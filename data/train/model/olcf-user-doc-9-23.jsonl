{"doc":"2023_olcf_system_changes","text":"2023 Notable System Changes\n\n\n\n\n\nAs the OLCF brings Frontier into full production and begins preparations for future resources, you should be aware of plans that will impact Summit, Andes, and the Alpine filesystem in 2023.\n\nPlease pay attention to the following key dates as you plan your science campaigns and data migration for the remainder of the year:\n\n\nThe table presented below outlines the important dates and events related to the 2023 OLCF system changes. These changes primarily affect the Summit and Alpine supercomputers, which are used for high-performance computing at the Oak Ridge Leadership Computing Facility (OLCF). On September 19, the submission system for 2024 Summit proposals will open, allowing researchers to submit their project proposals for consideration. However, on December 18, the last day to execute batch jobs on Summit, indicating that the system will no longer be available for use. The same day, December 18, is also the last day to use Alpine from Andes, another supercomputer at OLCF. On December 19, Alpine will become read-only and will only be accessible from designated DTNs (Data Transfer Nodes). Users are advised to begin transferring their data from Alpine to other systems before this date. On January 1, Alpine will be decommissioned, and all remaining data will be permanently deleted. In early 2024, Summit will return to service for projects with 2024 allocations, and the new Alpine II filesystem will also become available. This table serves as a helpful reference for OLCF users to plan and prepare for the upcoming system changes. \n\n| Date | Event |\n|------|-------|\n| Sep 19 | Summit proposal submission system opens for 2024 proposals. |\n| Dec 18 | Last day to execute batch jobs on Summit. |\n| Dec 18 | Last day to use Alpine from Andes. |\n| Dec 19 | Alpine becomes read-only and available only from DTNs. |\n| Jan 01 | Alpine decommissioned. ALL REMAINING DATA WILL BE PERMANENTLY DELETED. |\n| Early 2024 | Summit available for projects with 2024 allocation. |\n| Early 2024 | Alpine II filesystem available. |\n\n\n\nSep 19, 2023 - Submission system opens for 2024 Summit proposals\n\n<string>:42: (WARNING/2) Title underline too short.\n\nSep 19, 2023 - Submission system opens for 2024 Summit proposals\n--------------------------------------------------------------\n\nThe Department of Energy is extending Summit operations through October 2024, enabling researchers to pursue projects on one of the world’s leading AI-enabled open science supercomputing platforms.  OLCF will allocate Summit through new programs for calendar year 2024.\n\nSummitPLUS is one of the new allocation programs that will be used to allocate a significant portion of the system for 2024. The program is open to researchers from academia, government laboratories, federal agencies, and industry. We welcome proposals for computationally ready projects from investigators who are new to Summit, as well as from previous INCITE, ALCC, DD, ECP awardees and projects. We encourage proposals on emerging paradigms for computational campaigns including data-intensive science and AI/ML.\n\nIndividuals or teams interested in SummitPLUS must submit a proposal through the https://my.olcf.ornl.gov/ portal from September 19 to October 30.  Once on the portal, go to “New Accounts” across the top and then “Project Application”. Choose the SummitPLUS form from the dropdown list.\n\nSummit will continue to be allocated in node hours, and a typical SummitPLUS award will be between 100,000 – 250,000 node hours. The proposals will undergo review and the OLCF will notify awardees in mid-to-late November. Projects are anticipated to start in mid-to-late January 2024.\n\nTimeline\n\nProposals accepted beginning September 19\n\nProposals will undergo review and the OLCF will notify awardees in mid-to-late November.\n\nProjects are anticipated to start in mid-to-late January 2024.\n\n\n\nDec 18, 2023 - Last day to execute batch jobs on Summit\n\nYour project may continue to submit jobs on Summit through your current project's end date (which varies by allocation program) or December 18th (whichever comes first).  The last day batch jobs from current projects will run on Summit is December 18, 2023.\n\nSummit will accept batch jobs prior to 08:00 on December 18, but only batch jobs that will complete prior to 08:00 Dec 18 will run.  All batch jobs remaining in the queue at 08:00, Dec 18 will be deleted.\n\n\n\nDec 18, 2023 - Last day to use Alpine on Andes\n\nAlpine will be unmounted from Andes on December 19.  Jobs must be modified to use Orion as their scratch filesystem prior to this day.\n\n\n\nDec 19, 2023 - Alpine becomes read-only\n\nIn preparation for Alpine's decommission on January 01, Alpine will become read-only from all OLCF systems on December 19.\n\nTo assist you with moving your data off of Alpine, the DTN's mount the new Orion filesystem and all projects with access to Alpine have now been granted access to the Orion filesystem.\n\nPlease do not wait to migrate needed data, begin migrating all needed data now.\n\nWe highly encourage all teams to start migrating and/or deleting data from the Alpine filesystem now.  If you wait too late in the year to begin the transition, you will run the risk of running out of time to move your data before the system is decommissioned.  It is important to note that any data remaining on the Alpine filesystem after December 31, 2023, will truly be unavailable and not recoverable in any way as the system will be dismantled and the drives will be shredded.\n\nData migration\n\nMoving data off-site\n\nGlobus is the suggested tool to move data off-site\n\nStandard tools such as rsync and scp can also be used through the DTN, but may be slower and require more manual intervention than Globus\n\nCopying data directly from Alpine (GPFS) to Orion (Lustre)\n\nGlobus is the suggested tool to transfer needed data from Alpine to Orion.\n\nGlobus should be used when transfer large amounts of data.\n\nStandard tools such as rsync and cp can also be used. The DTN mounts both filesystems and should be used when transferring with rsync and cp tools. These methods should not be used to transfer large amounts of data.\n\nCopying data to the HPSS archive system\n\nThe hsi and htar utilities can be used to to transfer data from the Orion filesystem to the HPSS. The tools can also be used to transfer data from the HPSS to the Orion filesystem.\n\nGlobus is also available to transfer data directly to the HPSS\n\nPlease do not use the HPSS as a method to migrate data\n\nDue to the large amounts of data on the Alpine scratch filesystem and the limited available space on the HPSS archive system, we strongly recommend not using the HPSS to transfer data between Alpine and Orion.\n\nDue to available bandwidth, transferring data through the HPSS will be a slower route than using Globus to transfer directly between Alpine and Orion.\n\nTransferring data through the HPSS is a multi-step process and will be slower than direct transfers using Globus.\n\nGlobus is the suggested tool to migrate data off of Alpine.  Please do not use HPSS as a data migration method.\n\n\n\nJan 01, 2024 - Alpine decommissioned\n\nOn January 01, data remaining on the GPFS filesystem, Alpine, will no longer be accessible and will be permanently deleted . Following this date, the OLCF will no longer be able to retrieve data remaining on Alpine.\n\nDue to the large amount of data on the filesystems, we strongly urge you to start transferring your data now, and do not wait until later in the year.\n\nJan 01, all remaining Alpine data will be PERMANENTLY DELETED.  Do not wait to move needed data.\n\n\n\nEarly 2024 - Summit available for projects with 2024 allocation.\n\nSummit will be returned to service early 2024.\n\nProjects awarded a 2024 Summit allocation will be able to log into Summit and submit batch jobs once the system has been made available.\n\nPlease note, Summit will mount a new filesystem once returned to service.\n\nData stored on Alpine at the time of its decommission on January 01 will not be available.\n\nUsers will be responsible for transferring data onto Summit's new filesystem\n\n\n\nEarly 2024 - Alpine II filesystem available\n\nAlpine II will be available early 2024.\n\nThe previous center-wide GPFS scratch filesystem, Alpine, will be decommissioned in January 2024. A new scratch filesystem will be made available for projects with 2024 Summit allocations in early 2024. Users will be responsible for transferring any needed data onto the new scratch filesystem once available."}
{"doc":"account_pages","text":"Account Pages\n\n\n\nAt any time, you can view account pages by clicking on the \"My Account\" link in the top\nnavigation menu:\n\nlink to my account page\n\nAccount Context\n\nThere is only (1) account context in myOLCF: \"you\" as the currently-authenticated user. This account\ncontext is linked to the OLCF Moderate account that you used to authenticate to myOLCF.\n\naccount page left navigation menu\n\nAvailable Pages\n\nThe left navigation menu also includes an expandable item with links\nto account-centric pages.\n\n\nThe table above presents a comprehensive overview of the content that can be found on various account pages. The first page, \"My Profile,\" allows users to view and update their personal account profile information. This includes details such as name, contact information, and any other relevant data. The next page, \"For My Approval,\" displays a list of requests that require the user's feedback or approval. This could include requests for access to certain resources or changes to account permissions. The \"My Account Applications\" page provides a list of all pending and completed membership requests. This is useful for users to track the status of their requests and see which projects they are currently a member of. The \"Join Another Project\" page allows users to submit a request to join an additional project, providing them with access to new resources and collaborations. Finally, the \"Tickets\" page displays an interactive list of all tickets submitted to the OLCF help-desk. This allows users to easily track the status of their support requests and communicate with the help-desk team. Overall, these account pages provide users with a comprehensive and organized way to manage their account information, requests, and tickets. \n\n| Page               | Content                                                      |\n| ------------------ | ------------------------------------------------------------ |\n| My Profile         | Your account profile data, and forms to update them          |\n| For My Approval    | An interactive list of requests that need your feedback      |\n| My Account Applications | A list of your pending and completed membership requests     |\n| Join Another Project | A form to request membership on an additional project        |\n| Tickets            | An interactive list of your tickets with the OLCF help-desk  |"}
{"doc":"accounts_and_projects","text":"New to the Oak Ridge Leadership Computing Facility?\n\nWelcome! The information below introduces how we structure user\naccounts, projects, and system allocations. It's all you need to know\nabout getting to work. In general, OLCF resources are granted to\nprojects in allocations, and are made available to the users associated\nwith each project.\n\n\n\nRequest a New Allocation\n\nAccess to OLCF resources is limited to approved projects and their\nusers. The type of project (INCITE, ALCC, or Director's Discretion) will\ndetermine the application and review procedures. *Quarterly reports are\nrequired from industrial Director's Discretion projects only.\n\n\nThe table provides a comparison of three different types of allocations for accounts and projects: INCITE, Director's Discretion, and ALCC. INCITE is a large allocation that is only available once per year, while Director's Discretion offers both large and small allocations that can be applied for at any time. ALCC is also a large allocation, but it is only available once per year. The duration for all three allocations is 1 year, with the option for Director's Discretion to be up to 12 months. The priority for INCITE and ALCC is high, while Director's Discretion is medium. All three allocations require a closeout report, but only INCITE and ALCC require quarterly reports. The table also provides information on where to apply for each allocation, with INCITE and ALCC having specific websites for application and Director's Discretion requiring application through a personal account. Overall, this table provides a comprehensive overview of the different types of allocations available for accounts and projects, including their availability, duration, priority, and reporting requirements. \n\n| Allocation Type | Availability | Duration | Priority | Closeout Report | Quarterly Reports | Where to Apply |\n| --- | --- | --- | --- | --- | --- | --- |\n| INCITE | Once per year | 1 year | High | Yes | Yes | https://doeleadershipcomputing.org/proposal/call-for-proposals/ |\n| Director's Discretion | At any time | Up to 12 months | Medium | Yes | No* | https://my.olcf.ornl.gov/project-application-new |\n| ALCC | Once per year | 1 year | High | Yes | Yes | http://science.energy.gov/ascr/facilities/accessing-ascr-facilities/alcc/ |\n\n\n\nWhat are the differences between project types?\n\nINCITE – The Novel Computational Impact on Theory and Experiment\n(INCITE) program invites proposals for large-scale, computationally\nintensive research projects to run at the OLCF. The INCITE program\nawards sizeable allocations (typically, millions of processor-hours per\nproject) on some of the world’s most powerful supercomputers to address\ngrand challenges in science and engineering. There is an annual call for\nINCITE proposals and awards are made on an annual basis. Please visit\nthe Department of Energy Leadership Computing\nwebsite for more information\nand to submit a\nproposal.\n\nALCC –\nThe ASCR Leadership Computing Challenge (ALCC) is open to scientists\nfrom the research community in national laboratories, academia and\nindustry. The ALCC program allocates computational resources at the OLCF\nfor special situations of interest to the Department with an emphasis on\nhigh-risk, high-payoff simulations in areas directly related to the\nDepartment’s energy mission in areas such as advancing the clean energy\nagenda and understanding the Earth’s climate, for national emergencies,\nor for broadening the community of researchers capable of using\nleadership computing resources. For more information or to submit a\nproposal, please visit the ASCR Leadership Computing Challenge\nwebpage.\n\nDD – Director’s Discretion (DD) projects are dedicated to leadership\ncomputing preparation, INCITE and ALCC scaling, and application\nperformance to maximize scientific application efficiency and\nproductivity on leadership computing platforms. The OLCF Resource\nUtilization Council, as well as independent referees, review and approve\nall DD requests. Applications are accepted year-round via the OLCF\nDirector's Discretion Project\nApplication. Select\n\"OLCF Director's Discretionary Project\" from the drop down menu to begin.\n\nVendor – OLCF resources are also available to ORNL vendors.\nApplications may be submitted year-round by completing the Vendor\nProject\nApplication. Select\n\"OLCF Vendors Program\" from the drop down menu to begin.\n\nIf you have questions about project types or application procedures,\nfeel free to contact the OLCF Accounts Team at accounts@ccs.ornl.gov.\n\nApproved projects will be granted an allocation of core-hours for a\nperiod of time on one or more of the OLCF systems.\n\nWhat happens after a project request is approved?\n\nOnce a project request is approved, an OLCF Accounts Manager will\ncommunicate the following steps for activation to the project's PI.\n\nA signed Principal Investigator’s PI Agreement must be submitted with\nthe project application.\n\nExport Control: The project request will be reviewed by ORNL Export\nControl to determine whether sensitive or proprietary data will be\ngenerated or used. The results of this review will be forwarded to\nthe PI. If the project request is deemed sensitive and/or\nproprietary, the OLCF Security Team will schedule a conference call\nwith the PI to discuss the data protection needs.\n\nORNL Personnel Access System (PAS): All PI’s are required to be\nentered into the ORNL PAS system. An OLCF Accounts Manager will send\nthe PI a PAS invitation to submit all the pertinent information.\nPlease note that processing a PAS request may take 15 or more days.\n\nUser Agreement/Appendix A or Subcontract: A User Agreement/Appendix A\nor Subcontract must be executed between UT-Battelle and the PI’s\ninstitution. If our records indicate this requirement has not been\nmet, all necessary documents will be provided to the applicant by an\nOLCF Accounts Manager.\n\nUpon completion of the above steps, the PI will be notified that the\nproject has been created, and provided with the project ID and system\nallocation details. At this time, project participants may apply for\nuser accounts.\n\nGuidance on Frontier Allocation Requests\n\nFrontier has a total of 9,408 AMD compute nodes, with each node consisting of\n[1x] 64-core AMD “Optimized 3rd Gen EPYC” CPU with access to 512 GB of DDR4\nmemory. Each node also contains [4x] AMD MI250X accelerators, each with 2\nGraphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can\nthink of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth\nmemory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU,\nallowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of\n36+36 GB/s. The 2 GCDs on the same MI250X are connected with Infinity Fabric\nGPU-GPU with a peak bandwidth of 200 GB/s. The OLCF Director's Discretionary\n(DD) program allocates approximately 10% of the available Frontier hours in a\ncalendar year. Frontier is allocated in *node* hours, and a typical DD\nproject is awarded between 15,000 - 20,000 *node* hours. For more information\nabout Frontier, please visit the frontier-user-guide <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-user-guide>.\n\n\n\nApplying for a user account\n\nCollaborators involved with an approved and activated OLCF project can\napply for a user account associated with it. There are several steps in\nreceiving a user account, and we're here to help you through them.\n\nProject PIs do not receive a user account with project\ncreation, and must also apply.\n\nIf you will be contributing to multiple projects, your user\naccount will need to be associated with each. For instructions on joining\nadditional projects with an existing account, see the\nGet access to additional projects<get-additional-projects> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Get access to additional projects<get-additional-projects>> section below.\n\nFirst-time users should apply for an account using the Account Request\nForm.\n\nWhen our accounts team begins processing your application, you will receive an automated\nemail containing an unique 36-character confirmation code. Make note of it; you can use\nit to check the status of your application at any time.\n\nThe principal investigator (PI) of the project must approve your\naccount and system access. We will make the project PI aware of your request.\n\nForeign national participants will be sent an Oak Ridge National Lab\n(ORNL) Personnel Access System (PAS) request specific for the\nfacility and cyber-only access. After receiving your response, it\ntakes between 15-35 days for approval.\n\nFully-executed Institutional User Agreements with each institution having\nparticipants are required. If our records indicate your institution\nneeds to sign either an Institutional User Agreement and/or Appendix A, the proper\nform(s), along with instructions, will be sent via email.\n\nIf you are processing sensitive or proprietary data, additional\npaperwork is required and will be sent to you.\n\nIf you need an RSA SecurID token from our facility, the token and\nadditional paperwork will be sent to you via email to complete identity proofing.\n\n\n\nChecking the status of your application\n\nYou can check the general status of your application at any time using the myOLCF self-service\nportal's account status page.\nFor more information, see the myOLCF self-service portal documentation<myolcf-overview> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#myOLCF self-service portal documentation<myolcf-overview>>.\nIf you need to make further inquiries about your application, you may email our\nAccounts Team at accounts@ccs.ornl.gov.\n\nWhen all of the above steps are completed, your user account will be\ncreated and you will be notified by email. Now that you have a user\naccount and it has been associated with a project, you're ready to get\nto work. This website provides extensive documentation for OLCF systems,\nand can help you efficiently use your project's allocation. We recommend\nreading the System User Guides<system-user-guides> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#System User Guides<system-user-guides>> for the machines you will be using often.\n\n\n\nGet access to additional projects\n\nIf you already have a user account at the OLCF, your existing credentials can be\nleveraged across multiple projects.\n\nIf your user account has an associated RSA SecurID (i.e. you have an \"OLCF Moderate\" account), you\ngain access to another project by logging in to the myOLCF self-service portal\nand filling out the application under My Account > Join Another Project. For more information,\nsee the myOLCF self-service portal documentation<myolcf-overview> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#myOLCF self-service portal documentation<myolcf-overview>>.\n\nIf your user account has only an associated password (i.e. you have an \"OLCF Open\" account) you\ngain access to another project by filling out the Account Request Form; logging in to myOLCF is only available\nto users with RSA SecurID tokens at this time.\n\nIn either case, once the PI of that project has been contacted and granted permission, your user account\nwill be added to the relevant charge accounts and unix groups, and you will see these additions\nwhen you log in."}
{"doc":"andes_user_guide","text":"Andes User Guide\n\n\n\nSystem Overview\n\nAndes is a 704-compute node commodity-type linux cluster. The primary purpose of Andes is to provide a\nconduit for large-scale scientific discovery via pre/post processing and\nanalysis of simulation data generated on Summit.\n\n\n\nCompute nodes\n\nAndes contains 704 compute nodes and 9 GPU nodes. Andes has two partitions:\n\n\nThe table above provides a detailed breakdown of the specifications for the Andes user guide. The first column, \"Partition\", indicates the different types of partitions available, with the default being \"batch\". The \"Node Count\" column shows the number of nodes available for each partition, with 704 nodes for the default batch partition and 9 nodes for the GPU partition. The \"Memory\" column displays the amount of memory available for each partition, with 256 GB for the default batch partition and 1 TB for the GPU partition. The \"GPU\" column indicates the type of GPU available for each partition, with the default batch partition having no GPU and the GPU partition having 2 NVIDIA K80 GPUs. Finally, the \"CPU\" column shows the type and number of CPUs available for each partition, with the default batch partition having 2 AMD EPYC 7302 16Core Processors with a total of 32 cores per node, and the GPU partition having 2 Intel Xeon E5-2695 processors with a total of 28 cores and 56 hyper-threading per node. This table provides a comprehensive overview of the hardware specifications for the Andes user guide, allowing users to choose the most suitable partition for their needs.\n\n| Partition | Node Count | Memory | GPU | CPU |\n| --- | --- | --- | --- | --- |\n| batch (default) | 704 | 256 GB | N/A | [2x] AMD EPYC 7302 16Core Processor 3.0 GHz, 16 cores (total 32 cores per node) |\n| gpu | 9 | 1 TB | [2x] NVIDIA K80 | [2x] Intel Xeon E5-2695 @2.3 GHz - 14 cores, 28 HT (total 28 cores, 56 HT per node) |\n\n\n\nBatch Partition\n\nThe first 704 nodes make up the batch partition, where each node contains two\n16-core 3.0 GHz AMD EPYC 7302 processors with AMD's Simultaneous Multithreading\n(SMT) Technology and 256GB of main memory.  Each CPU in this partition features 16 physical\ncores, for a total of 32 physical cores per node.\n\nGPU Partition\n\nAndes also has 9 large memory/GPU nodes, which make up the gpu partition.\nThese nodes each have 1TB of main memory and two NVIDIA K80 GPUs in addition to\ntwo 14-core 2.30 GHz Intel Xeon processors with HT Technology. Each CPU in this\npartition features 14 physical cores, for a total of 28 physical cores per\nnode.  With Hyper-Threading enabled, these nodes have 56 logical cores that can\nexecute 56 hardware threads for increased parallelism.\n\nTo access the gpu partition, batch job submissions should request -p gpu.\n\nPlease see the batch-queues-on-andes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#batch-queues-on-andes> section to learn about the queuing\npolicies for these two partitions. Both compute partitions are accessible\nthrough the same batch queue from Andes’s andes-login-nodes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-login-nodes>.\n\nAndes features a S8500 Series HDR Infiniband interconnect, with a maximum theoretical\ntransfer rate of 200 Gb/s.\n\n\n\nLogin nodes\n\nAndes features 8 login nodes which are identical to the batch partition\ncompute nodes.  The login nodes provide an environment for editing, compiling,\nand launching codes onto the compute nodes. All Andes users will access the\nsystem through these same login nodes, and as such, any CPU- or\nmemory-intensive tasks on these nodes could interrupt service to other users.\nAs a courtesy, we ask that you refrain from doing any analysis or visualization\ntasks on the login nodes.\n\n<string>:74: (INFO/1) Possible title underline, too short for the title.\nTreating it as ordinary text because it's so short.\n\nTo connect to Andes, ssh to andes.olcf.ornl.gov\n\nssh username@andes.olcf.ornl.gov\n\nFor more information on connecting to OLCF resources, see connecting-to-olcf <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#connecting-to-olcf>.\n\n\n\nFile systems\n\nThe OLCF's center-wide data-alpine-ibm-spectrum-scale-filesystem <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-alpine-ibm-spectrum-scale-filesystem> name Alpine\nis available on Andes for computational work.  An NFS-based file system provides\ndata-user-home-directories-nfs <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-user-home-directories-nfs> and data-project-home-directories-nfs <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-project-home-directories-nfs>.\nAdditionally, the OLCF's data-hpss <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-hpss> provides archival spaces.\n\nShell and programming environments\n\nOLCF systems provide hundreds of software packages and scientific libraries\npre-installed at the system-level for users to take advantage of. To facilitate\nthis, environment management tools are employed to handle necessary changes to\nthe shell dynamically. The sections below provide information about using the\nmanagement tools at the OLCF.\n\n\n\nDefault shell\n\nA user's default shell is selected when completing the user account request\nform. The chosen shell is set across all OLCF resources.  Currently, supported\nshells include:\n\nbash\n\ntsch\n\ncsh\n\nksh\n\nIf you would like to have your default shell changed, please contact the\nOLCF user assistance center at\nhelp@olcf.ornl.gov.\n\n\n\nEnvironment management with lmod\n\nThe modules software package allows you to dynamically modify your user\nenvironment by using pre-written modulefiles. Environment modules are provided\nthrough Lmod, a Lua-based module\nsystem for dynamically altering shell environments.  By managing changes to the\nshell’s environment variables (such as path, ld_library_path, and\npkg_config_path), Lmod allows you to alter the software available in your\nshell environment without the risk of creating package and version combinations\nthat cannot coexist in a single environment.\n\nLmod is a recursive environment module system, meaning it is aware of module\ncompatibility and actively alters the environment to protect against conflicts.\nMessages to stderr are issued upon Lmod implicitly altering the environment.\nEnvironment modules are structured hierarchically by compiler family such that\npackages built with a given compiler will only be accessible if the compiler\nfamily is first present in the environment.\n\nnote: Lmod can interpret both Lua modulefiles and legacy Tcl\nmodulefiles. However, long and logic-heavy Tcl modulefiles may require\nporting to Lua.\n\nGeneral usage\n\nTypical use of Lmod is very similar to that of interacting with modulefiles on\nother OLCF systems. The interface to Lmod is provided by the module command:\n\n\nThe table presented in this section is a comprehensive guide for users of the Andes system. It provides a detailed description of the various commands that can be used to manage and utilize modules on the Andes platform. The first command, \"module -t list\", displays a concise list of all the modules currently loaded on the system. The next command, \"module avail\", presents a table of all the available modules that can be loaded. Users can also access help information about a specific module by using the \"module help <modulename>\" command. The \"module show <modulename>\" command displays the changes made to the environment by a particular module. The \"module spider <string>\" command allows users to search for modules based on a specific string. To load a module, users can use the \"module load <modulename> [...]\" command, which also allows for multiple modules to be loaded at once. The \"module use <path>\" command adds a specified path to the modulefile search cache and MODULESPATH, while the \"module unuse <path>\" command removes a path from these locations. The \"module purge\" command unloads all currently loaded modules, and the \"module reset\" command resets loaded modules to their system defaults. Finally, the \"module update\" command reloads all currently loaded modules. This table serves as a valuable resource for Andes users, providing them with the necessary information to effectively manage and utilize modules on the system.\n\n| Command         | Description                                                  |\n| --------------- | ------------------------------------------------------------ |\n| module -t list  | Shows a terse list of the currently loaded modules.          |\n| module avail    | Shows a table of the currently available modules.            |\n| module help <modulename> | Shows help information about <modulename>.           |\n| module show <modulename> | Shows the environment changes made by the <modulename> modulefile. |\n| module spider <string> | Searches all possible modules according to <string>.        |\n| module load <modulename> [...] | Loads the given <modulename>(s) into the current environment. |\n| module use <path> | Adds <path> to the modulefile search cache and MODULESPATH.  |\n| module unuse <path> | Removes <path> from the modulefile search cache and MODULESPATH. |\n| module purge    | Unloads all modules.                                        |\n| module reset    | Resets loaded modules to system defaults.                   |\n| module update   | Reloads all currently loaded modules.                       |\n\n\n\nModules are changed recursively. Some commands, such as\nmodule swap, are available to maintain compatibility with scripts\nusing Tcl Environment Modules, but are not necessary since Lmod\nrecursively processes loaded modules and automatically resolves\nconflicts.\n\nSearching for modules\n\nModules with dependencies are only available when the underlying dependencies,\nsuch as compiler families, are loaded. Thus, module avail will only display\nmodules that are compatible with the current state of the environment. To search\nthe entire hierarchy across all possible dependencies, the spider\nsub-command can be used as summarized in the following table.\n\n\nThe table presents a list of commands and their corresponding descriptions for the Andes user guide. The first command, \"module spider\", is used to display the entire possible graph of modules available. This can be useful for users to get an overview of all the modules that are available for use. The second command, \"module spider <modulename>\", allows users to search for a specific module by name in the graph of possible modules. This can be helpful for users who know the name of the module they need to use. The third command, \"module spider <modulename>/<version>\", allows users to search for a specific version of a module in the graph of possible modules. This can be useful for users who need to use a specific version of a module for their work. The last command, \"module spider <string>\", allows users to search for modulefiles that contain a specific string. This can be helpful for users who are not sure of the exact name of the module they need, but know a keyword or phrase related to it. Overall, these commands provide users with a convenient way to search for and access the modules they need for their work.\n\n| Command | Description |\n|---------|-------------|\n| module spider | Shows the entire possible graph of modules |\n| module spider <modulename> | Searches for modules named <modulename> in the graph of possible modules |\n| module spider <modulename>/<version> | Searches for a specific version of <modulename> in the graph of possible modules |\n| module spider <string> | Searches for modulefiles containing <string> |\n\n\n\nDefining custom module collections\n\nLmod supports caching commonly used collections of environment modules on a\nper-user basis in $home/.lmod.d. To create a collection called \"NAME\" from\nthe currently loaded modules, simply call module save NAME. omitting \"NAME\"\nwill set the user’s default collection. Saved collections can be recalled and\nexamined with the commands summarized in the following table.\n\n\nThe table presents a list of commands and their corresponding descriptions for the Andes user guide. The first command, \"module restore NAME\", allows users to recall a specific saved user collection with the given name. This is useful for retrieving previously saved settings or configurations. The second command, \"module restore\", recalls the user-defined defaults, which are the default settings and configurations set by the user. The third command, \"module reset\", resets all loaded modules to the system defaults, which are the default settings and configurations set by the system. The fourth command, \"module restore system\", recalls the system defaults, which are the default settings and configurations set by the system. Lastly, the \"module savelist\" command displays a list of all user-defined saved collections, providing an overview of the saved settings and configurations available for recall. This table serves as a quick reference guide for users to easily access and manage their saved collections and defaults within the Andes user guide.\n\n| Command | Description |\n| --- | --- |\n| module restore NAME | Recalls a specific saved user collection titled \"NAME\" |\n| module restore | Recalls the user-defined defaults |\n| module reset | Resets loaded modules to system defaults |\n| module restore system | Recalls the system defaults |\n| module savelist | Shows the list user-defined saved collections |\n\n\n\nYou should use unique names when creating collections to\nspecify the application (and possibly branch) you are working on. For\nexample, app1-development, app1-production, and\napp2-production.\n\nIn order to avoid conflicts between user-defined collections\non multiple compute systems that share a home file system (e.g.\n/ccs/home/[username]), Lmod appends the hostname of each system to the\nfiles saved in in your ~/.lmod.d directory (using the environment\nvariable lmod_system_name). This ensures that only collections\nappended with the name of the current system are visible.\n\nThe following screencast shows an example of setting up user-defined module\ncollections on Summit. https://vimeo.com/293582400\n\n\n\nInstalled Software\n\nThe OLCF provides hundreds of pre-installed software packages and scientific\nlibraries for your use, in addition to taking software installation requests. See the\nsoftware page for complete\ndetails on existing installs.\n\nCompiling\n\nCompiling code on andes is typical of commodity or Beowulf-style HPC Linux\nclusters.\n\nAvailable compilers\n\nThe following compilers are available on Andes:\n\nintel, intel composer xe (default)\n\npgi, the portland group compilar suite\n\ngcc, the gnu compiler collection\n\nUpon login, default versions of the intel compiler and openmpi (message passing\ninterface) libraries are automatically added to each user's environment. Users\ndo not need to make any environment changes to use the default version of intel\nand openmpi.\n\n\n\nChanging compilers\n\nIf a different compiler is required, it is important to use the correct\nenvironment for each compiler. To aid users in pairing the correct compiler and\nenvironment, the module system on andes automatically pulls in libraries compiled\nwith a given compiler when changing compilers. The compiler modules will load\nthe correct pairing of compiler version, message passing libraries, and other\nitems required to build and run code. To change the default loaded intel\nenvironment to the gcc environment for example, use:\n\n$ module load gcc\n\nThis will automatically unload the current compiler and system libraries\nassociated with it, load the new compiler environment and automatically load\nassociated system libraries as well.\n\nChanging versions of the same compiler\n\nTo use a specific compiler version, you must first ensure the compiler's\nmodule is loaded, and then swap to the correct compiler version. For example,\nthe following will configure the environment to use the gcc compilers, then load\na non-default gcc compiler version:\n\n$ module load gcc\n$ module swap gcc gcc/4.7.1\n\n\n\nnote: we recommend the following general guidelines for using the\nprogramming environment modules:\n\nDo not purge all modules; rather, use the default module environment\nprovided at the time of login, and modify it.\n\nDo not swap moab, torque, or mysql modules after loading a\nprogramming environment modulefile.\n\n\n\nCompiler wrappers\n\nCommodity clusters at the olcf can be accessed via the following wrapper\nprograms:\n\nmpicc to invoke the c compiler\n\nmpicc, mpicxx, or mpic++ to invoke the c++ compiler\n\nmpif77 or mpif90 to invoke appropriate versions of the\nfortran compiler\n\nThese wrapper programs are cognizant of your currently loaded modules, and will\nensure that your code links against our openmpi installation.  more information\nabout using openmpi at our center can be found in our software documentation.\n\nCompiling threaded codes\n\nWhen building threaded codes, compiler-specific flags must be included to ensure\na proper build.\n\nOpenmp\n\nFor pgi, add \"-mp\" to the build line.\n\n$ mpicc -mp test.c -o test.x\n$ export OMP_NUM_THREADS=2\n\nFor gnu, add \"-fopenmp\" to the build line.\n\n$ mpicc -fopenmp test.c -o test.x\n$ export OMP_NUM_THREADS=2\n\nFor intel, add \"-qopenmp\" to the build line.\n\n$ mpicc -qopenmp test.c -o test.x\n$ export OMP_NUM_THREADS=2\n\nFor information on running threaded codes, please see the andes-thread-layout <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-thread-layout>\nsubsection of the andes-running-jobs <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-running-jobs> section in this user guide.\n\n\n\nRunning Jobs\n\nIn High Performance Computing (HPC), computational work is performed by jobs.\nIndividual jobs produce data that lend relevant insight into grand challenges in\nscience and engineering. As such, the timely, efficient execution of jobs is the\nprimary concern in the operation of any HPC system.\n\nA job on a commodity cluster typically comprises a few different components:\n\nA batch submission script.\n\nA binary executable.\n\nA set of input files for the executable.\n\nA set of output files created by the executable.\n\nAnd the process for running a job, in general, is to:\n\nPrepare executables and input files.\n\nWrite a batch script.\n\nSubmit the batch script to the batch scheduler.\n\nOptionally monitor the job before and during execution.\n\nThe following sections describe in detail how to create, submit, and manage jobs\nfor execution on commodity clusters.\n\n\n\nLogin vs Compute Nodes on Commodity Clusters\n\nLogin Nodes\n\n<string>:403: (INFO/1) Duplicate implicit target name: \"login nodes\".\n\nWhen you log into an OLCF cluster, you are placed on a login node.  Login node\nresources are shared by all users of the system. Because of this, users should\nbe mindful when performing tasks on a login node.\n\nLogin nodes should be used for basic tasks such as file editing, code\ncompilation, data backup, and job submission. Login nodes should not be used\nfor memory- or compute-intensive tasks. Users should also limit the number of\nsimultaneous tasks performed on the login resources. For example, a user should\nnot run (10) simultaneous tar processes on a login node.\n\nCompute-intensive, memory-intensive, or otherwise disruptive processes\nrunning on login nodes may be killed without warning.\n\nSlurm\n\nMost OLCF resources now use the Slurm batch scheduler. Previously, most OLCF resources\nused the Moab scheduler. Summit and other IBM hardware use the LSF scheduler.\nBelow is a comparison table of useful commands among the three schedulers.\n\n\nThe table presents a comparison between two popular job schedulers, LSF and Slurm, in terms of their commands for various tasks in the Andes user guide. The first task, viewing the batch queue, can be done using the \"jobstat\" command in LSF and the \"squeue\" command in Slurm. For submitting a batch script, LSF uses the \"bsub\" command while Slurm uses \"sbatch\". To submit an interactive batch job, LSF requires the additional \"-Is $SHELL\" option with the \"bsub\" command, while Slurm uses the \"salloc\" command. Finally, for running parallel code within a batch job, LSF uses the \"jsrun\" command while Slurm uses \"srun\". These commands are essential for managing and executing jobs on the Andes supercomputer, and understanding their differences can help users navigate and utilize the system effectively.\n\n| Task | LSF (Summit) | Slurm |\n|------|--------------|--------|\n| View batch queue | jobstat | squeue |\n| Submit batch script | bsub | sbatch |\n| Submit interactive batch job | bsub -Is $SHELL | salloc |\n| Run parallel code within batch job | jsrun | srun |\n\n\n\nWriting Batch Scripts\n\nBatch scripts, or job submission scripts, are the mechanism by which a user\nconfigures and submits a job for execution. A batch script is simply a shell\nscript that also includes commands to be interpreted by the batch scheduling\nsoftware (e.g. Slurm).\n\nBatch scripts are submitted to the batch scheduler, where they are then parsed\nfor the scheduling configuration options. The batch scheduler then places the\nscript in the appropriate queue, where it is designated as a batch job. Once the\nbatch jobs makes its way through the queue, the script will be executed on the\nprimary compute node of the allocated resources.\n\nComponents of a Batch Script\n\nBatch scripts are parsed into the following (3) sections:\n\nInterpreter Line\n\nThe first line of a script can be used to specify the script’s interpreter; this\nline is optional. If not used, the submitter’s default shell will be used. The\nline uses the hash-bang syntax, i.e., #!/path/to/shell.\n\nSlurm Submission Options\n\nThe Slurm submission options are preceded by the string #SBATCH, making them\nappear as comments to a shell. Slurm will look for #SBATCH options in a\nbatch script from the script’s first line through the first non-comment line. A\ncomment line begins with #. #SBATCH options entered after the first\nnon-comment line will not be read by Slurm.\n\nShell Commands\n\nThe shell commands follow the last #SBATCH option and represent the\nexecutable content of the batch job. If any #SBATCH lines follow executable\nstatements, they will be treated as comments only.\n\nThe execution section of a script will be interpreted by a shell and can contain\nmultiple lines of executables, shell commands, and comments.  when the job's\nqueue wait time is finished, commands within this section will be executed on\nthe primary compute node of the job's allocated resources. Under normal\ncircumstances, the batch job will exit the queue after the last line of the\nscript is executed.\n\nExample Batch Script\n\n#!/bin/bash\n#SBATCH -A XXXYYY\n#SBATCH -J test\n#SBATCH -N 2\n#SBATCH -t 1:00:00\n\ncd $SLURM_SUBMIT_DIR\ndate\nsrun -n 8 ./a.out\n\nThis batch script shows examples of the three sections outlined above:\n\nInterpreter Line\n\n<string>:509: (INFO/1) Duplicate implicit target name: \"interpreter line\".\n\n1: This line is optional and can be used to specify a shell to interpret the\nscript. In this example, the bash shell will be used.\n\nSlurm Options\n\n2: The job will be charged to the “XXXYYY” project.\n\n3: The job will be named test.\n\n4: The job will request (2) nodes.\n\n5: The job will request (1) hour walltime.\n\nShell Commands\n\n<string>:526: (INFO/1) Duplicate implicit target name: \"shell commands\".\n\n6: This line is left blank, so it will be ignored.\n\n7: This command will change the current directory to the directory\nfrom where the script was submitted.\n\n8: This command will run the date command.\n\n9: This command will run (8) MPI instances of the executable a.out\non the compute nodes allocated by the batch system.\n\nBatch scripts can be submitted for execution using the sbatch command.\nFor example, the following will submit the batch script named test.slurm:\n\nsbatch test.slurm\n\nIf successfully submitted, a Slurm job ID will be returned. This ID can be used\nto track the job. It is also helpful in troubleshooting a failed job; make a\nnote of the job ID for each of your jobs in case you must contact the OLCF User\nAssistance Center for support.\n\n\n\nInteractive Batch Jobs on Commodity Clusters\n\nBatch scripts are useful when one has a pre-determined group of commands to\nexecute, the results of which can be viewed at a later time. However, it is\noften necessary to run tasks on compute resources interactively.\n\nUsers are not allowed to access cluster compute nodes directly from a login\nnode. Instead, users must use an interactive batch job to allocate and gain\naccess to compute resources. This is done by using the Slurm salloc command.\nOther Slurm options are passed to salloc on the command line as well:\n\n$ salloc -A abc123 -p gpu -N 4 -t 1:00:00\n\nThis request will:\n\n\nThe table presented below is a part of the Andes user guide and provides information on how to use the \"salloc\" command to start an interactive session. The \"-A\" option allows the user to charge the session to a specific project, in this case, the \"abc123\" project. The \"-p gpu\" option specifies that the session should run in the gpu partition. The \"-N 4\" option is used to request 4 nodes for the session, while the \"-t 1:00:00\" option sets the time limit for the session to 1 hour. This table serves as a quick reference guide for users to easily understand and utilize the various options available with the \"salloc\" command. \n\n| Command | Description |\n|---------|-------------|\n| salloc | Start an interactive session |\n| -A | Charge to the abc123 project |\n| -p gpu | Run in the gpu partition |\n| -N 4 | Request (4) nodes |\n| -t 1:00:00 | Set time limit for (1) hour |\n\n\n\nAfter running this command, the job will wait until enough compute nodes are\navailable, just as any other batch job must. However, once the job starts, the\nuser will be given an interactive prompt on the primary compute node within the\nallocated resource pool. Commands may then be executed directly (instead of\nthrough a batch script).\n\nDebugging\n\nA common use of interactive batch is to aid in debugging efforts.  interactive\naccess to compute resources allows the ability to run a process to the point of\nfailure; however, unlike a batch job, the process can be restarted after brief\nchanges are made without losing the compute resource pool; thus speeding up the\ndebugging effort.\n\nChoosing a Job Size\n\nBecause interactive jobs must sit in the queue until enough resources become\navailable to allocate, it is useful to know when a job can start.\n\nUse the sbatch --test-only command to see when a job of a specific size\ncould be scheduled. For example, the snapshot below shows that a (2) node job\nwould start at 10:54.\n\n$ sbatch --test-only -N2 -t1:00:00 batch-script.slurm\n\n  sbatch: Job 1375 to start at 2019-08-06T10:54:01 using 64 processors on nodes andes[499-500] in partition batch\n\nThe queue is fluid, the given time is an estimate made from the current queue state and load. Future job submissions and job\ncompletions will alter the estimate.\n\n\n\nCommon Batch Options to Slurm\n\nThe following table summarizes frequently-used options to Slurm:\n\n\nThe table provides a detailed description of the various options that can be used in the Andes user guide. The first column lists the different options that can be used, such as -A, -N, -t, -p, -o, -e, --mail-type, --mail-user, -J, and --get-user-env. The second column explains the use of each option, such as causing the job time to be charged to a specific account, specifying the number of compute nodes to allocate, setting a maximum wall-clock time, allocating resources on a specific partition, writing standard output and error to a specified file, sending email notifications for different events, setting a job name, and exporting environment variables from the submitting shell to the batch job shell. The third column provides a detailed description of each option, including any additional information or recommendations. For example, the -A option requires a specific account string and the --get-user-env option is not recommended for use on the login nodes. This table serves as a useful reference for users of the Andes system, providing them with a clear understanding of the different options available and how to use them effectively.\n\n| Option | Use | Description |\n| ------ | --- | ----------- |\n| -A | #SBATCH -A <account> | Causes the job time to be charged to <account>. The account string, e.g. pjt000 is typically composed of three letters followed by three digits and optionally followed by a subproject identifier. The utility showproj can be used to list your valid assigned project ID(s). This option is required by all jobs. |\n| -N | #SBATCH -N <value> | Number of compute nodes to allocate. Jobs cannot request partial nodes. |\n| -t | #SBATCH -t <time> | Maximum wall-clock time. <time> is in the format HH:MM:SS. |\n| -p | #SBATCH -p <partition_name> | Allocates resources on specified partition. |\n| -o | #SBATCH -o <filename> | Writes standard output to <name> instead of <job_script>.o$SLURM_JOB_UID. $SLURM_JOB_UID is an environment variable created by Slurm that contains the batch job identifier. |\n| -e | #SBATCH -e <filename> | Writes standard error to <name> instead of <job_script>.e$SLURM_JOB_UID. |\n| --mail-type | #SBATCH --mail-type=FAIL | Sends email to the submitter when the job fails. |\n| | #SBATCH --mail-type=BEGIN | Sends email to the submitter when the job begins. |\n| | #SBATCH --mail-type=END | Sends email to the submitter when the job ends. |\n| --mail-user | #SBATCH --mail-user=<address> | Specifies email address to use for --mail-type options. |\n| -J | #SBATCH -J <name> | Sets the job name to <name> instead of the name of the job script. |\n| --get-user-env | #SBATCH --get-user-env | Exports all environment variables from the submitting shell into the batch job shell. Since the login nodes differ from the service nodes, using the –get-user-env option is not recommended. Users should create the needed environment within the batch job. |\n| --mem=0 | #SBATCH --mem=0 | Declare to use all the available memory of the node. |\n\n\n\nBecause the login nodes differ from the service nodes, using\nthe –get-user-env option is not recommended. Users should create the\nneeded environment within the batch job.\n\nFurther details and other Slurm options may be found through the sbatch man\npage.\n\n\n\nBatch Environment Variables\n\nSlurm sets multiple environment variables at submission time. The following\nSlurm variables are useful within batch scripts:\n\n\nThe table presented is a part of the Andes user guide and provides a detailed description of various variables used in the Andes batch job submission process. The first variable, $SLURM_SUBMIT_DIR, represents the directory from which the batch job was submitted. By default, a new job starts in the user's home directory, but this variable allows the user to get back to the directory of job submission using the command \"cd $SLURM_SUBMIT_DIR\". It is important to note that this may not be the same directory in which the batch script resides. The next variable, $SLURM_JOBID, is the job's full identifier and is commonly used to append the job's ID to the standard output and error files. The following variable, $SLURM_JOB_NUM_NODES, indicates the number of nodes requested for the job. The $SLURM_JOB_NAME variable represents the job name supplied by the user. Finally, the $SLURM_NODELIST variable provides a list of nodes assigned to the job. This table serves as a useful reference for users of Andes, providing a clear understanding of the various variables used in the batch job submission process.\n\n| Variable           | Description                                                  |\n| ------------------ | ------------------------------------------------------------ |\n| $SLURM_SUBMIT_DIR  | The directory from which the batch job was submitted. By default, a new job starts in the user's home directory. The user can get back to the directory of job submission using the command \"cd $SLURM_SUBMIT_DIR\". Note that this may not be the same directory in which the batch script resides. |\n| $SLURM_JOBID       | The job's full identifier. A common use for SLURM_JOBID is to append the job's ID to the standard output and error files. |\n| $SLURM_JOB_NUM_NODES | The number of nodes requested for the job.                   |\n| $SLURM_JOB_NAME    | The job name supplied by the user.                           |\n| $SLURM_NODELIST    | The list of nodes assigned to the job.                       |\n\n\n\n\n\nModifying Batch Jobs\n\nThe batch scheduler provides a number of utility commands for managing\nsubmitted jobs. See each utilities' man page for more information.\n\nRemoving and Holding Jobs\n\nscancel\n\nJobs in the queue in any state can be stopped and removed from the queue\nusing the command scancel.\n\n$ scancel 1234\n\nscontrol hold\n\nJobs in the queue in a non-running state may be placed on hold using the\nscontrol hold command. Jobs placed on hold will not be removed from the\nqueue, but they will not be eligible for execution.\n\n$ scontrol hold 1234\n\nscontrol release\n\nOnce on hold the job will not be eligible to run until it is released to\nreturn to a queued state. The scontrol release command can be used to\nremove a job from the held state.\n\n$ scontrol release 1234\n\n\n\nMonitoring Batch Jobs\n\nSlurm provides multiple tools to view queue, system, and job status. Below are\nthe most common and useful of these tools.\n\nJob Monitoring Commands\n\nsqueue\n\nThe Slurm utility squeue can be used to view the batch queue.\n\nTo see all jobs currently in the queue:\n\n$ squeue -l\n\nTo see all of your queued jobs:\n\n$ squeue -l -u $USER\n\nsacct\n\nThe Slurm utility sacct can be used to view jobs currently in the queue and\nthose completed within the last few days. The utility can also be used to see\njob steps in each batch job.\n\nTo see all jobs currently in the queue:\n\n$ sacct -a -X\n\nTo see all jobs including steps owned by userA currently in the queue:\n\n$ sacct -u userA\n\nTo see all steps submitted to job 123:\n\n$ sacct -j 123\n\nTo see all of your jobs that completed on 2019-06-10:\n\n$ sacct -S 2019-06-10T00:00:00 -E 2019-06-10T23:59:59 -o\"jobid,user,account%16,cluster,AllocNodes,Submit,Start,End,TimeLimit\" -X -P\n\nscontrol show job <jobid>\n\nProvides additional details of given job.\n\nsview\n\nThe sview tool provide a graphical queue monitoring tool. To use, you will\nneed an X server running on your local system. You will also need to tunnel X\ntraffic through your ssh connection:\n\nlocal-system> ssh -Y username@andes.ccs.ornl.gov\nandes-login> sview\n\n\n\nJob Execution\n\nOnce resources have been allocated through the batch system, users have the\noption of running commands on the allocated resources' primary compute node (a\nserial job) and/or running an MPI/OpenMP executable across all the resources in\nthe allocated resource pool simultaneously (a parallel job).\n\nSerial Job Execution\n\nThe executable portion of batch scripts is interpreted by the shell specified on\nthe first line of the script. If a shell is not specified, the submitting user’s\ndefault shell will be used.\n\nThe serial portion of the batch script may contain comments, shell commands,\nexecutable scripts, and compiled executables. These can be used in combination\nto, for example, navigate file systems, set up job execution, run serial\nexecutables, and even submit other batch jobs.\n\nAndes Compute Node Description\n\nThe following image represents a high level compute node that will be used below\nto display layout options.\n\n\n\nUsing srun\n\nBy default, commands will be executed on the job’s primary compute node,\nsometimes referred to as the job’s head node. The srun command is used to\nexecute an MPI binary on one or more compute nodes in parallel.\n\nsrun accepts the following common options:\n\n\nThe table presented below is a part of the Andes user guide and provides detailed information on the minimum number of nodes, total number of MPI tasks, and thread affinity control for the Andes software. The first column, labeled as \"N\", specifies the minimum number of nodes required for running the software. The second column, labeled as \"n\", indicates the total number of MPI tasks that can be used for running the software. The third column, labeled as \"--cpu-bind=no\", allows the code to control the thread affinity, providing more flexibility in managing the software's performance. The fourth column, labeled as \"c\", specifies the number of cores per MPI task, which can be adjusted according to the user's requirements. The last column, labeled as \"--cpu-bind=cores\", indicates the option to bind the software to specific cores for optimized performance. This table serves as a useful reference for users of the Andes software, providing them with essential information for efficient and effective usage.\n\n| N | Minimum number of nodes |\n| n | Total number of MPI tasks |\n| --cpu-bind=no | Allow code to control thread affinity |\n| c | Cores per MPI task |\n| --cpu-bind=cores | Bind to cores |\n\n\n\nIf you do not specify the number of MPI tasks to srun\nvia -n, the system will default to using only one task per node.\n\nMPI Task Layout\n\nEach compute node on Andes contains two sockets each with 16 cores.  Depending on\nyour job, it may be useful to control task layout within and across nodes.\n\nPhysical Core Binding\n\nThe following will run four copies of a.out, one per CPU, two per node with\nphysical core binding\n\n\n\nSimultaneous Multithreading Binding\n\nThe following will run four copies of a.out, one per SMT, two per node\nusing a round robin task layout between nodes:\n\n\n\n\n\nThread Layout\n\nThread per SMT\n\nThe following will run four copies of a.out. Each task will launch two threads.\nThe -c flag will provide room for the threads.\n\n\n\nNot adding enough resources using the -c flag,\nthreads may be placed on the same resource.\n\nMultiple Simultaneous Jobsteps\n\nMultiple simultaneous sruns can be executed within a batch job by placing each\nsrun in the background.\n\n#!/bin/bash\n#SBATCH -N 2\n#SBATCH -t 1:00:00\n#SBATCH -A prj123\n#SBATCH -J simultaneous-jobsteps\n\nsrun -n16 -N2 -c1 --cpu-bind=cores --exclusive ./a.out &\nsrun -n8 -N2 -c1 --cpu-bind=cores --exclusive ./b.out &\nsrun -n4 -N1 -c1 --cpu-bind=threads --exclusive ./c.out &\nwait\n\nThe wait command must be used in a batch script\nto prevent the shell from exiting before all backgrounded\nsruns have completed.\n\nThe --exclusive flag must be used to prevent\nresource sharing. Without the flag each backgrounded srun\nwill likely be placed on the same resources.\n\n\n\nBatch Queues on Andes\n\nThe compute nodes on Andes are separated into two partitions the \"batch partition\"\nand the \"GPU partition\" as described in the andes-compute-nodes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-compute-nodes> section. The scheduling\npolicies for the individual partitions are as follows:\n\nBatch Partition Policy (default)\n\nJobs that do not specify a partition will run in the 704 node batch partition:\n\n\nThe table presents information regarding the usage policy for the Andes user guide. It is divided into three bins, A, B, and C, based on the number of nodes required for a job. Bin A allows for 1 to 16 nodes and has a maximum duration of 48 hours. The policy for this bin states that a user can have a maximum of 4 jobs running and 4 jobs eligible at any given time. Bin B allows for 17 to 64 nodes and has a maximum duration of 36 hours. Bin C allows for 65 to 384 nodes and has a maximum duration of 3 hours. The policy for bins B and C is not specified, indicating that there may be different policies in place for these bins. This table provides important information for users of Andes, allowing them to plan their jobs and understand the limitations and restrictions in place for each bin. \n\n| Bin | Node Count | Duration | Policy |\n| --- | --- | --- | --- |\n| A | 1 - 16 Nodes | 0 - 48 hr | max 4 jobs running and 4 jobs eligible per user in bins A, B, and C |\n| B | 17 - 64 Nodes | 0 - 36 hr | Not specified |\n| C | 65 - 384 Nodes | 0 - 3 hr | Not specified |\n\n\n\nGPU Partition Policy\n\nTo access the 9 node GPU Partition batch job submissions should request -p\ngpu\n\n\nThe table presented is a part of the Andes user guide and provides information on the node count, duration, and policy for running jobs. The first column, \"Node Count,\" specifies the number of nodes that can be used for running jobs. In this case, the range is 1-2 nodes. The second column, \"Duration,\" indicates the time frame within which the jobs can be run. The duration is 0-48 hours, meaning that the jobs can be run for a maximum of 48 hours. The last column, \"Policy,\" outlines the policy for running jobs on Andes. It states that only one job can be running per user at a time. This table serves as a useful reference for users of Andes, providing them with important information on the number of nodes, duration, and policy for running jobs. \n\n| Node Count | Duration | Policy |\n|------------|----------|--------|\n| 1-2 Nodes  | 0 - 48 hrs | max 1 job running per user |\n\n\n\nThe queue structure was designed based on user feedback and\nanalysis of batch jobs over the recent years. However, we understand that\nthe structure may not meet the needs of all users. If this structure\nlimits your use of the system, please let us know. We want Andes to be a\nuseful OLCF resource and will work with you providing exceptions or even\nchanging the queue structure if necessary.\n\nIf your jobs require resources outside these queue policies such as higher priority or longer walltimes, please contact help@olcf.ornl.gov.\n\nAllocation Overuse Policy\n\nProjects that overrun their allocation are still allowed to run on OLCF systems,\nalthough at a reduced priority. Like the adjustment for the number of processors\nrequested above, this is an adjustment to the apparent submit time of the job.\nHowever, this adjustment has the effect of making jobs appear much younger than\njobs submitted under projects that have not exceeded their allocation. In\naddition to the priority change, these jobs are also limited in the amount of\nwall time that can be used.\n\nFor example, consider that job1 is submitted at the same time as job2.\nThe project associated with job1 is over its allocation, while the project\nfor job2 is not. The batch system will consider job2 to have been\nwaiting for a longer time than job1. In addition, projects that are at 125%\nof their allocated time will be limited to only one running job at a time. The\nadjustment to the apparent submit time depends upon the percentage that the\nproject is over its allocation, as shown in the table below:\n\n\nThe table presented below is a part of the Andes User Guide and provides information on the allocation usage, priority reduction, and job eligibility and running status. The first column, \"% Of Allocation Used\", indicates the percentage of the allocated resources that have been used. The second column, \"Priority Reduction\", shows the number of days that the priority of a job will be reduced if the allocation usage is below 100%. The third column, \"Number Eligible-to-Run\", displays the maximum number of jobs that can be eligible to run at a given time. The last column, \"Number Running\", shows the maximum number of jobs that can be running simultaneously. The table also includes three different ranges of allocation usage - less than 100%, 100% to 125%, and greater than 125%. For allocation usage below 100%, there is no reduction in priority and an unlimited number of jobs can be eligible and running. For allocation usage between 100% to 125%, the priority of a job will be reduced for 30 days and an unlimited number of jobs can be eligible and running. However, for allocation usage above 125%, the priority of a job will be reduced for 365 days and only one job can be running at a time. This table serves as a useful reference for users to understand the allocation usage and job eligibility and running status in Andes. \n\n| % Of Allocation Used | Priority Reduction | Number Eligible-to-Run | Number Running |\n|----------------------|---------------------|------------------------|----------------|\n| < 100%               | 0 days              | 4 jobs                 | unlimited jobs |\n| 100% to 125%         | 30 days             | 4 jobs                 | unlimited jobs |\n| > 125%               | 365 days            | 4 jobs                 | 1 job          |\n\n\n\nJob Accounting on Andes\n\nJobs on Andes are scheduled in full node increments; a node's cores cannot be\nallocated to multiple jobs. Because the OLCF charges based on what a job makes\nunavailable to other users, a job is charged for an entire node even if it\nuses only one core on a node. To simplify the process, users are given a\nmultiples of entire nodes through Slurm.\n\nAllocations on Andes are separate from those on Summit and other OLCF resources.\n\nNode-Hour Calculation\n\nThe node-hour charge for each batch job will be calculated as follows:\n\nnode-hours = nodes requested * ( batch job endtime - batch job starttime )\n\nWhere batch job starttime is the time the job moves into a running state, and\nbatch job endtime is the time the job exits a running state.\n\nA batch job's usage is calculated solely on requested nodes and the batch job's\nstart and end time. The number of cores actually used within any particular node\nwithin the batch job is not used in the calculation. For example, if a job\nrequests (6) nodes through the batch script, runs for (1) hour, uses only (2)\nCPU cores per node, the job will still be charged for 6 nodes * 1 hour = 6\nnode-hours.\n\nViewing Usage\n\nUtilization is calculated daily using batch jobs which complete between 00:00\nand 23:59 of the previous day. For example, if a job moves into a run state on\nTuesday and completes Wednesday, the job's utilization will be recorded\nThursday. Only batch jobs which write an end record are used to calculate\nutilization. Batch jobs which do not write end records due to system failure or\nother reasons are not used when calculating utilization. Jobs which fail because\nof run-time errors (e.g. the user's application causes a segmentation fault) are\ncounted against the allocation.\n\nEach user may view usage for projects on which they are members from the command\nline tool showusage and the myOLCF site.\n\nOn the Command Line via showusage\n\nThe showusage utility can be used to view your usage from January 01\nthrough midnight of the previous day. For example:\n\n$ showusage\n  Usage:\n                           Project Totals\n  Project             Allocation      Usage      Remaining     Usage\n  _________________|______________|___________|____________|______________\n  abc123           |  20000       |   126.3   |  19873.7   |   1560.80\n\nThe -h option will list more usage details.\n\nOn the Web via myOLCF\n\nMore detailed metrics may be found on each project's usage section of the myOLCF\nsite. The following information is available\nfor each project:\n\nYTD usage by system, subproject, and project member\n\nMonthly usage by system, subproject, and project member\n\nYTD usage by job size groupings for each system, subproject, and\nproject member\n\nWeekly usage by job size groupings for each system, and subproject\n\nBatch system priorities by project and subproject\n\nProject members\n\nThe myOLCF site is provided to aid in the utilization and management of OLCF\nallocations. See the myOLCF Documentation <https://docs.olcf.ornl.gov/services_and_applications/myolcf/index.html> for more information.\n\nIf you have any questions or have a request for additional data,\nplease contact the OLCF User Assistance Center.\n\n\n\n\n\nDebugging\n\n<string>:1129: (INFO/1) Duplicate implicit target name: \"debugging\".\n\nLinaro DDT\n\nLinaro DDT is an advanced debugging tool used for scalar, multi-threaded,\nand large-scale parallel applications. In addition to traditional\ndebugging features (setting breakpoints, stepping through code,\nexamining variables), DDT also supports attaching to already-running\nprocesses and memory debugging. In-depth details of DDT can be found in\nthe Official DDT User Guide, and\ninstructions for how to use it on OLCF systems can be found on the\nDebugging Software <https://docs.olcf.ornl.gov/software/debugging/index.html> page. DDT is the\nOLCF's recommended debugging software for large parallel applications.\n\nOne of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Andes from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Andes. For guidance in setting up the remote client see the Debugging Software <https://docs.olcf.ornl.gov/software/debugging/index.html> page.\n\nGDB\n\nGDB, the GNU Project Debugger,\nis a command-line debugger useful for traditional debugging and\ninvestigating code crashes. GDB lets you debug programs written in Ada,\nC, C++, Objective-C, Pascal (and many other languages).\n\nGDB is available on andes via the gdb module:\n\nmodule load gdb\n\nTo use GDB to debug your application run:\n\ngdb ./path_to_executable\n\nAdditional information about GDB usage can befound on the GDB Documentation Page.\n\nValgrind\n\nValgrind is an instrumentation framework for\nbuilding dynamic analysis tools. There are Valgrind tools that can\nautomatically detect many memory management and threading bugs, and\nprofile your programs in detail. You can also use Valgrind to build new\ntools.\n\nThe Valgrind distribution currently includes five production-quality\ntools: a memory error detector, a thread error detector, a cache and\nbranch-prediction profiler, a call-graph generating cache profiler,\nand a heap profiler. It also includes two experimental tools: a data\nrace detector, and an instant memory leak detector.\n\nThe Valgrind tool suite provides a number of debugging and\nprofiling tools. The most popular is Memcheck, a memory checking tool\nwhich can detect many common memory errors such as:\n\nTouching memory you shouldn’t (eg. overrunning heap block boundaries,\nor reading/writing freed memory).\n\nUsing values before they have been initialized.\n\nIncorrect freeing of memory, such as double-freeing heap blocks.\n\nMemory leaks.\n\nValgrind is available on Andes via the valgrind module:\n\nmodule load valgrind\n\nAdditional information about Valgrind usage and OLCF-provided builds can\nbe found on the Valgrind Software\nPage.\n\n\n\nVisualization tools\n\nParaView\n\nInformation regarding ParaView, and how to run it on both Andes and Summit, has moved\nto the Software Section. Click HERE <https://docs.olcf.ornl.gov/software/viz_tools/paraview.html> to go to the new page.\n\nVisIt\n\nInformation regarding VisIt, and how to run it on both Andes and Summit, has moved\nto the Software Section. Click HERE <https://docs.olcf.ornl.gov/software/viz_tools/visit.html> to go to the new page.\n\nRemote Visualization using VNC (non-GPU)\n\nIn addition to the instructions below, Benjamin\nHernandez of the OLCF\nAdvanced Technologies\nSection\npresented a related talk, GPU Rendering in Rhea and\nTitan,\nduring the 2016 OLCF User Meeting.\n\nStep 1 (local system)\n\nInstall a vncviewer (turbovnc, tigervnc, etc.) on your local machine.  When\nrunning vncviewer for the first time, it will ask to set a password for this and\nfuture vnc sessions.\n\nStep 2 (terminal 1)\n\nFrom an Andes connection launch a batch job and execute the below matlab-vnc.sh\nscript to start the vncserver and run matlab within:\n\nlocalsytem: ssh -X username@andes.olcf.ornl.gov\n\nandes: salloc -A abc123 -N 1 -t 1:00:00 --x11=batch\n\nandes: ./matlab-vnc.sh\n\n$ ./matlab-vnc.sh\n\nStarting vncserver\n\nDesktop 'TurboVNC: andes79.olcf.ornl.gov:1 (userA)' started on display andes79.olcf.ornl.gov:1\n\nStarting applications specified in /ccs/home/userA/.vnc/xstartup.turbovnc\nLog file is /ccs/home/userA/.vnc/andes79.olcf.ornl.gov:1.log\n\n**************************************************************************\nInstructions\n\nIn a new terminal, open a tunneling connection with andes79.olcf.ornl.gov and port 5901\nexample:\n     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov\n     andes: ssh -4L 5901:localhost:5901 andes79\n\n**************************************************************************\n\nMATLAB is selecting SOFTWARE OPENGL rendering.\n\nStep 3 (terminal 2)\n\nIn a second terminal on your local system open a tunneling connection following\nthe instructions given by the vnc start-up script:\n\nlocalsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov\n\nandes: ssh -4L 5901:localhost:5901 andes79\n\nStep 4 (local system)\n\nLaunch the vncviewer. When you launch the vncviewer that you downloaded you will\nneed to specify localhost:5901. You will also set a password for the initial\nconnection or enter the created password for subsequent connections.\n\nmatlab-vnc.sh (non-GPU rendering)\n\n#!/bin/sh\n\nwhat()\n{\n   hostname\n}\necho \"Starting vncserver\"\n\n/opt/TurboVNC/bin/vncserver :1 -geometry 1920x1080 -depth 24\n\necho\necho\necho \"**************************************************************************\"\necho \"Instructions\"\necho\necho \"In a new terminal, open a tunneling connection with $(what) and port 5901\"\necho \"example:\"\necho \"   localsystom: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov \"\necho \"   andes: ssh -4L 5901:localhost:5901 $(what) \"\necho\necho \"**************************************************************************\"\necho\necho\n\nexport DISPLAY=:1\n\nmodule load matlab\nmatlab\nvncserver -kill :1\n\nRemote Visualization using VNC (GPU nodes)\n\nStep 1 (local system)\n\n<string>:1329: (INFO/1) Duplicate implicit target name: \"step 1 (local system)\".\n\nInstall a vncviewer (turbovnc, tigervnc, etc.) on your local machine.  When\nrunning vncviewer for the first time, it will ask to set a password for this and\nfuture vnc sessions.\n\nStep 2 (terminal 1)\n\n<string>:1336: (INFO/1) Duplicate implicit target name: \"step 2 (terminal 1)\".\n\nFrom an Andes connection launch a batch job and execute the below vmd-vgl.sh\nscript to start the vncserver and run vmd within:\n\nlocalsytem: ssh -X username@andes.olcf.ornl.gov\n\nandes: salloc -A abc123 -N 1 -t 1:00:00 -p gpu --x11=batch\n\nandes: ./vmd-vgl.sh\n\n$ ./vmd-vgl.sh\n\nStarting X\n\n\nX.Org X Server 1.20.3\nX Protocol Version 11, Revision 0\nBuild Operating System:  4.14.0-49.el7a.noaead.x86_64\nCurrent Operating System: Linux andes-gpu5.olcf.ornl.gov 4.18.0-147.8.1.el8_1.x86_64 #1 SMP Wed Feb 26 03:08:15 UTC 2020 x86_64\nKernel command line: selinux=0 audit=0 panic=10 biosdevname=0 console=ttyS1,115200n8 nouveau.modeset=0 rd.driver.blacklist=nouveau ip=dhcp BOOTIF=54:9f:35:25:a3:50 root=anchor init=/sbin/init dropbear_auth_key=/root-key.pub squashfs_mount_only=1 overlayfs_size=4096m overlayfs_write=/ image=andes_gpu:prod_20201109-73f962-12c93c6 initrd=initrd-4.18.0-147.8.1.el8_1.x86_64-anchor-0.1.4-4632674.el7-andes-mlnx\nBuild Date: 13 September 2019  02:55:13PM\nBuild ID: xorg-x11-server 1.20.3-11.el8\nCurrent version of pixman: 0.36.0\n    Before reporting problems, check http://wiki.x.org\n    to make sure that you have the latest version.\nMarkers: (--) probed, (**) from config file, (==) default setting,\n    (++) from command line, (!!) notice, (II) informational,\n    (WW) warning, (EE) error, (NI) not implemented, (??) unknown.\n(==) Log file: \"/var/log/Xorg.0.log\", Time: Thu Nov 26 22:14:04 2020\n(==) Using config file: \"/etc/X11/xorg.conf\"\n(==) Using config directory: \"/etc/X11/xorg.conf.d\"\n(==) Using system config directory \"/usr/share/X11/xorg.conf.d\"\nStarting vncserver\n\nDesktop 'TurboVNC: andes-gpu5.olcf.ornl.gov:1 (userA)' started on display andes-g                                                                             pu5.olcf.ornl.gov:1\n\nStarting applications specified in /ccs/home/userA/.vnc/xstartup.turbovnc\nLog file is /ccs/home/userA/.vnc/andes-gpu5.olcf.ornl.gov:1.log\n\n**************************************************************************\nInstructions\n\nIn a new terminal, open a tunneling connection with andes-gpu5.olcf.ornl.gov and                                                                              port 5901\nexample:\n     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov\n     andes: ssh -4L 5901:localhost:5901 andes-gpu5\n\n**************************************************************************\n\n\n/sw/andes/spack-envs/base/opt/linux-rhel8-x86_64/gcc-8.3.1/vmd-1.9.3-javakxxmgnha3ah4hqcv2rpx4paunyzf/lib/vmd_LINUXAMD64: /lib64/libGL.so.1: no version information available (required by /sw/andes/spack-envs/base/opt/linux-rhel8-x86_64/gcc-8.3.1/vmd-1.9.3-javakxxmgnha3ah4hqcv2rpx4paunyzf/lib/vmd_LINUXAMD64)\nInfo) VMD for LINUXAMD64, version 1.9.3 (November 30, 2016)\nInfo) http://www.ks.uiuc.edu/Research/vmd/\nInfo) Email questions and bug reports to vmd@ks.uiuc.edu\nInfo) Please include this reference in published work using VMD:\nInfo)    Humphrey, W., Dalke, A. and Schulten, K., `VMD - Visual\nInfo)    Molecular Dynamics', J. Molec. Graphics 1996, 14.1, 33-38.\nInfo) -------------------------------------------------------------\nInfo) Multithreading available, 56 CPUs detected.\nInfo)   CPU features: SSE2 AVX AVX2 FMA\nInfo) Free system memory: 986GB (97%)\nInfo) Creating CUDA device pool and initializing hardware...\nInfo) Detected 4 available CUDA accelerators:\nInfo) [0] Tesla K80          13 SM_3.7 @ 0.82 GHz, 11GB RAM, KTO, AE2, ZCP\nInfo) [1] Tesla K80          13 SM_3.7 @ 0.82 GHz, 11GB RAM, AE2, ZCP\nInfo) [2] Tesla K80          13 SM_3.7 @ 0.82 GHz, 11GB RAM, AE2, ZCP\nInfo) [3] Tesla K80          13 SM_3.7 @ 0.82 GHz, 11GB RAM, AE2, ZCP\nWarning) Detected X11 'Composite' extension: if incorrect display occurs\nWarning) try disabling this X server option.  Most OpenGL drivers\nWarning) disable stereoscopic display when 'Composite' is enabled.\nInfo) OpenGL renderer: Tesla K80/PCIe/SSE2\nInfo)   Features: STENCIL MSAA(4) MDE CVA MTX NPOT PP PS GLSL(OVFGS)\nInfo)   Full GLSL rendering mode is available.\nInfo)   Textures: 2-D (16384x16384), 3-D (2048x2048x2048), Multitexture (4)\nInfo) Detected 4 available TachyonL/OptiX ray tracing accelerators\nInfo)   Compiling 1 OptiX shaders on 4 target GPUs...\nInfo) Dynamically loaded 2 plugins in directory:\nInfo) /sw/andes/spack-envs/base/opt/linux-rhel8-x86_64/gcc-8.3.1/vmd-1.9.3-javakxxmgnha3ah4hqcv2rpx4paunyzf/lib/plugins/LINUXAMD64/molfile\nvmd >\n\nStep 3 (terminal 2)\n\n<string>:1418: (INFO/1) Duplicate implicit target name: \"step 3 (terminal 2)\".\n\nIn a second terminal on your local system open a tunneling connection following\nthe instructions given by the vnc start-up script:\n\nlocalsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov\n\nandes: ssh -4L 5901:localhost:5901 andes-gpu5\n\nStep 4 (local system)\n\n<string>:1427: (INFO/1) Duplicate implicit target name: \"step 4 (local system)\".\n\nLaunch the vncviewer. When you launch the vncviewer that you downloaded you will\nneed to specify localhost:5901. You will also set a passoword for the initial\nconnection or enter the created password for subsequent connections.\n\nvmd-vgl.sh (GPU rendering)\n\n#!/bin/sh\n\nwhat()\n{\n    hostname\n}\necho\necho \"Starting X\"\nxinit &\nsleep 5\necho \"Starting vncserver\"\n\n/opt/TurboVNC/bin/vncserver :1 -geometry 1920x1080 -depth 24\n\necho\necho\necho \"**************************************************************************\"\necho \"Instructions\"\necho\necho \"In a new terminal, open a tunneling connection with $(what) and port 5901\"\necho \"example:\"\necho \"   localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov \"\necho \"   andes: ssh -4L 5901:localhost:5901 $(what) \"\necho\necho \"**************************************************************************\"\necho\necho\nexport DISPLAY=:1\nmodule load vmd\nvglrun vmd\nvncserver -kill :1\n\nRemote Visualization using Nice DCV (GPU nodes only)\n\nNice DCV is currently undergoing maintenance. Instead, please use the VNC options detailed above.\n\nStep 1 (terminal 1)\n\nLaunch an interactive job:\n\nlocalsytem: ssh username@andes.olcf.ornl.gov\nandes: salloc -A PROJECT_ID -p gpu -N 1 -t 60:00 -M andes --constraint=DCV\n\nRun the following commands:\n\n$ xinit &\n$ export DISPLAY=:0\n$ dcv create-session --gl-display :0 mySessionName\n$ hostname  // will be used to open a tunneling connection with this node\n$ andes-gpuN\n\nStep 2 (terminal 2)\n\nOpen a tunneling connection with gpu node N, given by hostname:\n\nlocalsystem: ssh username@andes.olcf.ornl.gov -L 8443:andes-gpuN:8443\n\nOpen your web browser using the following link and use your credentials to\naccess OLCF systems: https://localhost:8443 When finished, kill the dcv\nsession in first terminal:\n\n$ dcv close-session mySessionName\n$ kill %1"}
{"doc":"apache-spark","text":"Apache Spark\n\nOverview\n\n<string>:3: (INFO/1) Duplicate implicit target name: \"apache spark\".\n\nApache Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.\nIn this document we provide instructions to run multi-node Spark cluster on Andes system and show an example of pyspark job.\n\nGetting Started\n\nDownload Spark from the Apache Spark download page.\nI used Spark-3.1, but it should work for newer versions as well.\nuntar the downloaded file and rename the directory as spark.\n\nTo setup a Spark cluster, we use the following Slurm script to request compute nodes.\nThe slurm script requests four nodes and spawns a Spark cluster having a master node and three worker nodes.\nNumber of worker nodes can be increased or decreased by changing the value of -N option in the Slurm script.\n\n#!/bin/bash\n#SBATCH --mem=0\n#SBATCH -A <ABC1234>\n#SBATCH -t 1:00:00\n#SBATCH -N 4\n#SBATCH -J spark_test\n#SBATCH -o o.spark_test\n#SBATCH -e e.spark_test\n\nmodule load spark\nmodule load python\n\nnodes=($(scontrol show hostnames ${SLURM_JOB_NODELIST} | sort | uniq ))\nnumnodes=${#nodes[@]}\nlast=$(( $numnodes - 1 ))\nexport SCRATCH=<SCRATCH_DIRECTORY>\nexport SPARK_HOME=<PATH/WHERE/SPARK/DIRECTORY/IS>/spark\nmaster=${nodes[0]}\nmasterurl=\"spark://${master}.olcf.ornl.gov:7077\"\nssh ${nodes[0]} \"cd ${SPARK_HOME}; source /etc/profile ; module load spark; ./sbin/start-master.sh\"\nfor i in $( seq 1 $last )\ndo\n    ssh ${nodes[$i]} \"cd ${SPARK_HOME}; source /etc/profile ; module load spark; ./sbin/start-worker.sh ${masterurl}\"\ndone\n\nssh ${nodes[0]} \"cd ${SPARK_HOME}; source /etc/profile ; module load spark; /usr/bin/time -v ./bin/spark-submit --deploy-mode client --executor-cores 32 --executor-memory 250G --conf spark.standalone.submit.waitAppCompletion=true --master $masterurl spark_test.py\"\nwait\necho 'end'\nexit\n\nThe Slurm script submits a test python script (spark_test.py) described below. This script runs a pyspark code to test the Spark cluster.\nCopy the content below and save it as a spark_test.py file in the SPARK_HOME directory.\nYou can also change the spark_test.py file's path, but you will have to update the Slurm script appropriately.\n\n#spark_test.py\nimport random\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\n\n\nspark = SparkSession.builder.appName('Test-app').getOrCreate()\n\n#Generate sample dataset\ncola_list = ['2022-01-01', '2022-01-02', '2022-01-03' ]\ncolb_list = ['CSC', 'PHY', 'MAT', 'ENG', 'CHE', 'ENV', 'BIO', 'PHRM']\ncolc_list = [100, 200, 300, 400, 500, 600, 700, 800, 900]\n\n\n# declaring a random.seed value to generate same data in every run\nrandom.seed(1)\nsample_data = []\nfor idx in range(1000):\n    sample_data.append([random.choice(cola_list), random.choice(colb_list), random.choice(colc_list)])\n\ncolumns= [\"date\", \"org\", \"value\"]\n#creating a Spark dataframe\ndf = spark.createDataFrame(data = sample_data, schema = columns)\n\nres = (df.groupBy('date','org')\n       .agg(F.count('value').alias('count_value')))\nres.show()\n\nIf the spark cluster is setup and the spark-test.py executes successfully, the output in the log file o.spark_test\nshould look similar to the table below.\n\n+----------+----+-----------+\n|      date| org|count_value|\n+----------+----+-----------+\n|2022-01-03| BIO|         37|\n|2022-01-02| ENV|         53|\n|2022-01-03| CHE|         39|\n|2022-01-03| PHY|         46|\n|2022-01-01| CSC|         45|\n|2022-01-03| CSC|         48|\n|2022-01-01| BIO|         39|\n|2022-01-01| MAT|         42|\n|2022-01-02| CHE|         44|\n|2022-01-03| ENV|         33|\n|2022-01-01| ENG|         33|\n|2022-01-02| ENG|         28|\n|2022-01-01| ENV|         33|\n|2022-01-02| CSC|         45|\n|2022-01-02| MAT|         51|\n|2022-01-01| PHY|         38|\n|2022-01-01|PHRM|         40|\n|2022-01-03|PHRM|         42|\n|2022-01-02|PHRM|         43|\n|2022-01-03| ENG|         56|\n+----------+----+-----------+\nonly showing top 20 rows\n\nSpark also provides a web UI to monitor cluster, and you can access it on your local machine by port forwarding the master node to local machine.\n\nFor example, if master node is running on andes338, you can run the following code on your local machine terminal.\n\nssh -N <USERNAME>@andes-login1.olcf.ornl.gov -L 8080:andes338.olcf.ornl.gov:8080\n\nThen access the Spark dashboard using address http://localhost:8080/ on a web browser on your local machine.\n\nThe spark documentation is very useful tool, go through it to find the Spark capabilities."}
{"doc":"archiving","text":"This page has been deprecated, and relevant information is now available on data-storage-and-transfers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers>. Please update any bookmarks to use that page.\n\nHPSS Data Archival System\n\nCurrently, HSI and HTAR are offered for archiving data into HPSS or retrieving\ndata from the HPSS archive. For optimal transfer performance, we recommend\nsending a file of 768 GB or larger to HPSS. The minimum file size that we\nrecommend sending is 512 MB. HPSS will handle files between 0K and 512 MB, but\nwrite and read performance will be negatively affected. For files smaller than\n512 MB we recommend bundling them with HTAR to achieve an archive file of at\nleast 512 MB.\n\nUsing Globus\n\nThe OLCF users have access to a new functionality, using Globus to transfer\nfiles to HPSS through the endpoint \"OLCF HPSS\". Globus has restriction of 8\nactive transfers across all the users. Each user has a limit of 3 active\ntransfers, so it is required to transfer a lot of data on each transfer than\nless data across many transfers. If a folder is constituted with mixed files\nincluding thousands of small files (less than 1MB each one), it would be better\nto tar the small files.  Otherwise, if the files are larger, Globus will handle\nthem. To transfer the files, follow these steps:\n\nVisit www.globus.org and login\n\n\n\nThen select the organization that you belong, if you don't work for ORNL, do\nnot select ORNL. If your organization is not in the list, create a Globus\naccount\n\n\n\nSearch for the endpoint OLCF DTN\n\n\n\n\n\nDeclare path\n\n\n\nOpen a second panel to declare the new endpoint called OLCF HPSS and use\nthe appropriate path for HPSS\n\n\n\n\n\nSelect your file/folder and click start. hen an activity report will appear\nand you can click on it to see the status. When the transfer is finished or\nfailed, you will receive an email\n\n\n\n\n\n\n\n\n\nUsing HSI\n\nWhen retrieving data from a tar archive larger than 1 TB, we recommend that you\npull only the files that you need rather than the full archive.  Examples of\nthis will be given in the htar section below. Issuing the command hsi will\nstart HSI in interactive mode. Alternatively, you can use:\n\nhsi [options] command(s)\n\n...to execute a set of HSI commands and then return. To list you files on the\nHPSS, you might use:\n\nhsi ls\n\nhsi commands are similar to ftp commands. For example, hsi get and\nhsi put are used to retrieve and store individual files, and hsi mget\nand hsi mput can be used to retrieve multiple files. To send a file to HPSS,\nyou might use:\n\nhsi put a.out : /hpss/prod/[projid]/users/[userid]/a.out\n\nTo retrieve one, you might use:\n\nhsi get /hpss/prod/[projid]/proj-shared/a.out\n\nHere is a list of commonly used hsi commands.\n\n\nThe table above provides a comprehensive list of commands and their corresponding functions for archiving files in HPSS (High Performance Storage System). The first column lists the commands, while the second column describes their specific functions. The \"cd\" command allows users to change their current directory within HPSS. The \"get\" and \"mget\" commands are used to copy one or more files from HPSS to the local system. The \"cget\" command is a conditional get, which only copies the file if it does not already exist in the local system. The \"cp\" command is used to copy a file within HPSS. The \"rm\" and \"mdelete\" commands are used to remove one or more files from HPSS. The \"ls\" command lists the contents of a directory in HPSS. The \"put\" and \"mput\" commands are used to copy one or more local files to HPSS. The \"cput\" command is a conditional put, which only copies the file into HPSS if it is not already present. The \"pwd\" command prints the current directory in HPSS. The \"mv\" command is used to rename an HPSS file. The \"mkdir\" command creates a new directory in HPSS. Lastly, the \"rmdir\" command is used to delete an existing directory in HPSS. These commands provide users with a variety of options for managing and organizing their files in HPSS, making it a powerful tool for archiving and storing large amounts of data.\n\n| Command | Function |\n|---------|----------|\n| cd | Change current directory |\n| get, mget | Copy one or more HPSS-resident files to local files |\n| cget | Conditional get - get the file only if it doesn't already exist |\n| cp | Copy a file within HPSS |\n| rm, mdelete | Remove one or more files from HPSS |\n| ls | List a directory |\n| put, mput | Copy one or more local files to HPSS |\n| cput | Conditional put - copy the file into HPSS unless it is already there |\n| pwd | Print current directory |\n| mv | Rename an HPSS file |\n| mkdir | Create an HPSS directory |\n| rmdir | Delete an HPSS directory |\n\n\n\nAdditional HSI Documentation\n\nThere is interactive documentation on the hsi command available by running:\n\nhsi help\n\nAdditional documentation can be found on the HPSS Collaboration website.\n\nUsing HTAR\n\nThe htar command provides an interface very similar to the traditional\ntar command found on UNIX systems. It is used as a command-line interface.\nThe basic syntax of htar is:\n\nhtar -{c|K|t|x|X} -f tarfile [directories] [files]\n\nAs with the standard Unix tar utility the -c, -x, and -t\noptions, respectively, function to create, extract, and list tar archive files.\nThe -K option verifies an existing tarfile in HPSS and the -X option can\nbe used to re-create the index file for an existing archive. For example, to\nstore all files in the directory dir1 to a file named\n/hpss/prod/[projid]/users/[userid]/allfiles.tar on HPSS, use the command:\n\nhtar -cvf /hpss/prod/[projid]/users/[userid]/allfiles.tar dir1/*\n\nTo retrieve these files:\n\nhtar -xvf  /hpss/prod/[projid]/users/[userid]/allfiles.tar\n\nhtar will overwrite files of the same name in the target directory.  When\npossible, extract only the files you need from large archives. To display the\nnames of the files in the project1.tar archive file within the HPSS home\ndirectory:\n\nhtar -vtf  /hpss/prod/[projid]/users/[userid]/project1.tar\n\nTo extract only one file, executable.out, from the project1 directory in\nthe Archive file called `` /hpss/prod/[projid]/users/[userid]/project1.tar``:\n\nhtar -xm -f project1.tar project1/ executable.out\n\nTo extract all files from the project1/src directory in the archive file\ncalled project1.tar, and use the time of extraction as the modification\ntime, use the following command:\n\nhtar -xm -f  /hpss/prod/[projid]/users/[userid]/project1.tar project1/src\n\nHTAR Limitations\n\nThe htar utility has several limitations.\n\nApending data\n\nYou cannot add or append files to an existing archive.\n\nFile Path Length\n\nFile path names within an htar archive of the form prefix/name are limited\nto 154 characters for the prefix and 99 characters for the file name. Link names\ncannot exceed 99 characters.\n\nSize\n\nThere are limits to the size and number of files that can be placed in an HTAR\narchive.\n\n\nThe table provides information on the limitations for archiving files. The first column lists the maximum individual file size, which is 68GB. This limit is due to the POSIX (Portable Operating System Interface) standard, which sets a maximum file size of 2^63-1 bytes. This means that any single file larger than 68GB cannot be archived. The second column lists the maximum number of files that can be included in a single archive, which is 1 million. This means that if you have more than 1 million files, they will need to be split into multiple archives. These limitations are important to keep in mind when organizing and storing large amounts of data, as exceeding them can result in errors or difficulties in accessing the archived files. \n\n| Individual File Size Maximum | Maximum Number of Files per Archive |\n|------------------------------|-------------------------------------|\n| 68GB                         | 1 million                           |\n\nFor example, when attempting to HTAR a directory with one member file larger\nthat 64GB, the following error message will appear:\n\n$ htar -cvf  /hpss/prod/[projid]/users/[userid]/hpss_test.tar hpss_test/\n\nINFO: File too large for htar to handle: hpss_test/75GB.dat (75161927680 bytes)\nERROR: 1 oversize member files found - please correct and retry\nERROR: [FATAL] error(s) generating filename list\nHTAR: HTAR FAILED\n\nAdditional HTAR Documentation\n\nFor more information about htar, execute man htar."}
{"doc":"ascent_user_guide","text":"Ascent\n\nSystem Overview\n\n\n\nAscent is a stand-alone 18-node system with the same architecture and design\nas Summit. It's most often utilized as a resource for OLCF training events,\nworkshops, and conferences. Ascent exists in the NCCS Open Security Enclave,\nwhich is subject to fewer restrictions than the Moderate Security Enclave that\nhouses systems such as Summit. This means that participants in training events can go through a streamlined version of the approval process before being granted access.\n\nAs the Ascent user environment is almost identical to Summit, additional\ninformation can be found in the training-system-ascent <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#training-system-ascent> section of the\nsummit-user-guide <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#summit-user-guide>."}
{"doc":"authenticating","text":"Authenticating\n\n\n\nOLCF Moderate Accounts\n\nmyOLCF is currently available to OLCF Moderate user accounts; i.e., users that authenticate to\nOLCF systems with an RSA SecurID token. Visit https://my.olcf.ornl.gov and authenticate with your\nOLCF Moderate username and RSA SecurID PASSCODE (PIN followed by the 6-digit tokencode).\n\nThe myOLCF login page\n\nOLCF Open Accounts\n\nOLCF Open user accounts, i.e., users that authenticate to OLCF systems with a password,\ncannot access myOLCF at this time, as we are still investigating the feasibility of\nsupporting password-only authentication."}
{"doc":"citadel_user_guide","text":"Citadel User Guide\n\n\n\nThe OLCF's Scalable Protected Infrastructure (SPI) provides resources and protocols that enable researchers to process protected data at scale.  The SPI is built around a framework of security protocals that allows researchers to process large datasets containing private information.  Using this framework researchers can use the center's large HPC resources to compute data containing protected health information (PHI), personally identifiable information (PII), data protected under International Traffic in Arms Regulations, and other types of data that require privacy.\n\nThe SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.\n\nThe SPI provides access to the OLCF's Summit resource for compute.  To safely separate SPI and non-SPI workflows, SPI workflows must use a separate login node named Citadel<spi-compute-citadel> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Citadel<spi-compute-citadel>>.  Citadel provides a login node specifically for SPI workflows.\n\nSummit Documentation\n\nThe SPI resource, Citadel<spi-compute-citadel> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Citadel<spi-compute-citadel>>, utilizes Summit's compute resources but adds measures to ensure separation of SPI and non-SPI workflows and data.  At a high level Citadel<spi-compute-citadel> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Citadel<spi-compute-citadel>> is a Summit login node for SPI workflows. Because of this, the Summit User Guide<summit-documentation-resources> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Summit User Guide<summit-documentation-resources>> provides much of the needed documentation on system details and use.\n\nMore information on the Programming Environment, Compiling, and Running Batch Jobs can be found in the Summit User Guide<summit-documentation-resources> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Summit User Guide<summit-documentation-resources>>.\n\nCitadel (SPI) Documentation\n\nFor notable differences between Citadel and Summit, please see the SPI<spi-compute-citadel> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#SPI<spi-compute-citadel>> documentation.\n\nThe login node used by Citadel mirrors the Summit login nodes in hardare and software.  The login node also provides access to the same compute resources as are accessible from Summit."}
{"doc":"conda_basics","text":"Conda Basics\n\nBecause there is no conda module on Frontier, this guide only applies if you installed a personal Miniconda first.\nSee our Installing Miniconda Guide <https://docs.olcf.ornl.gov/software/python/miniconda.html> for more details.\n\nThis guide has been shortened and adapted from a challenge in OLCF's Hands-On with Summit GitHub repository (Python: Conda Basics).\n\nThe guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.\nFor those that just want a quick-reference list of common conda commands, see the conda-quick <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#conda-quick> section.\n\nOverview\n\nThis guide introduces a user to the basic workflow of using conda environments, as well as providing an example of how to create a conda environment that uses a different version of Python than the base environment uses on Summit.\nAlthough Summit is being used in this guide, all of the concepts still apply to other OLCF systems.\nIf using Frontier, this assumes you already have installed your own version of conda (e.g., by installing Miniconda <https://docs.olcf.ornl.gov/software/python/miniconda.html>).\n\nOLCF Systems this guide applies to:\n\nSummit\n\nAndes\n\nFrontier (if using conda)\n\nInspecting and setting up an environment\n\nFirst, load the python module and the gnu compiler module on Summit (most Python packages assume use of GCC)\n\n$ module load gcc\n$ module load python\n\nThe above module load python does not apply to Frontier (since you will be using a personal Miniconda instead).\n\nThis puts you in the \"base\" conda environment, which is the default Python environment after loading the module.\nTo see a list of environments, use the command conda env list:\n\n$ conda env list\n\n# conda environments:\n#\nbase                  *  /sw/summit/python/3.8/anaconda3/2020.07-rhel8\n\nThis also is a great way to keep track of the locations and names of all other environments that have been created.\nThe current environment is indicated by *.\n\nTo see what packages are installed in the active environment, use conda list:\n\n$ conda list\n\n# packages in environment at /sw/summit/python/3.8/anaconda3/2020.07-rhel8:\n#\n# Name                    Version                   Build  Channel\n_ipyw_jlab_nb_ext_conf    0.1.0                    py38_0\n_libgcc_mutex             0.1                        main\nalabaster                 0.7.12                     py_0\nanaconda                  2020.07                  py38_0\nanaconda-client           1.7.2                    py38_0\nanaconda-project          0.8.4                      py_0\nasn1crypto                1.3.0                    py38_0\nastroid                   2.4.2                    py38_0\nastropy                   4.0.1.post1      py38h7b6447c_1\n.\n.\n.\n\nYou can find the version of Python that exists in this base environment by executing:\n\n$ python --version\n\nPython 3.8.3\n\nCreating a new environment\n\nFor this guide, you are going to install a different version of Python.\n\nTo do so, create a new environment using the conda create command:\n\n$ conda create -p /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/py3711-summit python=3.7.11\n\nThe -p flag specifies the desired path and name of your new virtual environment.\nThe directory structure is case sensitive, so be sure to insert <YOUR_PROJECT_ID> as lowercase.\nDirectories will be created if they do not exist already (provided you have write-access in that location).\nInstead, one can solely use the --name <your_env_name> flag which will automatically use your $HOME directory.\n\nIt is highly recommended to create new environments in the \"Project Home\" directory (on Summit, this is /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>).\nThis space avoids purges, allows for potential collaboration within your project, and works better with the compute nodes.\nIt is also recommended, for convenience, that you use environment names that indicate the hostname, as virtual environments created on one system will not necessarily work on others.\n\nAfter executing the conda create command, you will be prompted to install \"the following NEW packages\" -- type \"y\" then hit Enter/Return.\nDownloads of the fresh packages will start and eventually you should see something similar to:\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n#     $ conda activate /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/py3711-summit\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\nDue to the specific nature of conda on Summit, you must use source activate and source deactivate instead of conda activate and conda deactivate.\nLet's activate the new environment:\n\n$ source activate /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/py3711-summit\n\nThe path to the environment should now be displayed in \"( )\" at the beginning of your terminal lines, which indicate that you are currently using that specific conda environment.\nAnd if you check with conda env list again, you should see that the * marker has moved to your newly activated environment:\n\n$ conda env list\n\n# conda environments:\n#\n                      *  /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/py3711-summit\nbase                     /sw/summit/python/3.8/anaconda3/2020.07-rhel8\n\nInstalling packages\n\nNext, let's install a package (NumPy).\nThere are a few different approaches.\n\nInstalling with pip\n\nOne way to install packages into your conda environment is to build packages from source using pip.\nThis approach is useful if a specific package or package version is not available in the conda repository, or if the pre-compiled binaries don't work on the HPC resources (which is common).\nHowever, building from source means you need to take care of some of the dependencies yourself, especially for optimization.\nPip is available to use after installing Python into your conda environment, which you have already done.\n\nBecause issues can arise when using conda and pip together (see link in conda-refs <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#conda-refs>), it is recommended to do this only if absolutely necessary.\n\nTo build a package from source, use pip install --no-binary=<package_name> <package_name>:\n\n$ CC=gcc pip install --no-binary=numpy numpy\n\nThe CC=gcc flag will ensure that you are using the proper compiler and wrapper.\nBuilding from source results in a longer installation time for packages, so you may need to wait a few minutes for the install to finish.\n\nYou have successfully built NumPy from source in your conda environment;\nhowever, you did not link in any additional linear algebra packages, so this version of NumPy is not optimized.\nLet's install a more optimized version using a different method instead, but first you must uninstall the pip-installed NumPy:\n\n$ pip uninstall numpy\n\nInstalling with conda commands\n\nThe traditional, and more basic, approach to installing/uninstalling packages into a conda environment is to use the commands conda install and conda remove.\nInstalling packages with this method checks the Anaconda Distribution Repository for pre-built binary packages to install.\nLet's do this to install NumPy:\n\n$ conda install numpy\n\nBecause NumPy depends on other packages for optimization, this will also install all of its dependencies.\nYou have just installed an optimized version of NumPy, now let's test it.\n\nTesting your new environment\n\nLet's run a test to make sure everything installed properly.\nSince you are running a small test, you can do this without having to run on a compute node.\n\nRemember, at larger scales both your performance and your fellow users' performance will suffer if you do not run on the compute nodes.\nIt is always highly recommended to run on the compute nodes (through the use of a batch job or interactive batch job).\n\nMake sure you're in a Python shell first, then print out the versions of Python and NumPy:\n\n$ python3\n\n>>> import platform\n>>> import numpy\n>>> py_vers = platform.python_version()\n>>> np_vers = numpy.__version__\n>>> print(\"Hello from Python\", py_vers)\nHello from Python 3.7.11\n>>> print(\"You are using NumPy\", np_vers)\nYou are using NumPy 1.20.3\n\nAdditional Tips\n\nCloning the base environment:\n\nIt is not recommended to try to install new packages into the base environment.\nInstead, you can clone the base environment for yourself and install packages into the clone.\nTo clone an environment, you must use the --clone <env_to_clone> flag when creating a new conda environment.\nAn example for cloning the base environment into your Project Home directory on Summit is provided below:\n\n$ conda create -p /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/baseclone-summit --clone base\n$ source activate /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/baseclone-summit\n\nAdding known environment locations:\n\nFor a conda environment to be callable by a \"name\", it must be installed in one of the envs_dirs directories.\nThe list of known directories can be seen by executing:\n\n$ conda config --show envs_dirs\n\nOn OLCF systems, the default location is your $HOME directory.\nIf you plan to frequently create environments in a different location other than the default (such as /ccs/proj/...), then there is an option to add directories to the envs_dirs list.\n\nFor example, to track conda environments in a subdirectory called summit in Project Home you would execute:\n\n$ conda config --append envs_dirs /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit\n\nThis will create a .condarc file in your $HOME directory if you do not have one already, which will now contain this new envs_dirs location.\nThis will now enable you to use the --name env_name flag when using conda commands for environments stored in the summit directory, instead of having to use the -p /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/env_name flag and specifying the full path to the environment.\nFor example, you can do source activate py3711-summit instead of source activate /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/py3711-summit.\n\nExporting (sharing) an environment:\n\nYou may want to share your environment with someone else.\nOne way to do this is by creating your environment in a shared location where other users can access it.\nA different way (the method described below) is to export a list of all the packages and versions of your environment (an environment.yml file).\nIf a different user provides conda the list you made, conda will install all the same package versions and recreate your environment for them -- essentially \"sharing\" your environment.\nTo export your environment list:\n\n$ source activate my_env\n$ conda env export > environment.yml\n\nYou can then email or otherwise provide the environment.yml file to the desired person.\nThe person would then be able to create the environment like so:\n\n$ conda env create -f environment.yml\n\n\n\nQuick-Reference Commands\n\nList environments:\n\n$ conda env list\n\nList installed packages in current environment:\n\n$ conda list\n\nCreating an environment with Python version X.Y:\n\nFor a specific path:\n\n$ conda create -p /path/to/your/my_env python=X.Y\n\nFor a specific name:\n\n$ conda create -n my_env python=X.Y\n\nDeleting an environment:\n\nFor a specific path:\n\n$ conda env remove -p /path/to/your/my_env\n\nFor a specific name:\n\n$ conda env remove -n my_env\n\nCopying an environment:\n\nFor a specific path:\n\n$ conda create -p /path/to/new_env --clone old_env\n\nFor a specific name:\n\n$ conda create -n new_env --clone old_env\n\nActivating/Deactivating an environment:\n\n$ source activate my_env\n$ source deactivate # deactivates the current environment\n\nInstalling/Uninstalling packages:\n\nUsing conda:\n\n$ conda install package_name\n$ conda remove package_name\n\nUsing pip:\n\n$ pip install package_name\n$ pip uninstall package_name\n$ pip install --no-binary=package_name package_name # builds from source\n\n\n\nAdditional Resources\n\nConda User Guide\n\nAnaconda Package List\n\nPip User Guide\n\nUsing Pip In A Conda Environment"}
{"doc":"containers_on_summit","text":"Containers on Summit\n\n\n\nUsers can build container images with Podman, convert it to a SIF file, and run the\ncontainer using the Singularity runtime. This page is intended for users with some\nfamiliarity with building and running containers.\n\nBasic Information\n\nUsers will make use of two applications on Summit - Podman and Singularity - in their\ncontainer build and run workflow. Both are available without needing to load any modules.\n\nPodman is a container framework from Red Hat that functions as a drop in replacement for\nDocker. On Summit, we will use Podman for building container images and converting them\ninto tar files for Singularity to use. Podman makes use of Dockerfiles to describe the\nimages to be built. We cannot use Podman to run\ncontainers as it doesn't properly support MPI on Summit, and Podman does not support\nstoring its container images on GPFS or NFS.\n\nDue to Podman's lack of support for storage on GPFS and NFS, container images will be\nbuilt on the login nodes using the node-local NVMe on the login node. This NVMe is mounted\nin /tmp/containers. Users should treat this storage as temporary. Any data (container\nimage layers or otherwise) in this storage will be purged if the node is ever rebooted or\nwhen it gets full.  So any images created with Podman need to be converted to tar files\nusing podman save and stored elsewhere if you wish to preserve your image.\n\nThe NVMes on the login nodes are not shared storage across all login nodes. Each login\nnode is different with an independent NVMe. So you will not find the container layers\nbuilt on one login node to appear in another login node. Every time you login to\nSummit, the load balancer may put you on a different login node than where you were\nworking on building your container. Even if you try to stick to the same login node,\nthe NVMes should be treated as temporary storage as they can be purged anytime, during\na reboot or for some other reason. It is the user's responsibility to save any in\nprogress or completed container image build as a tar file or sif file before you close\na session.\n\nSingularity is a container framework from Sylabs. On Summit, we will use Singularity\nsolely as the runtime. We will convert the tar files of the container images Podman\ncreates into sif files, store those sif files on GPFS, and run them with\nJsrun. Singularity also allows building images but ordinary users cannot utilize that on\nSummit due to additional permissions not allowed for regular users.\n\nUsers will be building and running containers on Summit without root permissions\ni.e. containers on Summit are rootless.  This means users can get the benefits of\ncontainers without needing additional privileges. This is necessary for a shared system\nlike Summit. And this is part of the reason why Docker doesn't work on Summit. Podman and\nSingularity provides rootless support but to different extents hence why users need to use\na combination of both.\n\nSetup before Building\n\nUsers will need to set up a file in their home directory\n/ccs/home/<username>/.config/containers/storage.conf with the following content:\n\n[storage]\ndriver = \"overlay\"\ngraphroot = \"/tmp/containers/<user>\"\n\n[storage.options]\nadditionalimagestores = [\n]\n\n[storage.options.overlay]\nignore_chown_errors = \"true\"\nmount_program = \"/usr/bin/fuse-overlayfs\"\nmountopt = \"nodev,metacopy=on\"\n\n[storage.options.thinpool]\n\n<user> in the graphroot = \"/tmp/containers/<user>\" in the above file should be\nreplaced with your username. This will ensure that Podman will use the NVMe mounted in /tmp/containers for storage during container image builds.\n\nBuild and Run Workflow\n\nAs an example, let's build and run a very simple container image to demonstrate the workflow.\n\nBuilding a Simple Image\n\nCreate a directory called simplecontainer on home or GPFS and cd into it.\n\nCreate a file named simple.dockerfile with the following contents.\n\nFROM quay.io/centos/centos:stream8\nRUN dnf -y install epel-release && dnf -y install fakeroot\nRUN fakeroot dnf upgrade -y && fakeroot dnf update -y\nRUN fakeroot dnf install -y wget hostname libevent\nENTRYPOINT [\"/bin/bash\"]\n\nYou will notice the use of the fakeroot command when doing package installs with dnf. This is necessary as some some package installations require root permissions on container which the container builder does not have. So fakeroot allows dnf to think it is running as root and allows the installation to succeed.\n\nSingularity requires libevent installed in any container you build in order for it to work correctly with the jsrun job launcher.\n\nBuild the container image with podman build -t simple -f simple.dockerfile ..\n\nThe -t flag names the container image and the -f flag indicates the file to use for building the image.\n\nRun podman image ls to see the list of images. localhost/simple should be among them. Any container created without an explicit url to a container registry in its name will automatically have the localhost prefix.\n\n$ podman image ls\nREPOSITORY             TAG      IMAGE ID      CREATED      SIZE\nlocalhost/simple       latest   e47dbfde3e99  3 hours ago  687 MB\nquay.io/centos/centos  stream8  ad6f8b5e7f64  8 days ago   497 MB\n\nConvert this Podman container image into a tar file with podman save -o simple.tar localhost/simple.\n\nConvert the tar file into a Singularity sif file with  singularity build --disable-cache simple.sif docker-archive://simple.tar\n\nYou will also notice that we use centos:stream8 as our base image in the example. If you're planning on building a container image from scratch instead of using the OLCF MPI base image , use a centos:stream8 image with fakeroot installed as demonstrated above as your starting point (we talk about the OLCF MPI base image later in the olcf-mpi-base-image <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#olcf-mpi-base-image> section). Ubuntu would be difficult to use as a starting point since apt-get requires root from the get-go, and you can't even do a apt-get -y fakeroot to get you started. Other distributions haven't been tested. Using centos for this case for now is the most user friendly option).\n\nUsing a Container Registry to Build and Save your Images\n\nIf you are familiar with using a container registry like DockerHub, you can use that to save your Podman container images\nand use Singularity to pull from the registry and build the sif file. Below, we will use DockerHub as the example but there are many\nother container registries that you can use.\n\nUsing the simple example from the previous section, build the container image with podman build -t docker.io/<username>/simple -f simple.dockerfile . where <username> is your user on DockerHub.\n\npodman push uses the URL in the container image's name to push to the appropriate registry.\n\nCheck if your image is created\n\n$ podman image ls\nREPOSITORY                         TAG      IMAGE ID      CREATED      SIZE\ndocker.io/subilabrahamornl/simple  latest   e47dbfde3e99  3 hours ago  687 MB\nlocalhost/simple                   latest   e47dbfde3e99  3 hours ago  687 MB\nquay.io/centos/centos              stream8  ad6f8b5e7f64  8 days ago   497 MB\n\nRun podman login docker.io and enter your account's username and password so that Podman is logged in to the container registry before pushing.\n\nPush the container image to the registry with podman push docker.io/<username>/simple.\n\nYou can now create a Singularity sif file with singularity build --disable-cache --docker-login simple.sif docker://docker.io/<username>/simple.\n\nThis will ask you to enter your Docker username and password again for Singularity to download the image from Dockerhub and convert it to a sif file.\n\nThe reason we include the --disable-cache flag is because Singularity's caching can\nfill up your home directory without you realizing it. And if the home directory is\nfull, Singularity builds will fail. If you wish to make use of the cache, you can set\nthe environment variable\nSINGULARITY_CACHEDIR=/tmp/containers/<user>/singularitycache or something like that\nso that the NVMe storage is used as the cache.\n\nRunning a Simple Container in a Batch Job\n\nAs a simple example, we will run hostname with the Singularity container.\n\nCreate a file submit.lsf with the contents below.\n\n#!/bin/bash\n# Begin LSF Directives\n#BSUB -P STF007\n#BSUB -W 0:10\n#BSUB -q debug\n#BSUB -nnodes 1\n#BSUB -J simple_container_job\n#BSUB -o simple_container_job.%J\n#BSUB -e simple_container_job.%J\n\njsrun -n2 singularity exec ./simple.sif hostname\n\nSubmit the job with bsub submit.lsf. This should produce an output that looks like:\n\nh41n08\nh41n08\n\nHere, Jsrun starts 2 separate Singularity container runtimes since we pass the -n2 flag to start two processes. Each Singularity container runtime then loads the container image simple.sif and executes the hostname command from that container. If we had requested 2 nodes in the batch script and had run jsrun -n2 -r1 singularity exec ./simple.sif hostname, Jsrun would've started a Singularity runtime on each node and the output would look something like\n\nh41n08\nh41n09\n\n\n\nRunning an MPI program with the OLCF MPI base image\n\nCreating Singularity containers that run MPI programs require a few additional steps.\n\nOLCF provides an MPI base image that you can use for MPI programs. You can pull it with Podman with podman pull code.ornl.gov:4567/olcfcontainers/olcfbaseimages/mpiimage-centos-cuda\n\nLet's build an simple MPI example container using the prebuilt MPI base image from the repository.\n\nCreate a new directory mpiexample.\n\nCreate a file mpiexample.c with the following contents.\n\n#include <stdio.h>\n#include <mpi.h>\n\nint main (int argc, char *argv[])\n{\nint rank, size;\nMPI_Comm comm;\n\ncomm = MPI_COMM_WORLD;\nMPI_Init (&argc, &argv);\nMPI_Comm_rank (comm, &rank);\nMPI_Comm_size (comm, &size);\n\nprintf(\"Hello from rank %d\\n\", rank);\n\nMPI_Barrier(comm);\nMPI_Finalize();\n}\n\nCreate a file named mpiexample.dockerfile with the following contents\n\nFROM code.ornl.gov:4567/olcfcontainers/olcfbaseimages/mpiimage-centos-cuda:latest\nRUN mkdir /app\nCOPY mpiexample.c /app\nRUN cd /app && mpicc -o mpiexample mpiexample.c\n\nThe MPI base image only supports gcc/9.1.0 at the moment in order to be able to compile an MPI program during the container build.\nSo run the following commands to build the Podman image and convert it to the Singularity format.\n\nmodule purge\nmodule load DefApps\nmodule load gcc/9.1.0\nmodule -t list\npodman build -v $MPI_ROOT:$MPI_ROOT -f mpiexample.dockerfile -t mpiexample:latest .;\npodman save -o mpiexampleimage.tar localhost/mpiexample:latest;\nsingularity build --disable-cache mpiexampleimage.sif docker-archive://mpiexampleimage.tar;\n\nIt's possible the singularity build step might get killed due to reaching cgroup memory limit. To get around this, you can start an interactive job and build the singularity image with\n\njsrun -n1 -c42 -brs singularity build --disable-cache mpiexampleimage.sif docker-archive://mpiexampleimage.tar;\n\n(remember to do the above in /gpfs or specify the full path for the sif file somewhere in GPFS. If you try to save the sif file in your home directory you will error out because NFS is read-only from the compute nodes).\n\nCreate the following submit script submit.lsf. Make sure you replace the #BSUB -P STF007 line with your own project ID.\n\n#BSUB -P STF007\n#BSUB -W 0:30\n#BSUB -nnodes 2\n#BSUB -J singularity\n#BSUB -o singularity.%J\n#BSUB -e singularity.%J\n\nmodule purge\nmodule load DefApps\nmodule load  gcc/9.1.0\n\nsource /gpfs/alpine/stf007/world-shared/containers/utils/requiredmpilibs.source\n\njsrun -n 8 -r4  singularity exec --bind $MPI_ROOT:$MPI_ROOT,/autofs/nccs-svm1_home1,/autofs/nccs-svm1_home1:/ccs/home mpiexampleimage.sif /app/mpiexample\n\n# uncomment the below to run the preinstalled osubenchmarks from the container.\n#jsrun -n 8 -r 4 singularity exec --bind $MPI_ROOT:$MPI_ROOT,/autofs/nccs-svm1_home1,/autofs/nccs-svm1_home1:/ccs/home mpiimage.sif /osu-micro-benchmarks-5.7/mpi/collective/osu_allgather\n\nYou can view the Dockerfiles used to build the MPI base image at the code.ornl.gov\nrepository. These Dockerfiles are\nbuildable on Summit yourself by cloning the repository and running the ./build in the\nindividual directories in the repository. This allows you the freedom to modify these base\nimages to your own needs if you don't need all the components in the base images. You may\nrun into the cgroup memory limit when building so kill the podman process, log out, and\ntry running the build again if that happens when building.\n\nRunning a single node GPU program with the OLCF MPI base image\n\nSingularity provides the ability to access the GPUs from the containers, allowing you to containerize GPU programs.\nThe OLCF provided MPI base image already has CUDA libraries preinstalled and can be used for CUDA programs as well. You can pull it with Podman with podman pull code.ornl.gov:4567/olcfcontainers/olcfbaseimages/mpiimage-centos-cuda.\n\n<string>:5: (INFO/1) Duplicate explicit target name: \"code.ornl.gov repository\".\n\nThe OLCF provided MPI base image currently has CUDA 11.0.3 and CuDNN 8.2. If these don't fit your needs, you can build your own base image by modifying the files from the code.ornl.gov repository.\n\nLet's build an simple CUDA example container using the MPI base image from the repository.\n\nCreate a new directory gpuexample.\n\nCreate a file cudaexample.cu with the following contents\n\n#include <stdio.h>\n#define N 1000\n\n__global__\nvoid add(int *a, int *b) {\n    int i = blockIdx.x;\n    if (i<N) {\n        b[i] = 2*a[i];\n    }\n}\n\nint main() {\n    int ha[N], hb[N];\n\n    int *da, *db;\n    cudaMalloc((void **)&da, N*sizeof(int));\n    cudaMalloc((void **)&db, N*sizeof(int));\n\n    for (int i = 0; i<N; ++i) {\n        ha[i] = i;\n    }\ncudaMemcpy(da, ha, N*sizeof(int), cudaMemcpyHostToDevice);\n\nadd<<<N, 1>>>(da, db);\n\ncudaMemcpy(hb, db, N*sizeof(int), cudaMemcpyDeviceToHost);\n\nfor (int i = 0; i<N; ++i) {\n    if(i+i != hb[i]) {\n        printf(\"Something went wrong in the GPU calculation\\n\");\n    }\n}\nprintf(\"COMPLETE!\");\n     cudaFree(da);\n     cudaFree(db);\n\n     return 0;\n}\n\nCreate a file named gpuexample.dockerfile with the following contents\n\nFROM code.ornl.gov:4567/olcfcontainers/olcfbaseimages/mpiimage-centos-cuda:latest\nRUN mkdir /app\nCOPY cudaexample.cu /app\nRUN cd /app && nvcc -o cudaexample cudaexample.cu\n\nRun the following commands to build the container image with Podman and convert it to Singularity\n\npodman build -f gpuexample.dockerfile -t gpuexample:latest .;\npodman save -o gpuexampleimage.tar localhost/gpuexample:latest;\nsingularity build --disable-cache gpuexampleimage.sif docker-archive://gpuexampleimage.tar;\n\nIt's possible the singularity build step might get killed due to reaching cgroup memory limit. To get around this, you can start an interactive job and build the singularity image with\n\njsrun -n1 -c42 -brs singularity build --disable-cache gpuexampleimage.sif docker-archive://gpuexampleimage.tar;\n\n(remember to do this in /gpfs or specify the full path for the sif file somewhere in\nGPFS. If you try to save the sif file in your home directory you will error out because\nNFS is read-only from the compute nodes).\n\nCreate the following submit script submit.lsf. Make sure you replace the #BSUB -P\nSTF007 line with your own project ID.\n\n#BSUB -P STF007\n#BSUB -W 0:30\n#BSUB -nnodes 1\n#BSUB -J singularity\n#BSUB -o singularity.%J\n#BSUB -e singularity.%J\n\njsrun -n 1 -c 1 -g 1 singularity exec --nv gpuexampleimage.sif /app/cudaexample\n\nThe --nv flag is needed to tell Singularity to make use of the GPU.\n\nRunning a CUDA-Aware MPI program with the OLCF MPI base image\n\nYou can run containers with CUDA-aware MPI as well. CUDA-aware MPI allows transferring GPU\ndata with MPI without needing to copy the data over to CPU memory first. Read more\nCUDA-Aware MPI <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#CUDA-Aware MPI>.\n\nLet's build and run a container that will demonstrate CUDA-aware MPI.\n\nCreate a new directory cudaawarempiexample.\n\nRun the below wget commands to obtain the example code and Makefile from the OLCF\ntutorial example page.\n\nwget -O Makefile https://raw.githubusercontent.com/olcf-tutorials/MPI_ping_pong/master/cuda_aware/Makefile\nwget -O ping_pong_cuda_aware.cu https://raw.githubusercontent.com/olcf-tutorials/MPI_ping_pong/master/cuda_aware/ping_pong_cuda_aware.cu\n\nCreate a file named cudaawarempiexample.dockerfile with the following contents\n\nFROM code.ornl.gov:4567/olcfcontainers/olcfbaseimages/mpiimage-centos-cuda:latest\nRUN mkdir /app\nCOPY ping_pong_cuda_aware.cu Makefile /app\nRUN cd /app && make\n\nRun the following commands to build the container image with Podman and convert it to Singularity\n\nmodule purge\nmodule load DefApps\nmodule load gcc/9.1.0\nmodule -t list\npodman build --build-arg mpi_root=$MPI_ROOT -v $MPI_ROOT:$MPI_ROOT -f cudaawarempiexample.dockerfile -t cudaawarempiexample:latest .;\npodman save -o cudaawarempiexampleimage.tar localhost/cudaawarempiexample:latest;\nsingularity build --disable-cache cudaawarempiexampleimage.sif docker-archive://cudaawarempiexampleimage.tar;\n\nIt's possible the singularity build step might get killed due to reaching cgroup memory limit. To get around this, you can start an interactive job and build the singularity image with\n\njsrun -n1 -c42 -brs singularity build cudaawarempiexampleimage.sif docker-archive://cudaawarempiexampleimage.tar;\n\n(remember to do this in /gpfs or specify the full path for the sif file somewhere in\nGPFS. If you try to save the sif file in your home directory you will error out because\nNFS is read-only from the compute nodes).\n\nCreate the following submit script submit.lsf. Make sure you replace the #BSUB -P STF007 line with your own project ID.\n\n#BSUB -P STF007\n#BSUB -W 0:30\n#BSUB -nnodes 2\n#BSUB -J singularity\n#BSUB -o singularity.%J\n#BSUB -e singularity.%J\n\nmodule purge\nmodule load DefApps\nmodule load  gcc/9.1.0\n\nsource /gpfs/alpine/stf007/world-shared/containers/utils/requiredmpilibs.source\n\njsrun --smpiargs=\"-gpu\" -n 2 -a 1 -r 1 -c 42 -g 6 singularity exec --nv --bind $MPI_ROOT:$MPI_ROOT,/autofs/nccs-svm1_home1,/autofs/nccs-svm1_home1:/ccs/home cudaawarempiexampleimage.sif /app/pp_cuda_aware\n\nThe --nv flag is needed to tell Singularity to make use of the GPU.\n\nTips, Tricks, and Things to Watch Out For\n\nRun podman system prune and then run podman image rm --force $(podman image ls\n-aq) several times to clean out all the dangling images and layers if you want to do a\nfull reset.\n\nSometimes you may want to do a full purge of your container storage area. Your user\nshould own all the files in your /tmp/containers location. Recursively add write\npermissions to all files by running chmod -R +w /tmp/containers/<username> and then\nrun rm -r /tmp/containers/<username>.\n\nSometimes you may need to kill your podman process because you may have gotten killed\ndue to hitting cgroup limit. You can do so with pkill podman, then log out and log\nback in to reset your cgroup usage.\n\nIf you already have a \"image.tar\" file created with podman save from earlier that\nyou are trying to replace, you will need to delete it first before running any other\npodman save to replace it. podman save won't overwrite the tar file for you.\n\nNot using the --disable-cache flag in your singularity build commands could\ncause your home directory to get quickly filled by singularity caching image data. You\ncan set the cache to a location in /tmp/containers with export\nSINGULARITY_CACHEDIR=/tmp/containers/<username>/singularitycache if you want to avoid\nusing the --disable-cache flag.\n\nIf you see an error that looks something like ERRO[0000] stat /run/user/16248: no such\nfile or directory or Error: Cannot connect to the Podman socket, make sure there is\na Podman REST API service running.: error creating tmpdir: mkdir /run/user/12341:\npermission denied, try logging out and logging back in. If that fails, then after\nlogging in run ssh login<number> where login<number> is the login node you are\ncurrently logged in to. If all else fails, write to the help@olcf.ornl.gov and we can\nsee if the issue can be fixed from there.\n\nIf you're trying to mount your home directory with --bind\n/ccs/home/<user>:/ccs/home/<user> in your singularity exec command, it might not\nwork correctly. /ccs/home/user is an alias to /autofs/nccs-svm1_home1/user or\n/autofs/nccs-svm1_home2/user. You can find out which one is yours with stat\n/ccs/home/user and then mount your home directory with --bind\n/autofs/nccs-svm1_home1/user:/ccs/home/user to make /ccs/home/user visible within\nyour container."}
{"doc":"crusher_quick_start_guide","text":"Crusher Quick-Start Guide\n\n\n\n\n\nSystem Overview\n\nCrusher is an National Center for Computational Sciences (NCCS) moderate-security system that contains identical hardware and similar software as the upcoming Frontier system. It is used as an early-access testbed for Center for Accelerated Application Readiness (CAAR) and Exascale Computing Project (ECP) teams as well as NCCS staff and our vendor partners. The system has 2 cabinets, the first with 128 compute nodes and the second with 64 compute nodes, for a total of 192 compute nodes.\n\n\n\nCrusher Compute Nodes\n\nEach Crusher compute node consists of [1x] 64-core AMD EPYC 7A53 \"Optimized 3rd Gen EPYC\" CPU (with 2 hardware threads per physical core) with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X, each with 2 Graphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can think of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth memory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU, allowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of 36+36 GB/s. The 2 GCDs on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The GCDs on different MI250X are connected with Infinity Fabric GPU-GPU in the arrangement shown in the Crusher Node Diagram below, where the peak bandwidth ranges from 50-100 GB/s based on the number of Infinity Fabric connections between individual GCDs.\n\nTERMINOLOGY:\n\nThe 8 GCDs contained in the 4 MI250X will show as 8 separate GPUs according to Slurm, ROCR_VISIBLE_DEVICES, and the ROCr runtime, so from this point forward in the quick-start guide, we will simply refer to the GCDs as GPUs.\n\nCrusher node architecture diagram\n\nThere are [4x] NUMA domains per node and [2x] L3 cache regions per NUMA for a total of [8x] L3 cache regions. The 8 GPUs are each associated with one of the L3 regions as follows:\n\nNUMA 0:\n\nhardware threads 000-007, 064-071 | GPU 4\n\nhardware threads 008-015, 072-079 | GPU 5\n\nNUMA 1:\n\nhardware threads 016-023, 080-087 | GPU 2\n\nhardware threads 024-031, 088-095 | GPU 3\n\nNUMA 2:\n\nhardware threads 032-039, 096-103 | GPU 6\n\nhardware threads 040-047, 104-111 | GPU 7\n\nNUMA 3:\n\nhardware threads 048-055, 112-119 | GPU 0\n\nhardware threads 056-063, 120-127 | GPU 1\n\nSystem Interconnect\n\nThe Crusher nodes are connected with [4x] HPE Slingshot 200 Gbps (25 GB/s) NICs providing a node-injection bandwidth of 800 Gbps (100 GB/s).\n\nFile Systems\n\nCrusher is connected to the center-wide IBM Spectrum Scale™ filesystem providing 250 PB of storage capacity with a peak write speed of 2.5 TB/s. Crusher also has access to the center-wide NFS-based filesystem (which provides user and project home areas). While Crusher does not have direct access to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the dtn-user-guide <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#dtn-user-guide> to move data to/from HPSS.\n\nGPUs\n\nCrusher contains a total of 768 AMD MI250X. The AMD MI250X has a peak performance of 53 TFLOPS in double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 26.5 TFLOPS (double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in both directions simultaneously).\n\n\n\nConnecting\n\nTo connect to Crusher, ssh to crusher.olcf.ornl.gov. For example:\n\n$ ssh <username>@crusher.olcf.ornl.gov\n\nFor more information on connecting to OLCF resources, see connecting-to-olcf <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#connecting-to-olcf>.\n\n\n\nData and Storage\n\nFor more detailed information about center-wide file systems and data archiving available on Crusher, please refer to the pages on data-storage-and-transfers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers>, but the two subsections below give a quick overview of NFS and Lustre storage spaces.\n\nNFS Filesystem\n\n\nThe table provides a comprehensive overview of the content presented in the Crusher Quick Start Guide. It outlines the different areas and paths within the system, along with their corresponding types, permissions, quotas, and backup options. The first row, labeled \"User Home,\" shows that the path for user home directories is located at \"/ccs/home/[userid]\" and is of type NFS (Network File System). The permissions for this area are set by the user, and the quota is limited to 50 GB. Backups are available for this area, but it is not automatically purged. The retention period for backups is set at 90 days, and the user only has read-only access to this area on the compute nodes. The second row, labeled \"Project Home,\" shows that the path for project home directories is located at \"/ccs/proj/[projid]\" and is also of type NFS. The permissions for this area are set at 770, indicating that it is accessible to the project group and the user. The quota for this area is also limited to 50 GB, and backups are available but not automatically purged. The retention period for backups is also set at 90 days, and users only have read-only access to this area on the compute nodes. This table provides important information for users to understand the different areas and their corresponding settings within the Crusher system.\n\n| Area         | Path                 | Type | Permissions | Quota | Backups | Purged | Retention | On Compute Nodes |\n|--------------|----------------------|------|-------------|-------|---------|--------|-----------|------------------|\n| User Home    | /ccs/home/[userid]   | NFS  | User set    | 50 GB | Yes     | No     | 90 days   | Read-only        |\n| Project Home | /ccs/proj/[projid]   | NFS  | 770         | 50 GB | Yes     | No     | 90 days   | Read-only        |\n\n\n\nLustre Filesystem\n\n\nThe table provides a detailed breakdown of the different areas and paths within the Lustre HPE ClusterStor system, as well as the type of access, permissions, and quotas associated with each. The first row, labeled \"Member Work,\" pertains to the scratch space for individual users within a specific project. The path for this area is located in the /lustre/orion/[projid]/scratch/[userid] directory, where [projid] represents the project ID and [userid] represents the user's ID. The type of access for this area is Lustre HPE ClusterStor, with a permission level of 700, allowing for read, write, and execute capabilities. The quota for this area is set at 50 TB, meaning that users can store up to 50 TB of data in their scratch space. Backups are not enabled for this area, and any data that has not been accessed within 90 days will be automatically purged. There is no specific retention period set for this area. Finally, this area is accessible on compute nodes, meaning that users can access and work with their data while running computations on the cluster. The next two rows, labeled \"Project Work\" and \"World Work,\" follow a similar format, with the main difference being the path and permissions associated with each. The \"Project Work\" area is located in the /lustre/orion/[projid]/proj-shared directory and has a permission level of 770, allowing for read, write, and execute capabilities for all members of the project. The \"World Work\" area is located in the /lustre/orion/[projid]/world-shared directory and has a permission level of 775, allowing for read, write, and execute capabilities for all users on the cluster. Both of these areas have the same quota, backup, purged, retention, and on compute nodes settings as the \"Member Work\" area. Overall, this table provides important information for users of the Lustre HPE ClusterStor system, outlining the different areas and their associated settings to ensure efficient and organized data management. \n\n| Area        | Path                                      | Type              | Permissions | Quota | Backups | Purged  | Retention | On Compute Nodes |\n|-------------|-------------------------------------------|-------------------|-------------|-------|---------|---------|-----------|------------------|\n| Member Work | /lustre/orion/[projid]/scratch/[userid]   | Lustre HPE ClusterStor | 700         | 50 TB | No      | 90 days | N/A       | Yes              |\n| Project Work| /lustre/orion/[projid]/proj-shared        | Lustre HPE ClusterStor | 770         | 50 TB | No      | 90 days | N/A       | Yes              |\n| World Work  | /lustre/orion/[projid]/world-shared       | Lustre HPE ClusterStor | 775         | 50 TB | No      | 90 days | N/A       | Yes              |\n\n\n\n\n\nProgramming Environment\n\nCrusher users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.\n\nEnvironment Modules (Lmod)\n\nEnvironment modules are provided through Lmod, a Lua-based module system for dynamically altering shell environments. By managing changes to the shell’s environment variables (such as PATH, LD_LIBRARY_PATH, and PKG_CONFIG_PATH), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.\n\nGeneral Usage\n\nThe interface to Lmod is provided by the module command:\n\n\nThe table presented is a quick start guide for using the \"module\" command in the Crusher system. The \"module\" command is a tool used for managing software modules in a computing environment. The first column lists the different commands that can be used with the \"module\" command, while the second column provides a brief description of each command's function. The \"module -t list\" command displays a concise list of all the modules currently loaded in the system. The \"module avail\" command shows a table of all the available modules that can be loaded. The \"module help <modulename>\" command provides help information for a specific module. The \"module show <modulename>\" command displays the environment changes made by a specific module. The \"module spider <string>\" command searches for all possible modules based on a given string. The \"module load <modulename> [...]\" command loads the specified module(s) into the current environment. The \"module use <path>\" command adds a given path to the modulefile search cache and MODULESPATH. The \"module unuse <path>\" command removes a given path from the modulefile search cache and MODULESPATH. The \"module purge\" command unloads all currently loaded modules. The \"module reset\" command resets all loaded modules to their system defaults. Lastly, the \"module update\" command reloads all currently loaded modules. This quick start guide provides a comprehensive overview of the different commands and their functions, making it easier for users to manage software modules in the Crusher system.\n\n| Command         | Description                                                  |\n| --------------- | ------------------------------------------------------------ |\n| module -t list  | Shows a terse list of the currently loaded modules           |\n| module avail    | Shows a table of the currently available modules            |\n| module help <modulename> | Shows help information about <modulename>          |\n| module show <modulename> | Shows the environment changes made by the <modulename> modulefile |\n| module spider <string> | Searches all possible modules according to <string>         |\n| module load <modulename> [...] | Loads the given <modulename>(s) into the current environment |\n| module use <path> | Adds <path> to the modulefile search cache and MODULESPATH   |\n| module unuse <path> | Removes <path> from the modulefile search cache and MODULESPATH |\n| module purge    | Unloads all modules                                         |\n| module reset    | Resets loaded modules to system defaults                    |\n| module update   | Reloads all currently loaded modules                        |\n\n\n\nSearching for Modules\n\nModules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the spider sub-command can be used as summarized in the following table.\n\n\nThe table provides a quick start guide for using the \"module spider\" command in a crusher system. The \"module spider\" command is used to search for and display information about available modules in the system. The first row of the table explains that using the \"module spider\" command without any additional arguments will show the entire possible graph of modules. This means that all available modules in the system will be displayed in a hierarchical structure, showing their dependencies and relationships. The second row specifies that using the \"module spider\" command with a specific module name will search for that module in the graph and display its information. This is useful for finding a specific module without having to go through the entire graph. The third row states that using the \"module spider\" command with a specific module name and version will search for that specific version of the module in the graph. This is helpful for finding and loading a specific version of a module. The last row explains that using the \"module spider\" command with a string will search for any modulefiles containing that string. This is useful for finding modules that may have similar names or keywords. Overall, the table provides a comprehensive guide for using the \"module spider\" command to navigate and access modules in the crusher system.\n\n| Command | Description |\n|---------|-------------|\n| module spider | Shows the entire possible graph of modules |\n| module spider <modulename> | Searches for modules named <modulename> in the graph of possible modules |\n| module spider <modulename>/<version> | Searches for a specific version of <modulename> in the graph of possible modules |\n| module spider <string> | Searches for modulefiles containing <string> |\n\n\n\nCompilers\n\nCray, AMD, and GCC compilers are provided through modules on Crusher. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers.\n\nIt is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) whenever possible. See the next section for more details.\n\n\nThe table above provides a comprehensive overview of the programming environments, compilers, and compiler modules available for the Crusher system. The first column lists the vendors, including Cray, AMD, and GCC. The second column specifies the programming environment, which is the set of tools and libraries used for software development. The third column indicates the compiler module, which is a specific version of the compiler that is used to translate source code into machine code. The fourth column lists the language supported by each compiler, including C, C++, and Fortran. The fifth column specifies the compiler wrapper, which is a tool that helps to manage the compilation process. Finally, the last column lists the actual compiler used, such as craycc, amdclang, or ${GCC_PATH}/bin/gcc. This table serves as a quick start guide for users of the Crusher system, providing them with all the necessary information to select the appropriate programming environment and compiler for their needs.\n\n| Vendor | Programming Environment | Compiler Module | Language | Compiler Wrapper | Compiler |\n|--------|--------------------------|-----------------|----------|------------------|----------|\n| Cray   | PrgEnv-cray              | cce             | C        | cc               | craycc   |\n|        |                          |                 | C++      | CC               | craycxx or crayCC |\n|        |                          |                 | Fortran  | ftn              | crayftn  |\n| AMD    | PrgEnv-amd               | rocm            | C        | cc               | amdclang |\n|        |                          |                 | C++      | CC               | amdclang++ |\n|        |                          |                 | Fortran  | ftn              | amdflang |\n| GCC    | PrgEnv-gnu               | gcc             | C        | cc               | ${GCC_PATH}/bin/gcc |\n|        |                          |                 | C++      | CC               | ${GCC_PATH}/bin/g++ |\n|        |                          |                 | Fortran  | ftn              | ${GCC_PATH}/bin/gfortran |\n\n\n\nCray Programming Environment and Compiler Wrappers\n\nCray provides PrgEnv-<compiler> modules (e.g., PrgEnv-cray) that load compatible components of a specific compiler toolchain. The components include the specified compiler as well as MPI, LibSci, and other libraries. Loading the PrgEnv-<compiler> modules also defines a set of compiler wrappers for that compiler toolchain that automatically add include paths and link in libraries for Cray software. Compiler wrappers are provided for C (cc), C++ (CC), and Fortran (ftn).\n\nUse the -craype-verbose flag to display the full include and link information used by the Cray compiler wrappers. This must be called on a file to see the full output (e.g., CC -craype-verbose test.cpp).\n\nMPI\n\nThe MPI implementation available on Crusher is Cray's MPICH, which is \"GPU-aware\" so GPU buffers can be passed directly to MPI calls.\n\n\n\nCompiling\n\nThis section covers how to compile for different programming models using the different compilers covered in the previous section.\n\nMPI\n\n<string>:230: (INFO/1) Duplicate implicit target name: \"mpi\".\n\n\nThe table provides a quick start guide for using a crusher, specifically focusing on the implementation, module, compiler, and header files & linking. The implementation listed is Cray MPICH, which is a high-performance implementation of the Message Passing Interface (MPI) standard. The module is cray-mpich, which is the specific module that needs to be loaded in order to use the Cray MPICH implementation. The compiler options listed are cc, CC, and ftn, which are the Cray compiler wrappers for C, C++, and Fortran, respectively. These wrappers allow for easy compilation and linking of MPI programs. The last column lists the necessary header files and linking information, which is already built into the Cray compiler wrappers for MPI. This means that users do not need to manually specify these files and linking options when compiling their programs. The table also includes a blank row for hipcc, indicating that this guide can be adapted for using the HIP programming model with the crusher. \n\n| Implementation | Module | Compiler | Header Files & Linking |\n|----------------|---------|----------|------------------------|\n| Cray MPICH     | cray-mpich | cc, CC, ftn (Cray compiler wrappers) | MPI header files and linking is built into the Cray compiler wrappers |\n| hipcc          |         |          |                        |\n\n\n\nGPU-Aware MPI\n\nTo use GPU-aware Cray MPICH, users must set the following modules and environment variables:\n\nmodule load craype-accel-amd-gfx90a\nmodule load rocm\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nThere are extra steps needed to enable GPU-aware MPI on Crusher, which depend on the compiler that is used (see 1. and 2. below).\n\n1. Compiling with the Cray compiler wrappers, cc or CC\n\nTo use GPU-aware Cray MPICH with the Cray compiler wrappers, the following environment variables must be set before compiling. These variables are automatically set by the cray-mpich modulefile:\n\n## These must be set before compiling so the executable picks up GTL\nPE_MPICH_GTL_DIR_amd_gfx90a=\"-L${CRAY_MPICH_ROOTDIR}/gtl/lib\"\nPE_MPICH_GTL_LIBS_amd_gfx90a=\"-lmpi_gtl_hsa\"\n\nIn addition, the following header files and libraries must be included:\n\n-I${ROCM_PATH}/include\n-L${ROCM_PATH}/lib -lamdhip64\n\nwhere the include path implies that #include <hip/hip_runtime.h> is included in the source file.\n\n2. Compiling with hipcc\n\nTo use GPU-aware Cray MPICH with hipcc, users must include appropriate headers, libraries, and flags:\n\n-I${MPICH_DIR}/include\n-L${MPICH_DIR}/lib -lmpi \\\n  ${CRAY_XPMEM_POST_LINK_OPTS} -lxpmem \\\n  ${PE_MPICH_GTL_DIR_amd_gfx90a} ${PE_MPICH_GTL_LIBS_amd_gfx90a}\n\nHIPFLAGS = --amdgpu-target=gfx90a\n\nDetermining the Compatibility of Cray MPICH and ROCm\n\nReleases of cray-mpich are each built with a specific version of ROCm, and compatibility across multiple versions is not guaranteed. OLCF will maintain compatible default modules when possible. If using non-default modules, you can determine compatibility by reviewing the Product and OS Dependencies section in the cray-mpich release notes. This can be displayed by running module show cray-mpich/<version>. If the notes indicate compatibility with AMD ROCM X.Y or later, only use rocm/X.Y.Z modules. If using a non-default version of cray-mpich, you must add ${CRAY_MPICH_ROOTDIR}/gtl/lib to either your LD_LIBRARY_PATH at run time or your executable's rpath at build time.\n\nThe compatibility table below was determined by linker testing with all current combinations of cray-mpich and rocm modules on Crusher.\n\n\nThe following table provides a comprehensive overview of the compatibility between different versions of the cray-mpich and ROCm software. The first column lists the versions of cray-mpich, while the second column lists the corresponding versions of ROCm that are compatible with each version of cray-mpich. The table shows that versions 8.1.12, 8.1.14, and 8.1.15 of cray-mpich are compatible with versions 4.5.2 and 4.5.0 of ROCm. Similarly, versions 8.1.16, 8.1.17, 8.1.18, 8.1.19, 8.1.21, and 8.1.23 of cray-mpich are compatible with versions 5.4.0, 5.3.0, 5.2.0, 5.1.0, 5.0.2, and 5.0.0 of ROCm. This information is important for users who are looking to use both cray-mpich and ROCm in their projects, as it ensures that they are using compatible versions of both software. \n\n| cray-mpich | ROCm |\n|------------|------|\n| 8.1.12     | 4.5.2, 4.5.0 |\n| 8.1.14     | 4.5.2, 4.5.0 |\n| 8.1.15     | 5.4.0, 5.3.0, 5.2.0, 5.1.0, 5.0.2, 5.0.0 |\n| 8.1.16     | 5.4.0, 5.3.0, 5.2.0, 5.1.0, 5.0.2, 5.0.0 |\n| 8.1.17     | 5.4.0, 5.3.0, 5.2.0, 5.1.0, 5.0.2, 5.0.0 |\n| 8.1.18     | 5.4.0, 5.3.0, 5.2.0, 5.1.0, 5.0.2, 5.0.0 |\n| 8.1.19     | 5.4.0, 5.3.0, 5.2.0, 5.1.0, 5.0.2, 5.0.0 |\n| 8.1.21     | 5.4.0, 5.3.0, 5.2.0, 5.1.0, 5.0.2, 5.0.0 |\n| 8.1.23     | 5.4.0, 5.3.0, 5.2.0, 5.1.0, 5.0.2, 5.0.0 |\n\n\n\nOpenMP\n\nThis section shows how to compile with OpenMP using the different compilers covered above.\n\n\nThe table above provides a comprehensive overview of the different modules and compilers available for the Crusher quick start guide. The first column lists the vendors, which include Cray, AMD, and GCC. The second column specifies the module used for each vendor, such as cce for Cray and ftn for Fortran (which wraps crayftn). The third column indicates the language supported by each module, with C and C++ for Cray and Fortran for AMD. The fourth column lists the compiler used for each vendor, with no specific compiler mentioned for Cray and AMD, and GCC for GCC. The last column specifies the OpenMP flag for CPU thread, with -fopenmp being the flag used for all vendors. This table serves as a useful reference for those looking to use the Crusher quick start guide, as it outlines the different options available for modules, languages, compilers, and OpenMP flags. \n\n| Vendor | Module | Language | Compiler | OpenMP flag (CPU thread) |\n|--------|--------|----------|----------|-------------------------|\n| Cray   | cce    | C, C++   |          | -fopenmp                |\n| Fortran| ftn    | Fortran  | crayftn  |                         |\n| AMD    | rocm   |          |          | -fopenmp                |\n| GCC    | gcc    |          |          | -fopenmp                |\n\n\n\nOpenMP GPU Offload\n\nThis section shows how to compile with OpenMP Offload using the different compilers covered above.\n\nMake sure the craype-accel-amd-gfx90a module is loaded when using OpenMP offload.\n\n\nThe table above provides a comprehensive overview of the Crusher quick start guide, specifically focusing on the different vendors, modules, languages, compilers, and OpenMP flags (GPU) used in the guide. The first row lists the vendor as Cray, with the module being cce and the language being C. The compiler used is not specified, but the OpenMP flag for GPU is included. The second row lists the vendor as Cray again, but this time the module is not specified and the language is C++. The compiler used is also not specified, but the OpenMP flag for GPU is included as \"-fopenmp\". The third row lists the vendor as Cray once more, with the module being ftn (which wraps crayftn) and the language being Fortran. The compiler used is not specified, but the OpenMP flag for GPU is not included. The final row lists the vendor as AMD, with no module or language specified. The compiler used is also not specified, but the OpenMP flag for GPU is included as \"-fopenmp\". This table provides a clear breakdown of the different components involved in the Crusher quick start guide, making it a useful reference for those looking to utilize this guide. \n\n| Vendor | Module | Language | Compiler | OpenMP flag (GPU) |\n|--------|--------|----------|----------|-------------------|\n| Cray   | cce    | C        |          |                   |\n| Cray   |        | C++      |          | -fopenmp          |\n| Cray   | ftn    | Fortran  |          |                   |\n| AMD    | rocm   |          |          | -fopenmp          |\n\n\n\nIf invoking amdclang, amdclang++, or amdflang directly, or using hipcc you will need to add:\n-fopenmp -target x86_64-pc-linux-gnu -fopenmp-targets=amdgcn-amd-amdhsa -Xopenmp-target=amdgcn-amd-amdhsa -march=gfx90a.\n\nHIP\n\nThis section shows how to compile HIP codes using the Cray compiler wrappers and hipcc compiler driver.\n\nMake sure the craype-accel-amd-gfx90a module is loaded when compiling HIP with the Cray compiler wrappers.\n\n\nThe table titled \"Compiler Quick Start Guide\" provides a comprehensive overview of the necessary components for compiling and linking with the hipcc compiler. It includes information on the required compile/link flags, header files, and libraries that are needed for successful compilation. The table is organized into three columns, with the first column listing the compiler name as \"hipcc\". The second column provides a description of the specific components needed, such as compile/link flags, header files, and libraries. The third column lists the specific names or paths of these components. This table serves as a helpful reference for those using the hipcc compiler, providing a clear and concise breakdown of the necessary elements for successful compilation. \n\n| Compiler | Compile/Link Flags, Header Files, and Libraries |\n| --- | --- |\n| hipcc | -std=c++11 -lhipcl -lOpenCL -I/usr/local/include -L/usr/local/lib |\n\n\n\nHIP + OpenMP CPU Threading\n\nThis section describes how to compile HIP + OpenMP CPU threading hybrid codes.\nFor all compiler toolchains, HIP kernels and kernel launch calls (ie hipLaunchKernelGGL) cannot be implemented in the same file that requires -fopenmp.\nHIP API calls (hipMalloc, hipMemcpy) are allowed in files that require -fopenmp.\nHIP source files should be compiled into object files using the instructions in the HIP section, with the -c flag added to generate an object file.\nOpenMP and other non-HIP source files should be compiled into object files using the instructions in the OpenMP section.\nThen these object files should be linked using the following link flags: -fopenmp -L${ROCM_PATH}/lib -lamdhip64.\n\nSYCL\n\nThis section shows how to compile SYCL codes using the DPC++ compiler.\n\nMake sure the ums ums015 dpcpp module is loaded when compiling SYCL with clang or clang++.\n\n\nThe table provides a comprehensive overview of the necessary components and steps for compiling and linking a crusher program. It includes a list of compiler options and flags, such as optimization levels and debugging symbols, that can be used to customize the compilation process. Additionally, the table lists the required header files and libraries that need to be included in the program for it to function properly. This includes standard libraries as well as any specific libraries needed for crusher. The table serves as a quick start guide for developers looking to get started with crusher, providing all the essential information in a concise and organized format. \n\n| Compiler | Compile/Link Flags, Header Files, and Libraries |\n| --- | --- |\n| Optimization Levels | -O0, -O1, -O2, -O3 |\n| Debugging Symbols | -g, -ggdb, -g3 |\n| Header Files | stdio.h, stdlib.h, string.h, crusher.h |\n| Libraries | libc, libcrusher, libm |\n\n\n\nThese compilers are built weekly from the latest open-source rather than releases. As such, these compilers will get new features and updates quickly but may break on occasion. If you experience regressions, please load an older version of the module rather than the latest.\n\n\n\nRunning Jobs\n\nThis section describes how to run programs on the Crusher compute nodes, including a brief overview of Slurm and also how to map processes and threads to CPU cores and GPUs.\n\nSlurm Workload Manager\n\nSlurm is the workload manager used to interact with the compute nodes on Crusher. In the following subsections, the most commonly used Slurm commands for submitting, running, and monitoring jobs will be covered, but users are encouraged to visit the official documentation and man pages for more information.\n\nBatch Scheduler and Job Launcher\n\nSlurm provides 3 ways of submitting and launching jobs on Crusher's compute nodes: batch scripts, interactive, and single-command. The Slurm commands associated with these methods are shown in the table below and examples of their use can be found in the related subsections. Please note that regardless of the submission method used, the job will launch on compute nodes, with the first compute in the allocation serving as head-node.\n\n\nThe table below is a quick start guide for using a crusher, which is a type of computer program used for data processing. The first column lists three commands that are commonly used when working with a crusher: sbatch, salloc, and srun. Sbatch is used to submit a batch job to the system, while salloc is used to allocate resources for a job. Srun is used to launch a job and execute commands on the allocated resources. The second column provides a brief description of each command, explaining its purpose and function. This table serves as a useful reference for those new to using a crusher, providing a clear and concise overview of the essential commands needed to get started. \n\n| Command | Description |\n|---------|-------------|\n| sbatch  | Submits a batch job to the system |\n| salloc  | Allocates resources for a job |\n| srun    | Launches a job and executes commands on allocated resources |\n\n\n\nBatch Scripts\n\nA batch script can be used to submit a job to run on the compute nodes at a later time. In this case, stdout and stderr will be written to a file(s) that can be opened after the job completes. Here is an example of a simple batch script:\n\n#!/bin/bash\n#SBATCH -A <project_id>\n#SBATCH -J <job_name>\n#SBATCH -o %x-%j.out\n#SBATCH -t 00:05:00\n#SBATCH -p <partition>\n#SBATCH -N 2\n\nsrun -n4 --ntasks-per-node=2 ./a.out\n\nThe Slurm submission options are preceded by #SBATCH, making them appear as comments to a shell (since comments begin with #). Slurm will look for submission options from the first line through the first non-comment line. Options encountered after the first non-comment line will not be read by Slurm. In the example script, the lines are:\n\n\nThe table provides a quick start guide for using a crusher, with detailed instructions on how to set up and run a parallel job. The first line is an optional shell interpreter line, followed by the OLCF project to charge for the job. The user is then prompted to enter a job name and specify the desired stdout file name, with the option to use placeholders for the job name and id. The next line requires the user to input the requested walltime in HH:MM:SS format. The batch queue and number of compute nodes can also be specified. A blank line is included for clarity before the final line, which provides the srun command to launch the parallel job with 4 processes, 2 per node. This table serves as a useful reference for users to quickly and efficiently set up and run their jobs on a crusher.\n\n| Line | Description |\n|------|-------------|\n| 1 | [Optional] shell interpreter line |\n| 2 | OLCF project to charge |\n| 3 | Job name |\n| 4 | stdout file name ( %x represents job name, %j represents job id) |\n| 5 | Walltime requested (HH:MM:SS) |\n| 6 | Batch queue |\n| 7 | Number of compute nodes requested |\n| 8 | Blank line |\n| 9 | srun command to launch parallel job (requesting 4 processes - 2 per node) |\n\n\n\nInteractive Jobs\n\nTo request an interactive job where multiple job steps (i.e., multiple srun commands) can be launched on the allocated compute node(s), the salloc command can be used:\n\n$ salloc -A <project_id> -J <job_name> -t 00:05:00 -p <partition> -N 2\nsalloc: Granted job allocation 4258\nsalloc: Waiting for resource configuration\nsalloc: Nodes crusher[010-011] are ready for job\n\n$ srun -n 4 --ntasks-per-node=2 ./a.out\n<output printed to terminal>\n\n$ srun -n 2 --ntasks-per-node=1 ./a.out\n<output printed to terminal>\n\nHere, salloc is used to request an allocation of 2 compute nodes for 5 minutes. Once the resources become available, the user is granted access to the compute nodes (crusher010 and crusher011 in this case) and can launch job steps on them using srun.\n\n\n\nSingle Command (non-interactive)\n\n$ srun -A <project_id> -t 00:05:00 -p <partition> -N 2 -n 4 --ntasks-per-node=2 ./a.out\n<output printed to terminal>\n\nThe job name and output options have been removed since stdout/stderr are typically desired in the terminal window in this usage mode.\n\nCommon Slurm Submission Options\n\nThe table below summarizes commonly-used Slurm job submission options:\n\n\nThe table presented is a quick start guide for using a crusher, which is a type of machine used for crushing materials. The guide provides a list of parameters and their corresponding descriptions that can be used when operating the crusher. The first parameter, <project_id>, is used to specify the project ID that will be charged for the use of the crusher. The next parameter, <job_name>, allows the user to assign a name to the job being run on the crusher. The <partition> parameter is used to select the batch queue or partition on which the job will be executed. The <time> parameter specifies the maximum amount of wall clock time in hours, minutes, and seconds that the job is allowed to run for. The <number_of_nodes> parameter determines the number of compute nodes that will be used for the job. The <file_name> parameters, <o> and <e>, are used to specify the names of the standard output and error files, respectively. Finally, the <threads-per-core> parameter allows the user to specify the number of active hardware threads per core, with the default being 1 and the maximum being 2. This table serves as a useful reference for users who are new to operating a crusher and need guidance on the various parameters that can be used. \n\n| Parameter | Description |\n|-----------|-------------|\n| -A <project_id> | Project ID to charge |\n| -J <job_name> | Name of job |\n| -p <partition> | Partition / batch queue |\n| -t <time> | Wall clock time <HH:MM:SS> |\n| -N <number_of_nodes> | Number of compute nodes |\n| -o <file_name> | Standard output file name |\n| -e <file_name> | Standard error file name |\n| --threads-per-core=<threads> | Number of active hardware threads per core [1 (default) or 2] |\n\n\n\nFor more information about these and/or other options, please see the sbatch man page.\n\nOther Common Slurm Commands\n\nThe table below summarizes commonly-used Slurm commands:\n\n\nThe table provides a quick start guide for using a crusher, which is a type of machine used for crushing materials. The first column, \"sinfo,\" refers to a command that displays information about the nodes and partitions on the cluster. The second column, \"squeue,\" displays the current job queue on the cluster, including the job ID, user, and status. The third column, \"sacct,\" displays accounting information for completed jobs, such as start and end times, CPU usage, and memory usage. The fourth column, \"scancel,\" allows users to cancel a job that is currently running or in the queue. The last column, \"scontrol,\" provides a variety of control options for managing jobs, nodes, and partitions on the cluster. This table serves as a helpful reference for users who are new to using a crusher and need to quickly access important information and commands. \n\n| Command | Description |\n|---------|-------------|\n| sinfo   | Displays information about nodes and partitions on the cluster |\n| squeue  | Displays the current job queue on the cluster |\n| sacct   | Displays accounting information for completed jobs |\n| scancel | Allows users to cancel a job |\n| scontrol| Provides control options for managing jobs, nodes, and partitions |\n\n\n\n\n\nSlurm Compute Node Partitions\n\nCrusher's compute nodes are contained within a single Slurm partition (queue) for both CAAR and ECP projects. Please see the table below for details.\n\nPartition\n\nThe CAAR and ECP \"batch\" partition consists of 192 total compute nodes. On a per-project basis, each user can have 2 running and 2 eligible jobs at a time, with up to 20 jobs submitted.\n\n\nThe table provides a quick start guide for using a crusher, specifically outlining the maximum walltime allowed for different numbers of nodes. The first column lists the range of nodes, starting from 1 and increasing in increments of 8 until reaching 160. The second column specifies the maximum walltime allowed for each range of nodes, with the first range of 1-8 nodes having a maximum walltime of 8 hours. As the number of nodes increases, the maximum walltime decreases, with 9-64 nodes having a maximum walltime of 4 hours and 65-160 nodes having a maximum walltime of 2 hours. This table serves as a helpful reference for users to ensure they are utilizing the appropriate amount of nodes and walltime for their crusher tasks. \n\n| Number of Nodes | Max Walltime |\n|-----------------|--------------|\n| 1 - 8           | 8 hours      |\n| 9 - 64          | 4 hours      |\n| 65 - 160        | 2 hours      |\n\n\n\nIf CAAR or ECP teams require a temporary exception to this policy, please email help@olcf.ornl.gov with your request and it will be given to the OLCF Resource Utilization Council (RUC) for review.\n\nProcess and Thread Mapping\n\nThis section describes how to map processes (e.g., MPI ranks) and process threads (e.g., OpenMP threads) to the CPUs and GPUs on Crusher. The crusher-compute-nodes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#crusher-compute-nodes> diagram will be helpful when reading this section to understand which physical CPU cores (and hardware threads) your processes and threads run on.\n\nUsers are highly encouraged to use the CPU- and GPU-mapping programs used in the following sections to check their understanding of the job steps (i.e., srun commands) the intend to use in their actual jobs.\n\nCPU Mapping\n\nIn this sub-section, a simple MPI+OpenMP \"Hello, World\" program (hello_mpi_omp) will be used to clarify the mappings. Slurm's interactive <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#interactive> method was used to request an allocation of 1 compute node for these examples: salloc -A <project_id> -t 30 -p <parition> -N 1\n\nThe srun options used in this section are (see man srun for more information):\n\n\nThe table provided is a quick start guide for using a crusher, which is a type of machine used for crushing materials. The first column lists the different options available for setting the number of CPUs to be used, with the command line option \"-c\" and the argument \"--cpus-per-task=<ncpus>\". The second column specifies the option for setting the number of threads per core, with the argument \"--threads-per-core=<threads>\". The third column provides the option for binding the CPUs to specific threads, with the argument \"--cpu-bind=threads\". These options allow for customization and optimization of the crusher's performance based on the available hardware resources. By using these options, users can efficiently utilize the crusher for their specific needs. \n\n| Option | Argument |\n|--------|----------|\n| -c     | --cpus-per-task=<ncpus> |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n| --threads-per-core=<threads> | |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n|        |          |\n| --cpu-bind=threads | |\n\n\n\nIn the srun man page (and so the table above), threads refers to hardware threads.\n\n2 MPI ranks - each with 2 OpenMP threads\n\nIn this example, the intent is to launch 2 MPI ranks, each of which spawn 2 OpenMP threads, and have all of the 4 OpenMP threads run on different physical CPU cores.\n\nFirst (INCORRECT) attempt\n\nTo set the number of OpenMP threads spawned per MPI rank, the OMP_NUM_THREADS environment variable can be used. To set the number of MPI ranks launched, the srun flag -n can be used.\n\n$ export OMP_NUM_THREADS=2\n$ srun -N1 -n2 ./hello_mpi_omp | sort\n\nWARNING: Requested total thread count and/or thread affinity may result in\noversubscription of available CPU resources!  Performance may be degraded.\nExplicitly set OMP_WAIT_POLICY=PASSIVE or ACTIVE to suppress this message.\nSet CRAY_OMP_CHECK_AFFINITY=TRUE to print detailed thread-affinity messages.\nWARNING: Requested total thread count and/or thread affinity may result in\noversubscription of available CPU resources!  Performance may be degraded.\nExplicitly set OMP_WAIT_POLICY=PASSIVE or ACTIVE to suppress this message.\nSet CRAY_OMP_CHECK_AFFINITY=TRUE to print detailed thread-affinity messages.\n\nMPI 000 - OMP 000 - HWT 001 - Node crusher001\nMPI 000 - OMP 001 - HWT 001 - Node crusher001\nMPI 001 - OMP 000 - HWT 009 - Node crusher001\nMPI 001 - OMP 001 - HWT 009 - Node crusher001\n\nThe first thing to notice here is the WARNING about oversubscribing the available CPU cores. Also, the output shows each MPI rank did spawn 2 OpenMP threads, but both OpenMP threads ran on the same hardware thread (for a given MPI rank). This was not the intended behavior; each OpenMP thread was meant to run on its own physical CPU core.\n\nThe problem here arises from two default settings; 1) each MPI rank is only allocated 1 physical CPU core (-c 1) and, 2) only 1 hardware thread per physical CPU core is enabled (--threads-per-core=1). So in this case, each MPI rank only has 1 physical core (with 1 hardware thread) to run on - including any threads the process spawns - hence the WARNING and undesired behavior.\n\nSecond (CORRECT) attempt\n\nIn order for each OpenMP thread to run on its own physical CPU core, each MPI rank should be given 2 physical CPU cores (-c 2). Now the OpenMP threads will be mapped to unique hardware threads on separate physical CPU cores.\n\n$ export OMP_NUM_THREADS=2\n$ srun -N1 -n2 -c2 ./hello_mpi_omp | sort\n\nMPI 000 - OMP 000 - HWT 001 - Node crusher001\nMPI 000 - OMP 001 - HWT 002 - Node crusher001\nMPI 001 - OMP 000 - HWT 009 - Node crusher001\nMPI 001 - OMP 001 - HWT 010 - Node crusher001\n\nNow the output shows that each OpenMP thread ran on (one of the hardware threads of) its own physical CPU core. More specifically (see the Crusher Compute Node diagram), OpenMP thread 000 of MPI rank 000 ran on hardware thread 001 (i.e., physical CPU core 01), OpenMP thread 001 of MPI rank 000 ran on hardware thread 002 (i.e., physical CPU core 02), OpenMP thread 000 of MPI rank 001 ran on hardware thread 009 (i.e., physical CPU core 09), and OpenMP thread 001 of MPI rank 001 ran on hardware thread 010 (i.e., physical CPU core 10) - as intended.\n\nThird attempt - Using multiple threads per core\n\nTo use both available hardware threads per core, the job must be allocated with --threads-per-core=2 (as opposed to only the job step - i.e., srun command). That value will then be inherited by srun unless explcitly overridden with --threads-per-core=1.\n\n$ salloc -N1 -A <project_id> -t <time> -p <partition> --threads-per-core=2\n\n$ export OMP_NUM_THREADS=2\n$ srun -N1 -n2 -c2 ./hello_mpi_omp | sort\n\nMPI 000 - OMP 000 - HWT 001 - Node crusher001\nMPI 000 - OMP 001 - HWT 065 - Node crusher001\nMPI 001 - OMP 000 - HWT 009 - Node crusher001\nMPI 001 - OMP 001 - HWT 073 - Node crusher001\n\nComparing this output to the Crusher Compute Node diagram, we see that each pair of OpenMP threads is contained within a single physical core. MPI rank 000 ran on hardware threads 001 and 065 (i.e. physical CPU core 01) and MPI rank 001 ran on hardware threads 009 and 073 (i.e. physical CPU core 09).\n\nThere are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.\n\nGPU Mapping\n\nIn this sub-section, an MPI+OpenMP+HIP \"Hello, World\" program (hello_jobstep) will be used to clarify the GPU mappings. Again, Slurm's interactive <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#interactive> method was used to request an allocation of 2 compute nodes for these examples: salloc -A <project_id> -t 30 -p <parition> -N 2. The CPU mapping part of this example is very similar to the example used above in the CPU Mapping sub-section, so the focus here will be on the GPU mapping part.\n\nThe following srun options will be used in the examples below. See man srun for a complete list of options and more information.\n\n\nThe table presents a quick start guide for using the Crusher, a high-performance computing system. It provides a detailed description of the various options and parameters that can be specified when running a job on the system. The first two options, --gpus and --gpus-per-node, allow the user to specify the number of GPUs required for the job and the number of GPUs per node, respectively. The next option, --gpu-bind=closest, binds each task to a GPU on the same NUMA domain as the CPU core the MPI rank is running on. This helps optimize performance by reducing data transfer between the CPU and GPU. The --gpu-bind=map_gpu:<list> option allows the user to specify which specific GPUs to bind tasks to, using a list of GPU IDs. If the number of tasks exceeds the number of elements in the list, the list will be reused starting from the beginning. The --ntasks-per-gpu option allows the user to request a specific number of tasks to be invoked for each GPU. Finally, the --distribution option specifies the distribution of MPI ranks across compute nodes, sockets, and cores. The default values are block:cyclic:cyclic, meaning that ranks will be distributed in a block pattern across nodes, in a cyclic pattern across sockets, and in a cyclic pattern across cores. This table serves as a useful reference for users of the Crusher system, providing them with the necessary information to optimize their job performance.\n\n| Option | Description |\n| --- | --- |\n| --gpus | Specify the number of GPUs required for the job (total GPUs across all nodes). |\n| --gpus-per-node | Specify the number of GPUs per node required for the job. |\n| --gpu-bind=closest | Binds each task to the GPU which is on the same NUMA domain as the CPU core the MPI rank is running on. |\n| --gpu-bind=map_gpu:<list> | Bind tasks to specific GPUs by setting GPU masks on tasks (or ranks) as specified where <list> is <gpu_id_for_task_0>,<gpu_id_for_task_1>,.... If the number of tasks (or ranks) exceeds the number of elements in this list, elements in the list will be reused as needed starting from the beginning of the list. To simplify support for large task counts, the lists may follow a map with an asterisk and repetition count. (For example map_gpu:0*4,1*4) |\n| --ntasks-per-gpu=<ntasks> | Request that there are ntasks tasks invoked for every GPU. |\n| --distribution=<value>[:<value>][:<value>] | Specifies the distribution of MPI ranks across compute nodes, sockets (L3 regions on Crusher), and cores, respectively. The default values are block:cyclic:cyclic. |\n\n\n\nDue to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.\n\nIn general, GPU mapping can be accomplished in different ways. For example, an application might map MPI ranks to GPUs programmatically within the code using, say, hipSetDevice. In this case, since all GPUs on a node are available to all MPI ranks on that node by default, there might not be a need to map to GPUs using Slurm (just do it in the code). However, in another application, there might be a reason to make only a subset of GPUs available to the MPI ranks on a node. It is this latter case that the following examples refer to.\n\nMapping 1 task per GPU\n\nIn the following examples, each MPI rank (and its OpenMP threads) will be mapped to a single GPU.\n\nExample 0: 1 MPI rank with 1 OpenMP thread and 1 GPU (single-node)\n\nSomewhat counterintuitively, this common test case is currently among the most difficult. Slurm ignores GPU bindings for nodes with only a single task, so we do not use --gpu-bind here. We must allocate only a single GPU to ensure that only one GPU is available to the task, and since we get the first GPU available we should bind the task to the CPU closest to the allocated GPU.\n\n$ export OMP_NUM_THREADS=1\n$ srun -N1 -n1 -c1 --cpu-bind=map_cpu:49 --gpus=1 ./hello_jobstep\n\nMPI 000 - OMP 000 - HWT 049 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\n\nExample 1: 8 MPI ranks - each with 2 OpenMP threads and 1 GPU (single-node)\n\nThis example launches 8 MPI ranks (-n8), each with 2 physical CPU cores (-c2) to launch 2 OpenMP threads (OMP_NUM_THREADS=2) on. In addition, each MPI rank (and its 2 OpenMP threads) should have access to only 1 GPU. To accomplish the GPU mapping, two new srun options will be used:\n\n--gpus-per-node specifies the number of GPUs required for the job\n\n--gpu-bind=closest binds each task to the GPU which is closest.\n\nWithout these additional flags, all MPI ranks would have access to all GPUs (which is the default behavior).\n\n$ export OMP_NUM_THREADS=2\n$ srun -N1 -n8 -c2 --gpus-per-node=8 --gpu-bind=closest ./hello_jobstep | sort\n\nMPI 000 - OMP 000 - HWT 001 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 001 - HWT 002 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 009 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 001 - HWT 010 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 002 - OMP 000 - HWT 017 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 001 - HWT 018 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 003 - OMP 000 - HWT 025 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 001 - HWT 026 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 004 - OMP 000 - HWT 033 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 001 - HWT 034 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 005 - OMP 000 - HWT 041 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 001 - HWT 042 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 006 - OMP 000 - HWT 049 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 001 - HWT 050 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 007 - OMP 000 - HWT 057 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 001 - HWT 058 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nThe output from the program contains a lot of information, so let's unpack it. First, there are different IDs associated with the GPUs so it is important to describe them before moving on. GPU_ID is the node-level (or global) GPU ID, which is labeled as one might expect from looking at the Crusher Node Diagram: 0, 1, 2, 3, 4, 5, 6, 7. RT_GPU_ID is the HIP runtime GPU ID, which can be though of as each MPI rank's local GPU ID number (with zero-based indexing). So in the output above, each MPI rank has access to only 1 unique GPU - where MPI 000 has access to \"global\" GPU 4, MPI 001 has access to \"global\" GPU 5, etc., but all MPI ranks show a HIP runtime GPU ID of 0. The reason is that each MPI rank only \"sees\" one GPU and so the HIP runtime labels it as \"0\", even though it might be global GPU ID 0, 1, 2, 3, 4, 5, 6, or 7. The GPU's bus ID is included to definitively show that different GPUs are being used.\n\nHere is a summary of the different GPU IDs reported by the example program:\n\nGPU_ID is the node-level (or global) GPU ID read from ROCR_VISIBLE_DEVICES. If this environment variable is not set (either by the user or by Slurm), the value of GPU_ID will be set to N/A by this program.\n\nRT_GPU_ID is the HIP runtime GPU ID (as reported from, say hipGetDevice).\n\nBus_ID is the physical bus ID associated with the GPUs. Comparing the bus IDs is meant to definitively show that different GPUs are being used.\n\nSo the job step (i.e., srun command) used above gave the desired output. Each MPI rank spawned 2 OpenMP threads and had access to a unique GPU. The --gpus-per-node=8 allocated 8 GPUs for node and the --gpu-bind=closest ensured that the closest GPU to each rank was the one used.\n\nThis example shows an important peculiarity of the Crusher nodes; the \"closest\" GPUs to each MPI rank are not in sequential order. For example, MPI rank 000 and its two OpenMP threads ran on hardware threads 000 and 001. As can be seen in the Crusher node diagram, these two hardware threads reside in the same L3 cache region, and that L3 region is connected via Infinity Fabric (blue line in the diagram) to GPU 4. This is an important distinction that can affect performance if not considered carefully.\n\nExample 2: 16 MPI ranks - each with 2 OpenMP threads and 1 GPU (multi-node)\n\nThis example will extend Example 1 to run on 2 nodes. As the output shows, it is a very straightforward exercise of changing the number of nodes to 2 (-N2) and the number of MPI ranks to 16 (-n16).\n\n$ export OMP_NUM_THREADS=2\n$ srun -N2 -n16 -c2 --gpus-per-node=8 --gpu-bind=closest ./hello_jobstep | sort\n\nMPI 000 - OMP 000 - HWT 001 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 001 - HWT 002 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 009 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 001 - HWT 010 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 002 - OMP 000 - HWT 017 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 001 - HWT 018 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 003 - OMP 000 - HWT 025 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 001 - HWT 026 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 004 - OMP 000 - HWT 033 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 001 - HWT 034 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 005 - OMP 000 - HWT 041 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 001 - HWT 042 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 006 - OMP 000 - HWT 049 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 001 - HWT 050 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 007 - OMP 000 - HWT 057 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 001 - HWT 058 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 008 - OMP 000 - HWT 001 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 008 - OMP 001 - HWT 002 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 009 - OMP 000 - HWT 009 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 009 - OMP 001 - HWT 010 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 010 - OMP 000 - HWT 017 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 010 - OMP 001 - HWT 018 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 011 - OMP 000 - HWT 025 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 011 - OMP 001 - HWT 026 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 012 - OMP 000 - HWT 033 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 012 - OMP 001 - HWT 034 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 013 - OMP 000 - HWT 041 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 013 - OMP 001 - HWT 042 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 014 - OMP 000 - HWT 049 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 001 - HWT 050 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 015 - OMP 000 - HWT 057 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 001 - HWT 058 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nExample 3: 8 MPI ranks - each with 2 OpenMP threads and 1 *specific* GPU (single-node)\n\nThis example will be very similar to Example 1, but instead of using --gpu-bind=closest to map each MPI rank to the closest GPU, --gpu-bind=map_gpu will be used to map each MPI rank to a specific GPU. The map_gpu option takes a comma-separated list of GPU IDs to specify how the MPI ranks are mapped to GPUs, where the form of the comma-separated list is <gpu_id_for_task_0>, <gpu_id_for_task_1>,....\n\n$ export OMP_NUM_THREADS=2\n$ srun -N1 -n8 -c2 --gpus-per-node=8 --gpu-bind=map_gpu:4,5,2,3,6,7,0,1 ./hello_jobstep | sort\n\nMPI 000 - OMP 000 - HWT 001 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 001 - HWT 002 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 009 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 001 - HWT 010 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 002 - OMP 000 - HWT 017 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 001 - HWT 018 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 003 - OMP 000 - HWT 025 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 001 - HWT 026 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 004 - OMP 000 - HWT 033 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 001 - HWT 034 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 005 - OMP 000 - HWT 041 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 001 - HWT 042 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 006 - OMP 000 - HWT 049 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 001 - HWT 050 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 007 - OMP 000 - HWT 057 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 001 - HWT 058 - Node crusher001 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nHere, the output is the same as the results from Example 1. This is because the 8 GPU IDs in the comma-separated list happen to specify the GPUs within the same L3 cache region that the MPI ranks are in. So MPI 000 is mapped to GPU 4, MPI 001 is mapped to GPU 5, etc.\n\nWhile this level of control over mapping MPI ranks to GPUs might be useful for some applications, it is always important to consider the implication of the mapping. For example, if the order of the GPU IDs in the map_gpu option is reversed, the MPI ranks and the GPUs they are mapped to would be in different L3 cache regions, which could potentially lead to poorer performance.\n\nExample 4: 16 MPI ranks - each with 2 OpenMP threads and 1 *specific* GPU (multi-node)\n\nExtending Examples 2 and 3 to run on 2 nodes is also a straightforward exercise by changing the number of nodes to 2 (-N2) and the number of MPI ranks to 16 (-n16).\n\n$ export OMP_NUM_THREADS=2\n$ srun -N2 -n16 -c2 --gpus-per-node=8 --gpu-bind=map_gpu:4,5,2,3,6,7,0,1 ./hello_jobstep | sort\n\nMPI 000 - OMP 000 - HWT 001 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 001 - HWT 002 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 009 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 001 - HWT 010 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 002 - OMP 000 - HWT 017 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 001 - HWT 018 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 003 - OMP 000 - HWT 025 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 001 - HWT 026 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 004 - OMP 000 - HWT 033 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 001 - HWT 034 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 005 - OMP 000 - HWT 041 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 001 - HWT 042 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 006 - OMP 000 - HWT 049 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 001 - HWT 050 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 007 - OMP 000 - HWT 057 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 001 - HWT 058 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 008 - OMP 000 - HWT 001 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 008 - OMP 001 - HWT 002 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 009 - OMP 000 - HWT 009 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 009 - OMP 001 - HWT 010 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 010 - OMP 000 - HWT 017 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 010 - OMP 001 - HWT 018 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 011 - OMP 000 - HWT 025 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 011 - OMP 001 - HWT 026 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 012 - OMP 000 - HWT 033 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 012 - OMP 001 - HWT 034 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 013 - OMP 000 - HWT 041 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 013 - OMP 001 - HWT 042 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 014 - OMP 000 - HWT 049 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 001 - HWT 050 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 015 - OMP 000 - HWT 057 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 001 - HWT 058 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nMapping multiple MPI ranks to a single GPU\n\nIn the following examples, 2 MPI ranks will be mapped to 1 GPU. For the sake of brevity, OMP_NUM_THREADS will be set to 1, so -c1 will be used unless otherwise specified.\n\nOn AMD's MI250X, multi-process service (MPS) is not needed since multiple MPI ranks per GPU is supported natively.\n\nExample 5: 16 MPI ranks - where 2 ranks share a GPU (round-robin, single-node)\n\nThis example launches 16 MPI ranks (-n16), each with 1 physical CPU core (-c1) to launch 1 OpenMP thread (OMP_NUM_THREADS=1) on. The MPI ranks will be assigned to GPUs in a round-robin fashion so that each of the 8 GPUs on the node are shared by 2 MPI ranks. To accomplish this GPU mapping, a new srun options will be used:\n\n--ntasks-per-gpu specifies the number of MPI ranks that will share access to a GPU.\n\n$ export OMP_NUM_THREADS=1\n$ srun -N1 -n16 -c1 --ntasks-per-gpu=2 --gpu-bind=closest ./hello_jobstep | sort\n\nMPI 000 - OMP 000 - HWT 001 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 009 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 002 - OMP 000 - HWT 017 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 003 - OMP 000 - HWT 025 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 004 - OMP 000 - HWT 033 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 005 - OMP 000 - HWT 041 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 006 - OMP 000 - HWT 049 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 007 - OMP 000 - HWT 057 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 008 - OMP 000 - HWT 002 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 009 - OMP 000 - HWT 010 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 010 - OMP 000 - HWT 018 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 011 - OMP 000 - HWT 026 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 012 - OMP 000 - HWT 034 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 013 - OMP 000 - HWT 042 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 014 - OMP 000 - HWT 050 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 015 - OMP 000 - HWT 058 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nThe output shows the round-robin (cyclic) distribution of MPI ranks to GPUs. In fact, it is a round-robin distribution of MPI ranks to L3 cache regions (the default distribution). The GPU mapping is a consequence of where the MPI ranks are distributed; --gpu-bind=closest simply maps the GPU in an L3 cache region to the MPI ranks in the same L3 region.\n\nExample 6: 32 MPI ranks - where 2 ranks share a GPU (round-robin, multi-node)\n\nThis example is an extension of Example 5 to run on 2 nodes.\n\n$ export OMP_NUM_THREADS=1\n$ srun -N2 -n32 -c1 --ntasks-per-gpu=2 --gpu-bind=closest ./hello_jobstep | sort\n\nMPI 000 - OMP 000 - HWT 001 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 009 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 002 - OMP 000 - HWT 017 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 003 - OMP 000 - HWT 025 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 004 - OMP 000 - HWT 033 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 005 - OMP 000 - HWT 041 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 006 - OMP 000 - HWT 049 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 007 - OMP 000 - HWT 057 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 008 - OMP 000 - HWT 002 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 009 - OMP 000 - HWT 010 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 010 - OMP 000 - HWT 018 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 011 - OMP 000 - HWT 026 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 012 - OMP 000 - HWT 034 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 013 - OMP 000 - HWT 042 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 014 - OMP 000 - HWT 050 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 015 - OMP 000 - HWT 058 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 016 - OMP 000 - HWT 001 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 017 - OMP 000 - HWT 009 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 018 - OMP 000 - HWT 017 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 019 - OMP 000 - HWT 025 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 020 - OMP 000 - HWT 033 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 021 - OMP 000 - HWT 041 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 022 - OMP 000 - HWT 049 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 023 - OMP 000 - HWT 057 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 024 - OMP 000 - HWT 002 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 025 - OMP 000 - HWT 010 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 026 - OMP 000 - HWT 018 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 027 - OMP 000 - HWT 026 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 028 - OMP 000 - HWT 034 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 029 - OMP 000 - HWT 042 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 030 - OMP 000 - HWT 050 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 031 - OMP 000 - HWT 058 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nExample 7: 16 MPI ranks - where 2 ranks share a GPU (packed, single-node)\n\nThis example launches 16 MPI ranks (-n16), each with 4 physical CPU cores (-c4) to launch 1 OpenMP thread (OMP_NUM_THREADS=1) on. Because it is using 4 physical CPU cores per task, core specialization needs to be disabled (-S 0). The MPI ranks will be assigned to GPUs in a packed fashion so that each of the 8 GPUs on the node are shared by 2 MPI ranks. Similar to Example 5, -ntasks-per-gpu=2 will be used, but a new srun flag will be used to change the default round-robin (cyclic) distribution of MPI ranks across NUMA domains:\n\n--distribution=<value>[:<value>][:<value>] specifies the distribution of MPI ranks across compute nodes, sockets (L3 cache regions on Crusher), and cores, respectively. The default values are block:cyclic:cyclic, which is where the cyclic assignment comes from in the previous examples.\n\nIn the job step for this example, --distribution=*:block is used, where * represents the default value of block for the distribution of MPI ranks across compute nodes and the distribution of MPI ranks across L3 cache regions has been changed to block from its default value of cyclic.\n\nBecause the distribution across L3 cache regions has been changed to a \"packed\" (block) configuration, caution must be taken to ensure MPI ranks end up in the L3 cache regions where the GPUs they intend to be mapped to are located. To accomplish this, the number of physical CPU cores assigned to an MPI rank was increased - in this case to 4. Doing so ensures that only 2 MPI ranks can fit into a single L3 cache region. If the value of -c was left at 1, all 8 MPI ranks would be \"packed\" into the first L3 region, where the \"closest\" GPU would be GPU 4 - the only GPU in that L3 region.\n\nNotice that this is not a workaround like in Example 6, but a requirement due to the block distribution of MPI ranks across NUMA domains.\n\n$ salloc -N 1 -S 0 ...\n<job starts>\n$ export OMP_NUM_THREADS=1\n$ srun -N1 -n16 -c3 --ntasks-per-gpu=2 --gpu-bind=closest --distribution=*:block ./hello_jobstep | sort\n\nMPI 000 - OMP 000 - HWT 002 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 006 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 002 - OMP 000 - HWT 010 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 003 - OMP 000 - HWT 014 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 004 - OMP 000 - HWT 016 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 005 - OMP 000 - HWT 021 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 006 - OMP 000 - HWT 027 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 007 - OMP 000 - HWT 031 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 008 - OMP 000 - HWT 034 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 009 - OMP 000 - HWT 037 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 010 - OMP 000 - HWT 042 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 011 - OMP 000 - HWT 045 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 012 - OMP 000 - HWT 049 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 013 - OMP 000 - HWT 053 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 000 - HWT 057 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 000 - HWT 061 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nThe overall effect of using --distribution=*:block and increasing the number of physical CPU cores available to each MPI rank is to place the first two MPI ranks in the first L3 cache region with GPU 4, the next two MPI ranks in the second L3 cache region with GPU 5, and so on.\n\nExample 8: 32 MPI ranks - where 2 ranks share a GPU (packed, multi-node)\n\nThis example is an extension of Example 7 to use 2 compute nodes. Again, because it is using 4 physical CPU cores per task, core specialization needs to be disabled (-S 0). With the appropriate changes put in place in Example 7, it is a straightforward exercise to change to using 2 nodes (-N2) and 32 MPI ranks (-n32).\n\n$ export OMP_NUM_THREADS=1\n$ srun -N2 -n32 -c4 --ntasks-per-gpu=2 --gpu-bind=closest --distribution=*:block ./hello_jobstep | sort\n\nMPI 000 - OMP 000 - HWT 001 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 005 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 002 - OMP 000 - HWT 009 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 003 - OMP 000 - HWT 014 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 004 - OMP 000 - HWT 017 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 005 - OMP 000 - HWT 020 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 006 - OMP 000 - HWT 025 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 007 - OMP 000 - HWT 029 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 008 - OMP 000 - HWT 033 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 009 - OMP 000 - HWT 037 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 010 - OMP 000 - HWT 042 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 011 - OMP 000 - HWT 045 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 012 - OMP 000 - HWT 049 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 013 - OMP 000 - HWT 054 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 000 - HWT 058 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 000 - HWT 063 - Node crusher002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 016 - OMP 000 - HWT 001 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 017 - OMP 000 - HWT 005 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 018 - OMP 000 - HWT 009 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 019 - OMP 000 - HWT 014 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 020 - OMP 000 - HWT 017 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 021 - OMP 000 - HWT 021 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 022 - OMP 000 - HWT 026 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 023 - OMP 000 - HWT 028 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 024 - OMP 000 - HWT 033 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 025 - OMP 000 - HWT 037 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 026 - OMP 000 - HWT 042 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 027 - OMP 000 - HWT 045 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 028 - OMP 000 - HWT 049 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 029 - OMP 000 - HWT 052 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 030 - OMP 000 - HWT 059 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 031 - OMP 000 - HWT 061 - Node crusher004 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nExample 9: 4 independent and simultaneous job steps in a single allocation\n\nThis example shows how to run multiple job steps simultaneously in a single allocation. The example below demonstrates running 4 independent, single rank MPI executions on a single node, however the example could be extrapolated to more complex invocations using the above examples.\n\nSubmission script:\n\n#!/bin/bash\n#SBATCH -A <projid>\n#SBATCH -N 1\n#SBATCH -t 10\n\nsrun -N1 -c1 --gpus-per-task=1 --exact ./hello_jobstep &\nsleep 1\nsrun -N1 -c1 --gpus-per-task=1 --exact ./hello_jobstep &\nsleep 1\nsrun -N1 -c1 --gpus-per-task=1 --exact ./hello_jobstep &\nsleep 1\nsrun -N1 -c1 --gpus-per-task=1 --exact ./hello_jobstep &\nwait\n\nOutput:\n\nMPI 000 - OMP 000 - HWT 017 - Node crusher166 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 000 - OMP 000 - HWT 057 - Node crusher166 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 000 - OMP 000 - HWT 049 - Node crusher166 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 000 - OMP 000 - HWT 025 - Node crusher166 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\n\nThe --exact parameter is important to avoid the error message srun: Job <job id> step creation temporarily disabled, retrying (Requested nodes are busy). The wait command is also critical, or your job script and allocation will immediately end after launching your jobs in the background. The sleep command is currently required to work around a known issue that causes MPICH ERROR. See crusher-known-issues <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#crusher-known-issues> for more information and alternative workarounds. sleep will no longer be needed in a future update.\n\nThis may result in a sub-optimal alignment of CPU and GPU on the node, as shown in the example output. Unfortunately, at the moment there is not a workaround for this, however improvements are possible in future SLURM updates.\n\nMultiple GPUs per MPI rank\n\nAs mentioned previously, all GPUs are accessible by all MPI ranks by default, so it is possible to programatically map any combination of GPUs to MPI ranks. It should be noted however that Cray MPICH does not support GPU-aware MPI for multiple GPUs per rank, so this binding is not suggested.\n\nHowever, there is currently no way to use Slurm to map multiple GPUs to a single MPI rank. If this functionality is needed for an application, please submit a ticket by emailing help@olcf.ornl.gov.\n\nThere are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_jobstep program and test whether or not processes and threads are running where intended.\n\nNVMe Usage\n\nEach Crusher compute node has [2x] 1.92 TB NVMe devices (SSDs) with a peak sequential performance of 5500 MB/s (read) and 2000 MB/s (write). To use the NVMe, users must request access during job allocation using the -C nvme option to sbatch, salloc, or srun. Once the devices have been granted to a job, users can access them at /mnt/bb/<userid>. Users are responsible for moving data to/from the NVMe before/after their jobs. Here is a simple example script:\n\n#!/bin/bash\n#SBATCH -A <projid>\n#SBATCH -J nvme_test\n#SBATCH -o %x-%j.out\n#SBATCH -t 00:05:00\n#SBATCH -p batch\n#SBATCH -N 1\n#SBATCH -C nvme\n\ndate\n\n# Change directory to user scratch space (Orion)\ncd /lustre/orion/<projid>/scratch/<userid>\n\necho \" \"\necho \"*****ORIGINAL FILE*****\"\ncat test.txt\necho \"***********************\"\n\n# Move file from Orion to SSD\nmv test.txt /mnt/bb/<userid>\n\n# Edit file from compute node\nsrun -n1 hostname >> /mnt/bb/<userid>/test.txt\n\n# Move file from SSD back to Orion\nmv /mnt/bb/<userid>/test.txt .\n\necho \" \"\necho \"*****UPDATED FILE******\"\ncat test.txt\necho \"***********************\"\n\nAnd here is the output from the script:\n\n$ cat nvme_test-<jobid>.out\nFri Oct 8 12:28:18 EDT 2021\n\n*****ORIGINAL FILE*****\nThis is my file. There are many like it but this one is mine.\n***********************\n\n*****UPDATED FILE******\nThis is my file. There are many like it but this one is mine.\ncrusher025\n***********************\n\nTips for Launching at Scale\n\nSBCAST your executable and libraries\n\nSlurm contains a utility called sbcast. This program takes a file and broadcasts it to each node's node-local storage (ie, /tmp, NVMe).\nThis is useful for sharing large input files, binaries and shared libraries, while reducing the overhead on shared file systems and overhead at startup.\nThis is highly recommended at scale if you have multiple shared libraries on Lustre/NFS file systems.\n\nSBCASTing a single file\n\nHere is a simple example of a file sbcast from a user's scratch space on Lustre to each node's NVMe drive:\n\n#!/bin/bash\n#SBATCH -A <projid>\n#SBATCH -J sbcast_to_nvme\n#SBATCH -o %x-%j.out\n#SBATCH -t 00:05:00\n#SBATCH -p batch\n#SBATCH -N 2\n#SBATCH -C nvme\n\ndate\n\n# Change directory to user scratch space (Orion)\ncd /lustre/orion/<projid>/scratch/<userid>\n\necho \"This is an example file\" > test.txt\necho\necho \"*****ORIGINAL FILE*****\"\ncat test.txt\necho \"***********************\"\n\n# SBCAST file from Orion to NVMe -- NOTE: ``-C nvme`` is required to use the NVMe drive\nsbcast -pf test.txt /mnt/bb/$USER/test.txt\nif [ ! \"$?\" == \"0\" ]; then\n    # CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,\n    # your application may pick up partially complete shared library files, which would give you confusing errors.\n    echo \"SBCAST failed!\"\n    exit 1\nfi\n\necho\necho \"*****DISPLAYING FILES ON EACH NODE IN THE ALLOCATION*****\"\n# Check to see if file exists\nsrun -N ${SLURM_NNODES} -n ${SLURM_NNODES} --ntasks-per-node=1 bash -c \"echo \\\"\\$(hostname): \\$(ls -lh /mnt/bb/$USER/test.txt)\\\"\"\necho \"*********************************************************\"\n\necho\n# Showing the file on the current node -- this will be the same on all other nodes in the allocation\necho \"*****SBCAST FILE ON CURRENT NODE******\"\ncat /mnt/bb/$USER/test.txt\necho \"**************************************\"\n\nand here is the output from that script:\n\nFri 03 Mar 2023 03:43:30 PM EST\n\n*****ORIGINAL FILE*****\nThis is an example file\n***********************\n\n*****DISPLAYING FILES ON EACH NODE IN THE ALLOCATION*****\ncrusher001: -rw-r--r-- 1 hagertnl hagertnl 24 Mar  3 15:43 /mnt/bb/hagertnl/test.txt\ncrusher002: -rw-r--r-- 1 hagertnl hagertnl 24 Mar  3 15:43 /mnt/bb/hagertnl/test.txt\n*********************************************************\n\n*****SBCAST FILE ON CURRENT NODE******\nThis is an example file\n**************************************\n\nSBCASTing a binary with libraries stored on shared file systems\n\nsbcast also handles binaries and their libraries:\n\n#!/bin/bash\n#SBATCH -A <projid>\n#SBATCH -J sbcast_binary_to_nvme\n#SBATCH -o %x-%j.out\n#SBATCH -t 00:05:00\n#SBATCH -p batch\n#SBATCH -N 2\n#SBATCH -C nvme\n\ndate\n\n# Change directory to user scratch space (Orion)\ncd /lustre/orion/<projid>/scratch/<userid>\n\n# For this example, I use a HIP-enabled LAMMPS binary, with dependencies to MPI, HIP, and HWLOC\nexe=\"lmp\"\n\necho \"*****ldd ./${exe}*****\"\nldd ./${exe}\necho \"*************************\"\n\n# SBCAST executable from Orion to NVMe -- NOTE: ``-C nvme`` is needed in SBATCH headers to use the NVMe drive\n# NOTE: dlopen'd files will NOT be picked up by sbcast\n# SBCAST automatically excludes several directories: /lib,/usr/lib,/lib64,/usr/lib64,/opt\n#   - These directories are node-local and are very fast to read from, so SBCASTing them isn't critical\n#   - see ``$ scontrol show config | grep BcastExclude`` for current list\n#   - OLCF-provided libraries in ``/sw`` are not on the exclusion list. ``/sw`` is an NFS shared file system\n#   - To override, add ``--exclude=NONE`` to arguments\nsbcast --send-libs -pf ${exe} /mnt/bb/$USER/${exe}\nif [ ! \"$?\" == \"0\" ]; then\n    # CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,\n    # your application may pick up partially complete shared library files, which would give you confusing errors.\n    echo \"SBCAST failed!\"\n    exit 1\nfi\n\n# Check to see if file exists\necho \"*****ls -lh /mnt/bb/$USER*****\"\nls -lh /mnt/bb/$USER/\necho \"*****ls -lh /mnt/bb/$USER/${exe}_libs*****\"\nls -lh /mnt/bb/$USER/${exe}_libs\n\n# SBCAST sends all libraries detected by `ld` (minus any excluded), and stores them in the same directory in each node's node-local storage\n# Any libraries opened by `dlopen` are NOT sent, since they are not known by the linker at run-time.\n\n# At minimum: prepend the node-local path to LD_LIBRARY_PATH to pick up the SBCAST libraries\n# It is also recommended that you **remove** any paths that you don't need, like those that contain the libraries that you just SBCAST'd\n# Failure to remove may result in unnecessary calls to stat shared file systems\nexport LD_LIBRARY_PATH=\"/mnt/bb/$USER/${exe}_libs:${LD_LIBRARY_PATH}\"\n\n# If you SBCAST **all** your libraries (ie, `--exclude=NONE`), you may use the following line:\n#export LD_LIBRARY_PATH=\"/mnt/bb/$USER/${exe}_libs:$(pkg-config --variable=libdir libfabric)\"\n# Use with caution -- certain libraries may use ``dlopen`` at runtime, and that is NOT covered by sbcast\n# If you use this option, we recommend you contact OLCF Help Desk for the latest list of additional steps required\n\n# You may notice that some libraries are still linked from /sw/frontier, even after SBCASTing.\n# This is because the Spack-build modules use RPATH to find their dependencies.\necho \"*****ldd /mnt/bb/$USER/${exe}*****\"\nldd /mnt/bb/$USER/${exe}\necho \"*************************************\"\n\nand here is the output from that script:\n\nTue 28 Mar 2023 05:01:41 PM EDT\n*****ldd ./lmp*****\n    linux-vdso.so.1 (0x00007fffeda02000)\n    libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007fffed5bb000)\n    libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fffed398000)\n    libm.so.6 => /lib64/libm.so.6 (0x00007fffed04d000)\n    librt.so.1 => /lib64/librt.so.1 (0x00007fffece44000)\n    libamdhip64.so.4 => /opt/rocm-4.5.2/lib/libamdhip64.so.4 (0x00007fffec052000)\n    libmpi_cray.so.12 => /opt/cray/pe/lib64/libmpi_cray.so.12 (0x00007fffe96cc000)\n    libmpi_gtl_hsa.so.0 => /opt/cray/pe/lib64/libmpi_gtl_hsa.so.0 (0x00007fffe9469000)\n    libhsa-runtime64.so.1 => /opt/rocm-5.3.0/lib/libhsa-runtime64.so.1 (0x00007fffe8fdc000)\n    libhwloc.so.15 => /sw/crusher/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/hwloc-2.5.0-4p6jkgf5ez6wr27pytkzyptppzpugu3e/lib/libhwloc.so.15 (0x00007fffe8d82000)\n    libdl.so.2 => /lib64/libdl.so.2 (0x00007fffe8b7e000)\n    libhipfft.so => /opt/rocm-5.3.0/lib/libhipfft.so (0x00007fffed9d2000)\n    libstdc++.so.6 => /usr/lib64/libstdc++.so.6 (0x00007fffe875b000)\n    libc.so.6 => /lib64/libc.so.6 (0x00007fffe8366000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007fffed7da000)\n    libamd_comgr.so.2 => /opt/rocm-5.3.0/lib/libamd_comgr.so.2 (0x00007fffe06e0000)\n    libnuma.so.1 => /usr/lib64/libnuma.so.1 (0x00007fffe04d4000)\n    libfabric.so.1 => /opt/cray/libfabric/1.15.2.0/lib64/libfabric.so.1 (0x00007fffe01e2000)\n    libatomic.so.1 => /usr/lib64/libatomic.so.1 (0x00007fffdffd9000)\n    libpmi.so.0 => /opt/cray/pe/lib64/libpmi.so.0 (0x00007fffdfdd7000)\n    libpmi2.so.0 => /opt/cray/pe/lib64/libpmi2.so.0 (0x00007fffdfb9e000)\n    libquadmath.so.0 => /usr/lib64/libquadmath.so.0 (0x00007fffdf959000)\n    libmodules.so.1 => /opt/cray/pe/lib64/cce/libmodules.so.1 (0x00007fffed9b5000)\n    libfi.so.1 => /opt/cray/pe/lib64/cce/libfi.so.1 (0x00007fffdf3b4000)\n    libcraymath.so.1 => /opt/cray/pe/lib64/cce/libcraymath.so.1 (0x00007fffed8ce000)\n    libf.so.1 => /opt/cray/pe/lib64/cce/libf.so.1 (0x00007fffed83b000)\n    libu.so.1 => /opt/cray/pe/lib64/cce/libu.so.1 (0x00007fffdf2ab000)\n    libcsup.so.1 => /opt/cray/pe/lib64/cce/libcsup.so.1 (0x00007fffed832000)\n    libamdhip64.so.5 => /opt/rocm-5.3.0/lib/libamdhip64.so.5 (0x00007fffdda8a000)\n    libelf.so.1 => /usr/lib64/libelf.so.1 (0x00007fffdd871000)\n    libdrm.so.2 => /usr/lib64/libdrm.so.2 (0x00007fffdd65d000)\n    libdrm_amdgpu.so.1 => /usr/lib64/libdrm_amdgpu.so.1 (0x00007fffdd453000)\n    libpciaccess.so.0 => /sw/crusher/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libpciaccess-0.16-6evcvl74myxfxdrddmnsvbc7sgfx6s5j/lib/libpciaccess.so.0 (0x00007fffdd24a000)\n    libxml2.so.2 => /sw/crusher/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libxml2-2.9.12-h7fe2yauotu3xit7rsaixaowd3noknur/lib/libxml2.so.2 (0x00007fffdcee6000)\n    librocfft.so.0 => /opt/rocm-5.3.0/lib/librocfft.so.0 (0x00007fffdca1a000)\n    libz.so.1 => /lib64/libz.so.1 (0x00007fffdc803000)\n    libtinfo.so.6 => /lib64/libtinfo.so.6 (0x00007fffdc5d5000)\n    libcxi.so.1 => /usr/lib64/libcxi.so.1 (0x00007fffdc3b0000)\n    libcurl.so.4 => /usr/lib64/libcurl.so.4 (0x00007fffdc311000)\n    libjson-c.so.3 => /usr/lib64/libjson-c.so.3 (0x00007fffdc101000)\n    libpals.so.0 => /opt/cray/pe/lib64/libpals.so.0 (0x00007fffdbefc000)\n    libgfortran.so.5 => /opt/cray/pe/gcc-libs/libgfortran.so.5 (0x00007fffdba50000)\n    liblzma.so.5 => /sw/crusher/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/xz-5.2.5-zwra4gpckelt5umczowf3jtmeiz3yd7u/lib/liblzma.so.5 (0x00007fffdb82a000)\n    libiconv.so.2 => /sw/crusher/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libiconv-1.16-coz5dzhtoeq5unhjirayfn2xftnxk43l/lib/libiconv.so.2 (0x00007fffdb52e000)\n    librocfft-device-0.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-0.so.0 (0x00007fffa11a0000)\n    librocfft-device-1.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-1.so.0 (0x00007fff6491b000)\n    librocfft-device-2.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-2.so.0 (0x00007fff2a828000)\n    librocfft-device-3.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-3.so.0 (0x00007ffef7eee000)\n    libnghttp2.so.14 => /usr/lib64/libnghttp2.so.14 (0x00007ffef7cc6000)\n    libidn2.so.0 => /usr/lib64/libidn2.so.0 (0x00007ffef7aa9000)\n    libssh.so.4 => /usr/lib64/libssh.so.4 (0x00007ffef783b000)\n    libpsl.so.5 => /usr/lib64/libpsl.so.5 (0x00007ffef7629000)\n    libssl.so.1.1 => /usr/lib64/libssl.so.1.1 (0x00007ffef758a000)\n    libcrypto.so.1.1 => /usr/lib64/libcrypto.so.1.1 (0x00007ffef724a000)\n    libgssapi_krb5.so.2 => /usr/lib64/libgssapi_krb5.so.2 (0x00007ffef6ff8000)\n    libldap_r-2.4.so.2 => /usr/lib64/libldap_r-2.4.so.2 (0x00007ffef6da4000)\n    liblber-2.4.so.2 => /usr/lib64/liblber-2.4.so.2 (0x00007ffef6b95000)\n    libzstd.so.1 => /usr/lib64/libzstd.so.1 (0x00007ffef6865000)\n    libbrotlidec.so.1 => /usr/lib64/libbrotlidec.so.1 (0x00007ffef6659000)\n    libunistring.so.2 => /usr/lib64/libunistring.so.2 (0x00007ffef62d6000)\n    libjitterentropy.so.3 => /usr/lib64/libjitterentropy.so.3 (0x00007ffef60cf000)\n    libkrb5.so.3 => /usr/lib64/libkrb5.so.3 (0x00007ffef5df6000)\n    libk5crypto.so.3 => /usr/lib64/libk5crypto.so.3 (0x00007ffef5bde000)\n    libcom_err.so.2 => /lib64/libcom_err.so.2 (0x00007ffef59da000)\n    libkrb5support.so.0 => /usr/lib64/libkrb5support.so.0 (0x00007ffef57cb000)\n    libresolv.so.2 => /lib64/libresolv.so.2 (0x00007ffef55b3000)\n    libsasl2.so.3 => /usr/lib64/libsasl2.so.3 (0x00007ffef5396000)\n    libbrotlicommon.so.1 => /usr/lib64/libbrotlicommon.so.1 (0x00007ffef5175000)\n    libkeyutils.so.1 => /usr/lib64/libkeyutils.so.1 (0x00007ffef4f70000)\n    libselinux.so.1 => /lib64/libselinux.so.1 (0x00007ffef4d47000)\n    libpcre.so.1 => /usr/lib64/libpcre.so.1 (0x00007ffef4abe000)\n*************************\n*****ls -lh /mnt/bb/hagertnl*****\ntotal 236M\n-rwxr-xr-x 1 hagertnl hagertnl 236M Mar 28 17:01 lmp\ndrwx------ 2 hagertnl hagertnl  114 Mar 28 17:01 lmp_libs\n*****ls -lh /mnt/bb/hagertnl/lmp_libs*****\ntotal 9.2M\n-rwxr-xr-x 1 hagertnl hagertnl 1.6M Oct  6  2021 libhwloc.so.15\n-rwxr-xr-x 1 hagertnl hagertnl 1.6M Oct  6  2021 libiconv.so.2\n-rwxr-xr-x 1 hagertnl hagertnl 783K Oct  6  2021 liblzma.so.5\n-rwxr-xr-x 1 hagertnl hagertnl 149K Oct  6  2021 libpciaccess.so.0\n-rwxr-xr-x 1 hagertnl hagertnl 5.2M Oct  6  2021 libxml2.so.2\n*****ldd /mnt/bb/hagertnl/lmp*****\n    linux-vdso.so.1 (0x00007fffeda02000)\n    libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007fffed5bb000)\n    libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fffed398000)\n    libm.so.6 => /lib64/libm.so.6 (0x00007fffed04d000)\n    librt.so.1 => /lib64/librt.so.1 (0x00007fffece44000)\n    libamdhip64.so.4 => /opt/rocm-4.5.2/lib/libamdhip64.so.4 (0x00007fffec052000)\n    libmpi_cray.so.12 => /opt/cray/pe/lib64/libmpi_cray.so.12 (0x00007fffe96cc000)\n    libmpi_gtl_hsa.so.0 => /opt/cray/pe/lib64/libmpi_gtl_hsa.so.0 (0x00007fffe9469000)\n    libhsa-runtime64.so.1 => /opt/rocm-5.3.0/lib/libhsa-runtime64.so.1 (0x00007fffe8fdc000)\n    libhwloc.so.15 => /mnt/bb/hagertnl/lmp_libs/libhwloc.so.15 (0x00007fffe8d82000)\n    libdl.so.2 => /lib64/libdl.so.2 (0x00007fffe8b7e000)\n    libhipfft.so => /opt/rocm-5.3.0/lib/libhipfft.so (0x00007fffed9d2000)\n    libstdc++.so.6 => /usr/lib64/libstdc++.so.6 (0x00007fffe875b000)\n    libc.so.6 => /lib64/libc.so.6 (0x00007fffe8366000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007fffed7da000)\n    libamd_comgr.so.2 => /opt/rocm-5.3.0/lib/libamd_comgr.so.2 (0x00007fffe06e0000)\n    libnuma.so.1 => /usr/lib64/libnuma.so.1 (0x00007fffe04d4000)\n    libfabric.so.1 => /opt/cray/libfabric/1.15.2.0/lib64/libfabric.so.1 (0x00007fffe01e2000)\n    libatomic.so.1 => /usr/lib64/libatomic.so.1 (0x00007fffdffd9000)\n    libpmi.so.0 => /opt/cray/pe/lib64/libpmi.so.0 (0x00007fffdfdd7000)\n    libpmi2.so.0 => /opt/cray/pe/lib64/libpmi2.so.0 (0x00007fffdfb9e000)\n    libquadmath.so.0 => /usr/lib64/libquadmath.so.0 (0x00007fffdf959000)\n    libmodules.so.1 => /opt/cray/pe/lib64/cce/libmodules.so.1 (0x00007fffed9b5000)\n    libfi.so.1 => /opt/cray/pe/lib64/cce/libfi.so.1 (0x00007fffdf3b4000)\n    libcraymath.so.1 => /opt/cray/pe/lib64/cce/libcraymath.so.1 (0x00007fffed8ce000)\n    libf.so.1 => /opt/cray/pe/lib64/cce/libf.so.1 (0x00007fffed83b000)\n    libu.so.1 => /opt/cray/pe/lib64/cce/libu.so.1 (0x00007fffdf2ab000)\n    libcsup.so.1 => /opt/cray/pe/lib64/cce/libcsup.so.1 (0x00007fffed832000)\n    libamdhip64.so.5 => /opt/rocm-5.3.0/lib/libamdhip64.so.5 (0x00007fffdda8a000)\n    libelf.so.1 => /usr/lib64/libelf.so.1 (0x00007fffdd871000)\n    libdrm.so.2 => /usr/lib64/libdrm.so.2 (0x00007fffdd65d000)\n    libdrm_amdgpu.so.1 => /usr/lib64/libdrm_amdgpu.so.1 (0x00007fffdd453000)\n    libpciaccess.so.0 => /sw/crusher/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libpciaccess-0.16-6evcvl74myxfxdrddmnsvbc7sgfx6s5j/lib/libpciaccess.so.0 (0x00007fffdd24a000)\n    libxml2.so.2 => /sw/crusher/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libxml2-2.9.12-h7fe2yauotu3xit7rsaixaowd3noknur/lib/libxml2.so.2 (0x00007fffdcee6000)\n    librocfft.so.0 => /opt/rocm-5.3.0/lib/librocfft.so.0 (0x00007fffdca1a000)\n    libz.so.1 => /lib64/libz.so.1 (0x00007fffdc803000)\n    libtinfo.so.6 => /lib64/libtinfo.so.6 (0x00007fffdc5d5000)\n    libcxi.so.1 => /usr/lib64/libcxi.so.1 (0x00007fffdc3b0000)\n    libcurl.so.4 => /usr/lib64/libcurl.so.4 (0x00007fffdc311000)\n    libjson-c.so.3 => /usr/lib64/libjson-c.so.3 (0x00007fffdc101000)\n    libpals.so.0 => /opt/cray/pe/lib64/libpals.so.0 (0x00007fffdbefc000)\n    libgfortran.so.5 => /opt/cray/pe/gcc-libs/libgfortran.so.5 (0x00007fffdba50000)\n    liblzma.so.5 => /sw/crusher/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/xz-5.2.5-zwra4gpckelt5umczowf3jtmeiz3yd7u/lib/liblzma.so.5 (0x00007fffdb82a000)\n    libiconv.so.2 => /sw/crusher/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libiconv-1.16-coz5dzhtoeq5unhjirayfn2xftnxk43l/lib/libiconv.so.2 (0x00007fffdb52e000)\n    librocfft-device-0.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-0.so.0 (0x00007fffa11a0000)\n    librocfft-device-1.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-1.so.0 (0x00007fff6491b000)\n    librocfft-device-2.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-2.so.0 (0x00007fff2a828000)\n    librocfft-device-3.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-3.so.0 (0x00007ffef7eee000)\n    libnghttp2.so.14 => /usr/lib64/libnghttp2.so.14 (0x00007ffef7cc6000)\n    libidn2.so.0 => /usr/lib64/libidn2.so.0 (0x00007ffef7aa9000)\n    libssh.so.4 => /usr/lib64/libssh.so.4 (0x00007ffef783b000)\n    libpsl.so.5 => /usr/lib64/libpsl.so.5 (0x00007ffef7629000)\n    libssl.so.1.1 => /usr/lib64/libssl.so.1.1 (0x00007ffef758a000)\n    libcrypto.so.1.1 => /usr/lib64/libcrypto.so.1.1 (0x00007ffef724a000)\n    libgssapi_krb5.so.2 => /usr/lib64/libgssapi_krb5.so.2 (0x00007ffef6ff8000)\n    libldap_r-2.4.so.2 => /usr/lib64/libldap_r-2.4.so.2 (0x00007ffef6da4000)\n    liblber-2.4.so.2 => /usr/lib64/liblber-2.4.so.2 (0x00007ffef6b95000)\n    libzstd.so.1 => /usr/lib64/libzstd.so.1 (0x00007ffef6865000)\n    libbrotlidec.so.1 => /usr/lib64/libbrotlidec.so.1 (0x00007ffef6659000)\n    libunistring.so.2 => /usr/lib64/libunistring.so.2 (0x00007ffef62d6000)\n    libjitterentropy.so.3 => /usr/lib64/libjitterentropy.so.3 (0x00007ffef60cf000)\n    libkrb5.so.3 => /usr/lib64/libkrb5.so.3 (0x00007ffef5df6000)\n    libk5crypto.so.3 => /usr/lib64/libk5crypto.so.3 (0x00007ffef5bde000)\n    libcom_err.so.2 => /lib64/libcom_err.so.2 (0x00007ffef59da000)\n    libkrb5support.so.0 => /usr/lib64/libkrb5support.so.0 (0x00007ffef57cb000)\n    libresolv.so.2 => /lib64/libresolv.so.2 (0x00007ffef55b3000)\n    libsasl2.so.3 => /usr/lib64/libsasl2.so.3 (0x00007ffef5396000)\n    libbrotlicommon.so.1 => /usr/lib64/libbrotlicommon.so.1 (0x00007ffef5175000)\n    libkeyutils.so.1 => /usr/lib64/libkeyutils.so.1 (0x00007ffef4f70000)\n    libselinux.so.1 => /lib64/libselinux.so.1 (0x00007ffef4d47000)\n    libpcre.so.1 => /usr/lib64/libpcre.so.1 (0x00007ffef4abe000)\n*************************************\n\nNotice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable.\nOnce libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.\n\nAlternative: SBCASTing a binary with all libraries\n\nAs mentioned above, you can use --exclude=NONE on sbcast to send all libraries along with the binary.\nUsing --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time.\nA job script for the previous example, modified for sending all libraries is shown below.\n\n#!/bin/bash\n#SBATCH -A <projid>\n#SBATCH -J sbcast_binary_to_nvme\n#SBATCH -o %x-%j.out\n#SBATCH -t 00:05:00\n#SBATCH -p batch\n#SBATCH -N 2\n#SBATCH -C nvme\n\ndate\n\n# Change directory to user scratch space (Orion)\ncd /lustre/orion/<projid>/scratch/<userid>\n\n# For this example, I use a HIP-enabled LAMMPS binary, with dependencies to MPI, HIP, and HWLOC\nexe=\"lmp\"\n\necho \"*****ldd ./${exe}*****\"\nldd ./${exe}\necho \"*************************\"\n\n# SBCAST executable from Orion to NVMe -- NOTE: ``-C nvme`` is needed in SBATCH headers to use the NVMe drive\n# NOTE: dlopen'd files will NOT be picked up by sbcast\nsbcast --send-libs --exclude=NONE -pf ${exe} /mnt/bb/$USER/${exe}\nif [ ! \"$?\" == \"0\" ]; then\n    # CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,\n    # your application may pick up partially complete shared library files, which would give you confusing errors.\n    echo \"SBCAST failed!\"\n    exit 1\nfi\n\n# Check to see if file exists\necho \"*****ls -lh /mnt/bb/$USER*****\"\nls -lh /mnt/bb/$USER/\necho \"*****ls -lh /mnt/bb/$USER/${exe}_libs*****\"\nls -lh /mnt/bb/$USER/${exe}_libs\n\n# SBCAST sends all libraries detected by `ld` (minus any excluded), and stores them in the same directory in each node's node-local storage\n# Any libraries opened by `dlopen` are NOT sent, since they are not known by the linker at run-time.\n\n# All required libraries now reside in /mnt/bb/$USER/${exe}_libs\nexport LD_LIBRARY_PATH=\"/mnt/bb/$USER/${exe}_libs\"\n\n# libfabric dlopen's several libraries:\nexport LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:$(pkg-config --variable=libdir libfabric)\"\n\n# cray-mpich dlopen's libhsa-runtime64.so and libamdhip64.so (non-versioned), so symlink on each node:\nsrun -N ${SLURM_NNODES} -n ${SLURM_NNODES} --ntasks-per-node=1 --label -D /mnt/bb/$USER/${exe}_libs \\\n    bash -c \"if [ -f libhsa-runtime64.so.1 ]; then ln -s libhsa-runtime64.so.1 libhsa-runtime64.so; fi;\n    if [ -f libamdhip64.so.5 ]; then ln -s libamdhip64.so.5 libamdhip64.so; fi\"\n\n# RocBLAS has over 1,000 device libraries that may be `dlopen`'d by RocBLAS during a run.\n# It's impractical to SBCAST all of these, so you can set this path instead, if you use RocBLAS:\n#export ROCBLAS_TENSILE_LIBPATH=${ROCM_PATH}/lib/rocblas/library\n\n# You may notice that some libraries are still linked from /sw/crusher, even after SBCASTing.\n# This is because the Spack-build modules use RPATH to find their dependencies. This behavior cannot be changed.\necho \"*****ldd /mnt/bb/$USER/${exe}*****\"\nldd /mnt/bb/$USER/${exe}\necho \"*************************************\"\n\nSome libraries still resolved to paths outside of /mnt/bb, and the reason for that is that the executable may have several paths in RPATH.\n\n\n\nProfiling Applications\n\nGetting Started with the HPE Performance Analysis Tools (PAT)\n\nThe Performance Analysis Tools (PAT), formerly CrayPAT, are a suite of utilities that enable users to capture and analyze performance data generated during program execution. These tools provide an integrated infrastructure for measurement, analysis, and visualization of computation, communication, I/O, and memory utilization to help users optimize programs for faster execution and more efficient computing resource usage.\n\nThere are three programming interfaces available: (1) Perftools-lite, (2) Perftools, and (3) Perftools-preload.\n\nBelow are two examples that generate an instrumented executable using Perftools, which is an advanced interface that provides full-featured data collection and analysis capability, including full traces with timeline displays.\n\nThe first example generates an instrumented executable using a PrgEnv-amd build:\n\nmodule load PrgEnv-amd\nmodule load craype-accel-amd-gfx90a\nmodule load rocm\nmodule load perftools\n\nexport PATH=\"${PATH}:${ROCM_PATH}/llvm/bin\"\nexport CXX='CC -x hip'\nexport CXXFLAGS='-ggdb -O3 -std=c++17 –Wall'\nexport LD='CC'\nexport LDFLAGS=\"${CXXFLAGS} -L${ROCM_PATH}/lib\"\nexport LIBS='-lamdhip64'\n\nmake clean\nmake\n\npat_build -g hip,io,mpi -w -f <executable>\n\nThe second example generates an instrumened executable using a hipcc build:\n\nmodule load perftools\nmodule load craype-accel-amd-gfx90a\nmodule load rocm\n\nexport CXX='hipcc'\nexport CXXFLAGS=\"$(pat_opts include hipcc) \\\n  $(pat_opts pre_compile hipcc) -g -O3 -std=c++17 -Wall \\\n  --offload-arch=gfx90a -I${CRAY_MPICH_DIR}/include \\\n  $(pat_opts post_compile hipcc)\"\nexport LD='hipcc'\nexport LDFLAGS=\"$(pat_opts pre_link hipcc) ${CXXFLAGS} \\\n  -L${CRAY_MPICH_DIR}/lib ${PE_MPICH_GTL_DIR_amd_gfx908}\"\nexport LIBS=\"-lmpi ${PE_MPICH_GTL_LIBS_amd_gfx908} \\\n  $(pat_opts post_link hipcc)\"\n\nmake clean\nmake\n\npat_build -g hip,io,mpi -w -f <executable>\n\nThe pat_build command in the above examples generates an instrumented executable with +pat appended to the executable name (e.g., hello_jobstep+pat).\n\nWhen run, the instrumented executable will trace HIP, I/O, MPI, and all user functions and generate a folder of results (e.g., hello_jobstep+pat+39545-2t).\n\nTo analyze these results, use the pat_report command, e.g.:\n\npat_report hello_jobstep+pat+39545-2t\n\nThe resulting report includes profiles of functions, profiles of maximum function times, details on load imbalance, details on program energy and power usages, details on memory high water mark, and more.\n\nMore detailed information on the HPE Performance Analysis Tools can be found in the HPE Performance Analysis Tools User Guide.\n\nWhen using perftools-lite-gpu, there is a known issue causing ld.lld not to be found. A workaround this issue can be found here.\n\nGetting Started with HPCToolkit\n\nHPCToolkit is an integrated suite of tools for measurement and analysis of program performance on computers ranging from multicore desktop systems to the nation's largest supercomputers. HPCToolkit provides accurate measurements of a program's work, resource consumption, and inefficiency, correlates these metrics with the program's source code, works with multilingual, fully optimized binaries, has very low measurement overhead, and scales to large parallel systems. HPCToolkit's measurements provide support for analyzing a program execution cost, inefficiency, and scaling characteristics both within and across nodes of a parallel system.\n\nProgramming models supported by HPCToolkit include MPI, OpenMP, OpenACC, CUDA, OpenCL, DPC++, HIP, RAJA, Kokkos, and others.\n\nBelow is an example that generates a profile and loads the results in their GUI-based viewer.\n\nmodule use /gpfs/alpine/csc322/world-shared/modulefiles/x86_64\nmodule load hpctoolkit\n\n# 1. Profile and trace an application using CPU time and GPU performance counters\nsrun <srun_options> hpcrun -o <measurement_dir> -t -e CPUTIME -e gpu=amd <application>\n\n# 2. Analyze the binary of executables and its dependent libraries\nhpcstruct <measurement_dir>\n\n# 3. Combine measurements with program structure information and generate a database\nhpcprof -o <database_dir> <measurement_dir>\n\n# 4. Understand performance issues by analyzing profiles and traces with the GUI\nhpcviewer <database_dir>\n\nMore detailed information on HPCToolkit can be found in the HPCToolkit User's Manual.\n\nHPCToolkit does not require a recompile to profile the code. It is recommended to use the -g optimization flag for attribution to source lines.\n\nGetting Started with the ROCm Profiler\n\nrocprof gathers metrics on kernels run on AMD GPU architectures. The profiler works for HIP kernels, as well as offloaded kernels from OpenMP target offloading, OpenCL, and abstraction layers such as Kokkos.\nFor a simple view of kernels being run, rocprof --stats --timestamp on is a great place to start.\nWith the --stats option enabled, rocprof will generate a file that is named results.stats.csv by default, but named <output>.stats.csv if the -o flag is supplied.\nThis file will list all kernels being run, the number of times they are run, the total duration and the average duration (in nanoseconds) of the kernel, and the GPU usage percentage.\nMore detailed infromation on rocprof profiling modes can be found at ROCm Profiler documentation.\n\nIf you are using sbcast, you need to explicitly sbcast the AQL profiling library found in ${ROCM_PATH}/hsa-amd-aqlprofile/lib/libhsa-amd-aqlprofile64.so.\nA symbolic link to this library can also be found in ${ROCM_PATH}/lib.\nAlternatively, you may leave ${ROCM_PATH}/lib in your LD_LIBRARY_PATH.\n\nRoofline Profiling with the ROCm Profiler\n\nThe Roofline performance model is an increasingly popular way to demonstrate and understand application performance.\nThis section documents how to construct a simple roofline model for a single kernel using rocprof.\nThis roofline model is designed to be comparable to rooflines constructed by NVIDIA's NSight Compute.\nA roofline model plots the achieved performance (in floating-point operations per second, FLOPS/s) as a function of arithmetic (or operational) intensity (in FLOPS per Byte).\nThe model detailed here calculates the bytes moved as they move to and from the GPU's HBM.\n\nInteger instructions and cache levels are currently not documented here.\n\nTo get started, you will need to make an input file for rocprof, to be passed in through rocprof -i <input_file> --timestamp on -o my_output.csv <my_exe>.\nBelow is an example, and contains the information needed to roofline profile GPU 0, as seen by each rank:\n\npmc : TCC_EA_RDREQ_32B_sum TCC_EA_RDREQ_sum TCC_EA_WRREQ_sum TCC_EA_WRREQ_64B_sum SQ_INSTS_VALU_ADD_F16 SQ_INSTS_VALU_MUL_F16 SQ_INSTS_VALU_FMA_F16 SQ_INSTS_VALU_TRANS_F16 SQ_INSTS_VALU_ADD_F32 SQ_INSTS_VALU_MUL_F32 SQ_INSTS_VALU_FMA_F32 SQ_INSTS_VALU_TRANS_F32\npmc : SQ_INSTS_VALU_ADD_F64 SQ_INSTS_VALU_MUL_F64 SQ_INSTS_VALU_FMA_F64 SQ_INSTS_VALU_TRANS_F64 SQ_INSTS_VALU_MFMA_MOPS_F16 SQ_INSTS_VALU_MFMA_MOPS_BF16 SQ_INSTS_VALU_MFMA_MOPS_F32 SQ_INSTS_VALU_MFMA_MOPS_F64\ngpu: 0\n\nIn an application with more than one kernel, you should strongly consider filtering by kernel name by adding a line like: kernel: <kernel_name> to the rocprof input file.\n\nThis provides the minimum set of metrics used to construct a roofline model, in the minimum number of passes.\nEach line that begins with pmc indicates that the application will be re-run, and the metrics in that line will be collected.\nrocprof can collect up to 8 counters from each block (SQ, TCC) in each application re-run.\nTo gather metrics across multiple MPI ranks, you will need to use a command that redirects the output of rocprof to a unique file for each task.\nFor example:\n\nsrun -N 2 -n 16 --ntasks-per-node=8 --gpus-per-node=8 --gpu-bind=closest bash -c 'rocprof -o ${SLURM_JOBID}_${SLURM_PROCID}.csv -i <input_file> --timestamp on <exe>'\n\nThe gpu: filter in the rocprof input file identifies GPUs by the number the MPI rank would see them as. In the srun example above,\neach MPI rank only has 1 GPU, so each rank sees its GPU as GPU 0.\n\nTheoretical Roofline\n\nThe theoretical (not attainable) peak roofline constructs a theoretical maximum performance for each operational intensity.\n\ntheoretical peak is determined by the hardware specifications and is not attainable in practice. attaiable peak is the performance as measured by\nin-situ microbenchmarks designed to best utilize the hardware. achieved performance is what the profiled application actually achieves.\n\nThe theoretical roofline can be constructed as:\n\nFLOPS_{peak} = minimum(ArithmeticIntensity * BW_{HBM}, TheoreticalFLOPS)\n\nOn Crusher, the memory bandwidth for HBM is 1.6 TB/s, and the theoretical peak floating-point FLOPS/s when using vector registers is calculated by:\n\nTheoreticalFLOPS = 128 FLOP/cycle/CU * 110 CU * 1700000000 cycles/second = 23.9 TFLOP/s\n\nHowever, when using MFMA instructions, the theoretical peak floating-point FLOPS/s is calculated by:\n\nTheoreticalFLOPS = flop\\_per\\_cycle(precision) FLOP/cycle/CU * 110 CU * 1700000000 cycles/second\n\nwhere flop_per_cycle(precision) is the published floating-point operations per clock cycle, per compute unit.\nThose values are:\n\n\nThe table provides a detailed breakdown of the data types and their corresponding Flops/Clock/CU (floating-point operations per clock per compute unit) values for a crusher quick start guide. The first data type listed is FP64, which stands for 64-bit floating-point numbers. It has a Flops/Clock/CU value of 256, meaning that for every clock cycle, 256 floating-point operations can be performed on each compute unit. The next data type, FP32, stands for 32-bit floating-point numbers and also has a Flops/Clock/CU value of 256. This means that it can perform the same number of floating-point operations as FP64, but with smaller numbers. The third data type, FP16, stands for 16-bit floating-point numbers and has a higher Flops/Clock/CU value of 1024. This means that it can perform four times as many floating-point operations as FP64 and FP32. The last two data types, BF16 and INT8, both have a Flops/Clock/CU value of 1024. BF16 stands for bfloat16, a 16-bit floating-point format designed for deep learning applications, while INT8 stands for 8-bit integer numbers. Both of these data types have the same Flops/Clock/CU value, indicating that they can perform the same number of operations per clock cycle. This table is useful for understanding the capabilities of the crusher and determining which data types are best suited for different types of computations.\n\n| Data Type | Flops/Clock/CU |\n|-----------|-----------------|\n| FP64      | 256             |\n| FP32      | 256             |\n| FP16      | 1024            |\n| BF16      | 1024            |\n| INT8      | 1024            |\n\n\n\nAttainable peak rooflines are constructed using microbenchmarks, and are not currently discussed here.\nAttainable rooflines consider the limitations of cooling and power consumption and are more representative of what an application can achieve.\n\nAchieved FLOPS/s\n\nWe calculate the achieved performance at the desired level (here, double-precision floating point, FP64), by summing each metric count and weighting the FMA metric by 2, since a fused multiply-add is considered 2 floating point operations.\nAlso note that these SQ_INSTS_VALU_<ADD,MUL,TRANS> metrics are reported as per-simd, so we mutliply by the wavefront size as well.\nThe SQ_INSTS_VALU_MFMA_MOPS_* instructions should be multiplied by the Flops/Cycle/CU value listed above.\nWe use this equation to calculate the number of double-precision FLOPS:\n\nFP64\\_FLOPS =   64  *&(SQ\\_INSTS\\_VALU\\_ADD\\_F64         \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_MUL\\_F64       \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_TRANS\\_F64     \\\\\\\\\n                     &+ 2 * SQ\\_INSTS\\_VALU\\_FMA\\_F64)  \\\\\\\\\n              + 256 *&(SQ\\_INSTS\\_VALU\\_MFMA\\_MOPS\\_F64)\n\nWhen SQ_INSTS_VALU_MFMA_MOPS_*_F64 instructions are used, then 47.8 TF/s is considered the theoretical maximum FLOPS/s.\nIf only SQ_INSTS_VALU_<ADD,MUL,TRANS> are found, then 23.9 TF/s is the theoretical maximum FLOPS/s.\nThen, we divide the number of FLOPS by the elapsed time of the kernel to find FLOPS per second.\nThis is found from subtracting the rocprof metrics EndNs by BeginNs, provided by --timestamp on, then converting from nanoseconds to seconds by dividing by 1,000,000,000 (power(10,9)).\n\nFor ROCm/5.2.0 and earlier, there is a known issue with the timings provided by --timestamp on. See crusher-known-issues <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#crusher-known-issues>.\n\nCalculating for all precisions\n\nThe above formula can be adapted to compute the total FLOPS across all floating-point precisions (INT excluded).\n\nTOTAL\\_FLOPS =   64  *&(SQ\\_INSTS\\_VALU\\_ADD\\_F16         \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_MUL\\_F16       \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_TRANS\\_F16     \\\\\\\\\n                     &+ 2 * SQ\\_INSTS\\_VALU\\_FMA\\_F16)  \\\\\\\\\n              + 64  *&(SQ\\_INSTS\\_VALU\\_ADD\\_F32         \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_MUL\\_F32       \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_TRANS\\_F32     \\\\\\\\\n                     &+ 2 * SQ\\_INSTS\\_VALU\\_FMA\\_F32)  \\\\\\\\\n              + 64  *&(SQ\\_INSTS\\_VALU\\_ADD\\_F64         \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_MUL\\_F64       \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_TRANS\\_F64     \\\\\\\\\n                     &+ 2 * SQ\\_INSTS\\_VALU\\_FMA\\_F64)  \\\\\\\\\n              + 1024 &*(SQ\\_INSTS\\_VALU\\_MFMA\\_MOPS\\_F16) \\\\\\\\\n              + 1024 &*(SQ\\_INSTS\\_VALU\\_MFMA\\_MOPS\\_BF16) \\\\\\\\\n              + 256 *&(SQ\\_INSTS\\_VALU\\_MFMA\\_MOPS\\_F32) \\\\\\\\\n              + 256 *&(SQ\\_INSTS\\_VALU\\_MFMA\\_MOPS\\_F64) \\\\\\\\\n\nArithmetic Intensity\n\nArithmetic intensity calculates the ratio of FLOPS to bytes moved between HBM and L2 cache.\nWe calculated FLOPS above (FP64_FLOPS).\nWe can calculate the number of bytes moved using the rocprof metrics TCC_EA_WRREQ_64B, TCC_EA_WRREQ_sum, TCC_EA_RDREQ_32B, and TCC_EA_RDREQ_sum.\nTCC refers to the L2 cache, and EA is the interface between L2 and HBM.\nWRREQ and RDREQ are write-requests and read-requests, respectively.\nEach of these requests is either 32 bytes or 64 bytes.\nSo we calculate the number of bytes traveling over the EA interface as:\n\nBytesMoved = BytesWritten + BytesRead\n\nwhere\n\nBytesWritten = 64 * TCC\\_EA\\_WRREQ\\_64B\\_sum + 32 * (TCC\\_EA\\_WRREQ\\_sum - TCC\\_EA\\_WRREQ\\_64B\\_sum)\n\nBytesRead = 32 * TCC\\_EA\\_RDREQ\\_32B\\_sum + 64 * (TCC\\_EA\\_RDREQ\\_sum - TCC\\_EA\\_RDREQ\\_32B\\_sum)\n\n\n\nNotable Differences between Summit and Crusher\n\nThis section details 'tips and tricks' and information of interest to users when porting from Summit to Crusher.\n\nUsing reduced precision (FP16 and BF16 datatypes)\n\nUsers leveraging BF16 and FP16 datatypes for applications such as ML/AI training and low-precision matrix multiplication should be aware that the AMD MI250X GPU has different denormal handling than the V100 GPUs on Summit. On the MI250X, the V_DOT2 and the matrix instructions for FP16 and BF16 flush input and output denormal values to zero. FP32 and FP64 MFMA instructions do not flush input and output denormal values to zero.\n\nWhen training deep learning models using FP16 precision, some models may fail to converge with FP16 denorms flushed to zero. This occurs in operations encountering denormal values, and so is more likely to occur in FP16 because of a small dynamic range. BF16 numbers have a larger dynamic range than FP16 numbers and are less likely to encounter denormal values.\n\nAMD has provided a solution in ROCm 5.0 which modifies the behavior of Tensorflow, PyTorch, and rocBLAS. This modification starts with FP16 input values, casting the intermediate FP16 values to BF16, and then casting back to FP16 output after the accumulate FP32 operations. In this way, the input and output types are unchanged. The behavior is enabled by default in machine learning frameworks. This behavior requires user action in rocBLAS, via a special enum type. For more information, see the rocBLAS link below.\n\nIf you encounter significant differences when running using reduced precision, explore replacing non-converging models in FP16 with BF16, because of the greater dynamic range in BF16. We recommend using BF16 for ML models in general. If you have further questions or encounter issues, contact help@olcf.ornl.gov.\n\nAdditional information on MI250X reduced precision can be found at:\n\nThe MI250X ISA specification details the flush to zero denorm behavior at: https://developer.amd.com/wp-content/resources/CDNA2_Shader_ISA_18November2021.pdf (See page 41 and 46)\n\nAMD rocBLAS library reference guide details this behavior at: https://rocblas.readthedocs.io/en/master/API_Reference_Guide.html#mi200-gfx90a-considerations\n\nEnabling GPU Page Migration\n\nThe AMD MI250X and operating system on Crusher supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Crusher. In the following section we'll discuss how the heterogenous memory space on a Crusher node is surfaced within your application.\n\nThe accessibility of memory from GPU kernels and whether pages may migrate depends three factors: how the memory was allocated; the XNACK operating mode of the GPU; whether the kernel was compiled to support page migration. The latter two factors are intrinsically linked, as the MI250X GPU operating mode restricts the types of kernels which may run.\n\nXNACK (pronounced X-knack) refers to the AMD GPU's ability to retry memory accesses that fail due to a page fault. The XNACK mode of an MI250X can be changed by setting the environment variable HSA_XNACK before starting a process that uses the GPU. Valid values are 0 (disabled) and 1 (enabled), and all processes connected to a GPU must use the same XNACK setting. The default MI250X on Crusher is HSA_XNACK=0.\n\nIf HSA_XNACK=0, page faults in GPU kernels are not handled and will terminate the kernel. Therefore all memory locations accessed by the GPU must either be resident in the GPU HBM or mapped by the HIP runtime. Memory regions may be migrated between the host DDR4 and GPU HBM using explicit HIP library functions such as hipMemAdvise and hipPrefetchAsync, but memory will not be automatically migrated based on access patterns alone.\n\nIf HSA_XNACK=1, page faults in GPU kernels will trigger a page table lookup. If the memory location can be made accessible to the GPU, either by being migrated to GPU HBM or being mapped for remote access, the appropriate action will occur and the access will be replayed. Page migration  will happen between CPU DDR4 and GPU HBM according to page touch. The exceptions are if the programmer uses a HIP library call such as hipPrefetchAsync to request migration, or if a preferred location is set via hipMemAdvise, or if GPU HBM becomes full and the page must forcibly be evicted back to CPU DDR4 to make room for other data.\n\nIf ``HSA_XNACK=1``, page faults in GPU kernels will trigger a page table lookup. If the memory location can be made accessible to the GPU, either by being migrated to GPU HBM or being mapped for remote access, the appropriate action will occur and the access will be replayed. Once a memory region has been migrated to GPU HBM it typically stays there rather than migrating back to CPU DDR4. The exceptions are if the programmer uses a HIP library call such as ``hipPrefetchAsync`` to request migration, or if GPU HBM becomes full and the page must forcibly be evicted back to CPU DDR4 to make room for other data.\n\nMigration of Memory by Allocator and XNACK Mode\n\nMost applications that use \"managed\" or \"unified\" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Crusher. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.\n\nThe page migration behavior summarized by the following tables represents the current, observable behavior. Said behavior will likely change in the near future.\n\nCPU accesses to migratable memory may behave differently than other platforms you're used to. On Crusher, pages will not migrate from GPU HBM to CPU DDR4 based on access patterns alone. Once a page has migrated to GPU HBM it will remain there even if the CPU accesses it, and all accesses which do not resolve in the CPU cache will occur over the Infinity Fabric between the AMD \"Optimized 3rd Gen EPYC\" CPU and AMD MI250X GPU. Pages will only *automatically* migrate back to CPU DDR4 if they are forcibly evicted to free HBM capacity, although programmers may use HIP APIs to manually migrate memory regions.\n\nHSA_XNACK=1 Automatic Page Migration Enabled\n\n+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+\n| Allocator                                   | Initial Physical Location | CPU Access after GPU First Touch           | Default Behavior for GPU Access                    |\n+=============================================+===========================+============================================+====================================================+\n| System Allocator (malloc,new,allocate, etc) | Determined by first touch | Zero copy read/write                       | Migrate to GPU HBM on touch, then local read/write |\n+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+\n| hipMallocManaged                            | GPU HBM                   | Zero copy read/write                       | Populate in  GPU HBM, then local read/write        |\n+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+\n| hipHostMalloc                               | CPU DDR4                  | Local read/write                           | Zero copy read/write over Infinity Fabric          |\n+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+\n| hipMalloc                                   | GPU HBM                   | Zero copy read/write over Inifinity Fabric | Local read/write                                   |\n+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+\n\n\nThe table above provides a comprehensive overview of the different allocators and their corresponding behaviors in terms of physical location and access for both CPU and GPU. The first column lists the different allocators, including the System Allocator, hipMallocManaged, hipHostMalloc, and hipMalloc. The second column indicates the initial physical location of the allocated memory, with all allocators starting in the CPU DDR4. The third column specifies the CPU access after the GPU first touch, with all allocators migrating to the CPU DDR4. The last column outlines the default behavior for GPU access, with all allocators migrating to the GPU HBM on touch. It is worth noting that the System Allocator and hipMallocManaged have the same behavior, while hipHostMalloc and hipMalloc have different behaviors. hipHostMalloc allows for local read/write access, while hipMalloc enables zero copy read/write over Infinity Fabric. This table serves as a useful quick start guide for understanding the different behaviors of allocators and their corresponding memory locations and access. \n\n| Allocator | Initial Physical Location | CPU Access after GPU First Touch | Default Behavior for GPU Access |\n| --- | --- | --- | --- |\n| System Allocator (malloc, new, allocate, etc) | CPU DDR4 | Migrate to CPU DDR4 on touch | Migrate to GPU HBM on touch |\n| hipMallocManaged | CPU DDR4 | Migrate to CPU DDR4 on touch | Migrate to GPU HBM on touch |\n| hipHostMalloc | CPU DDR4 | Local read/write | Zero copy read/write over Infinity Fabric |\n| hipMalloc | GPU HBM | Zero copy read/write over Infinity Fabric | Local read/write |\n\n\n\nDisabling XNACK will not necessarily result in an application failure, as most types of memory can still be accessed by the AMD \"Optimized 3rd Gen EPYC\" CPU and AMD MI250X GPU. In most cases, however, the access will occur in a zero-copy fashion over the Infinity Fabric. The exception is memory allocated through standard system allocators such as malloc, which cannot be accessed directly from GPU kernels without previously being registered via a HIP runtime call such as hipHostRegister. Access to malloc'ed and unregistered memory from GPU kernels will result in fatal unhandled page faults. The table below shows how common allocators behave with XNACK disabled.\n\nHSA_XNACK=0 Automatic Page Migration Disabled\n\n\nThe table provides a quick start guide for using a crusher, specifically focusing on the different allocators available and their default behaviors for CPU and GPU access. The first column lists the different allocators, including the System Allocator and three different types of hipMalloc. The second column indicates the initial physical location for each allocator, with all of them being located in the CPU DDR4. The third and fourth columns detail the default behaviors for CPU and GPU access, respectively. The System Allocator has a default behavior of local read/write for CPU access, meaning it can read from and write to the local memory. However, for GPU access, it has a fatal unhandled page fault, indicating that it cannot handle GPU access. The hipMallocManaged allocator also has a default behavior of local read/write for CPU access, but for GPU access, it allows for zero copy read/write over Infinity Fabric, meaning it can directly access the GPU's memory without copying data. The hipHostMalloc allocator has the same default behaviors as hipMallocManaged. Finally, the hipMalloc allocator has a default behavior of zero copy read/write over Infinity Fabric for GPU access, but for CPU access, it allows for local read/write in the GPU's high-bandwidth memory (HBM). This table provides a comprehensive overview of the different allocators and their default behaviors, making it a useful resource for those looking to use a crusher. \n\n| Allocator | Initial Physical Location | Default Behavior for CPU Access | Default Behavior for GPU Access |\n| --- | --- | --- | --- |\n| System Allocator (malloc, new, allocate, etc) | CPU DDR4 | Local read/write | Fatal Unhandled Page Fault |\n| hipMallocManaged | CPU DDR4 | Local read/write | Zero copy read/write over Infinity Fabric |\n| hipHostMalloc | CPU DDR4 | Local read/write | Zero copy read/write over Infinity Fabric |\n| hipMalloc | GPU HBM | Zero copy read/write over Infinity Fabric | Local read/write |\n\n\n\nCompiling HIP kernels for specific XNACK modes\n\nAlthough XNACK is a capability of the MI250X GPU, it does require that kernels be able to recover from page faults. Both the ROCm and CCE HIP compilers will default to generating code that runs correctly with both XNACK enabled and disabled. Some applications may benefit from using the following compilation options to target specific XNACK modes.\n\nhipcc --amdgpu-target=gfx90a or CC --offload-arch=gfx90a -x hip\n\nKernels are compiled to a single \"xnack any\" binary, which will run correctly with both XNACK enabled and XNACK disabled.\n\nhipcc --amdgpu-target=gfx90a:xnack+ or CC --offload-arch=gfx90a:xnack+ -x hip\n\nKernels are compiled in \"xnack plus\" mode and will only be able to run on GPUs with HSA_XNACK=1 to enable XNACK. Performance may be better than \"xnack any\", but attempts to run with XNACK disabled will fail.\n\nhipcc --amdgpu-target=gfx90a:xnack- or CC --offload-arch=gfx90a:xnack- -x hip\n\nKernels are compiled in \"xnack minus\" mode and will only be able to run on GPUs with HSA_XNACK=0 and XNACK disabled. Performance may be better than \"xnack any\", but attempts to run with XNACK enabled will fail.\n\nhipcc --amdgpu-target=gfx90a:xnack- --amdgpu-target=gfx90a:xnack+ -x hip or CC --offload-arch=gfx90a:xnack- --offload-arch=gfx90a:xnack+ -x hip\n\nTwo versions of each kernel will be generated, one that runs with XNACK disabled and one that runs if XNACK is enabled. This is different from \"xnack any\" in that two versions of each kernel are compiled and HIP picks the appropriate one at runtime, rather than there being a single version compatible with both. A \"fat binary\" compiled in this way will have the same performance of \"xnack+\" with HSA_XNACK=1 and as \"xnack-\" with HSA_XNACK=0, but the final executable will be larger since it contains two copies of every kernel.\n\nIf the HIP runtime cannot find a kernel image that matches the XNACK mode of the device, it will fail with hipErrorNoBinaryForGpu.\n\n$ HSA_XNACK=0 srun -n 1 -N 1 -t 1 ./xnack_plus.exe\n\"hipErrorNoBinaryForGpu: Unable to find code object for all current devices!\"\nsrun: error: crusher002: task 0: Aborted\nsrun: launch/slurm: _step_signal: Terminating StepId=74100.0\n\nNOTE: This works in my shell because I used cpan to install the URI::Encode perl modules.\nThis won't work generically unless those get installed, so commenting out this block now.\n\nThe AMD tool `roc-obj-ls` will let you see what code objects are in a binary.\n\n.. code::\n    $ hipcc --amdgpu-target=gfx90a:xnack+ square.hipref.cpp -o xnack_plus.exe\n    $ roc-obj-ls -v xnack_plus.exe\n    Bundle# Entry ID:                                                              URI:\n    1       host-x86_64-unknown-linux                                           file://xnack_plus.exe#offset=8192&size=0\n    1       hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+                              file://xnack_plus.exe#offset=8192&size=9752\n\nIf no XNACK flag is specificed at compilation the default is \"xnack any\", and objects in `roc-obj-ls` with not have an XNACK mode specified.\n\n.. code::\n    $ hipcc --amdgpu-target=gfx90a square.hipref.cpp -o xnack_any.exe\n    $ roc-obj-ls -v xnack_any.exe\n    Bundle# Entry ID:                                                              URI:\n    1       host-x86_64-unknown-linux                                           file://xnack_any.exe#offset=8192&size=0\n    1       hipv4-amdgcn-amd-amdhsa--gfx90a                                     file://xnack_any.exe#offset=8192&size=9752\n\nOne way to diagnose hipErrorNoBinaryForGpu messages is to set the environment variable AMD_LOG_LEVEL to 1 or greater:\n\n$ AMD_LOG_LEVEL=1 HSA_XNACK=0 srun -n 1 -N 1 -t 1 ./xnack_plus.exe\n:1:rocdevice.cpp            :1573: 43966598070 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966598762 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966599392 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966599970 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966600550 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966601109 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966601673 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966602248 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:hip_code_object.cpp      :460 : 43966602806 us: hipErrorNoBinaryForGpu: Unable to find code object for all current devices!\n:1:hip_code_object.cpp      :461 : 43966602810 us:   Devices:\n:1:hip_code_object.cpp      :464 : 43966602811 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602811 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602812 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602813 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602813 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602814 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602814 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602815 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :468 : 43966602816 us:   Bundled Code Objects:\n:1:hip_code_object.cpp      :485 : 43966602817 us:     host-x86_64-unknown-linux - [Unsupported]\n:1:hip_code_object.cpp      :483 : 43966602818 us:     hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+ - [code object v4 is amdgcn-amd-amdhsa--gfx90a:xnack+]\n\"hipErrorNoBinaryForGpu: Unable to find code object for all current devices!\"\nsrun: error: crusher129: task 0: Aborted\nsrun: launch/slurm: _step_signal: Terminating StepId=74102.0\n\nThe above log messages indicate the type of image required by each device, given its current mode (amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack-) and the images found in the binary (hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+).\n\n\n\nFloating-Point (FP) Atomic Operations and Coarse/Fine Grained Memory Allocations\n\nThe Crusher system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities.\nThe AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs.\nThis new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.\n\nAMD GPUs can allocate two different types of memory locations: 1) Coarse grained and 2) Fine grained.\n\nCoarse grained memory is only guaranteed to be coherent outside of GPU kernels that modify it, enabling higher performance memory operations. Changes applied to coarse-grained memory by a GPU kernel are  only visible to the rest of the system (CPU or other GPUs) when the kernel has completed. A GPU kernel is only guaranteed to see changes applied to coarse grained memory by the rest of the system (CPU or other GPUs) if those changes were made before the kernel launched.\n\nFine grained memory allows CPUs and GPUs to synchronize (via atomics) and coherently communicate with each other while the GPU kernel is running, allowing more advanced programming patterns. The additional visibility impacts the performance of fine grained allocated memory.\n\nThe fast hardware-based Floating point (FP) atomic operations available on MI250X are assumed to be working on coarse grained memory regions; when these instructions are applied to a fine-grained memory region, they will silently produce a no-op. To avoid returning incorrect results, the compiler never emits hardware-based FP atomics instructions by default, even when applied to coarse grained memory regions. Currently, users can use the -munsafe-fp-atomics flag to force the compiler to emit hardware-based FP atomics.\nUsing hardware-based FP atomics translates in a substantial performance improvement over the default choice.\n\nUsers applying floating point atomic operations (e.g., atomicAdd) on memory regions allocated via regular hipMalloc() can safely apply the -munsafe-fp-atomics flags to their codes to get the best possible performance and leverage hardware supported floating point atomics.\nAtomic operations supported in hardware on non-FP datatypes  (e.g., INT32) will work correctly regardless of the nature of the memory region used.\n\nIn ROCm-5.1 and earlier versions, the flag -munsafe-fp-atomics is interpreted as a suggestion by the compiler, whereas from ROCm-5.2 the flag will always enforce the use of fast hardware-based FP atomics.\n\nThe following tables summarize the result granularity of various combinations of allocators, flags and arguments.\n\nFor hipHostMalloc(), the following table shows the nature of the memory returned based on the flag passed as argument.\n\n\nThe table presents information related to the quick start guide for using a crusher. The first column lists the different APIs available, specifically the hipHostMalloc() function. The second column indicates the different flags that can be used with the API, including hipHostMallocDefault and hipHostMallocNonCoherent. The third column provides the results that can be expected when using each flag, with \"Fine grained\" for hipHostMallocDefault and \"Coarse grained\" for hipHostMallocNonCoherent. This table is useful for understanding the different options available when using the hipHostMalloc() function and the corresponding results that can be achieved. It can serve as a reference for users to select the appropriate flag based on their specific needs and desired outcome. \n\n| API             | Flag                  | Results       |\n|-----------------|-----------------------|---------------|\n| hipHostMalloc() | hipHostMallocDefault  | Fine grained  |\n| hipHostMalloc() | hipHostMallocNonCoherent | Coarse grained |\n\n\n\nThe following table shows the nature of the memory returned based on the flag passed as argument to hipExtMallocWithFlags().\n\n\nThe table presents a quick start guide for using the crusher API, specifically the hipExtMallocWithFlags() function. This function allows for the allocation of memory on a device using different flags to specify the type of memory to be allocated. The first row of the table shows the use of the hipDeviceMallocDefault flag, which results in coarse grained memory allocation. This type of allocation is suitable for large data sets and is optimized for high bandwidth operations. The second row of the table uses the hipDeviceMallocFinegrained flag, which results in fine grained memory allocation. This type of allocation is suitable for smaller data sets and is optimized for low latency operations. By using the appropriate flag, users can optimize their memory allocation for their specific needs. \n\n| API | Flag | Result |\n| --- | --- | --- |\n| hipExtMallocWithFlags() | hipDeviceMallocDefault | Coarse grained |\n| hipExtMallocWithFlags() | hipDeviceMallocFinegrained | Fine grained |\n\n\n\nFinally, the following table summarizes the nature of the memory returned based on the flag passed as argument to hipMallocManaged() and the use of CPU regular malloc() routine with the possible use of hipMemAdvise().\n\n\nThe table above is a quick start guide for using the crusher API, specifically focusing on the hipMallocManaged() and malloc() functions. The first column lists the different API calls, while the second column provides information on the corresponding memory advice (MemAdvice) that can be used with the API. The third column indicates the expected result when using the API with the specified memory advice. \n\nThe first row of the table shows that using the hipMallocManaged() function without any memory advice will result in fine-grained memory allocation. This means that the memory will be divided into smaller chunks, allowing for more efficient use of memory. \n\nThe second row demonstrates that using the hipMallocManaged() function with the hipMemAdviseSetCoarseGrain memory advice will result in coarse-grained memory allocation. This means that the memory will be allocated in larger chunks, which may be more suitable for certain applications. \n\nThe third row shows that using the malloc() function without any memory advice will also result in fine-grained memory allocation. This is similar to the first row, but using a different API. \n\nFinally, the last row indicates that using the malloc() function with the hipMemAdviseSetCoarseGrain memory advice will also result in coarse-grained memory allocation. This is similar to the second row, but using a different API. \n\nIn summary, this table provides a clear overview of the different memory allocation options available when using the crusher API, allowing users to choose the most suitable option for their specific needs.\n\n| API | MemAdvice | Result |\n| --- | --- | --- |\n| hipMallocManaged() |  | Fine grained |\n| hipMallocManaged() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained |\n| malloc() |  | Fine grained |\n| malloc() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained |\n\nPerformance considerations for LDS FP atomicAdd()\n\nHardware FP atomic operations performed in LDS memory are usually always faster than an equivalent CAS loop, in particular when contention on LDS memory locations is high.\nBecause of a hardware design choice, FP32 LDS atomicAdd() operations can be slower than equivalent FP64 LDS atomicAdd(), in particular when contention on memory locations is low (e.g. random access pattern).\nThe aforementioned behavior is only true for FP atomicAdd() operations. Hardware atomic operations for CAS/Min/Max on FP32 are usually faster than the FP64 counterparts.\nIn cases when contention is very low, a FP32 CAS loop implementing an atomicAdd() operation could be faster than an hardware FP32 LDS atomicAdd().\nApplications using single precision FP atomicAdd() are encouraged to experiment with the use of double precision to evaluate the trade-off between high atomicAdd() performance vs. potential lower occupancy due to higher LDS usage.\n\n\n\nSystem Updates\n\n2023-09-19\n\nOn Tuesday, September 19, 2023, Crusher's system software was upgraded. The following changes took place:\n\nThe system was upgraded to Slingshot Host Software 2.1.0.\n\nROCm 5.6.0 and 5.7.0 are now available via the rocm/5.6.0 and rocm/5.7.0 modulefiles, respectively.\n\nHPE/Cray Programming Environments (PE) 23.09 is now available via the cpe/23.09 modulefile.\n\nROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.\n\n2023-07-18\n\nOn Tuesday, July 18, 2023, the Crusher TDS was upgraded to a new version of the system software stack. During the upgrade, the following changes took place:\n\nThe system was upgraded to Cray OS 2.5, Slingshot Host Software 2.0.2-112, and the AMD GPU 6.0.5 device driver (ROCm 5.5.1 release).\n\nROCm 5.5.1 is now available via the rocm/5.5.1 modulefile.\n\nHPE/Cray Programming Environments (PE) 23.05 is now available via the cpe/23.05 modulefile.\n\nHPE/Cray PE 23.05 introduces support for ROCm 5.5.1. However, due to issues identified during testing, ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.\n\n2023-04-05\n\nOn Wednesday, April 5, 2023, the Crusher TDS was upgraded to a new software stack.  A summary of the changes is included below.\n\nThe operating system was upgraded to Cray OS 2.4 based on SLES 15.4.\n\nHPE/Cray Programming Environment (PE):\n\nPE 22.12 was installed and is now default. This PE includes the following components:\n\nCray MPICH 8.1.23\n\nCray Libsci 22.12.1.1\n\nCCE 15.0.0\n\nPE 23.03 is now also available and includes:\n\nCray MPICH 8.1.25\n\nCray Libsci 23.02.1.1\n\nCCE 15.0.1\n\nROCm:\n\nROCm 5.3.0 is now default.\n\nROCm 5.4.0 and 5.4.3 are available.\n\nFile Systems:\n\nThe Orion Lustre parallel file system is now available on Crusher.\n\nThe Alpine GPFS file system remains available but will be permanently unmounted from Crusher on Tuesday, April 18, 2023. Please begin moving your data to the Orion file system as soon as possible.\n\n2022-12-29\n\nOn Thursday, December 29, 2022 the following system configuration settings will be updated on Crusher:\n\nLow-Noise Mode will be enabled: as a result, system processes will be constrained to core 0 on every node.\n\nSlurm's core specialization default will change: Slurm --core-spec or -S value will be set to 8. This will provide a symmetric distribution of cores per GCD to the application and will reserve one core per L3 cache region. After the outage, the default number of cores available to each GCD on a node will be 7. To change from the new default value, you can set --core-spec or -S in your job submission.\n\nThe default NIC mapping will be updated to MPICH_OFI_NIC_POLICY=NUMA to address known issues described in OLCFDEV-192 and OLCFDEV-1366.\n\n\n\nGetting Help\n\nIf you have problems or need helping running on Crusher, please submit a ticket\nby emailing help@olcf.ornl.gov.\n\n\n\nKnown Issues\n\nJIRA_CONTENT_HERE"}
{"doc":"cupy","text":"Installing CuPy\n\nThis guide has been adapted for Frontier following venv syntax. If you\nare using a personal Miniconda distribution on Frontier <https://docs.olcf.ornl.gov/software/python/miniconda.html>,\nthe workflow will be similar to the Summit or Andes conda scenario.  If\nthat is your use-case, then ignore the mention of the cray-python module in\nthe workflow (other modules still apply).\n\nThis guide has been adapted from a challenge in OLCF's Hands-On with Summit GitHub repository (Python: CuPy Basics).\n\nThe guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.\n\nOverview\n\nThis guide teaches you how to build CuPy from source into a custom virtual environment.\nOn Summit and Andes, this is done using conda, while on Frontier this is done using venv.\n\nIn this guide, you will:\n\nLearn how to install CuPy\n\nLearn the basics of CuPy\n\nCompare speeds to NumPy\n\nOLCF Systems this guide applies to:\n\nSummit\n\nFrontier\n\nAndes\n\nCuPy\n\nGPU computing has become a big part of the data science landscape, as array operations with NVIDIA GPUs can provide considerable speedups over CPU computing.\nAlthough GPU computing on Summit is often utilized in codes that are written in Fortran and C, GPU-related Python packages are quickly becoming popular in the data science community.\nOne of these packages is CuPy, a NumPy/SciPy-compatible array library accelerated with NVIDIA CUDA.\n\nCuPy is a library that implements NumPy arrays on NVIDIA GPUs by utilizing CUDA Toolkit libraries like cuBLAS, cuRAND, cuSOLVER, cuSPARSE, cuFFT, cuDNN and NCCL.\nAlthough optimized NumPy is a significant step up from Python in terms of speed, performance is still limited by the CPU (especially at larger data sizes) -- this is where CuPy comes in.\nBecause CuPy's interface is nearly a mirror of NumPy, it acts as a replacement to run existing NumPy/SciPy code on NVIDIA CUDA platforms, which helps speed up calculations further.\nCuPy supports most of the array operations that NumPy provides, including array indexing, math, and transformations.\nMost operations provide an immediate speed-up out of the box, and some operations are sped up by over a factor of 100 (see CuPy benchmark timings below, from the Single-GPU CuPy Speedups article).\n\n\n\nCompute nodes equipped with NVIDIA GPUs will be able to take full advantage of CuPy's capabilities on the system, providing significant speedups over NumPy-written code.\nCuPy with AMD GPUs is still being explored, and the same performance is not guaranteed (especially with larger data sizes).\nInstructions for Frontier are available in this guide, but users must note that the CuPy developers have labeled this method as experimental and has limitations.\n\n\n\nInstalling CuPy\n\n<string>:60: (INFO/1) Duplicate implicit target name: \"installing cupy\".\n\nBefore setting up your environment, you must exit and log back in so that you have a fresh login shell.\nThis is to ensure that no previously activated environments exist in your $PATH environment variable.\nAdditionally, you should execute module reset.\n\nBuilding CuPy from source is highly sensitive to the current environment variables set in your profile.\nBecause of this, it is extremely important that all the modules and environments you plan to load are done in the correct order, so that all the environment variables are set correctly.\n\nFirst, load the gnu compiler module (most Python packages assume GCC), relevant GPU module (necessary for CuPy), and the python module (allows you to create a new environment):\n\nSummit\n\n.. code-block:: bash\n\n   $ module load gcc/7.5.0 # might work with other GCC versions\n   $ module load cuda/11.0.2\n   $ module load python\n\nFrontier\n\n.. code-block:: bash\n\n   $ module load PrgEnv-gnu\n   $ module load amd-mixed/5.3.0\n   $ module load craype-accel-amd-gfx90a\n   $ module load cray-python # only if not using Miniconda on Frontier\n\n.. note::\n   If you are using a `Miniconda distribution on Frontier <https://docs.olcf.ornl.gov/software/python/miniconda.html>`, the above ``module load cray-python`` should not be loaded.\n\nAndes\n\n.. code-block:: bash\n\n   $ module load gcc/9.3.0 # works with older GCC versions if using cuda/10.2.89\n   $ module load cuda/11.0.2\n   $ module load python\n\nLoading a python module puts you in a \"base\" environment, but you need to create a new environment using the conda create command (Summit and Andes) or the venv command (Frontier):\n\nSummit\n\n.. code-block:: bash\n\n   $ conda create -p /ccs/proj/<project_id>/<user_id>/envs/summit/cupy-summit python=3.10\n\nFrontier\n\n.. code-block:: bash\n\n   $ python3 -m venv /ccs/proj/<project_id>/<user_id>/envs/frontier/cupy-frontier\n\nAndes\n\n.. code-block:: bash\n\n   $ conda create -p /ccs/proj/<project_id>/<user_id>/envs/andes/cupy-andes python=3.10\n\nAs noted in the python <https://docs.olcf.ornl.gov/software/python/index.html> page, it is highly recommended to create new environments in the \"Project Home\" directory.\n\nAfter following the prompts for creating your new environment, you can now activate it:\n\nSummit\n\n.. code-block:: bash\n\n   $ source activate /ccs/proj/<project_id>/<user_id>/envs/summit/cupy-summit\n\nFrontier\n\n.. code-block:: bash\n\n   $ source /ccs/proj/<project_id>/<user_id>/envs/frontier/cupy-frontier/bin/activate\n\nAndes\n\n.. code-block:: bash\n\n   $ source activate /ccs/proj/<project_id>/<user_id>/envs/andes/cupy-andes\n\nCuPy depends on NumPy, so let's install an optimized version of NumPy into your fresh environment:\n\nSummit\n\n.. code-block:: bash\n\n   $ conda install -c defaults --override-channels numpy scipy\n\nFrontier\n\n.. code-block:: bash\n\n   $ pip install --no-cache-dir --upgrade pip\n   $ pip install numpy scipy --no-cache-dir\n\nAndes\n\n.. code-block:: bash\n\n   $ conda install -c defaults --override-channels numpy scipy\n\nAfter following the prompts, NumPy and its linear algebra dependencies should successfully install.\nSciPy is an optional dependency, but it would allow you to use the additional SciPy-based routines in CuPy:\n\nFinally, install CuPy from source into your environment.\nTo make sure that you are building from source, and not a pre-compiled binary, use pip:\n\nCuPy v13.0.0 removed support for CUDA 10.2, 11.0, and 11.1.\nPlease try installing CuPy<13.0.0 if you run into issues with older CUDA versions.\nSee CuPy Release Notes for more details\nand other compatibility changes.\n\nSummit\n\n.. code-block:: bash\n\n   $ CC=gcc NVCC=nvcc pip install --no-cache-dir --no-binary=cupy cupy\n\nFrontier\n\n.. code-block:: bash\n\n   $ export CUPY_INSTALL_USE_HIP=1\n   $ export ROCM_HOME=/opt/rocm-5.3.0\n   $ export HCC_AMDGPU_TARGET=gfx90a\n   $ CC=gcc pip install --no-cache-dir --no-binary=cupy cupy\n\nAndes\n\n.. code-block:: bash\n\n   $ salloc -A PROJECT_ID -N1 -p gpu -t 01:00:00\n   $ export all_proxy=socks://proxy.ccs.ornl.gov:3128/\n   $ export ftp_proxy=ftp://proxy.ccs.ornl.gov:3128/\n   $ export http_proxy=http://proxy.ccs.ornl.gov:3128/\n   $ export https_proxy=http://proxy.ccs.ornl.gov:3128/\n   $ export no_proxy='localhost,127.0.0.0/8,*.ccs.ornl.gov'\n   $ CC=gcc NVCC=nvcc pip install --no-cache-dir --no-binary=cupy cupy\n\n.. note::\n    To be able to build CuPy on Andes, you must be within a compute job\n    on the GPU partition (even if you have the cuda module loaded).\n    This allows CuPy to see the GPU properly when linking and building.\n\nThe CC and NVCC flags ensure that you are passing the correct wrappers, while the various flags for Frontier tell CuPy to build for AMD GPUs.\nNote that, on Summit, if you are using the instructions for installing CuPy with OpenCE below, the cuda/11.0.3 module will automatically be loaded.\nThis installation takes, on average, 10-20 minutes to complete (due to building everything from scratch), so don't panic if it looks like the install timed-out.\nEventually you should see output similar to this (versions will vary):\n\nSuccessfully installed cupy-12.2.0 fastrlock-0.8.1\n\nInstalling CuPy in an OpenCE Environment (Summit only)\n\nIf you wish to use CuPy within a clone of the OpenCE environment, the installation process is very similar to what we do in the regular CuPy installation we saw above.\n\nThe open-ce/1.2.0-pyXY-0 (which is the current default) will not support this. So make sure you are using open-ce/1.5.0-pyXY-0 or higher.\n\nThe contents of the open-ce module cannot be modified so you need to make your own clone of the open-ce environment.\n\n$ module purge\n$ module load DefApps\n$ module unload xl\n$ module load open-ce/1.5.2-py39-0\n$ conda create --clone open-ce-1.5.2-py39-0 -p /ccs/proj/<project_id>/<user_id>/envs/summit/opence_cupy_summit\n$ conda activate /ccs/proj/<project_id>/<user_id>/envs/summit/opence_cupy_summit\n\nNext, install CuPy the way you did before. This installation will use the system GCC /usr/bin/gcc which is currently 8.3.1.\n\n$ CC=gcc NVCC=nvcc pip install --no-binary=cupy cupy\n\nNow, everytime you want to use this environment with CuPy on a new login or in a job, you will have to do the sequence of the following\n\nmodule purge\nmodule load DefApps\nmodule unload xl\nmodule load open-ce/1.5.2-py39-0\nconda activate /ccs/proj/<project_id>/<user_id>/envs/summit/opence_cupy_summit\n\nGetting Started With CuPy\n\nAssuming you are continuing from the previous sections, you do not need to\nload any modules. Otherwise, you need to load the modules associated with your\nsystem covered in the Installing CuPy section <cupy-envs> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Installing CuPy section <cupy-envs>>.\n\nWhen a kernel call is required in CuPy, it compiles a kernel code optimized for the shapes and data types of given arguments, sends it to the GPU device, and executes the kernel.\nDue to this, CuPy runs slower on its initial execution.\nThis slowdown will be resolved at the second execution because CuPy caches the kernel code sent to GPU device.\nBy default, the compiled code is cached to the $HOME/.cupy/kernel_cache directory, which the compute nodes will not be able to access.\nIt is good practice to change it to your scratch directory:\n\n$ export CUPY_CACHE_DIR=\"/gpfs/alpine/scratch/<YOUR_USER_ID>/<YOUR_PROJECT_ID>/.cupy/kernel_cache\"\n\nBefore you start testing CuPy with Python scripts, let's go over some of the basics.\nThe developers provide a great introduction to using CuPy in their user guide under the CuPy Basics section.\nWe will be following this walkthrough on Summit.\nThe syntax below assumes being in a Python shell with access to 4 GPUs (through a jsrun -g4 ... command).\n\nOn Frontier, running in an interactive job will return 8 GPUs available to CuPy.\n\nAs is the standard with NumPy being imported as \"np\", CuPy is often imported in a similar fashion:\n\n>>> import numpy as np\n>>> import cupy as cp\n\nSimilar to NumPy arrays, CuPy arrays can be declared with the cupy.ndarray class.\nNumPy arrays will be created on the CPU (the \"host\"), while CuPy arrays will be created on the GPU (the \"device\"):\n\n>>> x_cpu = np.array([1,2,3])\n>>> x_gpu = cp.array([1,2,3])\n\nManipulating a CuPy array can also be done in the same way as manipulating NumPy arrays:\n\n>>> x_cpu*2.\narray([2., 4., 6.])\n>>> x_gpu*2.\narray([2., 4., 6.])\n>>> l2_cpu = np.linalg.norm(x_cpu)\n>>> l2_gpu = cp.linalg.norm(x_gpu)\n>>> print(l2_cpu,l2_gpu)\n3.7416573867739413 3.7416573867739413\n\nUseful functions for initializing arrays like np.linspace, np.arange, and np.zeros also have a CuPy equivalent:\n\n>>> cp.zeros(3)\narray([0., 0., 0.])\n>>> cp.linspace(0,10,11)\narray([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n>>> cp.arange(0,11,1)\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n\nCuPy has a concept of a \"current device\", which is the current activated GPU device that will operate on an array or where future arrays will be allocated.\nMost of the time, if not explicitly declared or switched, the initial default device will be GPU 0.\nTo find out what device a CuPy array is allocated on, you can call the cupy.ndarray.device attribute:\n\n>>> x_gpu.device\n<CUDA Device 0>\n\nTo get a total number of devices that you can access, use the getDeviceCount function:\n\n>>> cp.cuda.runtime.getDeviceCount()\n4\n\nThe current device can be switched using cupy.cuda.Device(<DEVICE_ID>).use():\n\n>>> cp.cuda.Device(1).use()\n>>> x_gpu_1 = cp.array([1, 2, 3, 4, 5])\n>>> x_gpu_1.device\n<CUDA Device 1>\n\nSimilarly, you can temporarily switch to a device using the with context:\n\n>>> cp.cuda.Device(0).use()\n>>> with cp.cuda.Device(3):\n...    x_gpu_3 = cp.array([1, 2, 3, 4, 5])\n...\n>>> x_gpu_0 = cp.array([1, 2, 3, 4, 5])\n>>> x_gpu_0.device\n<CUDA Device 0>\n>>> x_gpu_3.device\n<CUDA Device 3>\n\nTrying to perform operations on an array stored on a different GPU will result in an error:\n\nThe below code block should not be run on Frontier, as it causes problems for the\nsubsequent code blocks further below. With recent updates to CuPy, peer access is\nenabled by default, which \"passes\" the below error. This causes problems with\nAMD GPUs, resulting in inaccurate data.\n\n>>> with cp.cuda.Device(0):\n...    x_gpu_0 = cp.array([1, 2, 3, 4, 5]) # create an array in GPU 0\n...\n>>> with cp.cuda.Device(1):\n...    x_gpu_0 * 2  # ERROR: trying to use x_gpu_0 on GPU 1\n...\nPerformanceWarning: The device where the array resides (0) is different from the current device (1). Peer access has been activated automatically.\n\nTo solve the above warning/error, you must transfer x_gpu_0 to \"Device 1\".\nA CuPy array can be transferred to a specific GPU using the cupy.asarray() function while on the specific device:\n\n>>> with cp.cuda.Device(1):\n...    cp.asarray(x_gpu_0) * 2  # fixes the error, moves x_gpu_0 to GPU 1\n...\narray([ 2,  4,  6,  8, 10])\n\nA NumPy array on the CPU can also be transferred to a GPU using the same cupy.asarray() function:\n\n>>> x_cpu = np.array([1, 1, 1]) # create an array on the CPU\n>>> x_gpu = cp.asarray(x_cpu)  # move the CPU array to the current device\n>>> x_gpu\narray([1, 1, 1])\n\nTo transfer from a GPU back to the CPU, you use the cupy.asnumpy() function instead:\n\n>>> x_gpu = cp.zeros(3)  # create an array on the current device\n>>> x_cpu = cp.asnumpy(x_gpu)  # move the GPU array to the CPU\n>>> x_cpu\narray([ 0., 0., 0.])\n\nAssociated with the concept of current devices are current \"streams\".\nIn CuPy, all CUDA operations are enqueued onto the current stream, and the queued tasks on the same stream will be executed in serial (but asynchronously with respect to the CPU).\nThis can result in some GPU operations finishing before some CPU operations.\nAs CuPy streams are out of the scope of this guide, you can find additional information in the CuPy User Guide.\n\nNumPy Speed Comparison (Summit only)\n\nAs noted in AMD+CuPy limitations,\ndata sizes explored here hang. So, this section currently does not apply to Frontier.\n\nNow that you know how to use CuPy, time to see the actual benefits that CuPy provides for large datasets.\nMore specifically, let's see how much faster CuPy can be than NumPy on Summit.\nYou won't need to fix any errors; this is mainly a demonstration on what CuPy is capable of.\n\nThere are a few things to consider when running on GPUs, which also apply to using CuPy:\n\nHigher precision means higher cost (time and space)\n\nThe structuring of your data is important\n\nThe larger the data, the better for GPUs (but needs careful planning)\n\nThese points are explored in the example script timings.py:\n\n# timings.py\nimport cupy as cp\nimport numpy as np\nimport time as tp\n\nA      = np.random.rand(3000,3000) # NumPy rand\nG      = cp.random.rand(3000,3000) # CuPy rand\nG32    = cp.random.rand(3000,3000,dtype=cp.float32) # Create float32 matrix instead of float64 (default)\nG32_9k = cp.random.rand(9000,1000,dtype=cp.float32) # Create float32 matrix of a different shape\n\nt1 = tp.time()\nnp.linalg.svd(A) # NumPy Singular Value Decomposition\nt2 = tp.time()\nprint(\"CPU time: \", t2-t1)\n\nt3 = tp.time()\ncp.linalg.svd(G) # CuPy Singular Value Decomposition\ncp.cuda.Stream.null.synchronize() # Waits for GPU to finish\nt4 = tp.time()\nprint(\"GPU time: \", t4-t3)\n\nt5 = tp.time()\ncp.linalg.svd(G32)\ncp.cuda.Stream.null.synchronize()\nt6 = tp.time()\nprint(\"GPU float32 time: \", t6-t5)\n\nt7 = tp.time()\ncp.linalg.svd(G32_9k)\ncp.cuda.Stream.null.synchronize()\nt8 = tp.time()\nprint(\"GPU float32 restructured time: \", t8-t7)\n\nThis script times the decomposition of a matrix with 9 million elements across four different methods.\nFirst, NumPy is timed for a 3000x3000 dimension matrix.\nThen, a 3000x3000 matrix in CuPy is timed.\nAs you will see shortly, the use of CuPy will result in a major performance boost when compared to NumPy, even though the matrices are structured the same way.\nThis is improved upon further by switching the data type to float32 from float64 (the default).\nLastly, a 9000x1000 matrix is timed, which contains the same number of elements as the original matrix, just rearranged.\nAlthough you may not expect it, the restructuring results in a big performance boost as well.\n\nBefore asking for a compute node, change into your GPFS scratch directory:\n\n$ cd $MEMBERWORK/<YOUR_PROJECT_ID>\n$ mkdir cupy_test\n$ cd cupy_test\n\nLet's see the boosts explicitly by running the timings.py script.\nTo do so, you must submit submit_timings to the queue:\n\nSummit\n\n.. code-block:: bash\n\n   $ bsub -L $SHELL submit_timings.lsf\n\nExample \"submit_timings\" batch script:\n\nSummit\n\n.. code-block:: bash\n\n   #!/bin/bash\n   #BSUB -P <PROJECT_ID>\n   #BSUB -W 00:05\n   #BSUB -nnodes 1\n   #BSUB -J cupy_timings\n   #BSUB -o cupy_timings.%J.out\n   #BSUB -e cupy_timings.%J.err\n\n   cd $LSB_OUTDIR\n   date\n\n   module load gcc/7.5.0\n   module load cuda/11.0.3\n   module load python\n\n   source activate /ccs/proj/<project_id>/<user_id>/envs/summit/cupy-summit\n   export CUPY_CACHE_DIR=\"${MEMBERWORK}/<project_id>/.cupy/kernel_cache\"\n\n   jsrun -n1 -g1 python3 timings.py\n\nAfter the job completes, in cupy_timings.<JOB_ID>.out you will see something similar to:\n\nCPU time:  21.632022380828857\nGPU time:  11.382664203643799\nGPU float32 time:  4.066986799240112\nGPU float32 restructured time:  0.8666532039642334\n\nThe exact numbers may be slightly different, but you should see a speedup factor of approximately 2 or better when comparing \"GPU time\" to \"CPU time\".\nSwitching to float32 was easier on memory for the GPU, which improved the time further.\nThings are even better when you look at \"GPU float32 restructured time\", which represents an additional factor of 4 speedup when compared to \"GPU float32 time\".\nOverall, using CuPy and restructuring the data led to a speedup factor of >20 when compared to traditional NumPy!\nThis factor would diminish with smaller datasets, but represents what CuPy is capable of at this scale.\n\nYou have now discovered what CuPy can provide!\nNow you can try speeding up your own codes by swapping CuPy and NumPy where you can.\n\nAdditional Resources\n\nCuPy User Guide\n\nCuPy Website\n\nCuPy API Reference\n\nCuPy Release Notes"}
{"doc":"debugging","text":"Debugging\n\nDebug a pod in CrashLoopBackoff\n\nIf we have a pod that is crash looping then it is exiting too quickly to spawn a shell inside the container. We can use oc debug to start a pod with that same image but instead of the image entrypoint we use /bin/sh instead.\n\n$ oc debug misbehaving-pod-1\nDefaulting container name to bad.\nUse 'oc describe pod/misbehaving-pod-1' to see all of the containers in this pod.\n\nDebugging with pod/misbehaving-pod-1, original command: <image entrypoint>\nWaiting for pod to start ...\nIf you don't see a command prompt, try pressing enter.\n/ $\n\nGet a shell inside a pod\n\nWhat if we want to get a shell inside of the container to debug?\n\noc run centos-debug-test --restart=Never --image=centos -- /usr/bin/sleep 1d\n\nLets make sure it successfully gets scheduled and started, look for Status: Running\n\noc describe pod centos-debug-test\n\nNow we have a running pod, lets check it out:\n\noc exec -it centos-debug-test /bin/bash\n\nNow lets delete the pod:\n\noc delete pod centos-debug-test"}
{"doc":"deployment","text":"Deployments\n\n\n\nOn top of Kubernetes ReplicaSets, OpenShift gives us even more support for software\nlifecycle with Deployments. A Deployment creates a ReplicaSet and has the added\nbenefit of controlling how new deployments get triggered and deployed.\n\nDeployments are sufficient for deploying a production service\n\nDeployments manages ReplicaSets which in turn manages a set of Pods\n\nCreating a Deployment\n\nBelow is an example Deployment:\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      name: my-app\n  template: { ... }\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 2\n  minReadySeconds: 0\n  paused: false\n\nLet's look at the individual parts of this definition, under spec.\n\nreplicas - the number of replicas to be passed down to the ReplicaSet\n\nselector - the selector to determine which pods are managed by the ReplicaSet.\n\ntemplate - a pod definition. Note that this pod template must have the selector from above in its metadata.labels.\n\nstrategy - see slate_deployment_strategies <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_deployment_strategies>\n\nrevisionHistoryLimit - the number of old revisions of ReplicaSets to keep around.\nThis can be used to have a rollback plan in case of a bad deployment.\n\nminReadySeconds - number of seconds after the readiness check succeeds that the pod is considered\n\"available\". By default, this is 0 and may be omitted.\n\npaused - set this to \"true\" to pause a deployment\n\nCreating Deployments\n\nHere is an example completed Deployment.\n\napiVersion: v1\nkind: Deployment\nmetadata:\n  name: recreate-example\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      deployment: recreate-example\n  strategy:\n    # We set the type of strategy to Recreate, which means that it will be scaled down prior to being scaled up\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        deployment: recreate-example\n    spec:\n      containers:\n      - image: openshift/deployment-example:v1\n        name: deployment-example\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n\nAs with other objects in OpenShift, you can create this Deployment with oc create -f {FILE_NAME}.\n\nOnce created, you can check the status of the ongoing deployment process with the command\n\noc rollout status deploy/{NAME}\n\nTo get basic information about the available revisions, if you had done any updates to the deployment since, run\n\noc rollout history deploy/{NAME}\n\nYou can view a specific revision with\n\noc rollout history deploy/{NAME} --revision=1\n\nFor more detailed information about a Deployment, use\n\noc describe deploy {NAME}\n\nTo roll back a deployment, run\n\noc rollout undo deploy/{NAME}\n\nWeb Interface\n\nWhen using the web interface, you can view and edit a Deployment, from the\nsidebar, go to Applications, then Deployments.\n\nDeployment Menu\n\nYou can get info on any deployment by clicking on it.\n\nTo edit this configuration, click Actions in the upper right hand corner, then Edit on whatever you wish to edit, or Edit Deployment if\nyou'd rather edit the YAML directly.\n\nEdit Deployment Config\n\n\n\nDeployment Strategies\n\nA deployment strategy is a method for upgrading an application. The goal of deployment strategies is\nto make an update with no downtime to the end users.\n\nThe two most common values here will be RollingUpdate and Recreate. The default is RollingUpdate.\n\nSince the end user usually will be accessing an application with a route, the\ndeployment strategy can focus on deployment configuration features. Here are a few examples of the\ndeployment configuration based strategies.\n\nAll of the below strategies use readiness checks to determine if a new pod is ready for use. If\nany readiness check fails, the deployment configuration will continue to try to run the pod until it\ntimes out. The default timeout is 10m. This value can be set in deployment.spec.strategy.params.TimeoutSeconds\n\nThe default strategy, if omitted, is RollingUpdate\n\nRolling Strategy\n\nA rolling deployment slowly replaces instances of the previous version with instances of the new\nversion. This deployment waits for new pods to become ready before scaling down the old replication\ncontroller. This strategy is easily aborted and reverted.\n\nA rolling deployment is best used when you want to take no downtime during an update, but you know your\napplication can support having old and new code running at the same time.\n\nHere is an example Rolling deployment:\n\nstrategy:\n  type: RollingUpdate\n  rollingUpdate:\n    maxSurge: \"20%\"\n    maxUnavailable: \"10%\"\n\nmaxSurge and maxUnavailable - maxUnavailable is the maximum number of pods that can be unavailable\nduring the update. maxSurge is the number of pods that can be scheduled above the original number\nof pods. Both values can be set to either a percentage (20%) or a positive integer (2). The default\nvalue for both is 25%.\n\nThese values can be used to tune a deployment for speed or availability. If you want to maintain full\ncapacity, set maxUnavailable to 0. The maxSurge value can be used to speed up the scale up. Note\nthat you still must stay below your project's pod quota.\n\nA RollingUpdate strategy follows this sequence:\n\nScale up the new ReplicaSet based on maxSurge.\n\nScale down the old ReplicaSet based on maxUnavailable.\n\nRepeat the scaling until the new replication controller has the desired replica count and the old\nreplication controller has 0.\n\nDuring the scale down, the strategy waits for pods to become \"ready\" to determine if scaling down\nmore will affect availability. If the new pods don't become \"ready\", the deployment will eventually\ntime out and revert to the old deployment.\n\nRecreate Strategy\n\nA recreate deployment scales the previous deployment down to 0 before starting the new deployment.\nThis is best used when a downtime is acceptable, and your application cannot handle having the old\nand new versions running at the same time.\n\nHere is an example recreate deployment:\n\nstrategy:\n  type: Recreate\n\nThe recreate strategy follows this sequence:\n\nScale down the old deployment to 0 replicas.\n\nScale up the new deployment to the number of desired replicas.\n\nMore Advanced Deployment Strategies with Routes\n\nWhile some strategies leverage features of Deployments, others leverage features of\nroutes. If you haven't read the docs on slate_services <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_services> or slate_routes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_routes>,\nread those first before trying these more advanced strategies.\n\nSince routes are intended for HTTP and HTTPS traffic, these strategies are best used for\nweb applications.\n\nBlue-Green Deployment\n\nBlue-green deployments are defined as running two versions of an application at the same time, then\nmoving traffic from the old production version (the green version) to the new production version (the\nblue version). You could use a Rolling Deployment Strategy for this, but for the\nsake of showing how route-based deployments work, we'll use a route.\n\nBlue-green deployment requires that your application can handle both old and new versions\nrunning at the same time. Be sure to think about your application and if it can handle this. For example, if the\nnew version of the software changes how a certain field in a database is read and written, then the old\nversion of the software won't be able to read the database changes, and your production instance could\nbreak. This is known as \"N-1 compatibility\" or \"backward compatibility\".\n\nThe below examples use commands to create everything for demonstration purposes but the\ngeneral best practice is to use yaml files for for all your deploying and configuration.\n\nCreate two copies of your application, one for the old service (green) and one for the\nnew (blue). Note that the image you are using here my-app:v<n> must have a port\nexposed with the EXPOSE <portnumber> in its Dockerfile when you built it. Otherwise,\nmake sure you create a service for it after the below commands. The below command\ncreates two DeploymentConfigs my-app-green and my-app-blue.\n\noc new-app my-app:v1 --name=my-app-green\noc new-app my-app:v2 --name=my-app-blue\n\nIf a service wasn't created automatically, create a service with the below commands,\nwhere the --port refers to the port the service exposes to incoming traffic, and the --target-port\nrefers to the port in the running pod to direct traffic to.\n\noc expose dc/my-app-blue --port=80 --target-port=8080\noc expose dc/my-app-green --port=80 --target-port=8080\n\nCreate a route which points to the old service (this is assuming your application)\n\noc create route edge my-app-route --service=my-app-green --hostname=my-app.apps.<cluster>.ccs.ornl.gov\n\nBrowse to your project at my-app.apps.<cluster>.ccs.ornl.gov and verify that the v1 version is displayed.\n\nEdit the route and change the service name to my-app-blue\n\noc patch route/my-app-route -p '{\"spec\":{\"to\":{\"name\":\"my-app-blue\"}}}'\n\nVerify the change has taken effect by refreshing your browser until you see the new version.\n\nA/B Deployment\n\nA/B Deployments are a popular way to try a new version of an application with a small subset of users\nin the production environment. With this strategy, you can specify that the older version gets most\nof the user requests while a limited fraction of users get sent to the new version. Since you can\ncontrol the amount of users which get sent to the new version, you can gradually increase the volume\nof requests to the new version and eventually stop using the old version. Remember that deployment\nconfigurations don't do any autoscaling of pods, so you may have to adjust the number of pod replicas\nfor each version to deal with the increased/decreased load.\n\nAs with blue-green deployment, A/B deployments require that your application has N-1 compatibility.\n\nTo set up an A/B environment:\n\nCreate the two applications and give them different names\n\noc new-app my-app:v1 --name=my-app-a\noc new-app my-app:v2 --name=my-app-b\n\nCreate a route to the A service (assuming the my-app configuration contains a service)\n\noc create route edge --service=my-app-a --name=my-app\n\nBrowse to the application at my-app.{PROJECT}.{CLUSTER}.ccs.ornl.gov to verify that you see the A version.\n\nEdit the route to include the second service with alternateBackends (see the slate_routes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_routes> for more information)\n\n   oc edit route my-app\n   ...\n   metadata:\n       name: my-app\n       annotations:\n           haproxy.router.openshift.io/balance: roundrobin\n   spec:\n       host: my-app.{PROJECT}.{CLUSTER}.ccs.ornl.gov\n       to:\n           kind: Service\n           name: my-app-a\n           weight: 10\n       alternateBackends:\n         - kind: Service\n           name: my-app-b\n           weight: 15\n\nIn the above example, ``my-app-a`` will get 10/25, or 2/5 of the requests, and ``my-app-b`` will get\n15/25, or 3/5\n\nMore information\n\nFor more information, checkout the upstream kubernetes doc on Deployments and the  Openshift doc on Route based deployment strategies."}
{"doc":"documents_and_forms","text":"Documents and Forms\n\n\n\nForms for Requesting a Project Allocation\n\nRequest a Director's Discretionary (DD) Project\nUse this form to request a Director's Discretionary (DD) Project. Select\n\"OLCF Director's Discretionary Program\" from the drop down menu.\n\nPrincipal Investigator\nAgreement The Oak Ridge\nLeadership Computing Facility must have a signed copy of this form on\nfile from the project's principal investigator(s) before any accounts\nfor the project will be processed.\n\nIndustry Principal Investigator's\nAgreement The Oak\nRidge Leadership Computing Facility must have a signed copy of this form\non file from the project's principal investigator(s) before any accounts\nfor the project will be processed.\n\n\n\nForms for Requesting an Account\n\nRequest an Account\nFirst-time users should use this form to request a new user account. See the section applying-for-a-user-account <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#applying-for-a-user-account>\nfor complete details.\n\nJoin an Additional Project\nExisting users with RSA SecurID tokens should log in to the myOLCF self-service portal to apply for additional projects.\nExisting users without RSA SecurID tokens should fill out the Account Application Form that can be found at the top left of the\nmyOLCF login page without needing to sign in.\nSee the section applying-for-a-user-account <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#applying-for-a-user-account> for complete details.\n\nNotary Token Verification Form\n(See Notary Instructions)\nFor certain resources, ORNL requires identity proofing to authenticate a\nuser’s identity and possession of the token prior to activation. The\nNotary Token Verification Form verifies the identity of the applicant\nand maintains records of the identity proofing credentials that may be\ndisclosed in the future to an authorized individual or investigative\nagency. Alternatively, you may schedule this verification with a member\nof our staff virtually my filling out the\nConference Scheduler Form.\nYou will need your Application Confirmation Number that was emailed to you\nby our accounts team to schedule in this manner. If you do not possess\na confirmation number (you are verifying a replacement token, for example),\nplease email us at help@olcf.ornl.gov to schedule.\n\nNondisclosure Agreement\nForm Required from\nsubcontractors only.\n\nSensitive Data Rules\nAll users\nmust agree to abide by all security measures described in this document\nwhen performing any work on OLCF resources that is not fundamental\nresearch and/or publicly available information.\n\n\n\nForms to Request Changes to Computers, Jobs, or Accounts\n\nYou no longer need to use a special form for the following requests. If you\nneed to request any of the following, please do so via email to help@olcf.ornl.gov.\n\nRelaxed queue limits for one or more jobs (longer walltime, higher priority)\n\nSystem reservation (a dedicated set of nodes at a specific date/time)\n\nIncreased disk quota\n\nPurge exemption for User/Group/World Work areas\n\nSoftware requests\n\n\n\nReport Templates\n\nCloseout Report Template\nUse this template if you have been asked to submit a closeout report for your\nproject.  Note this form does not apply to INCITE projects.  If you have been provided a template via email, only that template applies.\n\nIndustry Quarterly Report Template\nUse this template if you have an industry project to submit a quarterly\nreport.\n\n\n\nMiscellaneous Forms\n\nDirector's Discretion Review Form\nFor internal use only."}
{"doc":"dtn_user_guide","text":"Data Transfer Nodes (DTNs)\n\n\n\n\n\nSystem Overview\n\nThe Data Transfer Nodes (DTNs) are hosts specifically designed to provide optimized data transfer between OLCF systems and systems outside of the OLCF network. These nodes perform well on local-area transfers as well as the wide-area data transfers for which they are tuned. The OLCF recommends that users use these nodes to improve transfer speed and reduce load on computational systems’ login and service nodes. OLCF provides two sets of DTNs: one for systems in our moderate enclave and a second for systems in the open enclave.\n\n\n\nInteractive Access\n\nDTN access is automatically granted to all enabled OLCF users. For interactive access to DTNs (ssh/scp/sftp), connect to dtn.ccs.ornl.gov (for the moderate enclave) or opendtn.ccs.ornl.gov (for the open enclave). For example:\n\nssh username@dtn.ccs.ornl.gov\n\nFor more information on connecting to OLCF resources, see connecting-to-olcf <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#connecting-to-olcf>.\n\nAccess From Globus Online\n\nDTNs are also accessible via the \"OLCF DTN\" (for moderate) and \"NCCS Open DTN\" (for open) Globus endpoints. For more information on using Globus at OLCF see data-transferring-data-globus <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-transferring-data-globus>.\n\nBatch Queue (Slurm)\n\nThe moderate DTNs also support batch jobs. The system contains 8 nodes accessible through the DTN batch system.\n\nMost OLCF resources now use the Slurm batch scheduler, including the DTNs.\nBelow is a table of useful commands for Slurm.\n\n\nThe table presents a list of tasks and their corresponding Slurm commands in the DTN user guide. The first task is to view the batch queue, which can be done using the \"squeue\" command. This command allows users to see the status of their submitted batch jobs, including the job ID, name, partition, and status. The next task is to submit a batch script, which can be done using the \"sbatch\" command. This command allows users to submit a batch job with a script containing the necessary commands and instructions. The third task is to submit an interactive batch job, which can be done using the \"salloc\" command. This command allows users to allocate resources and run interactive batch jobs, which are useful for debugging and testing purposes. Overall, these commands are essential for managing and submitting batch jobs on the DTN system, providing users with a streamlined and efficient workflow.\n\n| Task | Slurm |\n|------|-------|\n| View batch queue | squeue |\n| Submit batch script | sbatch |\n| Submit interactive batch job | salloc |\n\n\n\nQueue Policy\n\n\nThe table presents information related to the DTN (Data Transfer Node) user guide. It includes three columns: Node Count, Duration, and Policy. The Node Count column specifies the number of nodes, which ranges from 1 to 4. The Duration column indicates the time frame, which is 0 to 24 hours. The Policy column outlines the maximum number of jobs that can be running per user, which is limited to 1. This table provides important guidelines for users of the DTN system, ensuring efficient and fair usage of resources. By limiting the number of jobs per user, it allows for equal distribution of resources and prevents any one user from monopolizing the system. This information is crucial for users to understand and follow in order to optimize their experience with the DTN system. \n\n| Node Count | Duration | Policy |\n|------------|----------|--------|\n| 1-4 Nodes  | 0 - 24 hrs | max 1 job running per user |"}
{"doc":"e4s","text":"E4S Software Stack\n\nThe Extreme-scale Scientific Software Stack (E4S)\nis a community effort to provide open source software packages for developing, deploying,\nand running scientific applications on high-performance computing (HPC) platforms.\nE4S provides from-source builds and containers of a broad collection of HPC software\npackages.\n\nFor information about the software packages included in the E4S project,\nvisit https://e4s-project.github.io/Resources/ProductInfo.html\n\nThe packages installed on Summit include only those that meet OLCF policy\nrequirements for facility-provided software; build successfully on Summit's\nhardware architectures for a given toolchain and runtime environment; and are\notherwise appropriate for use with the resource scheduler and communication\nfabrics supported by the OLCF.\n\nTherefore, not all the packages in the E4S project collection are necessarily\nprovided by the OLCF nor are these packages necessarily configured the same way\nfor each toolchain and runtime environment\n\nNot all packages are built for all compilers.\n\nSummit\n\nThe E4S software list is installed along side the existing software on Summit and\ncan be access via lmod modulefiles.\n\nAccess via modulefiles\n\nTo access the installed software, load the desired compiler via:\n\nmodule load < compiler/version >\n.. ie ..\nmodule load gcc/9.1.0\n.. or ..\nmodule load gcc/7.5.0\n\nThen use module avail to see the installed list of packages.\n\nE4S 21.08 Packages\n\nList of installed packages on Summit for E4S release 21.08:\n\n\nThe table above presents a comprehensive list of software and their corresponding versions that are available on e4s. E4s, or the Extreme-scale Scientific Software Stack, is a collection of software packages that are optimized for high-performance computing (HPC) systems. The first column of the table lists the names of the software, including adios2, aml, argobots, bolt, chai, darshan-runtime, darshan-util, dyninst, flit, flux-core, gasnet, ginkgo, gotcha, kokkos-kernels, legion, libunwind, loki, mercury, metall, mpark-variant, netlib-scalapack, ninja, nvhpc, papi, parallel-netcdf, pdt, plasma, qt, raja, superlu, swig, sz, tasmanian, tau, umap, umpire, upcxx, vtk-m, and zfp. The second column provides the loaded version of each software, with the most recent versions being used. The third column indicates the specific module name for each software, which is used to load the software onto the system. This table is a valuable resource for users of e4s, as it provides a quick and easy reference for the available software and their versions. By using this table, users can ensure that they are using the most up-to-date and optimized versions of the software for their HPC needs. \n\n| Software Name | Loaded Version | Module Name |\n|---------------|----------------|-------------|\n| adios2 | 2.7.1 | adios2/2.7.1 |\n| aml | 0.1.0 | aml/0.1.0 |\n| argobots | 1.1 | argobots/1.1 |\n| bolt | 2.0 | bolt/2.0 |\n| chai | 2.3.0 | chai/2.3.0 |\n| darshan-runtime | 3.3.1 | darshan-runtime/3.3.1 |\n| darshan-util | 3.3.1 | darshan-util/3.3.1 |\n| dyninst | 11.0.1 | dyninst/11.0.1 |\n| flit | 2.1.0 | flit/2.1.0 |\n| flux-core | 0.28.0 | flux-core/0.28.0 |\n| gasnet | 2021.3.0 | gasnet/2021.3.0 |\n| ginkgo | 1.3.0 | ginkgo/1.3.0 |\n| gotcha | 1.0.3 | gotcha/1.0.3 |\n| kokkos-kernels | 3.2.00 | kokkos-kernels/3.2.00 |\n| legion | 21.03.0 | legion/21.03.0 |\n| libunwind | 1.5.0 | libunwind/1.5.0 |\n| loki | 0.1.7 | loki/0.1.7 |\n| mercury | 2.0.1 | mercury/2.0.1 |\n| metall | 0.15 | metall/0.15 |\n| mpark-variant | 1.4.0 | mpark-variant/1.4.0 |\n| netlib-scalapack | 2.1.0 | netlib-scalapack/2.1.0 |\n| ninja | 1.10.2 | ninja/1.10.2 |\n| nvhpc | 21.7 | nvhpc/21.7 |\n| papi | 6.0.0.1 | papi/6.0.0.1 |\n| parallel-netcdf | 1.12.2 | parallel-netcdf/1.12.2 |\n| pdt | 3.25.1 | pdt/3.25.1 |\n| plasma | 20.9.20 | plasma/20.9.20 |\n| qt | 5.15.2 | qt/5.15.2 |\n| raja | 0.13.0 | raja/0.13.0 |\n| superlu | 5.2.2 | superlu/5.2.2 |\n| swig | 4.0.2 | swig/4.0.2 |\n| swig | 4.0.2-fortran | swig/4.0.2-fortran |\n| sz | 2.1.12 | sz/2.1.12 |\n| tasmanian | 7.5 | tasmanian/7.5 |\n| tau | 2.30.1 | tau/2.30.1 |\n| umap | 2.1.0 | umap/2.1.0 |\n| umpire | 4.1.2 | umpire/4.1.2 |\n| upcxx | 2021.3.0 | upcxx/2021.3.0 |\n| vtk-m | 1.6.0 | vtk-m/1.6.0 |\n| zfp | 0.5.5 | zfp/0.5.5 |\n\n\n\nE4S 21.05 Packages\n\nList of installed packages on Summit for E4S release 21.05:\n\n\nThe table above provides a comprehensive list of software names, loaded versions, and module names for e4s. E4s, or the Extreme-scale Scientific Software Stack, is a collection of software packages designed to support high-performance computing (HPC) applications. The software names listed in the table include adios2, aml, amrex, arborx, archer, argobots, ascent, axom, bolt, cabana, caliper, chai, conduit, darshan-runtime, darshan-util, dyninst, faodel, flecsi, flit, gasnet, ginkgo, globalarrays, gotcha, gmp, hdf5, heffte, hpctoolkit, hpx, hypre, kokkos-kernels, kokkos, legion, libquo, libunwind, magma, mercury, mpark-variant, mpifileutils, ninja, omega-h, openpmd-api, papi, papyrus, parallel-netcdf, pdt, petsc, plasma, precice, pumi, qt, qthreads, raja, rempi, slate, slepc, stc, strumpack, sundials, superlu, superlu-dist, swig, sz, tasmanian, tau, trilinos, turbine, umap, umpire, unifyfs, upcxx, and zfp. The loaded versions of these software packages range from 0.1.0 to 21.05, with some packages having multiple versions listed. The module names correspond to the specific module that needs to be loaded in order to use the software package. This table serves as a useful reference for users of e4s, providing them with the necessary information to access and utilize the various software packages available. \n\n| Software Name | Loaded Version | Module Name |\n| --- | --- | --- |\n| adios2 | 2.7.1 | adios2/2.7.1 |\n| aml | 0.1.0 | aml/0.1.0 |\n| amrex | 21.05 | amrex/21.05 |\n| arborx | 1.0 | arborx/1.0 |\n| archer | 2.0.0 | archer/2.0.0 |\n| argobots | 1.1 | argobots/1.1 |\n| ascent | 0.7.1 | ascent/0.7.1 |\n| axom | 0.5.0 | axom/0.5.0 |\n| bolt | 2.0 | bolt/2.0 |\n| cabana | 0.3.0 | cabana/0.3.0 |\n| caliper | 2.4.0 | caliper/2.4.0 |\n| chai | 2.3.0 | chai/2.3.0 |\n| conduit | 0.7.2 | conduit/0.7.2 |\n| darshan-runtime | 3.3.0 | darshan-runtime/3.3.0 |\n| darshan-util | 3.3.0 | darshan-util/3.3.0 |\n| dyninst | 11.0.0 | dyninst/11.0.0 |\n| faodel | 1.1906.1 | faodel/1.1906.1 |\n| flecsi | 1.4 | flecsi/1.4 |\n| flit | 2.1.0 | flit/2.1.0 |\n| gasnet | 2020.3.0 | gasnet/2020.3.0 |\n| ginkgo | 1.13.0 | ginkgo/1.13.0 |\n| globalarrays | 5.8 | globalarrays/5.8 |\n| gotcha | 1.0.3 | gotcha/1.0.3 |\n| gmp | 6.2.1 | gmp/6.2.1 |\n| gotcha | 1.0.3 | gotcha/1.0.3 |\n| hdf5 | 1.10.7 | hdf5/1.10.7 |\n| heffte | 2.0.0 | heffte/2.0.0 |\n| hpctoolkit | 2021.03.01 | hpctoolkit/2021.03.01 |\n| hpx | 1.6.0 | hpx/1.6.0 |\n| hypre | 2.20.0 | hypre/2.20.0 |\n| kokkos-kernels | 3.2.00 | kokkos-kernels/3.2.00 |\n| kokkos | 3.4.00 | kokkos/3.4.00 |\n| legion | 21.03.0 | legion/21.03.0 |\n| libquo | 1.3.1 | libquo/1.3.1 |\n| libunwind | 1.5.0 | libunwind/1.5.0 |\n| magma | 2.5.4 | magma/2.5.4 |\n| mercury | 2.0.1 | mercury/2.0.1 |\n| mpark-variant | 1.4.0 | mpark-variant/1.4.0 |\n| mpifileutils | 0.11 | mpifileutils/0.11 |\n| ninja | 1.10.2 | ninja/1.10.2 |\n| omega-h | 9.32.5 | omega-h/9.32.5 |\n| openpmd-api | 0.13.4 | openpmd-api/0.13.4 |\n| papi | 6.0.0.1 | papi/6.0.0.1 |\n| papyrus | 1.0.1 | papyrus/1.0.1 |\n| parallel-netcdf | 1.12.1 | parallel-netcdf/1.12.1 |\n| pdt | 3.25.1 | pdt/3.25.1 |\n| petsc | 3.15.0 | petsc/3.15.0 |\n| plasma | 20.9.20 | plasma/20.9.20 |\n| precice | 2.2.1 | precice/2.2.1 |\n| pumi | 2.2.5 | pumi/2.2.5 |\n| qt | 5.15.2 | qt/5.15.2 |\n| qthreads | 1.16 | qthreads/1.16-distrib |\n| raja | 0.13.0 | raja/0.13.0 |\n| rempi | 1.1.0 | rempi/1.1.0 |\n| slate | 2021.5.2 | slate/2021.05.2 |\n| slepc | 3.15.0 | slepc/3.15.0 |\n| stc | 0.9.0 | stc/0.9.0 |\n| strumpack | 5.1.1 | strumpack/5.1.1 |\n| sundials | 5.7.0 | sundials/5.7.0 |\n| superlu | 5.2.1 | superlu/5.2.1 |\n| superlu-dist | 6.4.1 | superlu-dist/6.4.1 |\n| swig | 4.0.2 | swig/4.0.2 |\n| sz | 2.1.11.1 | sz/2.1.11.1 |\n| tasmanian | 7.5 | tasmanian/7.5 |\n| tau | 2.30.1 | tau/2.30.1 |\n| trilinos | 13.0.1 | trilinos/13.0.1 |\n| turbine | 1.3.0 | turbine/1.3.0 |\n| umap | 2.1.0 | umap/2.1.0 |\n| umpire | 4.1.2 | umpire/4.1.2 |\n| unifyfs | 0.9.2 | unifyfs/0.9.2 |\n| upcxx | 2021.3.0 | upcxx/2021.3.0 |\n| zfp | 0.5.5 | zfp/0.5.5 |\n\n\n\nSpock\n\nThe E4S software list is installed along side the existing software on Spock and\ncan be access via lmod modulefiles.\n\nAccess via modulefiles\n\n<string>:194: (INFO/1) Duplicate implicit target name: \"access via modulefiles\".\n\nTo access the installed software, load the desired compiler via:\n\nmodule load < compiler/version >\n.. ie ..\nmodule load gcc/9.3.0\n.. or ..\nmodule load gcc/10.2.0\n\nThen use module avail to see the installed list of packages.\n\nE4S 21.08 Packages\n\n<string>:209: (INFO/1) Duplicate implicit target name: \"e4s 21.08 packages\".\n\nList of currently installed E4S packages on Spock (E4S release 21.08):\n\n\nThe table above presents a comprehensive list of software names, their loaded versions, and corresponding module names for e4s. E4s, or the Extreme-scale Scientific Software Stack, is a collection of software packages designed to support high-performance computing (HPC) applications. The software names listed in the table include adios2, aml, arborx, argobots, ascent, bolt, cabana, chai, conduit, darshan-util, datatransferkit, faodel, flecsi, flit, flux-core, fortrilinos, gasnet, globalarrays, gotcha, hdf5, heffte, hpx, hypre, kokkos, kokkos-kernels, legion, libquo, libunwind, loki, mercury, metall, mfem, mpark-variant, mpifileutils, netlib-scalapack, ninja, omega-h, openpmd-api, papi, papyrus, parallel-netcdf, pdt, petsc, pumi, qthreads, raja, sundials, superlu, superlu-dist, swig, sz, tasmanian, trilinos, turbine, umap, umpire, vtk-m, and zfp. Each software name is accompanied by its loaded version and corresponding module name, which is used to load the software into the HPC environment. This table serves as a useful reference for users of e4s, providing them with the necessary information to access and utilize the various software packages available in the stack. \n\n| Software Name | Loaded Version | Module Name |\n|---------------|----------------|-------------|\n| adios2 | 2.7.1 | adios2/2.7.1 |\n| aml | 0.1.0 | aml/0.1.0 |\n| arborx | 1.0 | arborx/1.0 |\n| argobots | 1.1 | argobots/1.1 |\n| ascent | 0.7.1 | ascent/0.7.1 |\n| bolt | 2.0 | bolt/2.0 |\n| cabana | 0.3.0 | cabana/0.3.0 |\n| chai | 2.3.0 | chai/2.3.0 |\n| conduit | 0.7.2 | conduit/0.7.2 |\n| darshan-util | 3.3.1 | darshan-util/3.3.1 |\n| datatransferkit | 3.1-rc2 | datatransferkit/3.1-rc2 |\n| faodel | 1.1906.1 | faodel/1.1906.1 |\n| flecsi | 1.4.2 | flecsi/1.4.2 |\n| flit | 2.1.0 | flit/2.1.0 |\n| flux-core | 0.28.0 | flux-core/0.28.0 |\n| fortrilinos | 2.0.0 | fortrilinos/2.0.0 |\n| gasnet | 2021.3.0 | gasnet/2021.3.0 |\n| globalarrays | 5.8 | globalarrays/5.8 |\n| gotcha | 1.0.3 | gotcha/1.0.3 |\n| hdf5 | 1.12.0 | hdf5/1.12.0 |\n| heffte | 2.1.0 | heffte/2.1.0 |\n| hpx | 1.7.1 | hpx/1.7.1 |\n| hypre | 2.22.0 | hypre/2.22.0 |\n| kokkos | 3.4.00 | kokkos/3.4.00 |\n| kokkos-kernels | 3.2.00 | kokkos-kernels/3.2.00 |\n| legion | 21.03.0 | legion/21.03.0 |\n| libquo | 1.3.1 | libquo/1.3.1 |\n| libunwind | 1.5.0 | libunwind/1.5.0 |\n| loki | 0.1.7 | loki/0.1.7 |\n| mercury | 2.0.1 | mercury/2.0.1 |\n| metall | 0.15 | metall/0.15 |\n| mfem | 4.3.0 | mfem/4.3.0 |\n| mpark-variant | 1.4.0 | mpark-variant/1.4.0 |\n| mpifileutils | 0.11 | mpifileutils/0.11 |\n| netlib-scalapack | 2.1.0 | netlib-scalapack/2.1.0 |\n| ninja | 1.10.2 | ninja/1.10.2 |\n| omega-h | 9.32.5 | omega-h/9.32.5 |\n| openpmd-api | 0.13.4 | openpmd-api/0.13.4 |\n| papi | 6.0.0.1 | papi/6.0.0.1 |\n| papyrus | 1.0.1 | papyrus/1.0.1 |\n| parallel-netcdf | 1.12.2 | parallel-netcdf/1.12.2 |\n| pdt | 3.25.1 | pdt/3.25.1 |\n| petsc | 3.15.3 | petsc/3.15.3 |\n| pumi | 2.2.6 | pumi/2.2.6 |\n| qthreads | 1.16 | qthreads/1.16 |\n| raja | 0.13.0 | raja/0.13.0 |\n| sundials | 5.7.0 | sundials/5.7.0 |\n| superlu | 5.2.2 | superlu/5.2.2 |\n| superlu-dist | 6.4.0 | superlu-dist/6.4.0 |\n| swig | 4.0.2 | swig/4.0.2 |\n| swig | 4.0.2-fortran | swig/4.0.2-fortran |\n| sz | 2.1.12 | sz/2.1.12 |\n| tasmanian | 7.5 | tasmanian/7.5 |\n| trilinos | 13.0.1 | trilinos/13.0.1 |\n| turbine | 1.3.0 | turbine/1.3.0 |\n| umap | 2.1.0 | umap/2.1.0 |\n| umpire | 4.1.2 | umpire/4.1.2 |\n| vtk-m | 1.6.0 | vtk-m/1.6.0 |\n| zfp | 0.5.5 | zfp/0.5.5 |\n\n\n\nE4S 21.05 Packages\n\n<string>:278: (INFO/1) Duplicate implicit target name: \"e4s 21.05 packages\".\n\nList of currently installed E4S packages on Spock (E4S release 21.05):\n\n\nThe table above provides a comprehensive list of software names, loaded versions, and module names for e4s. e4s, or the Extreme-scale Scientific Software Stack, is a collection of software packages and libraries designed to support high-performance computing and scientific research. The software names listed include adios2, arborx, cabana, caliper, conduit, faodel, flecsi, globalarrays, hdf5, heffte, hypre, libquo, mfem, omega-h, openpmd-api, papyrus, parallel-netcdf, petsc, precice, pumi, slate, slepc, stc, superlu-dist, trilinos, turbine, aml, argobots, bolt, chai, darshan-util, dyninst, flit, gmp, gotcha, hpctoolkit, hpx, kokkos-kernels, kokkos, legion, libunwind, mercury, mpark-variant, ninja, papi, pdt, qthreads, raja, superlu, swig, sz, tasmanian, umap, umpire, and zfp. The loaded versions listed range from 0.1.0 to 21.03.0, with the most recent versions being 2021.05.02 for slate and 2021.03.01 for hpctoolkit. The module names listed include adios2/2.7.1, arborx/1.0, cabana/0.3.0, caliper/2.5.0, conduit/0.7.2, faodel/1.1906.1, flecsi/1.4, globalarrays/5.8, hdf5/1.10.7, heffte/2.0.0, hypre/2.20.0, libquo/1.3.1, mfem/4.2.0, omega-h/9.32.5, openpmd-api/0.13.4, papyrus/1.0.1, parallel-netcdf/1.12.2, petsc/3.15.0, precice/2.2.1, pumi/2.2.5, slate/2021.05.02, slepc/3.15.0, stc/0.9.0, superlu-dist/6.4.0, trilinos/13.0.1, turbine/1.3.0, aml/0.1.0, argobots/1.1, bolt/2.0, chai/2.3.0, darshan-util/3.3.0, dyninst/11.0.0, flit/2.1.0, gmp/6.2.1, gotcha/1.0.3, hpctoolkit/2021.03.01, hpx/1.6.0, kokkos-kernels/3.2.00, kokkos/3.4.00, legion/21.03.0, libunwind/1.5.0, mercury/2.0.1, mpark-variant/1.4.0, ninja/1.10.2, papi/6.0.0.1, pdt/3.25.1, qthreads/1.16, raja/0.13.0, superlu/5.2.1, swig/4.0.2-fortran, swig/4.0.2, sz/2.1.11.1, tasmanian/7.5, umap/2.1.0, umpire/4.1.2, and zfp/0.5.5. These module names are used to load the corresponding software packages and libraries for use in high-performance computing and scientific research. Overall, this table provides a comprehensive overview of the software and modules available in e4s, making it a valuable resource for researchers and developers in the field. \n\n| Software Name | Loaded Version | Module Name |\n|---------------|----------------|-------------|\n| adios2 | 2.7.1 | adios2/2.7.1 |\n| arborx | 1.0 | arborx/1.0 |\n| cabana | 0.3.0 | cabana/0.3.0 |\n| caliper | 2.5.0 | caliper/2.5.0 |\n| conduit | 0.7.2 | conduit/0.7.2 |\n| faodel | 1.1906.1 | faodel/1.1906.1 |\n| flecsi | 1.4 | flecsi/1.4 |\n| globalarrays | 5.8 | globalarrays/5.8 |\n| hdf5 | 1.10.7 | hdf5/1.10.7 |\n| heffte | 2.0.0 | heffte/2.0.0 |\n| hypre | 2.20.0 | hypre/2.20.0 |\n| libquo | 1.3.1 | libquo/1.3.1 |\n| mfem | 4.2.0 | mfem/4.2.0 |\n| omega-h | 9.32.5 | omega-h/9.32.5 |\n| openpmd-api | 0.13.4 | openpmd-api/0.13.4 |\n| papyrus | 1.0.1 | papyrus/1.0.1 |\n| parallel-netcdf | 1.12.2 | parallel-netcdf/1.12.2 |\n| petsc | 3.15.0 | petsc/3.15.0 |\n| precice | 2.2.1 | precice/2.2.1 |\n| pumi | 2.2.5 | pumi/2.2.5 |\n| slate | 2021.05.02 | slate/2021.05.02 |\n| slepc | 3.15.0 | slepc/3.15.0 |\n| stc | 0.9.0 | stc/0.9.0 |\n| superlu-dist | 6.4.0 | superlu-dist/6.4.0 |\n| trilinos | 13.0.1 | trilinos/13.0.1 |\n| turbine | 1.3.0 | turbine/1.3.0 |\n| aml | 0.1.0 | aml/0.1.0 |\n| argobots | 1.1 | argobots/1.1 |\n| bolt | 2.0 | bolt/2.0 |\n| chai | 2.3.0 | chai/2.3.0 |\n| darshan-util | 3.3.0 | darshan-util/3.3.0 |\n| dyninst | 11.0.0 | dyninst/11.0.0 |\n| flit | 2.1.0 | flit/2.1.0 |\n| gmp | 6.2.1 | gmp/6.2.1 |\n| gotcha | 1.0.3 | gotcha/1.0.3 |\n| hpctoolkit | 2021.03.01 | hpctoolkit/2021.03.01 |\n| hpx | 1.6.0 | hpx/1.6.0 |\n| kokkos-kernels | 3.2.00 | kokkos-kernels/3.2.00 |\n| kokkos | 3.4.00 | kokkos/3.4.00 |\n| legion | 21.03.0 | legion/21.03.0 |\n| libunwind | 1.5.0 | libunwind/1.5.0 |\n| mercury | 2.0.1 | mercury/2.0.1 |\n| mpark-variant | 1.4.0 | mpark-variant/1.4.0 |\n| ninja | 1.10.2 | ninja/1.10.2 |\n| papi | 6.0.0.1 | papi/6.0.0.1 |\n| pdt | 3.25.1 | pdt/3.25.1 |\n| qthreads | 1.16 | qthreads/1.16 |\n| raja | 0.13.0 | raja/0.13.0 |\n| superlu | 5.2.1 | superlu/5.2.1 |\n| swig | 4.0.2-fortran | swig/4.0.2-fortran |\n| swig | 4.0.2 | swig/4.0.2 |\n| sz | 2.1.11.1 | sz/2.1.11.1 |\n| tasmanian | 7.5 | tasmanian/7.5 |\n| umap | 2.1.0 | umap/2.1.0 |\n| umpire | 4.1.2 | umpire/4.1.2 |\n| zfp | 0.5.5 | zfp/0.5.5 |\n| darshan-util | 3.3.0 | darshan-util/3.3.0 |\n| gmp | 6.2.1 | gmp/6.2.1 |\n| libunwind |\n\nadios2/2.7.1\naml/0.1.0\namrex/21.04\nbolt/2.0\ncaliper/2.5.0\ndyninst/10.2.1\nfaodel/1.1906.1\nflecsi/1.4\nflit/2.1.0\ngasnet/2020.3.0\nginkgo/1.3.0\nglobalarrays/5.8\ngotcha/1.0.3\nhdf5/1.10.7\nhpx/1.6.0\nkokkos-kernels/3.2.00\nlegion/20.03.0\nlibquo/1.3.1\nmercury/2.0.0\nmfem/4.2.0\nninja/1.10.2\nopenpmd-api/0.13.2\npapi/6.0.0.1\npapyrus/1.0.1\npdt/3.25.1\nprecice/2.2.0\npumi/2.2.5\nqthreads/1.16\nraja/0.13.0\nslate/2020.10.00\nslepc/3.15.0\nsundials/5.7.0\nsuperlu/5.2.1\nsuperlu-dist/6.4.0\nswig/4.0.2-fortran\nsz/2.1.11.1\ntasmanian/7.3\numap/2.1.0\numpire/4.1.2\nzfp/0.5.5"}
{"doc":"entk","text":"Ensemble Toolkit (EnTK)\n\n\n\nOverview\n\nThe Ensemble Toolkit (EnTK) is a Python library developed by the RADICAL\nResearch Group at Rutgers University for developing and executing large-scale\nensemble-based workflows. This tutorial shows how to get up and running with\nEnTK 1.13.0 on Summit specifically. For in-depth information about EnTK itself,\nplease refer to its\ndocumentation.\n\nPrerequisites\n\nBefore using EnTK itself, you will need MongoDB\nand RabbitMQ services running on\nSlate<slate> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Slate<slate>>. There are tutorials for MongoDB in this documentation,\nbut the tutorial for RabbitMQ is forthcoming.\n\nYou will need to know the connection information for both MongoDB and RabbitMQ\nso that EnTK can be configured to connect to the services.\n\nThen, to use EnTK on Summit, load the module as shown below:\n\n$ module load workflows\n$ module load entk/1.13.0\n\nRun the following command to verify that EnTK is available:\n\n$ radical-utils-version\n1.13.0\n\nHello world!\n\nTo run EnTK on Summit, you will create two files and then execute two commands\nfrom a Summit login node. Currently, EnTK must be run from a Summit login node,\nrather than within a batch job.\n\nFirst, create a setup file setup.bash that will load the correct modules and\ndefine environment variables.\n\nmodule load workflows\nmodule load entk/1.13.0\n\nexport RADICAL_PILOT_DBURL=\"mongodb://admin:password@apps.marble.ccs.ornl.gov:32767/test\"\nexport RMQ_HOSTNAME=\"apps.marble.ccs.ornl.gov\"\nexport RMQ_PORT=\"30256\"\nexport RMQ_USERNAME=\"admin\"\nexport RMQ_PASSWORD=\"password\"\n\nReplace, without renaming, the RADICAL_PILOT_DBURL environment variable in\nsetup.bash with the MongoDB connection string that corresponds to your own\nservice running on Slate. EnTK uses this environment variable directly.\n\nThen, replace the RMQ_ variables in setup.bash with the corresponding\nvalues for your RabbitMQ service on Slate. These variables can be renamed,\nbecause their only use is in the following example Python program.\n\nNow, create a demo.py3 file with the following lines:\n\nimport os\nfrom radical.entk import AppManager, Pipeline, Stage, Task\n\n# Create objects.\n\nappman = AppManager(\n    hostname=os.environ[\"RMQ_HOSTNAME\"],\n    port=int(os.environ[\"RMQ_PORT\"]),\n    username=os.environ[\"RMQ_USERNAME\"],\n    password=os.environ[\"RMQ_PASSWORD\"])\n\np = Pipeline()\n\ns = Stage()\n\nt = Task()\n\n# Use the objects to model the workflow.\n\nappman.resource_desc = {\n    \"resource\": \"ornl.summit\",\n    \"walltime\": 10,\n    \"cpus\":     1,\n    \"project\":  \"abc123\" # replace with your own project identifier\n}\n\nt.name = \"mytask\"\nt.executable = \"/bin/echo\"\nt.arguments = [\"Hello world!\"]\n\ns.add_tasks(t)\n\np.add_stages(s)\n\nappman.workflow = set([p])\n\n# Execute the workflow.\n\nappman.run()\n\nIn demo.py3, only one line needs to be changed, so that EnTK knows which\nproject identifier to use when submitting batch jobs to Summit.\n\nFinally, run the demo program by executing the following commands from a Summit\nlogin node:\n\n$ source setup.bash\n$ python3 demo.py3\n\nCongratulations! You should now see interactive output from EnTK while it\nlaunches and monitors your job on Summit."}
{"doc":"examples","text":"YAML Object Quick Reference\n\n\n\nExamples of basic Kubernetes objects meant to be used as a reference for those familiar with Kubernetes.\n\nCronJobs\n\nBasic Cronjob\n\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n# Name of cronjob\n  name: hello-openshift\nspec:\n# If the cronjob does not start in this time it will be marked as failed\n  startingDeadlineSeconds: 10\n  schedule: \"*/1 * * * *\"\n  # Cronjobs create jobs every schedule above. This is the template for the job to be created\n  jobTemplate:\n  # The job creates a pod. This is the spec for the pod\n    spec:\n      template:\n      # Container spec\n        spec:\n          containers:\n          # Container name\n          - name: hello-openshift\n          # Container image\n            image: image-registry.openshift-image-registry.svc:5000/openshift/ccs-rhel7-base-amd64\n            tag: latest\n            # Command to be run inside the container\n            args:\n              - /bin/sh\n              - -c\n              - echo \"hello openshift\"\n          restartPolicy: Never\n\nDeployments and Stateful Sets\n\nBasic Deployment\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  # deployment name\n  name: test-pod-deployment\nspec:\n  # number of replicas\n  replicas: 3\n  selector:\n    # this sets the label the deployment is looking for\n    matchLabels:\n      app: test-pod\n  template:\n    metadata:\n      # labels are how the deployments keep track of their objects. This sets a label on the pod\n      labels:\n        app: test-pod\n    spec:\n      containers:\n        # container name\n      - name: test-pod\n        # using the base image\n        image: \"image-registry.openshift-image-registry.svc:5000/openshift/ccs-rhel7-base-amd64\"\n        # Generic command that will not return\n        command: [\"cat\"]\n        # Need a tty if we are to SSH. Need stdin for tty\n        tty: true\n        stdin: true\n\nBasic Stateful Set\n\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  # statefulset name\n  name: test-pod-stateful-set\nspec:\n  # number of replicas\n  replicas: 3\n  selector:\n    # this sets the label the stateful set is looking for\n    matchLabels:\n      app: test-pod\n  template:\n    metadata:\n      # labels are how the stateful set keep track of their objects. This sets a label on the pod\n      labels:\n        app: test-pod\n    spec:\n      containers:\n        # container name\n      - name: test-pod\n        # using the base image\n        image: \"image-registry.openshift-image-registry.svc:5000/openshift/ccs-rhel7-base-amd64\"\n        # Generic command that will not return\n        command: [\"cat\"]\n        # Need a tty if we are to SSH. Need stdin for tty\n        tty: true\n        stdin: true\n\nPods\n\nBasic Pod you can create and get a shell in\n\napiVersion: v1\nkind: Pod\nmetadata:\n  # Pod name\n  name: test-pod\nspec:\n  containers:\n    # Container name\n    - name: test-container\n      # Using the base image\n      image: \"image-registry.openshift-image-registry.svc:5000/openshift/ccs-rhel7-base-amd64\"\n      # Generic command that will not return\n      command: [\"cat\"]\n      # Need a tty if we are to SSH. Need stdin for tty\n      tty: true\n      stdin: true\n\nPod that mounts a volume named test-pod-pvc\n\napiVersion: v1\nkind: Pod\nmetadata:\n  # Pod name\n  name: test-pod\nspec:\n  containers:\n    # Container name\n    - name: test-pod\n      # Using the base image\n      image: \"image-registry.openshift-image-registry.svc:5000/openshift/ccs-rhel7-base-amd64\"\n      # Generic command that will not return\n      command: [\"cat\"]\n      # Need a tty if we are to SSH. Need stdin for tty\n      tty: true\n      stdin: true\n      volumeMounts:\n        # Where in the pod the volume will be mounted\n        - mountPath: /etc/test-volume\n          # What the volume was named\n          name: test-pod-volume\n  volumes:\n      # Setting the name. What the volume will be referred to in the pod spec\n    - name: test-pod-volume\n      persistentVolumeClaim:\n        # The name of the already created pvc that the volume will be bound to\n        claimName: test-pod-pvc\n\nThe yaml that defines the PVC that is being mounted by the above pod can be found in the Volumes section\n\nRoles and Rolebindings\n\nRole\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n# Role Name\n  name: pod-reader\nrules:\n# \"\" indicates the core API group\n- apiGroups: [\"\"]\n# What object the verbs apply to\n  resources: [\"pods\"]\n# The API requests allowed on the above object\n  verbs: [\"get\", \"watch\", \"list\"]\n\nThe verbs match to HTTP verbs against the API. A list of that matching can be found here.\n\nRolebinding\n\napiVersion: rbac.authorization.k8s.io/v1\n# This role binding allows user \"2jl\" to read pods in the \"default\" namespace.\n# You need to already have a Role named \"pod-reader\" in that namespace.\nkind: RoleBinding\nmetadata:\n  # Name of the RoleBinding\n  name: read-pods\n  # Namespace for the RoleBinding\n  namespace: default\nsubjects:\n# You can specify more than one \"subject\"\n- kind: User\n  name: 2jl\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  # kind is what your binding is to. In this case a Role\n  kind: Role\n  # The Role you are binding the user to\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n\nRoutes, Services and Nodeports\n\nRoute\n\napiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n# Route Name\n  name: test-route\nspec:\n # The URL. Must be unique across cluster.\n  host: test-route-stf002platform-hello-openshift.apps.marble.ccs.ornl.gov\n  tls:\n  # redirects traffic from insecure port to secure port\n    insecureEdgeTerminationPolicy: Redirect\n    termination: edge\n  to:\n  # This is a route and thus points to a service\n    kind: Service\n  # name of the service to point to\n    name: test-service\n\nService\n\napiVersion: v1\nkind: Service\nmetadata:\n# Service name\n  name: test-service\nspec:\n  ports:\n  # Port name\n  - name: nginx\n  # The port being exposed by the service to the Route\n    port: 443\n  # The port on the pod being exposed to the Service\n    targetPort: 8080\n    protocol: TCP\n  selector:\n  # A label that will match a pod\n    app: test-route\n  sessionAffinity: None\n  # How the service is exposed. For routes the type would be ClusterIP\n  type: ClusterIP\n\nNote the above service is assuming that the pod is serving traffic on port 8080\n\nNodePort\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: nodeport\nspec:\n  ports:\n  # The nodeport port\n  - port: 8081\n  # The port that will be exposed on all nodes in the cluster. Must be in range 30000-32767. Can be left blank and randomly assigned by system.\n    nodePort: 322394\n  # The port on the pod being exposed\n    targetPort: 8080\n    protocol: TCP\n  selector:\n    app: test-nodeport\n  type: NodePort\n\nPersistent Volume Claims\n\nBasic PVC\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  # The name of the claim\n  name: test-pod-pvc\nspec:\n  # The type of storage being requested. This can be blank and it will be\n  # set to the default value, which is netapp-nfs, but it is good practice\n  # to explictly declare it.\n  storageClassName: netapp-nfs\n  # how the volume can be accessed. ReadWriteMany, or RWX as it is abbreviated,\n  # means the volume can be mounted as Read Write by multiple nodes\n  accessModes:\n  - ReadWriteMany\n  resources:\n    # the amount of storage being requested\n    requests:\n      storage: 1Gi\n\nBasic VolumeSnapshot\n\napiVersion: snapshot.storage.k8s.io/v1beta1\nkind: VolumeSnapshot\nmetadata:\n  # Snapshot name\n  name: pvc1-snap\nspec:\n  source:\n    # Persistent Volume to snapshot\n    persistentVolumeClaimName: test-pod-pvc"}
{"doc":"fireworks","text":"FireWorks\n\n\n\nOverview\n\nFireWorks is a free, open-source tool for defining, managing, and executing\nworkflows. Complex workflows can be defined using Python, JSON, or YAML, are\nstored using MongoDB, and can be monitored through a built-in web interface.\nWorkflow execution can be automated over arbitrary computing resources,\nincluding those that have a queueing system. FireWorks has been used to run\nmillions of workflows encompassing tens of millions of CPU-hours across diverse\napplication areas and in long-term production projects over the span of\nmultiple years.\n\nTo learn more about FireWorks, please refer to its extensive online\ndocumentation.\n\nPrerequisites\n\nBefore using FireWorks itself, you will need a MongoDB service running\non Slate<slate> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Slate<slate>>. A tutorial to deploy MongoDB is available in the\nSlate documentation.\n\nYou will need to know the connection information for both MongoDB so that\nFireWorks can be configured to connect to it.\n\nThen, to use FireWorks on Summit, load the module as shown below:\n\n$ module load workflows\n$ module load fireworks/2.0.2\n\nRun the following command to verify that FireWorks is available:\n\n$ rlaunch -v\nrlaunch v2.0.2\n\nHello world!\n\nTo run this FireWorks demo on Summit, you will create a Python file and then\nsubmit it as a batch job to LSF from a Summit node.\n\nThe contents for demo.py follow:\n\nimport os\n\nfrom fireworks import Firework, Workflow, LaunchPad, ScriptTask\nfrom fireworks.core.rocket_launcher import rapidfire\n\n# Set up and reset the LaunchPad using MongoDB URI string.\nlaunchpad = LaunchPad(host = os.getenv(\"MONGODB_URI\"), uri_mode = True)\nlaunchpad.reset('', require_password=False)\n\n# Create the individual FireWorks and Workflow.\nfw1 = Firework(ScriptTask.from_str('echo \"hello\"'), name = \"hello\")\nfw2 = Firework(ScriptTask.from_str('echo \"goodbye\"'), name = \"goodbye\")\nwf = Workflow([fw1, fw2], {fw1: fw2}, name = \"test workflow\")\n\n# Store workflow and launch it locally.\nlaunchpad.add_wf(wf)\nrapidfire(launchpad)\n\nFinally, create an LSF batch script called fireworks_demo.lsf, and\nchange abc123 to match your own project identifier:\n\n#BSUB -P abc123\n#BSUB -W 10\n#BSUB -nnodes 1\n\n#BSUB -J fireworks_demo\n#BSUB -o fireworks_demo.o%J\n#BSUB -e fireworks_demo.e%J\n\nmodule load workflows\nmodule load fireworks/2.0.2\n\n# Edit the following line to match your own MongoDB connection string.\nexport MONGODB_URI=\"mongodb://admin:password@apps.marble.ccs.ornl.gov:32767/test\"\n\njsrun -n 1 python3 demo.py\n\nFinally, submit the batch job to LSF by executing the following command from a\nSummit login node:\n\n$ bsub fireworks_demo.lsf\n\nCongratulations! Once the batch job completes, you will find new directories\nbeginning with launcher_ and containing FW.json files that detail\nexactly what happened."}
{"doc":"fix-writable-directories","text":"Fix Container Image Permissions\n\nRunning containers as non-root can be challenging when consuming container images from an upstream source\nsuch as the Docker Hub since many images are build with the intention of running as root. You may encounter\nthis issue if your container enters a CrashLoopBackoff state, check the logs for the container and if\nthere are Permission Denied issues then you may need to fix directory permissions in the image.\n\nMount an EmptyDir Volume\n\nIf the application needs access to a temporary space for doing something like generating a configuration file on launch you\ncan mount a EmptyDir volume in the PodSpec which will ensure that whatever user the container is running as will have access\nto write to that directory.\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: foo\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: busybox\n        name: busybox\n        resources: {}\n        volumeMounts:\n        - name: workdir\n          mountPath: /data\n      volumes:\n      - name: workdir\n        emptyDir: {}\nstatus: {}\n\nGenerated with oc create deployment foo --image busybox --dry-run -o yaml and then modified with volume mounts\n\nBuild a New Image\n\nIf you need to run a container in a project and mount NCCS home and project areas then we will need to build an new container\nand modify permissions to allow the project user the container will be running as to access the filesystem of the image.\n\nWe will use OpenShift to build a new image based on the upstream one and change owner of the directories that need to be\nwritable during container execution. Here is an example Dockerfile which derives from an upstream image and changes ownership\nof directories to the user id that the container will run as in the cluster.\n\nFor example, if we are using the UID 63114 for our NCCS project user and we need to write to /opt/application-data during\nthe runtime of the container image we could do this:\n\nFROM upstream-image:tag\nUSER 0\nRUN chown -R 63114 /opt/application-data\nUSER 63114\n\nWe will use this Dockerfile to generate a BuildConfig and then build a new image in our project that has the correct permissions.\n\ncat Dockerfile | oc new-build --dockerfile=- --to=my-image:tag\n\nThe build should start automatically, monitor it with oc logs bc/my-image -f.\n\nNow that we have a new image with our /opt/application-data directory owned by the right user we can either update an existing\ndeployment or create a new one with the image.\n\nNote that in this example, I am updating the Deployment and setting the image of the container named containername and\n--source=istag says I am using a ImageStream tag. The ImageStream in my OpenShift project stf002 is stf002/my-image:mytag.\n\n# Update an existing deployment called my-application\noc set image deploy/my-application containername=stf002/my-image:mytag --source=istag\n\n# Create a new deployment with the container\noc new-app -i my-image"}
{"doc":"frequently_asked_questions","text":"Frequently Asked Questions\n\nHow do I apply for an account?\n\nDetailed instructions for account application can be found in the\napplying-for-a-user-account <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#applying-for-a-user-account>\nsection of the accounts-and-projects <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#accounts-and-projects> page.\n\nWhat is the status of my application?\n\nDetailed instructions for checking the status of your account application\ncan be found in the checking-application-status <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#checking-application-status> section of the\naccounts-and-projects <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#accounts-and-projects> page.\n\nHow should I acknowledge the OLCF in my publications and presentations?\n\nUsers should acknowledge the OLCF in all publications and presentations\nthat speak to work performed on OLCF resources:\n\nThis research used resources of the Oak Ridge Leadership Computing\nFacility at the Oak Ridge National Laboratory, which is supported by the\nOffice of Science of the U.S. Department of Energy under Contract No.\nDE-AC05-00OR22725.\n\nWhat is a subproject?\n\nMany OLCF projects make use of optional subprojects. Subprojects provide\na useful means for\n\ndividing allocations among different applications, groups, or\nindividuals\n\ncontrolling priority\n\nmonitoring progress\n\nSimilar to individual user accounts being granted resources by their\nassociation with a project, individual user accounts are granted the\nresources of a subproject upon association with the subproject.\n\nSubprojects do not inherit the accesses of their primary\nprojects, and users can be associated with a subproject without\nassociation with the primary project.\n\nThe ID for a subproject must follow the format of: <6 character\nprimary project ID> + <1-4 character subproject suffix>. For\nexample, project ABC123 could have subprojects ABC123XYZ6 and ABC123P3.\nThe hours allocated for the primary project and subprojects must equal\nthe awarded allocation. All hours can be allocated to subprojects, or\nsome amount can be held as reserve as part of the primary project. It is\nrecommended that all users be assigned to a subproject(s). If all of a\nprimary project's awarded hours are allocated to its subprojects, all\nprimary project users must be associated with a subproject(s). If you\nhave any questions, or would like to request a subproject, please\ncontact the OLCF Accounts Team at accounts@ccs.ornl.gov.\n\nSubprojects are created at the request of project PIs.\n\nI no longer need my account. Who should I inform and what should I do with my OLCF issued RSA SecurID token?\n\nPlease inform the Accounts Team via email at accounts@ccs.ornl.gov and\nreturn the RSA SecurID Token to the following address depending on the\nmail service:\n\nFor US Mail:\n\nAccounts Team\nOak Ridge National Laboratory\nPO Box 2008, MS 6014\nOak Ridge, TN 37830-6014\n\nFor FedEx/UPS:\n\nAccounts Team\nOak Ridge National Laboratory\n1 Bethel Valley Road, MS 6014\nOak Ridge, TN 37831-6014\n\nMy SecurID token is broken/expired. What should I do?\n\nIf your project is still active and you require continued access to\nOLCF, you'll need to request a replacement fob. To do so, contact either\nthe User Assistance Team (help@olcf.ornl.gov) or the Accounts Team\n(accounts@olcf.ornl.gov). You do not\nneed to return the broken/expired RSA token to OLCF. Disposal and\nrecycling information can be found in the vendor's disposal\nstatement.\n\n\n\nGetting Help\n\nWhen submitting a ticket to help@olcf.ornl.gov requesting help, you will likely\nget faster resolution by supporting a few best practices:\n\nWhere possible, provide helpful details that can help speed the process. For\nexample: Project ID, relevant directories, job scripts, jobIDs, modules at\ncompile/runtime, host name, etc.\n\nWhen replying to a ticket, do not modify the subject line.\n\nDo not piggyback unrelated questions on existing tickets. This leads to slower\nresponse times and inflates ticket history.\n\nDo not open multiple tickets on the same unresolved topic. Doing so can\nfragment resources and slow down the time to resolution.\n\nPlease do not respond to previous tickets with new, unrelated issues. This can\nslow down response time and make finding relevant information harder thereby\nslowing down time to resolution.\n\nLet us know if you've solved the issue yourself (and let us know what worked!)\n\nMOST IMPORTANT: do not hesitate to contact us (help@olcf.ornl.gov); we will\nwork through the details with you.\n\nAdditional Resources\n\nWe're here to provide support at every step. We also provide a collection of\nTutorials for\napplied technical demonstrations, system-user-guides <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#system-user-guides>, Training Events, and the User Assistance\nCenter to answer\nquestions and resolve technical issues as they arise."}
{"doc":"frontier_user_guide","text":"Frontier User Guide\n\n\n\n\n\nNotable differences between Summit and Frontier:\n\nOrion scratch filesystem\n\nFrontier mounts Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace. Frontier will not mount Alpine and Summit will not mount Orion. Data will not be automatically transferred from Alpine to Orion, so we recommend that users move only needed data between the file systems with Globus.\n\nSee the frontier-data-storage <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-data-storage> section or this recording for more information.\n\nCray Programming Environment\n\nFrontier utilizes the Cray Programming Environment.  Many aspects including LMOD are similar to Summit's environment.  But Cray's compiler wrappers and the Cray and AMD compilers are worth noting.\n\nSee the frontier-compilers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers> section for more information.\n\nAMD GPUs\n\nEach frontier node has 4 AMD MI250X accelerators with two Graphic Compute Dies (GCDs) in each accelerator. The system identifies each GCD as an independent device (so for simplicity we use the term GPU when we talk about a GCD) for a total of 8 GPUs per node (compared to Summit's 6 Nvidia V100 GPUs per node). Each pair of GPUs is associated with a particular NUMA domain (see node diagram in frontier-nodes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes> section) which might affect how your application should lay out data and computation.\n\nSee the amd-gpus <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#amd-gpus> section for more information.\n\nProgramming Models\n\nSince Frontier uses AMD GPUs, code written in Nvidia's CUDA language will not work as is. They need to be converted to use HIP, which is AMD's GPU programming framework, or should be converted to some other GPU framework that supports AMD GPUs as a backend e.g. OpenMP Offload, Kokkos, RAJA, OCCA, SYCL/DPC++ etc .\n\nSee the amd-hip <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#amd-hip> section for more information and links about HIP.\n\nSlurm batch scheduler\n\nFrontier uses SchedMD's Slurm Workload Manager for job scheduling instead of IBM's LSF. Slurm provides similar functionality to LSF, albeit with different commands.  Notable are the separation in batch script submission (sbatch) and interactive batch submission (salloc).\n\nSee the frontier-slurm <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-slurm> section for more infomation including a LSF to Slurm command comparison.\n\nSrun job launcher\n\nFrontier uses Slurm's job launcher, srun, instead of Summit's jsrun to launch parallel jobs within a batch script.  Overall functionality is similar, but commands are notably different. Frontier's compute node layout <frontier-simple> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#compute node layout <frontier-simple>> should also be considered when selecting job layout.\n\nSee the frontier-srun <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-srun> section for more srun information, and see frontier-mapping <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-mapping> for srun examples on Frontier.\n\nOLCF Support\n\nIf you encounter any issues or have questions, please contact the OLCF via the following:\n\nEmail us at help@olcf.ornl.gov\n\nContact your OLCF liaison\n\nSign-up to attend OLCF Office Hours\n\nSystem Overview\n\nFrontier is a HPE Cray EX supercomputer located at the Oak Ridge Leadership Computing Facility. With a theoretical peak double-precision performance of approximately 2 exaflops (2 quintillion calculations per second), it is the fastest system in the world for a wide range of traditional computational science applications. The system has 74 Olympus rack HPE cabinets, each with 128 AMD compute nodes, and a total of 9,408 AMD compute nodes.\n\n\n\nFrontier Compute Nodes\n\nEach Frontier compute node consists of [1x] 64-core AMD \"Optimized 3rd Gen EPYC\" CPU (with 2 hardware threads per physical core) with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X, each with 2 Graphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can think of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth memory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU, allowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of 36+36 GB/s. The 2 GCDs on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The GCDs on different MI250X are connected with Infinity Fabric GPU-GPU in the arrangement shown in the Frontier Node Diagram below, where the peak bandwidth ranges from 50-100 GB/s based on the number of Infinity Fabric connections between individual GCDs.\n\nTERMINOLOGY:\n\nThe 8 GCDs contained in the 4 MI250X will show as 8 separate GPUs according to Slurm, ROCR_VISIBLE_DEVICES, and the ROCr runtime, so from this point forward in the quick-start guide, we will simply refer to the GCDs as GPUs.\n\nFrontier node architecture diagram\n\n\n\nThere are [4x] NUMA domains per node and [2x] L3 cache regions per NUMA for a total of [8x] L3 cache regions. The 8 GPUs are each associated with one of the L3 regions as follows:\n\nNUMA 0:\n\nhardware threads 000-007, 064-071 | GPU 4\n\nhardware threads 008-015, 072-079 | GPU 5\n\nNUMA 1:\n\nhardware threads 016-023, 080-087 | GPU 2\n\nhardware threads 024-031, 088-095 | GPU 3\n\nNUMA 2:\n\nhardware threads 032-039, 096-103 | GPU 6\n\nhardware threads 040-047, 104-111 | GPU 7\n\nNUMA 3:\n\nhardware threads 048-055, 112-119 | GPU 0\n\nhardware threads 056-063, 120-127 | GPU 1\n\nBy default, Frontier reserves the first core in each L3 cache region. Frontier uses low-noise mode,\nwhich constrains all system processes to core 0. Low-noise mode cannot be disabled by users.\nIn addition, Frontier uses SLURM core specialization (-S 8 flag at job allocation time, e.g., sbatch)\nto reserve one core from each L3 cache region, leaving 56 allocatable cores. Set -S 0 at job allocation to override this setting.\n\nNode Types\n\nOn Frontier, there are two major types of nodes you will encounter: Login and Compute. While these are\nsimilar in terms of hardware (see: frontier-nodes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes>), they differ considerably in their intended\nuse.\n\n\nThe table presents a comparison between the AMD and NVIDIA technologies used in the Frontier supercomputer. The first column lists the different components of the supercomputer, while the second column specifies the equivalent term used by each technology. The first row, \"Work-items or Threads,\" refers to the smallest unit of work that can be executed in parallel by the supercomputer. In AMD, this is referred to as \"Threads,\" while in NVIDIA, it is also known as \"Threads.\" The second row, \"Workgroup,\" represents a group of work-items or threads that can communicate and synchronize with each other. In AMD, this is called \"Block,\" while in NVIDIA, it is referred to as \"Block.\" The third row, \"Wavefront,\" refers to a group of work-items or threads that are executed together in a SIMD (Single Instruction Multiple Data) fashion. In AMD, this is known as \"Wavefront,\" while in NVIDIA, it is called \"Warp.\" The last row, \"Grid,\" represents a collection of workgroups or blocks that are executed together. In AMD, this is referred to as \"Grid,\" while in NVIDIA, it is known as \"Grid.\"\n\n| AMD | NVIDIA |\n| --- | --- |\n| Work-items or Threads | Threads |\n| Workgroup | Block |\n| Wavefront | Warp |\n| Grid | Grid |\n\n\n\nSystem Interconnect\n\nThe Frontier nodes are connected with [4x] HPE Slingshot 200 Gbps (25 GB/s) NICs providing a node-injection bandwidth of 800 Gbps (100 GB/s).\n\nFile Systems\n\nFrontier is connected to Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Data will not be automatically transferred from Alpine to Orion. Frontier also has access to the center-wide NFS-based filesystem (which provides user and project home areas). Each compute node has two 1.92TB Non-Volatile Memory storage devices. See frontier-data-storage <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-data-storage> for more information.\n\nFrontier connects to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the dtn-user-guide <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#dtn-user-guide> to move data to/from HPSS.\n\nOperating System\n\nFrontier is running Cray OS 2.4 based on SUSE Linux Enterprise Server (SLES) version 15.4.\n\nGPUs\n\nEach Frontier compute node contains 4 AMD MI250X. The AMD MI250X has a peak performance of 53 TFLOPS in double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 26.5 TFLOPS (double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in each direction simultaneously).\n\nConnecting\n\nTo connect to Frontier, ssh to frontier.olcf.ornl.gov. For example:\n\n$ ssh <username>@frontier.olcf.ornl.gov\n\nFor more information on connecting to OLCF resources, see connecting-to-olcf <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#connecting-to-olcf>.\n\nBy default, connecting to Frontier will automatically place the user on a random login node. If you need to access a specific login node, you will ssh to that node after your intial connection to Frontier.\n\n[<username>@login12.frontier ~]$ ssh <username>@login01.frontier.olcf.ornl.gov\n\nUsers can connect to any of the 17 Frontier login nodes by replacing login01 with their login node of choice.\n\n\n\n\n\nData and Storage\n\nTransition from Alpine to Orion\n\nFrontier mounts Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Frontier will not mount Alpine when Orion is in production.\n\nData will not be automatically transferred from Alpine to Orion. Users should consider the data needed from Alpine and transfer it. Globus is the preferred method and there is access to both Orion and Alpine through the Globus OLCF DTN endpoint. See data-transferring-data-globus <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-transferring-data-globus>. Use of HPSS to specifically stage data for the Alpine to Orion transfer is discouraged.\n\nOn Alpine, there was no user-exposed concept of file striping, the process of dividing a file between the storage elements of the filesystem. Orion uses a feature called Progressive File Layout (PFL) that changes the striping of files as they grow. Because of this, we ask users not to manually adjust the file striping. If you feel the default striping behavior of Orion is not meeting your needs, please contact help@olcf.ornl.gov.\n\nAs with Alpine, files older than 90 days are purged from Orion.  Please plan your data management and lifecycle at OLCF before generating the data.\n\nFor more detailed information about center-wide file systems and data archiving available on Frontier, please refer to the pages on data-storage-and-transfers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers>. The subsections below give a quick overview of NFS, Lustre,and HPSS storage spaces as well as the on node NVMe \"Burst Buffers\" (SSDs).\n\nNFS Filesystem\n\n\nThe table presents a list of commands and their corresponding descriptions for the Frontier supercomputer. The first row serves as the header, with the first column indicating the command and the second column providing a brief explanation of its function. The first command, \"module -t list\", displays a concise list of the modules currently loaded on the system. The second command, \"module avail\", shows a table of all available modules. The third command, \"module help <modulename>\", provides help information for a specific module. The fourth command, \"module show <modulename>\", displays the changes made to the environment by a specific module. The fifth command, \"module spider <string>\", allows for searching of all possible modules based on a given string. The sixth command, \"module load <modulename> [...]\", loads the specified module(s) into the current environment. The seventh command, \"module use <path>\", adds a given path to the modulefile search cache and MODULESPATH. The eighth command, \"module unuse <path>\", removes a given path from the modulefile search cache and MODULESPATH. The ninth command, \"module purge\", unloads all currently loaded modules. The tenth command, \"module reset\", resets all loaded modules to their system defaults. Lastly, the eleventh command, \"module update\", reloads all currently loaded modules. This table provides a comprehensive overview of the various commands available for managing modules on the Frontier supercomputer.\n\n| Command | Description |\n|---------|-------------|\n| module -t list | Shows a terse list of the currently loaded modules |\n| module avail | Shows a table of the currently available modules |\n| module help <modulename> | Shows help information about <modulename> |\n| module show <modulename> | Shows the environment changes made by the <modulename> modulefile |\n| module spider <string> | Searches all possible modules according to <string> |\n| module load <modulename> [...] | Loads the given <modulename>(s) into the current environment |\n| module use <path> | Adds <path> to the modulefile search cache and MODULESPATH |\n| module unuse <path> | Removes <path> from the modulefile search cache and MODULESPATH |\n| module purge | Unloads all modules |\n| module reset | Resets loaded modules to system defaults |\n| module update | Reloads all currently loaded modules |\n\nThough the NFS filesystem's User Home and Project Home areas are read/write from Frontier's compute nodes,\nwe strongly recommend that users launch and run jobs from the Lustre Orion parallel filesystem\ninstead due to its larger storage capacity and superior performance. Please see below for Lustre\nOrion filesystem storage areas and paths.\n\nLustre Filesystem\n\nTable-3-to-be-replaced\n\nHPSS Archival Storage\n\nPlease note that the HPSS is not mounted directly onto Frontier nodes. There are two main methods for accessing and moving data to/from the HPSS. The first is to use the command line utilities hsi and htar. The second is to use the Globus data transfer service. See data-hpss <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-hpss> for more information on both of these methods.\n\nTable-4-to-be-replaced\n\nNVMe\n\nEach compute node on Frontier has [2x] 1.92TB Non-Volatile Memory (NVMe) storage devices (SSDs), colloquially known as a \"Burst Buffer\" with a peak sequential performance of 5500 MB/s (read) and 2000 MB/s (write). The purpose of the Burst Buffer system is to bring improved I/O performance to appropriate workloads. Users are not required to use the NVMes. Data can also be written directly to the parallel filesystem.\n\n\n\nThe NVMes on Frontier are local to each node.\n\nNVMe Usage\n\nTo use the NVMe, users must request access during job allocation using the -C nvme option to sbatch, salloc, or srun. Once the devices have been granted to a job, users can access them at /mnt/bb/<userid>. Users are responsible for moving data to/from the NVMe before/after their jobs. Here is a simple example script:\n\n#!/bin/bash\n#SBATCH -A <projid>\n#SBATCH -J nvme_test\n#SBATCH -o %x-%j.out\n#SBATCH -t 00:05:00\n#SBATCH -p batch\n#SBATCH -N 1\n#SBATCH -C nvme\n\ndate\n\n# Change directory to user scratch space (GPFS)\ncd /gpfs/alpine/<projid>/scratch/<userid>\n\necho \" \"\necho \"*****ORIGINAL FILE*****\"\ncat test.txt\necho \"***********************\"\n\n# Move file from GPFS to SSD\nmv test.txt /mnt/bb/<userid>\n\n# Edit file from compute node\nsrun -n1 hostname >> /mnt/bb/<userid>/test.txt\n\n# Move file from SSD back to GPFS\nmv /mnt/bb/<userid>/test.txt .\n\necho \" \"\necho \"*****UPDATED FILE******\"\ncat test.txt\necho \"***********************\"\n\nAnd here is the output from the script:\n\n$ cat nvme_test-<jobid>.out\n\n*****ORIGINAL FILE*****\nThis is my file. There are many like it but this one is mine.\n***********************\n\n*****UPDATED FILE******\nThis is my file. There are many like it but this one is mine.\nfrontier0123\n***********************\n\nUsing Globus to Move Data to Orion\n\nThe following example is intended to help users who are making the transition from Summit to Frontier to move their data between Alpine GPFS and Orion Lustre. We strongly recommend using Globus for this transfer as it is the method that is most efficient for users and that causes the least contention on filesystems and data transfer nodes.\n\nGlobus Warnings:\n\nGlobus transfers do not preserve file permissions. Arriving files will have (rw-r--r--) permissions, meaning arriving file will have user read and write permissions and group and world read permissions. Note that the arriving files will not have any execute permissions, so you will need to use chmod to reset execute permissions before running a Globus-transferred executable.\n\nGlobus will overwrite files at the destination with identically named source files. This is done without warning.\n\nGlobus has restriction of 8 active transfers across all the users. Each user has a limit of 3 active transfers, so it is required to transfer a lot of data on each transfer than less data across many transfers.\n\nIf a folder is constituted with mixed files including thousands of small files (less than 1MB each one), it would be better to tar the small files.  Otherwise, if the files are larger, Globus will handle them.\n\n<string>:5: (INFO/1) Duplicate implicit target name: \"using globus to move data to orion\".\n\nHere is a recording of an example transfer from Alpine to Orion using Globus and the OLCF DTN: Using Globus to Move Data to Orion.\n\nBelow is a summary of the steps for data transfer given in the recording:\n\n1.      Login to globus.org using your globus ID and password. If you do not have a globusID, set one up here:\nGenerate a globusID.\n\nOnce you are logged in, Globus will open the “File Manager” page. Click the left side “Collection” text field in the File Manager and type “OLCF DTN”.\n\nWhen prompted, authenticate into the OLCF DTN endpoint using your OLCF username and PIN followed by your RSA passcode.\n\nClick in the left side “Path” box in the File Manager and enter the path to your data on Alpine. For example, /gpfs/alpine/stf007/proj-shared/my_alpine_data. You should see a list of your files and folders under the left “Path” Box.\n\nClick on all files or folders that you want to transfer in the list. This will highlight them.\n\nClick on the right side “Collection” box in the File Manager and type “OLCF DTN”\n\nClick in the right side “Path” box and enter the path where you want to put your data on Orion, for example, /lustre/orion/stf007/proj-shared/my_orion_data\n\nClick the left \"Start\" button.\n\nClick on “Activity“ in the left blue menu bar to monitor your transfer. Globus will send you an email when the transfer is complete.\n\n<string>:5: (INFO/1) Enumerated list start value not ordinal-1: \"2\" (ordinal 2)\n\n\n\nAMD GPUs\n\nThe AMD Instinct MI200 is built on advanced packaging technologies\nenabling two Graphic Compute Dies (GCDs) to be integrated\ninto a single package in the Open Compute Project (OCP) Accelerator Module (OAM)\nin the MI250 and MI250X products.\nEach GCD is build on the AMD CDNA 2 architecture.\nA single Frontier node contains 4 MI250X OAMs for the total of 8 GCDs.\n\nThe Slurm workload manager and the ROCr runtime treat each GCD as a separate GPU\nand visibility can be controlled using the ROCR_VISIBLE_DEVICES environment variable.\nTherefore, from this point on, the Frontier guide simply refers to a GCD as a GPU.\n\nEach GPU contains 110 Compute Units (CUs) grouped in 4 Compute Engines (CEs).\nPhysically, each GPU contains 112 CUs, but two are disabled.\nA command processor in each GPU receives API commands and transforms them into compute tasks.\nCompute tasks are managed by the 4 compute engines, which dispatch wavefronts to compute units.\nAll wavefronts from a single workgroup are assigned to the same CU.\nIn CUDA terminology, workgroups are \"blocks\", wavefronts are \"warps\", and work-items are \"threads\".\nThe terms are often used interchangeably.\n\nBlock diagram of the AMD Instinct MI200 multi-chip module\n\nThe 110 CUs in each GPU deliver peak performance of 26.5 TFLOPS in double precision.\nAlso, each GPU contains 64 GB of high-bandwidth memory (HBM2) accessible at a peak\nbandwidth of 1.6 TB/s.\nThe 2 GPUs in an MI250X are connected with [4x] GPU-to-GPU Infinity Fabric links\nproviding 200+200 GB/s of bandwidth.\n(Consult the diagram in the frontier-nodes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes> section for information\non how the accelerators are connected to each other, to the CPU, and to the network.\n\nThe X+X GB/s notation describes bidirectional bandwidth, meaning X GB/s in each direction.\n\nTODO: unified memory? If mi250x has it, what is it and how does it work\nTODO: link to HIP from scratch tutorial\nTODO: here are some references https://www.amd.com/system/files/documents/amd-cdna2-white-paper.pdf and https://www.amd.com/system/files/documents/amd-instinct-mi200-datasheet.pdf\n\n\n\nAMD vs NVIDIA Terminology\n\nTable-5-to-be-replaced\n\nWe will be using these terms interchangeably as they refer to the same concepts in GPU\nprogramming, with the exception that we will only be using \"wavefront\" (which refers to a\nunit of 64 threads) instead of \"warp\" (which refers to a unit of 32 threads) as they mean\ndifferent things.\n\nBlocks (workgroups), Threads (work items), Grids, Wavefronts\n\nWhen kernels are launched on a GPU, a \"grid\" of thread blocks are created, where the\nnumber of thread blocks in the grid and the number of threads within each block are\ndefined by the programmer. The number of blocks in the grid (grid size) and the number of\nthreads within each block (block size) can be specified in one, two, or three dimensions\nduring the kernel launch. Each thread can be identified with a unique id within the\nkernel, indexed along the X, Y, and Z dimensions.\n\nNumber of blocks that can be specified along each dimension in a grid: (2147483647, 2147483647, 2147483647)\n\nMax number of threads that can be specified along each dimension in a block: (1024, 1024, 1024)\n\nHowever, the total of number of threads in a block has an upper limit of 1024\n[i.e. (size of x dimension * size of y dimension * size of z dimension) cannot exceed\n1024].\n\nEach block (or workgroup) of threads is assigned to a single Compute Unit i.e. a single\nblock won’t be split across multiple CUs. The threads in a block are scheduled in units of\n64 threads called wavefronts (similar to warps in CUDA, but warps only have 32 threads\ninstead of 64). When launching a kernel, up to 64KB of block level shared memory called\nthe Local Data Store (LDS) can be statically or dynamically allocated. This shared memory\nbetween the threads in a block allows the threads to access block local data with much\nlower latency compared to using the HBM since the data is in the compute unit itself.\n\nThe Compute Unit\n\nBlock diagram of the AMD Instinct CDNA2 Compute Unit\n\nEach CU has 4 Matrix Core Units (the equivalent of NVIDIA's Tensor core units) and 4\n16-wide SIMD units. For a vector instruction that uses the SIMD units, each wavefront\n(which has 64 threads) is assigned to a single 16-wide SIMD unit such that the wavefront\nas a whole executes the instruction over 4 cycles, 16 threads per cycle. Since other\nwavefronts occupy the other three SIMD units at the same time, the total throughput still\nremains 1 instruction per cycle. Each CU maintains an instructions buffer for 10\nwavefronts and also maintains 256 registers where each register is 64 4-byte wide\nentries.\n\n\n\nHIP\n\nThe Heterogeneous Interface for Portability (HIP) is AMD’s dedicated GPU programming\nenvironment for designing high performance kernels on GPU hardware. HIP is a C++ runtime\nAPI and programming language that allows developers to create portable applications on\ndifferent platforms, including the AMD MI250X. This means that developers can write their GPU applications and with\nvery minimal changes be able to run their code in any environment.  The API is very\nsimilar to CUDA, so if you're already familiar with CUDA there is almost no additional\nwork to learn HIP. See here for a series\nof tutorials on programming with HIP and also converting existing CUDA code to HIP with the hipify tools .\n\nThings To Remember When Programming for AMD GPUs\n\nThe MI250X has different denormal handling for FP16 and BF16 datatypes, which is relevant for ML training. Prefer using the BF16 over the FP16 datatype for ML models as you are more likely to encounter denormal values with FP16 (which get flushed to zero, causing failure in convergence for some ML models). See more in using-reduced-precision <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#using-reduced-precision>.\n\nMemory can be automatically migrated to GPU from CPU on a page fault if XNACK operating mode is set.  No need to explicitly migrate data or provide managed memory. This is useful if you're migrating code from a programming model that relied on 'unified' or 'managed' memory. See more in enabling-gpu-page-migration <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#enabling-gpu-page-migration>. Information about how memory is accessed based on the allocator used and the XNACK mode can be found in migration-of-memory-allocator-xnack <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#migration-of-memory-allocator-xnack>.\n\nHIP has two kinds of memory allocations, coarse grained and fine grained, with tradeoffs between performance and coherence. Particularly relevant if you want to ues the hardware FP atomic instructions. See more in fp-atomic-ops-coarse-fine-allocations <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#fp-atomic-ops-coarse-fine-allocations>.\n\nFP32 atomicAdd operations on Local Data Store (i.e. block shared memory) can be slower than the equivalent FP64 operations. See more in performance-lds-atomicadd <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#performance-lds-atomicadd>.\n\nSee the frontier-compilers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers> section for information on compiling for AMD GPUs, and\nsee the tips-and-tricks <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks> section for some detailed information to keep in mind\nto run more efficiently on AMD GPUs.\n\nProgramming Environment\n\nFrontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.\n\nEnvironment Modules (Lmod)\n\nEnvironment modules are provided through Lmod, a Lua-based module system for dynamically altering shell environments. By managing changes to the shell’s environment variables (such as PATH, LD_LIBRARY_PATH, and PKG_CONFIG_PATH), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.\n\nGeneral Usage\n\nThe interface to Lmod is provided by the module command:\n\nTable-6-to-be-replaced\n\nSearching for Modules\n\nModules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the spider sub-command can be used as summarized in the following table.\n\nTable-7-to-be-replaced\n\nCompilers\n\nCray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers. Please see the following frontier-compilers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers> section for more detailed inforation on how to compile using these modules.\n\nCray Programming Environment and Compiler Wrappers\n\nCray provides PrgEnv-<compiler> modules (e.g., PrgEnv-cray) that load compatible components of a specific compiler toolchain. The components include the specified compiler as well as MPI, LibSci, and other libraries. Loading the PrgEnv-<compiler> modules also defines a set of compiler wrappers for that compiler toolchain that automatically add include paths and link in libraries for Cray software. Compiler wrappers are provided for C (cc), C++ (CC), and Fortran (ftn).\n\nUse the -craype-verbose flag to display the full include and link information used by the Cray compiler wrappers. This must be called on a file to see the full output (e.g., CC -craype-verbose test.cpp).\n\nMPI\n\nThe MPI implementation available on Frontier is Cray's MPICH, which is \"GPU-aware\" so GPU buffers can be passed directly to MPI calls.\n\n\n\n\n\nCompiling\n\nCompilers\n\n<string>:581: (INFO/1) Duplicate implicit target name: \"compilers\".\n\nCray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers.\n\nIt is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) whenever possible. See the next section for more details.\n\nTable-8-to-be-replaced\n\nCray Programming Environment and Compiler Wrappers\n\n<string>:614: (INFO/1) Duplicate implicit target name: \"cray programming environment and compiler wrappers\".\n\nCray provides PrgEnv-<compiler> modules (e.g., PrgEnv-cray) that load compatible components of a specific compiler toolchain. The components include the specified compiler as well as MPI, LibSci, and other libraries. Loading the PrgEnv-<compiler> modules also defines a set of compiler wrappers for that compiler toolchain that automatically add include paths and link in libraries for Cray software. Compiler wrappers are provided for C (cc), C++ (CC), and Fortran (ftn).\n\nFor example, to load the AMD programming environment, do:\n\nmodule load PrgEnv-amd\n\nThis module will setup your programming environment with paths to software and libraries that are compatible with AMD compilers.\n\nUse the -craype-verbose flag to display the full include and link information used by the Cray compiler wrappers. This must be called on a file to see the full output (e.g., CC -craype-verbose test.cpp).\n\n\n\nExposing The ROCm Toolchain to your Programming Environment\n\nIf you need to add the tools and libraries related to ROCm, the framework for targeting AMD GPUs, to your path, you will need to use a version of ROCm that is compatible with your programming environment.\n\nThe following modules help you expose the ROCm Toolchain to your programming Environment:\n\nTable-9-to-be-replaced\n\nBoth the CCE and ROCm compilers are Clang-based, so please be sure to use consistent (major) Clang versions when using them together. You can check which version of Clang is being used with CCE and ROCm by giving the --version flag to CC and hipcc, respectively.\n\nMPI\n\n<string>:653: (INFO/1) Duplicate implicit target name: \"mpi\".\n\nThe MPI implementation available on Frontier is Cray's MPICH, which is \"GPU-aware\" so GPU buffers can be passed directly to MPI calls.\n\nTable-10-to-be-replaced\n\nhipcc requires the ROCm Toolclain, See exposing-the-rocm-toolchain-to-your-programming-environment <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#exposing-the-rocm-toolchain-to-your-programming-environment>\n\nGPU-Aware MPI\n\nTo use GPU-aware Cray MPICH, with Frontier's PrgEnv modules, users must set the following modules and environment variables:\n\nIf using PrgEnv-amd:\n\nmodule load craype-accel-amd-gfx90a\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nIf using PrgEnv-cray:\n\nmodule load craype-accel-amd-gfx90a\nmodule load amd-mixed\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nThere are extra steps needed to enable GPU-aware MPI on Frontier, which depend on the compiler that is used (see 1. and 2. below).\n\n1. Compiling with the Cray compiler wrappers, cc or CC\n\nWhen using GPU-aware Cray MPICH with the Cray compiler wrappers, most of the needed libraries are automatically linked through the environment variables.\n\nThough, the following header files and libraries must be included explicitly:\n\n-I${ROCM_PATH}/include\n-L${ROCM_PATH}/lib -lamdhip64\n\nwhere the include path implies that #include <hip/hip_runtime.h> is included in the source file.\n\n2. Compiling with hipcc\n\nTo use hipcc with GPU-aware Cray MPICH, use the following environment variables to setup the needed header files and libraries.\n\n-I${MPICH_DIR}/include\n-L${MPICH_DIR}/lib -lmpi \\\n  ${CRAY_XPMEM_POST_LINK_OPTS} -lxpmem \\\n  ${PE_MPICH_GTL_DIR_amd_gfx90a} ${PE_MPICH_GTL_LIBS_amd_gfx90a}\n\nHIPFLAGS = --amdgpu-target=gfx90a\n\nDetermining the Compatibility of Cray MPICH and ROCm\n\nReleases of cray-mpich are each built with a specific version of ROCm, and compatibility across multiple versions is not guaranteed. OLCF will maintain compatible default modules when possible. If using non-default modules, you can determine compatibility by reviewing the Product and OS Dependencies section in the cray-mpich release notes. This can be displayed by running module show cray-mpich/<version>. If the notes indicate compatibility with AMD ROCM X.Y or later, only use rocm/X.Y.Z modules. If using a non-default version of cray-mpich, you must add ${CRAY_MPICH_ROOTDIR}/gtl/lib to either your LD_LIBRARY_PATH at run time or your executable's rpath at build time.\n\nThe compatibility table below was determined by linker testing with all current combinations of cray-mpich and ROCm-related modules on Frontier.\n\nTable-11-to-be-replaced\n\nOpenMP\n\nThis section shows how to compile with OpenMP using the different compilers covered above.\n\nTable-12-to-be-replaced\n\nOpenMP GPU Offload\n\nThis section shows how to compile with OpenMP Offload using the different compilers covered above.\n\nMake sure the craype-accel-amd-gfx90a module is loaded when using OpenMP offload.\n\nTable-13-to-be-replaced\n\nIf invoking amdclang, amdclang++, or amdflang directly for openmp offload, or using hipcc you will need to add:\n\n-fopenmp -target x86_64-pc-linux-gnu -fopenmp-targets=amdgcn-amd-amdhsa -Xopenmp-target=amdgcn-amd-amdhsa -march=gfx90a.\n\nOpenACC\n\nThis section shows how to compile code with OpenACC. Currently only the Cray compiler supports OpenACC for Fortran. The AMD and\nGNU programming environments do not support OpenACC at all.\nC and C++ support for OpenACC is provided by clacc which maintains a fork of the LLVM\ncompiler with added support for OpenACC. It can be obtained by loading the UMS modules\nums, ums025, and clacc.\n\nTable-14-to-be-replaced\n\nHIP\n\n<string>:831: (INFO/1) Duplicate implicit target name: \"hip\".\n\nThis section shows how to compile HIP codes using the Cray compiler wrappers and hipcc compiler driver.\n\nMake sure the craype-accel-amd-gfx90a module is loaded when compiling HIP with the Cray compiler wrappers.\n\nTable-15-to-be-replaced\n\nhipcc requires the ROCm Toolclain, See exposing-the-rocm-toolchain-to-your-programming-environment <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#exposing-the-rocm-toolchain-to-your-programming-environment>\n\nInformation about compiling code for different XNACK modes (which control page migration between GPU and CPU memory) can be found in the compiling-hip-kernels-for-xnack-modes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#compiling-hip-kernels-for-xnack-modes> section.\n\nHIP + OpenMP CPU Threading\n\nThis section shows how to compile HIP + OpenMP CPU threading hybrid codes.\n\nMake sure the craype-accel-amd-gfx90a module is loaded when compiling HIP with the Cray compiler wrappers.\n\nTable-16-to-be-replaced\n\nhipcc requires the ROCm Toolclain, See exposing-the-rocm-toolchain-to-your-programming-environment <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#exposing-the-rocm-toolchain-to-your-programming-environment>\n\n\n\n\n\nRunning Jobs\n\nComputational work on Frontier is performed by jobs. Jobs typically consist of several componenets:\n\nA batch submission script\n\nA binary executable\n\nA set of input files for the executable\n\nA set of output files created by the executable\n\nIn general, the process for running a job is to:\n\nPrepare executables and input files.\n\nWrite a batch script.\n\nSubmit the batch script to the batch scheduler.\n\nOptionally monitor the job before and during execution.\n\nThe following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.\n\nLogin vs Compute Nodes\n\nRecall from the System Overview that Frontier contains two node types: Login and Compute. When you connect to the system, you are placed on a login node. Login nodes are used for tasks such as code editing, compiling, etc. They are shared among all users of the system, so it is not appropriate to run tasks that are long/computationally intensive on login nodes. Users should also limit the number of simultaneous tasks on login nodes (e.g. concurrent tar commands, parallel make\n\nCompute nodes are the appropriate place for long-running, computationally-intensive tasks. When you start a batch job, your batch script (or interactive shell for batch-interactive jobs) runs on one of your allocated compute nodes.\n\nCompute-intensive, memory-intensive, or other disruptive processes running on login nodes may be killed without warning.\n\nUnlike Summit and Titan, there are no launch/batch nodes on Frontier. This means your batch script runs on a node allocated to you rather than a shared node. You still must use the job launcher (srun) to run parallel jobs across all of your nodes, but serial tasks need not be launched with srun.\n\n\n\nSimplified Node Layout\n\nTo easily visualize job examples (see frontier-mapping <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-mapping> further below), the\ncompute node diagram has been simplified to the picture shown below.\n\nSimplified Frontier node architecture diagram\n\nIn the diagram, each physical core on a Frontier compute node is composed\nof two logical cores that are represented by a pair of blue and grey boxes.\nFor a given physical core, the blue box represents the logical core of the\nfirst hardware thread, where the grey box represents the logical core of the\nsecond hardware thread.\n\n\n\nLow-noise Mode Layout\n\nFrontier uses low-noise mode and core specialization (-S flag at job\nallocation, e.g., sbatch).  Low-noise mode constrains all system processes\nto core 0.  Core specialization (by default, -S 8) reserves the first core\nin each L3 region.  This prevents the user running on the core that system\nprocesses are constrained to.  This also means that there are only 56\nallocatable cores by default instead of 64. Therefore, this modifies the\nsimplified node layout to:\n\nSimplified Frontier node architecture diagram (low-noise mode)\n\nTo override this default layout (not recommended), set -S 0 at job allocation.\n\n\n\nSlurm\n\nFrontier uses SchedMD's Slurm Workload Manager for scheduling and managing jobs. Slurm maintains similar functionality to other schedulers such as IBM's LSF, but provides unique control of Frontier's resources through custom commands and options specific to Slurm. A few important commands can be found in the conversion table below, but please visit SchedMD's Rosetta Stone of Workload Managers for a more complete conversion reference.\n\nSlurm documentation for each command is available via the man utility, and on the web at https://slurm.schedmd.com/man_index.html. Additional documentation is available at https://slurm.schedmd.com/documentation.html.\n\nSome common Slurm commands are summarized in the table below. More complete examples are given in the Monitoring and Modifying Batch Jobs section of this guide.\n\nTable-17-to-be-replaced\n\nBatch Scripts\n\nThe most common way to interact with the batch system is via batch scripts. A batch script is simply a shell script with added directives to request various resoruces from or provide certain information to the scheduling system.  Aside from these directives, the batch script is simply the series of commands needed to set up and run your job.\n\nTo submit a batch script, use the command sbatch myjob.sl\n\nConsider the following batch script:\n\n#!/bin/bash\n#SBATCH -A ABC123\n#SBATCH -J RunSim123\n#SBATCH -o %x-%j.out\n#SBATCH -t 1:00:00\n#SBATCH -p batch\n#SBATCH -N 1024\n\ncd $MEMBERWORK/abc123/Run.456\ncp $PROJWORK/abc123/RunData/Input.456 ./Input.456\nsrun ...\ncp my_output_file $PROJWORK/abc123/RunData/Output.456\n\nIn the script, Slurm directives are preceded by #SBATCH, making them appear as comments to the shell. Slurm looks for these directives through the first non-comment, non-whitespace line. Options after that will be ignored by Slurm (and the shell).\n\nTable-18-to-be-replaced\n\n\n\nInteractive Jobs\n\nMost users will find batch jobs an easy way to use the system, as they allow you to \"hand off\" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.\n\nSince all compute resources are managed and scheduled by Slurm, it is not possible to simply log into the system and immediately begin running parallel codes interactively. Rather, you must request the appropriate resources from Slurm and, if necessary, wait for them to become available. This is done through an \"interactive batch\" job. Interactive batch jobs are submitted with the salloc command. Resources are requested via the same options that are passed via #SBATCH in a regular batch script (but without the #SBATCH prefix). For example, to request an interactive batch job with the same resources that the batch script above requests, you would use salloc -A ABC123 -J RunSim123 -t 1:00:00 -p batch -N 1024. Note there is no option for an output file...you are running interactively, so standard output and standard error will be displayed to the terminal.\n\n\n\nCommon Slurm Options\n\nThe table below summarizes options for submitted jobs. Unless otherwise noted, they can be used for either batch scripts or interactive batch jobs. For scripts, they can be added on the sbatch command line or as a #BSUB directive in the batch script. (If they're specified in both places, the command line takes precedence.) This is only a subset of all available options. Check the Slurm Man Pages for a more complete list.\n\nTable-19-to-be-replaced\n\nSlurm Environment Variables\n\nSlurm reads a number of environment variables, many of which can provide the same information as the job options noted above. We recommend using the job options rather than environment variables to specify job options, as it allows you to have everything self-contained within the job submission script (rather than having to remember what options you set for a given job).\n\nSlurm also provides a number of environment variables within your running job. The following table summarizes those that may be particularly useful within your job (e.g. for naming output log files):\n\nTable-20-to-be-replaced\n\nJob States\n\nA job will transition through several states during its lifetime. Common ones include:\n\nTable-21-to-be-replaced\n\nJob Reason Codes\n\nIn addition to state codes, jobs that are pending will have a \"reason code\" to explain why the job is pending. Completed jobs will have a reason describing how the job ended. Some codes you might see include:\n\nTable-22-to-be-replaced\n\nMany other states and job reason codes exist. For a more complete description, see the squeue man page (either on the system or online).\n\nScheduling Policy\n\nIn a simple batch queue system, jobs run in a first-in, first-out (FIFO) order. This can lead to inefficient use of the system. If a large job is the next to run, a strict FIFO queue can cause nodes to sit idle while waiting for the large job to start. Backfilling would allow smaller, shorter jobs to use those resources that would otherwise remain idle until the large job starts. With the proper algorithm, they would do so without impacting the start time of the large job. While this does make more efficient use of the system, it encourages the submission of smaller jobs.\n\nThe DOE Leadership-Class Job Mandate\n\nAs a DOE Leadership Computing Facility, OLCF has a mandate that a large portion of Frontier's usage come from large, leadership-class (a.k.a. capability) jobs. To ensure that OLCF complies with this directive, we strongly encourage users to run jobs on Frontier that are as large as their code will allow. To that end, OLCF implements queue policies that enable large jobs to run in a timely fashion.\n\nThe OLCF implements queue policies that encourage the submission and timely execution of large, leadership-class jobs on Frontier.\n\nThe basic priority mechanism for jobs waiting in the queue is the time the job has been waiting in the queue. If your jobs require resources outside these policies such as higher priority or longer walltimes, please contact help@olcf.ornl.gov\n\nJob Priority by Node Count\n\nJobs are aged according to the job's requested node count (older\nage equals higher queue priority). Each job's requested node count\nplaces it into a specific bin. Each bin has a different aging\nparameter, which all jobs in the bin receive.\n\nTable-23-to-be-replaced\n\nbatch Queue Policy\n\nThe batch queue is the default queue for production work on Frontier. Most work on Frontier is handled through this queue. The following policies are enforced for the batch queue:\n\nLimit of four eligible-to-run jobs per user. (Jobs in excess of this number will be held, but will move to the eligible-to-run state at the appropriate time.)\n\nUsers may have only 100 jobs queued in the batch queue at any time (this includes jobs in all states). Additional jobs will be rejected at submit time.\n\ndebug Quality of Service Class\n\nThe debug quality of service (QOS) class can be used to access Frontier's compute resources for short non-production debug tasks. The QOS provides a higher priority compare to jobs of the same job size bin in production queues. Production work and job chaining using the debug QOS is prohibited. Each user is limited to one job in any state at any one point. Attempts to submit multiple jobs to this QOS will be rejected upon job submission.\n\nTo submit a job to the debug QOS, add the -q debug option to your sbatch or salloc command or #SBATCH -q debug to your job script.\n\nAllocation Overuse Policy\n\nProjects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used. For example, consider that job1 is submitted at the same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. Additionally, projects that are at 125% of their allocated time will be limited to only 3 running jobs at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:\n\nTable-24-to-be-replaced\n\nSystem Reservation Policy\n\nProjects may request to reserve a set of nodes for a period of time by contacting help@olcf.ornl.gov. If the reservation is granted, the reserved nodes will be blocked from general use for a given period of time. Only users that have been authorized to use the reservation can utilize those resources. Since no other users can access the reserved resources, it is crucial that groups given reservations take care to ensure the utilization on those resources remains high. To prevent reserved resources from remaining idle for an extended period of time, reservations are monitored for inactivity. If activity falls below 50% of the reserved resources for more than (30) minutes, the reservation will be canceled and the system will be returned to normal scheduling. A new reservation must be requested if this occurs.\n\nThe requesting project’s allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Frontier would be equivalent to using 16,000 Frontier node-hours of a project’s allocation.\n\nReservations should not be confused with priority requests. If quick turnaround is needed for a few jobs or for a period of time, a priority boost should be requested. A reservation should only be requested if users need to guarantee availability of a set of nodes at a given time, such as for a live demonstration at a conference.\n\nJob Dependencies\n\nOftentimes, a job will need data from some other job in the queue, but it's nonetheless convenient to submit the second job before the first finishes. Slurm allows you to submit a job with constraints that will keep it from running until these dependencies are met. These are specified with the -d option to Slurm. Common dependency flags are summarized below. In each of these examples, only a single jobid is shown but you can specify multiple job IDs as a colon-delimited list (i.e. #SBATCH -d afterok:12345:12346:12346). For the after dependency, you can optionally specify a +time value for each jobid.\n\nTable-25-to-be-replaced\n\nMonitoring and Modifying Batch Jobs\n\nscontrol hold and scontrol release: Holding and Releasing Jobs\n\nSometimes you may need to place a hold on a job to keep it from starting. For example, you may have submitted it assuming some needed data was in place but later realized that data is not yet available. This can be done with the scontrol hold command. Later, when the data is ready, you can release the job (i.e. tell the system that it's now OK to run the job) with the scontrol release command. For example:\n\nTable-26-to-be-replaced\n\nscontrol update: Changing Job Parameters\n\nThere may also be occasions where you want to modify a job that's waiting in the queue. For example, perhaps you requested 2,000 nodes but later realized this is a different data set and only needs 1,000 nodes. You can use the scontrol update command for this. For example:\n\nTable-27-to-be-replaced\n\nscancel: Cancel or Signal a Job\n\nIn addition to the --signal option for the sbatch/salloc commands described above <common-slurm-options> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#above <common-slurm-options>>, the scancel command can be used to manually signal a job. Typically, this is used to remove a job from the queue. In this use case, you do not need to specify a signal and can simply provide the jobid (i.e. scancel 12345). If you want to send some other signal to the job, use scancel the with the -s option. The -s option allows signals to be specified either by number or by name. Thus, if you want to send SIGUSR1 to a job, you would use scancel -s 10 12345 or scancel -s USR1 12345.\n\nsqueue: View the Queue\n\nThe squeue command is used to show the batch queue. You can filter the level of detail through several command-line options. For example:\n\nTable-28-to-be-replaced\n\nsacct: Get Job Accounting Information\n\nThe sacct command gives detailed information about jobs currently in the queue and recently-completed jobs. You can also use it to see the various steps within a batch jobs.\n\nTable-29-to-be-replaced\n\nscontrol show job: Get Detailed Job Information\n\nIn addition to holding, releasing, and updating the job, the scontrol command can show detailed job information via the show job subcommand. For example, scontrol show job 12345.\n\n\n\nSrun\n\nThe default job launcher for Frontier is srun . The srun command is used to execute an MPI binary on one or more compute nodes in parallel.\n\nSrun Format\n\nsrun  [OPTIONS... [executable [args...]]]\n\nSingle Command (non-interactive)\n\n$ srun -A <project_id> -t 00:05:00 -p <partition> -N 2 -n 4 --ntasks-per-node=2 ./a.out\n<output printed to terminal>\n\nThe job name and output options have been removed since stdout/stderr are typically desired in the terminal window in this usage mode.\n\nsrun accepts the following common options:\n\nTable-30-to-be-replaced\n\nBelow is a comparison table between srun and jsrun.\n\nTable-31-to-be-replaced\n\n\n\nProcess and Thread Mapping Examples\n\nThis section describes how to map processes (e.g., MPI ranks) and process\nthreads (e.g., OpenMP threads) to the CPUs, GPUs, and NICs on Frontier.\n\nUsers are highly encouraged to use the CPU- and GPU-mapping programs used in\nthe following sections to check their understanding of the job steps (i.e.,\nsrun commands) they intend to use in their actual jobs.\n\nFor the frontier-cpu-map <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-cpu-map> and frontier-multi-map <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-multi-map> sections:\n\nA simple MPI+OpenMP \"Hello, World\" program (hello_mpi_omp) will be used to clarify the\nmappings.\n\nFor the frontier-gpu-map <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-gpu-map> section:\n\nAn MPI+OpenMP+HIP \"Hello, World\" program (hello_jobstep) will be used to clarify the GPU\nmappings.\n\nAdditionally, it may be helpful to cross reference the\nsimplified Frontier node diagram <frontier-simple> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#simplified Frontier node diagram <frontier-simple>> -- specifically the\nlow-noise mode diagram <frontier-lownoise> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>>.\n\nUnless specified otherwise, the examples below assume the default low-noise\ncore specialization setting (-S 8).  This means that there are only 56\nallocatable cores by default instead of 64.  See the frontier-lownoise <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-lownoise>\nsection for more details.  Set -S 0 at job allocation to override this setting.\n\n\n\nCPU Mapping\n\nThis subsection covers how to map tasks to the CPU without the presence of\nadditional threads (i.e., solely MPI tasks -- no additional OpenMP threads).\n\nThe intent with both of the following examples is to launch 8 MPI ranks across\nthe node where each rank is assigned its own logical (and, in this case,\nphysical) core.  Using the -m distribution flag, we will cover two common\napproaches to assign the MPI ranks -- in a \"round-robin\" (cyclic)\nconfiguration and in a \"packed\" (block) configuration. Slurm's\nfrontier-interactive <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive> method was used to request an allocation of 1\ncompute node for these examples: salloc -A <project_id> -t 30 -p <parition>\n-N 1\n\nThere are many different ways users might choose to perform these mappings,\nso users are encouraged to clone the hello_mpi_omp program and test whether\nor not processes and threads are running where intended.\n\n8 MPI Ranks (round-robin)\n\nAssigning MPI ranks in a \"round-robin\" (cyclic) manner across L3 cache\nregions (sockets) is the default behavior on Frontier. This mode will assign\nconsecutive MPI tasks to different sockets before it tries to \"fill up\" a\nsocket.\n\nRecall that the -m flag behaves like: -m <node distribution>:<socket\ndistribution>.  Hence, the key setting to achieving the round-robin nature is\nthe -m block:cyclic flag, specifically the cyclic setting provided for\nthe \"socket distribution\". This ensures that the MPI tasks will be distributed\nacross sockets in a cyclic (round-robin) manner.\n\nThe below srun command will achieve the intended 8 MPI \"round-robin\" layout:\n\n$ export OMP_NUM_THREADS=1\n$ srun -N1 -n8 -c1 --cpu-bind=threads --threads-per-core=1 -m block:cyclic ./hello_mpi_omp | sort\n\nMPI 000 - OMP 000 - HWT 001 - Node frontier00144\nMPI 001 - OMP 000 - HWT 009 - Node frontier00144\nMPI 002 - OMP 000 - HWT 017 - Node frontier00144\nMPI 003 - OMP 000 - HWT 025 - Node frontier00144\nMPI 004 - OMP 000 - HWT 033 - Node frontier00144\nMPI 005 - OMP 000 - HWT 041 - Node frontier00144\nMPI 006 - OMP 000 - HWT 049 - Node frontier00144\nMPI 007 - OMP 000 - HWT 057 - Node frontier00144\n\n\n\nBreaking down the srun command, we have:\n\n-N1: indicates we are using 1 node\n\n-n8: indicates we are launching 8 MPI tasks\n\n-c1: indicates we are assigning 1 logical core per MPI task.\nIn this case, because of --threads-per-core=1, this also means 1 physical core per MPI task.\n\n--cpu-bind=threads: binds tasks to threads\n\n--threads-per-core=1: use a maximum of 1 hardware thread per physical core (i.e., only use 1 logical core per physical core)\n\n-m block:cyclic: distribute the tasks in a block layout across nodes (default), and in a cyclic (round-robin) layout across L3 sockets\n\n./hello_mpi_omp: launches the \"hello_mpi_omp\" executable\n\n| sort: sorts the output\n\nAlthough the above command used the default settings -c1,\n--cpu-bind=threads, --threads-per-core=1 and -m block:cyclic, it is\nalways better to be explicit with your srun command to have more control\nover your node layout. The above command is equivalent to srun -N1 -n8.\n\nAs you can see in the node diagram above, this results in the 8 MPI tasks\n(outlined in different colors) being distributed \"vertically\" across L3\nsockets.\n\n7 MPI Ranks (packed)\n\nInstead, you can assign MPI ranks so that the L3 regions are filled in a\n\"packed\" (block) manner.  This mode will assign consecutive MPI tasks to\nthe same L3 region (socket) until it is \"filled up\" or \"packed\" before\nassigning a task to a different socket.\n\nRecall that the -m flag behaves like: -m <node distribution>:<socket\ndistribution>.  Hence, the key setting to achieving the round-robin nature is\nthe -m block:block flag, specifically the block setting provided for\nthe \"socket distribution\". This ensures that the MPI tasks will be distributed\nin a packed manner.\n\nThe below srun command will achieve the intended 7 MPI \"packed\" layout:\n\n$ export OMP_NUM_THREADS=1\n$ srun -N1 -n7 -c1 --cpu-bind=threads --threads-per-core=1 -m block:block ./hello_mpi_omp | sort\n\nMPI 000 - OMP 000 - HWT 001 - Node frontier00144\nMPI 001 - OMP 000 - HWT 002 - Node frontier00144\nMPI 002 - OMP 000 - HWT 003 - Node frontier00144\nMPI 003 - OMP 000 - HWT 004 - Node frontier00144\nMPI 004 - OMP 000 - HWT 005 - Node frontier00144\nMPI 005 - OMP 000 - HWT 006 - Node frontier00144\nMPI 006 - OMP 000 - HWT 007 - Node frontier00144\n\n\n\nBreaking down the srun command, the only difference than the previous example is:\n\n-m block:block: distribute the tasks in a block layout across nodes (default), and in a block (packed) socket layout\n\nAs you can see in the node diagram above, this results in the 7 MPI tasks\n(outlined in different colors) being distributed \"horizontally\" within a\nsocket, rather than being spread across different L3 sockets like with the\nprevious example. However, if an 8th task was requested it would be assigned\nto the next L3 region on core 009.\n\n\n\nMultithreading\n\nBecause a Frontier compute node has two hardware threads available (2 logical\ncores per physical core), this enables the possibility of multithreading your\napplication (e.g., with OpenMP threads). Although the additional hardware\nthreads can be assigned to additional MPI tasks, this is not recommended. It is\nhighly recommended to only use 1 MPI task per physical core and to use OpenMP\nthreads instead on any additional logical cores gained when using both hardware\nthreads.\n\nThe following examples cover multithreading with hybrid MPI+OpenMP\napplications.  In these examples, Slurm's frontier-interactive <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive> method\nwas used to request an allocation of 1 compute node:\nsalloc -A <project_id> -t 30 -p <parition> -N 1\n\nThere are many different ways users might choose to perform these mappings,\nso users are encouraged to clone the hello_mpi_omp program and test whether\nor not processes and threads are running where intended.\n\n2 MPI ranks - each with 2 OpenMP threads\n\nIn this example, the intent is to launch 2 MPI ranks, each of which spawn 2\nOpenMP threads, and have all of the 4 OpenMP threads run on different physical\nCPU cores.\n\nFirst (INCORRECT) attempt\n\nTo set the number of OpenMP threads spawned per MPI rank, the\nOMP_NUM_THREADS environment variable can be used. To set the number of MPI\nranks launched, the srun flag -n can be used.\n\n$ export OMP_NUM_THREADS=2\n$ srun -N1 -n2 ./hello_mpi_omp | sort\n\nWARNING: Requested total thread count and/or thread affinity may result in\noversubscription of available CPU resources!  Performance may be degraded.\nExplicitly set OMP_WAIT_POLICY=PASSIVE or ACTIVE to suppress this message.\nSet CRAY_OMP_CHECK_AFFINITY=TRUE to print detailed thread-affinity messages.\nWARNING: Requested total thread count and/or thread affinity may result in\noversubscription of available CPU resources!  Performance may be degraded.\nExplicitly set OMP_WAIT_POLICY=PASSIVE or ACTIVE to suppress this message.\nSet CRAY_OMP_CHECK_AFFINITY=TRUE to print detailed thread-affinity messages.\n\nMPI 000 - OMP 000 - HWT 001 - Node frontier001\nMPI 000 - OMP 001 - HWT 001 - Node frontier001\nMPI 001 - OMP 000 - HWT 009 - Node frontier001\nMPI 001 - OMP 001 - HWT 009 - Node frontier001\n\nThe first thing to notice here is the WARNING about oversubscribing the\navailable CPU cores. Also, the output shows each MPI rank did spawn 2 OpenMP\nthreads, but both OpenMP threads ran on the same logical core (for a given\nMPI rank). This was not the intended behavior; each OpenMP thread was meant to\nrun on its own physical CPU core.\n\nThe problem here arises from two default settings; 1) each MPI rank is only\nallocated 1 logical core (-c 1) and, 2) only 1 hardware thread per physical\nCPU core is enabled (--threads-per-core=1).  When using\n--threads-per-core=1 and --cpu-bind=threads (the default setting), 1\nlogical core in -c is equivalent to 1 physical core.  So in this case, each\nMPI rank only has 1 physical core (with 1 hardware thread) to run on -\nincluding any threads the process spawns - hence the WARNING and undesired\nbehavior.\n\nSecond (CORRECT) attempt\n\nRecall that in this scenario, because of the --threads-per-core=1 setting,\n1 logical core is equivalent to 1 physical core when using -c.  Therefore,\nin order for each OpenMP thread to run on its own physical CPU core, each MPI\nrank should be given 2 physical CPU cores (-c 2).  Now the OpenMP threads\nwill be mapped to unique hardware threads on separate physical CPU cores.\n\n$ export OMP_NUM_THREADS=2\n$ srun -N1 -n2 -c2 ./hello_mpi_omp | sort\n\nMPI 000 - OMP 000 - HWT 001 - Node frontier001\nMPI 000 - OMP 001 - HWT 002 - Node frontier001\nMPI 001 - OMP 000 - HWT 009 - Node frontier001\nMPI 001 - OMP 001 - HWT 010 - Node frontier001\n\nNow the output shows that each OpenMP thread ran on its own physical CPU core.\nMore specifically (see the Frontier Compute Node diagram), OpenMP thread 000 of\nMPI rank 000 ran on logical core 001 (i.e., physical CPU core 01), OpenMP\nthread 001 of MPI rank 000 ran on logical core 002 (i.e., physical CPU core\n02), OpenMP thread 000 of MPI rank 001 ran on logical core 009 (i.e., physical\nCPU core 09), and OpenMP thread 001 of MPI rank 001 ran on logical core 010\n(i.e., physical CPU core 10) - as intended.\n\nThird attempt - Using multiple threads per core\n\nTo use both available hardware threads per core, the job must be allocated\nwith --threads-per-core=2 (as opposed to only the job step - i.e., srun\ncommand). That value will then be inherited by srun unless explcitly\noverridden with --threads-per-core=1. Because we are using\n--threads-per-core=2, the usage of -c goes back to purely meaning the\namount of logical cores (i.e., it is no longer equivalent to 1 physical core).\n\n$ salloc -N1 -A <project_id> -t <time> -p <partition> --threads-per-core=2\n\n$ export OMP_NUM_THREADS=2\n$ srun -N1 -n2 -c2 ./hello_mpi_omp | sort\n\nMPI 000 - OMP 000 - HWT 001 - Node frontier001\nMPI 000 - OMP 001 - HWT 065 - Node frontier001\nMPI 001 - OMP 000 - HWT 009 - Node frontier001\nMPI 001 - OMP 001 - HWT 073 - Node frontier001\n\nComparing this output to the Frontier Compute Node diagram, we see that each\npair of OpenMP threads is contained within a single physical core. MPI rank 000\nran on logical cores 001 and 065 (i.e. physical CPU core 01) and MPI rank\n001 ran on logical cores 009 and 073 (i.e. physical CPU core 09).\n\n\n\nGPU Mapping\n\nIn this sub-section, an MPI+OpenMP+HIP \"Hello, World\" program (hello_jobstep) will be used to show how to make\nonly specific GPUs available to processes - which we will refer to as \"GPU\nmapping\". Again, Slurm's frontier-interactive <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive> method was used to request\nan allocation of 2 compute nodes for these examples: salloc -A <project_id>\n-t 30 -p <parition> -N 2. The CPU mapping part of this example is very\nsimilar to the example used above in the Multithreading sub-section, so the\nfocus here will be on the GPU mapping part.\n\nIn general, GPU mapping can be accomplished in different ways. For example, an\napplication might map GPUs to MPI ranks programmatically within the code using,\nsay, hipSetDevice. In this case, there might not be a need to map GPUs using\nSlurm (since it can be done in the code itself). However, many applications\nexpect only 1 GPU to be available to each rank. It is this latter case that the\nfollowing examples refer to.\n\nAlso, recall that the CPU cores in a given L3 cache region are connected to a\nspecific GPU (see the Frontier Node Diagram and subsequent\nNote on NUMA domains <numa-note> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Note on NUMA domains <numa-note>> for more information). In the examples\nbelow, knowledge of these details will be assumed.\n\nThere are many different ways users might choose to perform these mappings,\nso users are encouraged to clone the hello_jobstep program and test whether\nprocesses and threads are mapped to the CPU cores and GPUs as intended..\n\nDue to the unique architecture of Frontier compute nodes and the way that\nSlurm currently allocates GPUs and CPU cores to job steps, it is suggested that\nall 8 GPUs on a node are allocated to the job step to ensure that optimal\nbindings are possible.\n\nhello_jobstep output\n\nBefore jumping into the examples, it is helpful to understand the output from the hello_jobstep program:\n\nTable-32-to-be-replaced\n\nMapping 1 GPU per task\n\nIn the following examples, 1 GPU will be mapped to each MPI rank (and any OpenMP threads it might spawn). The relevant srun options for GPU mapping used in these examples are:\n\nTable-33-to-be-replaced\n\nExample 1: 8 MPI ranks - each with 7 CPU cores and 1 GPU (single-node)\n\nThe most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see low-noise mode diagram <frontier-lownoise> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:\n\n$ OMP_NUM_THREADS=7 srun -N1 -n8 -c7 --gpus-per-task=1 --gpu-bind=closest ./hello_jobstep | sort\nMPI 000 - OMP 000 - HWT 001 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 001 - HWT 002 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 002 - HWT 003 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 003 - HWT 004 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 004 - HWT 005 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 005 - HWT 006 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 006 - HWT 007 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 009 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 001 - HWT 010 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 002 - HWT 011 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 003 - HWT 012 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 004 - HWT 013 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 005 - HWT 014 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 006 - HWT 015 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 002 - OMP 000 - HWT 017 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 001 - HWT 018 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 002 - HWT 019 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 003 - HWT 020 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 004 - HWT 021 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 005 - HWT 022 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 006 - HWT 023 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 003 - OMP 000 - HWT 025 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 001 - HWT 026 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 002 - HWT 027 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 003 - HWT 028 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 004 - HWT 029 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 005 - HWT 030 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 006 - HWT 031 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 004 - OMP 000 - HWT 033 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 001 - HWT 034 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 002 - HWT 035 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 003 - HWT 036 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 004 - HWT 037 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 005 - HWT 038 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 006 - HWT 039 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 005 - OMP 000 - HWT 041 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 001 - HWT 042 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 002 - HWT 043 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 003 - HWT 044 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 004 - HWT 045 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 005 - HWT 046 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 006 - HWT 047 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 006 - OMP 000 - HWT 049 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 001 - HWT 050 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 002 - HWT 051 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 003 - HWT 052 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 004 - HWT 053 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 005 - HWT 054 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 006 - HWT 055 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 007 - OMP 000 - HWT 057 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 001 - HWT 058 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 002 - HWT 059 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 003 - HWT 060 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 004 - HWT 061 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 005 - HWT 062 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 006 - HWT 063 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\n<string>:5: (INFO/1) Duplicate explicit target name: \"frontier node diagram\".\n\nAs has been pointed out previously in the Frontier documentation, notice that\nGPUs are NOT mapped to MPI ranks in sequential order (e.g., MPI rank 0 is\nmapped to physical CPU cores 1-7 and GPU 4, MPI rank 1 is mapped to physical\nCPU cores 9-15 and GPU 5), but this IS expected behavior. It is simply a\nconsequence of the Frontier node architectures as shown in the Frontier Node\nDiagram and\nsubsequent Note on NUMA domains <numa-note> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Note on NUMA domains <numa-note>>.\n\nExample 2: 1 MPI rank with 7 CPU cores and 1 GPU (single-node)\n\nWhen new users first attempt to run their application on Frontier, they often\nwant to test with 1 MPI rank that has access to 7 CPU cores and 1 GPU. Although\nthe job step used here is very similar to Example 1, the behavior is different:\n\n$ OMP_NUM_THREADS=7 srun -N1 -n1 -c7 --gpus-per-task=1 --gpu-bind=closest ./hello_jobstep | sort\nMPI 000 - OMP 000 - HWT 049 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 000 - OMP 001 - HWT 050 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 000 - OMP 002 - HWT 051 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 000 - OMP 003 - HWT 052 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 000 - OMP 004 - HWT 053 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 000 - OMP 005 - HWT 054 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 000 - OMP 006 - HWT 055 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\n\nNotice that our MPI rank did not get mapped to CPU cores 1-7 and GPU 4, but\ninstead to GPU 0 and CPU cores 49-55. The apparent reason for this can be found\nin the --gpu-bind section in the srun man page: GPU binding is\nignored if there is only one task.. Here, Slurm appears to give the first GPU\nit sees and maps it to the CPU cores that are closest. So although the mapping\ndoesn't occur as expected, the rank is still mapped to the correct GPU given\nthe CPU cores it ran on.\n\nExample 3: 16 MPI ranks - each with 7 CPU cores and 1 GPU (multi-node)\n\nThis example simply extends Example 1 to run on 2 nodes, which simply requires\nchanging the number of nodes to 2 (-N2) and the number of MPI ranks to 16\n(-n16).\n\n$ OMP_NUM_THREADS=7 srun -N2 -n16 -c7 --gpus-per-task=1 --gpu-bind=closest ./hello_jobstep | sort\nMPI 000 - OMP 000 - HWT 001 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 001 - HWT 002 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 002 - HWT 003 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 003 - HWT 004 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 004 - HWT 005 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 005 - HWT 006 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 006 - HWT 007 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 009 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 001 - HWT 010 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 002 - HWT 011 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 003 - HWT 012 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 004 - HWT 013 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 005 - HWT 014 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 006 - HWT 015 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 002 - OMP 000 - HWT 017 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 001 - HWT 018 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 002 - HWT 019 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 003 - HWT 020 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 004 - HWT 021 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 005 - HWT 022 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 006 - HWT 023 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 003 - OMP 000 - HWT 025 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 001 - HWT 026 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 002 - HWT 027 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 003 - HWT 028 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 004 - HWT 029 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 005 - HWT 030 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 006 - HWT 031 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 004 - OMP 000 - HWT 033 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 001 - HWT 034 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 002 - HWT 035 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 003 - HWT 036 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 004 - HWT 037 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 005 - HWT 038 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 006 - HWT 039 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 005 - OMP 000 - HWT 041 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 001 - HWT 042 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 002 - HWT 043 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 003 - HWT 044 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 004 - HWT 045 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 005 - HWT 046 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 006 - HWT 047 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 006 - OMP 000 - HWT 049 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 001 - HWT 050 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 002 - HWT 051 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 003 - HWT 052 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 004 - HWT 053 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 005 - HWT 054 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 006 - HWT 055 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 007 - OMP 000 - HWT 057 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 001 - HWT 058 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 002 - HWT 059 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 003 - HWT 060 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 004 - HWT 061 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 005 - HWT 062 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 006 - HWT 063 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 008 - OMP 000 - HWT 001 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 008 - OMP 001 - HWT 002 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 008 - OMP 002 - HWT 003 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 008 - OMP 003 - HWT 004 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 008 - OMP 004 - HWT 005 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 008 - OMP 005 - HWT 006 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 008 - OMP 006 - HWT 007 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 009 - OMP 000 - HWT 009 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 009 - OMP 001 - HWT 010 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 009 - OMP 002 - HWT 011 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 009 - OMP 003 - HWT 012 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 009 - OMP 004 - HWT 013 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 009 - OMP 005 - HWT 014 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 009 - OMP 006 - HWT 015 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 010 - OMP 000 - HWT 017 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 010 - OMP 001 - HWT 018 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 010 - OMP 002 - HWT 019 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 010 - OMP 003 - HWT 020 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 010 - OMP 004 - HWT 021 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 010 - OMP 005 - HWT 022 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 010 - OMP 006 - HWT 023 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 011 - OMP 000 - HWT 025 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 011 - OMP 001 - HWT 026 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 011 - OMP 002 - HWT 027 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 011 - OMP 003 - HWT 028 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 011 - OMP 004 - HWT 029 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 011 - OMP 005 - HWT 030 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 011 - OMP 006 - HWT 031 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 012 - OMP 000 - HWT 033 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 012 - OMP 001 - HWT 034 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 012 - OMP 002 - HWT 035 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 012 - OMP 003 - HWT 036 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 012 - OMP 004 - HWT 037 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 012 - OMP 005 - HWT 038 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 012 - OMP 006 - HWT 039 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 013 - OMP 000 - HWT 041 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 013 - OMP 001 - HWT 042 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 013 - OMP 002 - HWT 043 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 013 - OMP 003 - HWT 044 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 013 - OMP 004 - HWT 045 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 013 - OMP 005 - HWT 046 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 013 - OMP 006 - HWT 047 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 014 - OMP 000 - HWT 049 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 001 - HWT 050 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 002 - HWT 051 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 003 - HWT 052 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 004 - HWT 053 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 005 - HWT 054 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 006 - HWT 055 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 015 - OMP 000 - HWT 057 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 001 - HWT 058 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 002 - HWT 059 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 003 - HWT 060 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 004 - HWT 061 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 005 - HWT 062 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 006 - HWT 063 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nMapping multiple MPI ranks to a single GPU\n\nIn the following examples, 2 MPI ranks will be mapped to 1 GPU. For brevity,\nOMP_NUM_THREADS will be set to 1, so -c1 will be used unless\notherwise specified. A new srun option will also be introduced to\naccomplish the new mapping:\n\nTable-34-to-be-replaced\n\nOn AMD's MI250X, multi-process service (MPS) is not needed since multiple\nMPI ranks per GPU is supported natively.\n\nExample 4: 16 MPI ranks - where 2 ranks share a GPU (round-robin, single-node)\n\nThis example launches 16 MPI ranks (-n16), each with 1 physical CPU core\n(-c1) to launch 1 OpenMP thread (OMP_NUM_THREADS=1) on. The MPI ranks\nwill be assigned to GPUs in a round-robin fashion so that each of the 8 GPUs on\nthe node are shared by 2 MPI ranks.\n\n$ OMP_NUM_THREADS=1 srun -N1 -n16 -c1 --ntasks-per-gpu=2 --gpu-bind=closest ./hello_jobstep | sort\nMPI 000 - OMP 000 - HWT 001 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 009 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 002 - OMP 000 - HWT 017 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 003 - OMP 000 - HWT 025 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 004 - OMP 000 - HWT 033 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 005 - OMP 000 - HWT 041 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 006 - OMP 000 - HWT 049 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 007 - OMP 000 - HWT 057 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 008 - OMP 000 - HWT 002 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 009 - OMP 000 - HWT 010 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 010 - OMP 000 - HWT 018 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 011 - OMP 000 - HWT 026 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 012 - OMP 000 - HWT 034 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 013 - OMP 000 - HWT 042 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 014 - OMP 000 - HWT 050 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 015 - OMP 000 - HWT 058 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nThe output shows the round-robin (cyclic) distribution of MPI ranks to\nGPUs. In fact, it is a round-robin distribution of MPI ranks to L3 cache\nregions (the default distribution). The GPU mapping is a consequence of where\nthe MPI ranks are distributed; --gpu-bind=closest simply maps the GPU in an\nL3 cache region to the MPI ranks in the same L3 region.\n\nExample 5: 32 MPI ranks - where 2 ranks share a GPU (round-robin, multi-node)\n\nThis example is an extension of Example 4 to run on 2 nodes.\n\n$ OMP_NUM_THREADS=1 srun -N2 -n32 -c1 --ntasks-per-gpu=2 --gpu-bind=closest ./hello_jobstep | sort\nMPI 000 - OMP 000 - HWT 001 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 009 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 002 - OMP 000 - HWT 017 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 003 - OMP 000 - HWT 025 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 004 - OMP 000 - HWT 033 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 005 - OMP 000 - HWT 041 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 006 - OMP 000 - HWT 049 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 007 - OMP 000 - HWT 057 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 008 - OMP 000 - HWT 002 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 009 - OMP 000 - HWT 010 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 010 - OMP 000 - HWT 018 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 011 - OMP 000 - HWT 026 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 012 - OMP 000 - HWT 034 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 013 - OMP 000 - HWT 042 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 014 - OMP 000 - HWT 050 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 015 - OMP 000 - HWT 058 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 016 - OMP 000 - HWT 001 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 017 - OMP 000 - HWT 009 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 018 - OMP 000 - HWT 017 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 019 - OMP 000 - HWT 025 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 020 - OMP 000 - HWT 033 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 021 - OMP 000 - HWT 041 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 022 - OMP 000 - HWT 049 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 023 - OMP 000 - HWT 057 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 024 - OMP 000 - HWT 002 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 025 - OMP 000 - HWT 010 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 026 - OMP 000 - HWT 018 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 027 - OMP 000 - HWT 026 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 028 - OMP 000 - HWT 034 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 029 - OMP 000 - HWT 042 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 030 - OMP 000 - HWT 050 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 031 - OMP 000 - HWT 058 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nExample 6: 16 MPI ranks - where 2 ranks share a GPU (packed, single-node)\n\nThis example assumes the use of a core specialization of -S 0.  Because\nFrontier's default core specialization (-S 8) reserves the first core in\neach L3 region, the \"packed\" mode can be problematic because the 7 cores\navailable in each L3 region won't necessarily divide evenly. This can lead to\ntasks potentially spanning multiple L3 regions with its assigned cores, which\ncreates problems when Slurm tries to assign GPUs to a given task.\n\nThis example launches 16 MPI ranks (-n16), each with 4 physical CPU cores\n(-c4) to launch 1 OpenMP thread (OMP_NUM_THREADS=1) on. The MPI ranks\nwill be assigned to GPUs in a packed fashion so that each of the 8 GPUs on the\nnode are shared by 2 MPI ranks. Similar to Example 4, -ntasks-per-gpu=2\nwill be used, but a new srun flag will be used to change the default\nround-robin (cyclic) distribution of MPI ranks across NUMA domains:\n\nTable-35-to-be-replaced\n\nIn the job step for this example, --distribution=*:block is used, where\n* represents the default value of block for the distribution of MPI\nranks across compute nodes and the distribution of MPI ranks across L3 cache\nregions has been changed to block from its default value of cyclic.\n\nBecause the distribution across L3 cache regions has been changed to a\n\"packed\" (block) configuration, caution must be taken to ensure MPI ranks\nend up in the L3 cache regions where the GPUs they intend to be mapped to are\nlocated. To accomplish this, the number of physical CPU cores assigned to an\nMPI rank was increased - in this case to 4. Doing so ensures that only 2 MPI\nranks can fit into a single L3 cache region. If the value of -c was left at\n1, all 8 MPI ranks would be \"packed\" into the first L3 region, where the\n\"closest\" GPU would be GPU 4 - the only GPU in that L3 region.\n\n$ OMP_NUM_THREADS=1 srun -N1 -n16 -c4 --ntasks-per-gpu=2 --gpu-bind=closest --distribution=*:block ./hello_jobstep | sort\nMPI 000 - OMP 000 - HWT 000 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 004 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 002 - OMP 000 - HWT 008 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 003 - OMP 000 - HWT 012 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 004 - OMP 000 - HWT 016 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 005 - OMP 000 - HWT 020 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 006 - OMP 000 - HWT 024 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 007 - OMP 000 - HWT 028 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 008 - OMP 000 - HWT 032 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 009 - OMP 000 - HWT 036 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 010 - OMP 000 - HWT 040 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 011 - OMP 000 - HWT 044 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 012 - OMP 000 - HWT 048 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 013 - OMP 000 - HWT 052 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 000 - HWT 056 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 000 - HWT 060 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nThe overall effect of using --distribution=*:block and increasing the\nnumber of physical CPU cores available to each MPI rank is to place the first\ntwo MPI ranks in the first L3 cache region with GPU 4, the next two MPI ranks\nin the second L3 cache region with GPU 5, and so on.\n\nExample 7: 32 MPI ranks - where 2 ranks share a GPU (packed, multi-node)\n\nThis example assumes the use of a core specialization of -S 0.  Because\nFrontier's default core specialization (-S 8) reserves the first core in\neach L3 region, the \"packed\" mode can be problematic because the 7 cores\navailable in each L3 region won't necessarily divide evenly. This can lead to\ntasks potentially spanning multiple L3 regions with its assigned cores, which\ncreates problems when Slurm tries to assign GPUs to a given task.\n\nThis example is an extension of Example 6 to use 2 compute nodes. With the\nappropriate changes put in place in Example 6, it is a straightforward exercise\nto change to using 2 nodes (-N2) and 32 MPI ranks (-n32).\n\n$ OMP_NUM_THREADS=1 srun -N2 -n32 -c4 --ntasks-per-gpu=2 --gpu-bind=closest --distribution=*:block ./hello_jobstep | sort\nMPI 000 - OMP 000 - HWT 000 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 004 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 002 - OMP 000 - HWT 010 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 003 - OMP 000 - HWT 012 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 004 - OMP 000 - HWT 016 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 005 - OMP 000 - HWT 021 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 006 - OMP 000 - HWT 024 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 007 - OMP 000 - HWT 028 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 008 - OMP 000 - HWT 032 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 009 - OMP 000 - HWT 037 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 010 - OMP 000 - HWT 041 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 011 - OMP 000 - HWT 044 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 012 - OMP 000 - HWT 049 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 013 - OMP 000 - HWT 052 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 000 - HWT 056 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 000 - HWT 060 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 016 - OMP 000 - HWT 000 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 017 - OMP 000 - HWT 004 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 018 - OMP 000 - HWT 008 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 019 - OMP 000 - HWT 013 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 020 - OMP 000 - HWT 016 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 021 - OMP 000 - HWT 020 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 022 - OMP 000 - HWT 024 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 023 - OMP 000 - HWT 028 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 024 - OMP 000 - HWT 034 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 025 - OMP 000 - HWT 036 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 026 - OMP 000 - HWT 040 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 027 - OMP 000 - HWT 044 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 028 - OMP 000 - HWT 048 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 029 - OMP 000 - HWT 052 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 030 - OMP 000 - HWT 056 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 031 - OMP 000 - HWT 060 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nExample 8: 56 MPI ranks - where 7 ranks share a GPU (packed, single-node)\n\nAn alternative solution to Example 6 and 7's -S 8 issue is to use -c 1\ninstead.  There is no problem when running with 1 core per MPI rank (i.e., 7\nranks per GPU) because the task can’t span multiple L3s.\n\n$ OMP_NUM_THREADS=1 srun -N1 -n56 -c1 --ntasks-per-gpu=7 --gpu-bind=closest --distribution=*:block ./hello_jobstep | sort\nMPI 000 - OMP 000 - HWT 001 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 002 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 002 - OMP 000 - HWT 003 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 003 - OMP 000 - HWT 004 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 004 - OMP 000 - HWT 005 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 005 - OMP 000 - HWT 006 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 006 - OMP 000 - HWT 007 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 007 - OMP 000 - HWT 009 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 008 - OMP 000 - HWT 010 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 009 - OMP 000 - HWT 011 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 010 - OMP 000 - HWT 012 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 011 - OMP 000 - HWT 013 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 012 - OMP 000 - HWT 014 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 013 - OMP 000 - HWT 015 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 014 - OMP 000 - HWT 017 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 015 - OMP 000 - HWT 018 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 016 - OMP 000 - HWT 019 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 017 - OMP 000 - HWT 020 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 018 - OMP 000 - HWT 021 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 019 - OMP 000 - HWT 022 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 020 - OMP 000 - HWT 023 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 021 - OMP 000 - HWT 025 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 022 - OMP 000 - HWT 026 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 023 - OMP 000 - HWT 027 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 024 - OMP 000 - HWT 028 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 025 - OMP 000 - HWT 029 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 026 - OMP 000 - HWT 030 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 027 - OMP 000 - HWT 031 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 028 - OMP 000 - HWT 033 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 029 - OMP 000 - HWT 034 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 030 - OMP 000 - HWT 035 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 031 - OMP 000 - HWT 036 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 032 - OMP 000 - HWT 037 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 033 - OMP 000 - HWT 038 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 034 - OMP 000 - HWT 039 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 035 - OMP 000 - HWT 041 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 036 - OMP 000 - HWT 042 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 037 - OMP 000 - HWT 043 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 038 - OMP 000 - HWT 044 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 039 - OMP 000 - HWT 045 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 040 - OMP 000 - HWT 046 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 041 - OMP 000 - HWT 047 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 042 - OMP 000 - HWT 049 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 043 - OMP 000 - HWT 050 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 044 - OMP 000 - HWT 051 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 045 - OMP 000 - HWT 052 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 046 - OMP 000 - HWT 053 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 047 - OMP 000 - HWT 054 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 048 - OMP 000 - HWT 055 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 049 - OMP 000 - HWT 057 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 050 - OMP 000 - HWT 058 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 051 - OMP 000 - HWT 059 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 052 - OMP 000 - HWT 060 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 053 - OMP 000 - HWT 061 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 054 - OMP 000 - HWT 062 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 055 - OMP 000 - HWT 063 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nMultiple Independent Job Steps\n\nExample 9: 8 independent and simultaneous job steps running on a single node\n\nThis example shows how to run multiple independent, simultaneous job steps on a single compute node. Specifically, it shows how to run 8 independent hello_jobstep programs running on their own CPU core and GPU.\n\nSubmission script:\n\n#!/bin/bash\n\n#SBATCH -A stf016_frontier\n#SBATCH -N 1\n#SBATCH -t 5\n\nfor idx in {1..8};\n\n    do\n        date\n\n        OMP_NUM_THREADS=1 srun -u --gpus-per-task=1 --gpu-bind=closest -N1 -n1 -c1 ./hello_jobstep &\n\n        sleep 1\n    done\n\nwait\n\nOutput:\n\nFri 02 Jun 2023 03:33:45 PM EDT\nMPI 000 - OMP 000 - HWT 049 - Node frontier04724 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nFri 02 Jun 2023 03:33:46 PM EDT\nMPI 000 - OMP 000 - HWT 057 - Node frontier04724 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nFri 02 Jun 2023 03:33:47 PM EDT\nMPI 000 - OMP 000 - HWT 017 - Node frontier04724 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nFri 02 Jun 2023 03:33:48 PM EDT\nMPI 000 - OMP 000 - HWT 025 - Node frontier04724 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nFri 02 Jun 2023 03:33:49 PM EDT\nMPI 000 - OMP 000 - HWT 001 - Node frontier04724 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nFri 02 Jun 2023 03:33:50 PM EDT\nMPI 000 - OMP 000 - HWT 009 - Node frontier04724 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nFri 02 Jun 2023 03:33:51 PM EDT\nMPI 000 - OMP 000 - HWT 033 - Node frontier04724 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nFri 02 Jun 2023 03:33:52 PM EDT\nMPI 000 - OMP 000 - HWT 041 - Node frontier04724 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\n\nThe output shows that each independent process ran on its own CPU core and GPU\non the same single node. To show that the ranks ran simultaneously, date\nwas called before each job step and a 20 second sleep was added to the end of\nthe hello_jobstep program. So the output also shows that the first job step\nwas submitted at :45 and the subsequent job steps were all submitted\nbetween :46 and :52. But because each hello_jobstep sleeps for 20\nseconds, the subsequent job steps must have all been running while the first\njob step was still sleeping (and holding up its resources). And the same\nargument can be made for the other job steps.\n\nThe wait command is needed so the job script (and allocation) do not immediately end after launching the job steps in the background.\n\nThe sleep 1 is needed to give Slurm sufficient time to launch each job step.\n\nMultiple GPUs per MPI rank\n\nAs mentioned previously, all GPUs are accessible by all MPI ranks by default,\nso it is possible to programatically map any combination of GPUs to MPI\nranks. It should be noted however that Cray MPICH does not support GPU-aware\nMPI for multiple GPUs per rank, so this binding is not suggested.\n\nNIC Mapping\n\n<string>:5: (INFO/1) Duplicate explicit target name: \"frontier node diagram\".\n\nAs shown in the Frontier Node Diagram, each of the 4\nNICs on a compute node is connected to a specific MI250X, and each MI250X is\n(in turn) connected to a specific NUMA domain - so each NUMA domain is\ncorrelated to a specific NIC. By default, processes (e.g., MPI ranks) that are\nmapped to CPU cores in a specific NUMA domain are mapped (by CrayMPICH) to the\nNIC that is correlated to that NUMA domain.\n\nIf a user attempts to map a process to a set of cores that span more than 1\nNUMA domain using the default NIC mapping, they will see an error such as\nMPICH ERROR: Unable to use a NIC_POLICY of 'NUMA'. Rank 0 is not confined\nto a single NUMA node.. This is expected behavior for the default NIC\npolicy.\n\nThe default behavior can be changed by using the MPICH_OFI_NIC_POLICY\nenvironment variable (see man mpi for available options).\n\nTips for Launching at Scale\n\nSBCAST your executable and libraries\n\nSlurm contains a utility called sbcast. This program takes a file and broadcasts it to each node's node-local storage (ie, /tmp, NVMe).\nThis is useful for sharing large input files, binaries and shared libraries, while reducing the overhead on shared file systems and overhead at startup.\nThis is highly recommended at scale if you have multiple shared libraries on Lustre/NFS file systems.\n\nSBCASTing a single file\n\nHere is a simple example of a file sbcast from a user's scratch space on Lustre to each node's NVMe drive:\n\n#!/bin/bash\n#SBATCH -A <projid>\n#SBATCH -J sbcast_to_nvme\n#SBATCH -o %x-%j.out\n#SBATCH -t 00:05:00\n#SBATCH -p batch\n#SBATCH -N 2\n#SBATCH -C nvme\n\ndate\n\n# Change directory to user scratch space (Orion)\ncd /lustre/orion/<projid>/scratch/<userid>\n\necho \"This is an example file\" > test.txt\necho\necho \"*****ORIGINAL FILE*****\"\ncat test.txt\necho \"***********************\"\n\n# SBCAST file from Orion to NVMe -- NOTE: ``-C nvme`` is required to use the NVMe drive\nsbcast -pf test.txt /mnt/bb/$USER/test.txt\nif [ ! \"$?\" == \"0\" ]; then\n    # CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,\n    # your application may pick up partially complete shared library files, which would give you confusing errors.\n    echo \"SBCAST failed!\"\n    exit 1\nfi\n\necho\necho \"*****DISPLAYING FILES ON EACH NODE IN THE ALLOCATION*****\"\n# Check to see if file exists\nsrun -N ${SLURM_NNODES} -n ${SLURM_NNODES} --ntasks-per-node=1 bash -c \"echo \\\"\\$(hostname): \\$(ls -lh /mnt/bb/$USER/test.txt)\\\"\"\necho \"*********************************************************\"\n\necho\n# Showing the file on the current node -- this will be the same on all other nodes in the allocation\necho \"*****SBCAST FILE ON CURRENT NODE******\"\ncat /mnt/bb/$USER/test.txt\necho \"**************************************\"\n\nand here is the output from that script:\n\nFri 03 Mar 2023 03:43:30 PM EST\n\n*****ORIGINAL FILE*****\nThis is an example file\n***********************\n\n*****DISPLAYING FILES ON EACH NODE IN THE ALLOCATION*****\nfrontier00001: -rw-r--r-- 1 hagertnl hagertnl 24 Mar  3 15:43 /mnt/bb/hagertnl/test.txt\nfrontier00002: -rw-r--r-- 1 hagertnl hagertnl 24 Mar  3 15:43 /mnt/bb/hagertnl/test.txt\n*********************************************************\n\n*****SBCAST FILE ON CURRENT NODE******\nThis is an example file\n**************************************\n\nSBCASTing a binary with libraries stored on shared file systems\n\nsbcast also handles binaries and their libraries:\n\n#!/bin/bash\n#SBATCH -A <projid>\n#SBATCH -J sbcast_binary_to_nvme\n#SBATCH -o %x-%j.out\n#SBATCH -t 00:05:00\n#SBATCH -p batch\n#SBATCH -N 2\n#SBATCH -C nvme\n\ndate\n\n# Change directory to user scratch space (Orion)\ncd /lustre/orion/<projid>/scratch/<userid>\n\n# For this example, I use a HIP-enabled LAMMPS binary, with dependencies to MPI, HIP, and HWLOC\nexe=\"lmp\"\n\necho \"*****ldd ./${exe}*****\"\nldd ./${exe}\necho \"*************************\"\n\n# SBCAST executable from Orion to NVMe -- NOTE: ``-C nvme`` is needed in SBATCH headers to use the NVMe drive\n# NOTE: dlopen'd files will NOT be picked up by sbcast\n# SBCAST automatically excludes several directories: /lib,/usr/lib,/lib64,/usr/lib64,/opt\n#   - These directories are node-local and are very fast to read from, so SBCASTing them isn't critical\n#   - see ``$ scontrol show config | grep BcastExclude`` for current list\n#   - OLCF-provided libraries in ``/sw`` are not on the exclusion list. ``/sw`` is an NFS shared file system\n#   - To override, add ``--exclude=NONE`` to arguments\nsbcast --send-libs -pf ${exe} /mnt/bb/$USER/${exe}\nif [ ! \"$?\" == \"0\" ]; then\n    # CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,\n    # your application may pick up partially complete shared library files, which would give you confusing errors.\n    echo \"SBCAST failed!\"\n    exit 1\nfi\n\n# Check to see if file exists\necho \"*****ls -lh /mnt/bb/$USER*****\"\nls -lh /mnt/bb/$USER/\necho \"*****ls -lh /mnt/bb/$USER/${exe}_libs*****\"\nls -lh /mnt/bb/$USER/${exe}_libs\n\n# SBCAST sends all libraries detected by `ld` (minus any excluded), and stores them in the same directory in each node's node-local storage\n# Any libraries opened by `dlopen` are NOT sent, since they are not known by the linker at run-time.\n\n# At minimum: prepend the node-local path to LD_LIBRARY_PATH to pick up the SBCAST libraries\n# It is also recommended that you **remove** any paths that you don't need, like those that contain the libraries that you just SBCAST'd\n# Failure to remove may result in unnecessary calls to stat shared file systems\nexport LD_LIBRARY_PATH=\"/mnt/bb/$USER/${exe}_libs:${LD_LIBRARY_PATH}\"\n\n# If you SBCAST **all** your libraries (ie, `--exclude=NONE`), you may use the following line:\n#export LD_LIBRARY_PATH=\"/mnt/bb/$USER/${exe}_libs:$(pkg-config --variable=libdir libfabric)\"\n# Use with caution -- certain libraries may use ``dlopen`` at runtime, and that is NOT covered by sbcast\n# If you use this option, we recommend you contact OLCF Help Desk for the latest list of additional steps required\n\n# You may notice that some libraries are still linked from /sw/frontier, even after SBCASTing.\n# This is because the Spack-build modules use RPATH to find their dependencies.\necho \"*****ldd /mnt/bb/$USER/${exe}*****\"\nldd /mnt/bb/$USER/${exe}\necho \"*************************************\"\n\nand here is the output from that script:\n\nTue 28 Mar 2023 05:01:41 PM EDT\n*****ldd ./lmp*****\n    linux-vdso.so.1 (0x00007fffeda02000)\n    libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007fffed5bb000)\n    libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fffed398000)\n    libm.so.6 => /lib64/libm.so.6 (0x00007fffed04d000)\n    librt.so.1 => /lib64/librt.so.1 (0x00007fffece44000)\n    libamdhip64.so.4 => /opt/rocm-4.5.2/lib/libamdhip64.so.4 (0x00007fffec052000)\n    libmpi_cray.so.12 => /opt/cray/pe/lib64/libmpi_cray.so.12 (0x00007fffe96cc000)\n    libmpi_gtl_hsa.so.0 => /opt/cray/pe/lib64/libmpi_gtl_hsa.so.0 (0x00007fffe9469000)\n    libhsa-runtime64.so.1 => /opt/rocm-5.3.0/lib/libhsa-runtime64.so.1 (0x00007fffe8fdc000)\n    libhwloc.so.15 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/hwloc-2.5.0-4p6jkgf5ez6wr27pytkzyptppzpugu3e/lib/libhwloc.so.15 (0x00007fffe8d82000)\n    libdl.so.2 => /lib64/libdl.so.2 (0x00007fffe8b7e000)\n    libhipfft.so => /opt/rocm-5.3.0/lib/libhipfft.so (0x00007fffed9d2000)\n    libstdc++.so.6 => /usr/lib64/libstdc++.so.6 (0x00007fffe875b000)\n    libc.so.6 => /lib64/libc.so.6 (0x00007fffe8366000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007fffed7da000)\n    libamd_comgr.so.2 => /opt/rocm-5.3.0/lib/libamd_comgr.so.2 (0x00007fffe06e0000)\n    libnuma.so.1 => /usr/lib64/libnuma.so.1 (0x00007fffe04d4000)\n    libfabric.so.1 => /opt/cray/libfabric/1.15.2.0/lib64/libfabric.so.1 (0x00007fffe01e2000)\n    libatomic.so.1 => /usr/lib64/libatomic.so.1 (0x00007fffdffd9000)\n    libpmi.so.0 => /opt/cray/pe/lib64/libpmi.so.0 (0x00007fffdfdd7000)\n    libpmi2.so.0 => /opt/cray/pe/lib64/libpmi2.so.0 (0x00007fffdfb9e000)\n    libquadmath.so.0 => /usr/lib64/libquadmath.so.0 (0x00007fffdf959000)\n    libmodules.so.1 => /opt/cray/pe/lib64/cce/libmodules.so.1 (0x00007fffed9b5000)\n    libfi.so.1 => /opt/cray/pe/lib64/cce/libfi.so.1 (0x00007fffdf3b4000)\n    libcraymath.so.1 => /opt/cray/pe/lib64/cce/libcraymath.so.1 (0x00007fffed8ce000)\n    libf.so.1 => /opt/cray/pe/lib64/cce/libf.so.1 (0x00007fffed83b000)\n    libu.so.1 => /opt/cray/pe/lib64/cce/libu.so.1 (0x00007fffdf2ab000)\n    libcsup.so.1 => /opt/cray/pe/lib64/cce/libcsup.so.1 (0x00007fffed832000)\n    libamdhip64.so.5 => /opt/rocm-5.3.0/lib/libamdhip64.so.5 (0x00007fffdda8a000)\n    libelf.so.1 => /usr/lib64/libelf.so.1 (0x00007fffdd871000)\n    libdrm.so.2 => /usr/lib64/libdrm.so.2 (0x00007fffdd65d000)\n    libdrm_amdgpu.so.1 => /usr/lib64/libdrm_amdgpu.so.1 (0x00007fffdd453000)\n    libpciaccess.so.0 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libpciaccess-0.16-6evcvl74myxfxdrddmnsvbc7sgfx6s5j/lib/libpciaccess.so.0 (0x00007fffdd24a000)\n    libxml2.so.2 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libxml2-2.9.12-h7fe2yauotu3xit7rsaixaowd3noknur/lib/libxml2.so.2 (0x00007fffdcee6000)\n    librocfft.so.0 => /opt/rocm-5.3.0/lib/librocfft.so.0 (0x00007fffdca1a000)\n    libz.so.1 => /lib64/libz.so.1 (0x00007fffdc803000)\n    libtinfo.so.6 => /lib64/libtinfo.so.6 (0x00007fffdc5d5000)\n    libcxi.so.1 => /usr/lib64/libcxi.so.1 (0x00007fffdc3b0000)\n    libcurl.so.4 => /usr/lib64/libcurl.so.4 (0x00007fffdc311000)\n    libjson-c.so.3 => /usr/lib64/libjson-c.so.3 (0x00007fffdc101000)\n    libpals.so.0 => /opt/cray/pe/lib64/libpals.so.0 (0x00007fffdbefc000)\n    libgfortran.so.5 => /opt/cray/pe/gcc-libs/libgfortran.so.5 (0x00007fffdba50000)\n    liblzma.so.5 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/xz-5.2.5-zwra4gpckelt5umczowf3jtmeiz3yd7u/lib/liblzma.so.5 (0x00007fffdb82a000)\n    libiconv.so.2 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libiconv-1.16-coz5dzhtoeq5unhjirayfn2xftnxk43l/lib/libiconv.so.2 (0x00007fffdb52e000)\n    librocfft-device-0.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-0.so.0 (0x00007fffa11a0000)\n    librocfft-device-1.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-1.so.0 (0x00007fff6491b000)\n    librocfft-device-2.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-2.so.0 (0x00007fff2a828000)\n    librocfft-device-3.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-3.so.0 (0x00007ffef7eee000)\n    libnghttp2.so.14 => /usr/lib64/libnghttp2.so.14 (0x00007ffef7cc6000)\n    libidn2.so.0 => /usr/lib64/libidn2.so.0 (0x00007ffef7aa9000)\n    libssh.so.4 => /usr/lib64/libssh.so.4 (0x00007ffef783b000)\n    libpsl.so.5 => /usr/lib64/libpsl.so.5 (0x00007ffef7629000)\n    libssl.so.1.1 => /usr/lib64/libssl.so.1.1 (0x00007ffef758a000)\n    libcrypto.so.1.1 => /usr/lib64/libcrypto.so.1.1 (0x00007ffef724a000)\n    libgssapi_krb5.so.2 => /usr/lib64/libgssapi_krb5.so.2 (0x00007ffef6ff8000)\n    libldap_r-2.4.so.2 => /usr/lib64/libldap_r-2.4.so.2 (0x00007ffef6da4000)\n    liblber-2.4.so.2 => /usr/lib64/liblber-2.4.so.2 (0x00007ffef6b95000)\n    libzstd.so.1 => /usr/lib64/libzstd.so.1 (0x00007ffef6865000)\n    libbrotlidec.so.1 => /usr/lib64/libbrotlidec.so.1 (0x00007ffef6659000)\n    libunistring.so.2 => /usr/lib64/libunistring.so.2 (0x00007ffef62d6000)\n    libjitterentropy.so.3 => /usr/lib64/libjitterentropy.so.3 (0x00007ffef60cf000)\n    libkrb5.so.3 => /usr/lib64/libkrb5.so.3 (0x00007ffef5df6000)\n    libk5crypto.so.3 => /usr/lib64/libk5crypto.so.3 (0x00007ffef5bde000)\n    libcom_err.so.2 => /lib64/libcom_err.so.2 (0x00007ffef59da000)\n    libkrb5support.so.0 => /usr/lib64/libkrb5support.so.0 (0x00007ffef57cb000)\n    libresolv.so.2 => /lib64/libresolv.so.2 (0x00007ffef55b3000)\n    libsasl2.so.3 => /usr/lib64/libsasl2.so.3 (0x00007ffef5396000)\n    libbrotlicommon.so.1 => /usr/lib64/libbrotlicommon.so.1 (0x00007ffef5175000)\n    libkeyutils.so.1 => /usr/lib64/libkeyutils.so.1 (0x00007ffef4f70000)\n    libselinux.so.1 => /lib64/libselinux.so.1 (0x00007ffef4d47000)\n    libpcre.so.1 => /usr/lib64/libpcre.so.1 (0x00007ffef4abe000)\n*************************\n*****ls -lh /mnt/bb/hagertnl*****\ntotal 236M\n-rwxr-xr-x 1 hagertnl hagertnl 236M Mar 28 17:01 lmp\ndrwx------ 2 hagertnl hagertnl  114 Mar 28 17:01 lmp_libs\n*****ls -lh /mnt/bb/hagertnl/lmp_libs*****\ntotal 9.2M\n-rwxr-xr-x 1 hagertnl hagertnl 1.6M Oct  6  2021 libhwloc.so.15\n-rwxr-xr-x 1 hagertnl hagertnl 1.6M Oct  6  2021 libiconv.so.2\n-rwxr-xr-x 1 hagertnl hagertnl 783K Oct  6  2021 liblzma.so.5\n-rwxr-xr-x 1 hagertnl hagertnl 149K Oct  6  2021 libpciaccess.so.0\n-rwxr-xr-x 1 hagertnl hagertnl 5.2M Oct  6  2021 libxml2.so.2\n*****ldd /mnt/bb/hagertnl/lmp*****\n    linux-vdso.so.1 (0x00007fffeda02000)\n    libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007fffed5bb000)\n    libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fffed398000)\n    libm.so.6 => /lib64/libm.so.6 (0x00007fffed04d000)\n    librt.so.1 => /lib64/librt.so.1 (0x00007fffece44000)\n    libamdhip64.so.4 => /opt/rocm-4.5.2/lib/libamdhip64.so.4 (0x00007fffec052000)\n    libmpi_cray.so.12 => /opt/cray/pe/lib64/libmpi_cray.so.12 (0x00007fffe96cc000)\n    libmpi_gtl_hsa.so.0 => /opt/cray/pe/lib64/libmpi_gtl_hsa.so.0 (0x00007fffe9469000)\n    libhsa-runtime64.so.1 => /opt/rocm-5.3.0/lib/libhsa-runtime64.so.1 (0x00007fffe8fdc000)\n    libhwloc.so.15 => /mnt/bb/hagertnl/lmp_libs/libhwloc.so.15 (0x00007fffe8d82000)\n    libdl.so.2 => /lib64/libdl.so.2 (0x00007fffe8b7e000)\n    libhipfft.so => /opt/rocm-5.3.0/lib/libhipfft.so (0x00007fffed9d2000)\n    libstdc++.so.6 => /usr/lib64/libstdc++.so.6 (0x00007fffe875b000)\n    libc.so.6 => /lib64/libc.so.6 (0x00007fffe8366000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007fffed7da000)\n    libamd_comgr.so.2 => /opt/rocm-5.3.0/lib/libamd_comgr.so.2 (0x00007fffe06e0000)\n    libnuma.so.1 => /usr/lib64/libnuma.so.1 (0x00007fffe04d4000)\n    libfabric.so.1 => /opt/cray/libfabric/1.15.2.0/lib64/libfabric.so.1 (0x00007fffe01e2000)\n    libatomic.so.1 => /usr/lib64/libatomic.so.1 (0x00007fffdffd9000)\n    libpmi.so.0 => /opt/cray/pe/lib64/libpmi.so.0 (0x00007fffdfdd7000)\n    libpmi2.so.0 => /opt/cray/pe/lib64/libpmi2.so.0 (0x00007fffdfb9e000)\n    libquadmath.so.0 => /usr/lib64/libquadmath.so.0 (0x00007fffdf959000)\n    libmodules.so.1 => /opt/cray/pe/lib64/cce/libmodules.so.1 (0x00007fffed9b5000)\n    libfi.so.1 => /opt/cray/pe/lib64/cce/libfi.so.1 (0x00007fffdf3b4000)\n    libcraymath.so.1 => /opt/cray/pe/lib64/cce/libcraymath.so.1 (0x00007fffed8ce000)\n    libf.so.1 => /opt/cray/pe/lib64/cce/libf.so.1 (0x00007fffed83b000)\n    libu.so.1 => /opt/cray/pe/lib64/cce/libu.so.1 (0x00007fffdf2ab000)\n    libcsup.so.1 => /opt/cray/pe/lib64/cce/libcsup.so.1 (0x00007fffed832000)\n    libamdhip64.so.5 => /opt/rocm-5.3.0/lib/libamdhip64.so.5 (0x00007fffdda8a000)\n    libelf.so.1 => /usr/lib64/libelf.so.1 (0x00007fffdd871000)\n    libdrm.so.2 => /usr/lib64/libdrm.so.2 (0x00007fffdd65d000)\n    libdrm_amdgpu.so.1 => /usr/lib64/libdrm_amdgpu.so.1 (0x00007fffdd453000)\n    libpciaccess.so.0 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libpciaccess-0.16-6evcvl74myxfxdrddmnsvbc7sgfx6s5j/lib/libpciaccess.so.0 (0x00007fffdd24a000)\n    libxml2.so.2 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libxml2-2.9.12-h7fe2yauotu3xit7rsaixaowd3noknur/lib/libxml2.so.2 (0x00007fffdcee6000)\n    librocfft.so.0 => /opt/rocm-5.3.0/lib/librocfft.so.0 (0x00007fffdca1a000)\n    libz.so.1 => /lib64/libz.so.1 (0x00007fffdc803000)\n    libtinfo.so.6 => /lib64/libtinfo.so.6 (0x00007fffdc5d5000)\n    libcxi.so.1 => /usr/lib64/libcxi.so.1 (0x00007fffdc3b0000)\n    libcurl.so.4 => /usr/lib64/libcurl.so.4 (0x00007fffdc311000)\n    libjson-c.so.3 => /usr/lib64/libjson-c.so.3 (0x00007fffdc101000)\n    libpals.so.0 => /opt/cray/pe/lib64/libpals.so.0 (0x00007fffdbefc000)\n    libgfortran.so.5 => /opt/cray/pe/gcc-libs/libgfortran.so.5 (0x00007fffdba50000)\n    liblzma.so.5 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/xz-5.2.5-zwra4gpckelt5umczowf3jtmeiz3yd7u/lib/liblzma.so.5 (0x00007fffdb82a000)\n    libiconv.so.2 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libiconv-1.16-coz5dzhtoeq5unhjirayfn2xftnxk43l/lib/libiconv.so.2 (0x00007fffdb52e000)\n    librocfft-device-0.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-0.so.0 (0x00007fffa11a0000)\n    librocfft-device-1.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-1.so.0 (0x00007fff6491b000)\n    librocfft-device-2.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-2.so.0 (0x00007fff2a828000)\n    librocfft-device-3.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-3.so.0 (0x00007ffef7eee000)\n    libnghttp2.so.14 => /usr/lib64/libnghttp2.so.14 (0x00007ffef7cc6000)\n    libidn2.so.0 => /usr/lib64/libidn2.so.0 (0x00007ffef7aa9000)\n    libssh.so.4 => /usr/lib64/libssh.so.4 (0x00007ffef783b000)\n    libpsl.so.5 => /usr/lib64/libpsl.so.5 (0x00007ffef7629000)\n    libssl.so.1.1 => /usr/lib64/libssl.so.1.1 (0x00007ffef758a000)\n    libcrypto.so.1.1 => /usr/lib64/libcrypto.so.1.1 (0x00007ffef724a000)\n    libgssapi_krb5.so.2 => /usr/lib64/libgssapi_krb5.so.2 (0x00007ffef6ff8000)\n    libldap_r-2.4.so.2 => /usr/lib64/libldap_r-2.4.so.2 (0x00007ffef6da4000)\n    liblber-2.4.so.2 => /usr/lib64/liblber-2.4.so.2 (0x00007ffef6b95000)\n    libzstd.so.1 => /usr/lib64/libzstd.so.1 (0x00007ffef6865000)\n    libbrotlidec.so.1 => /usr/lib64/libbrotlidec.so.1 (0x00007ffef6659000)\n    libunistring.so.2 => /usr/lib64/libunistring.so.2 (0x00007ffef62d6000)\n    libjitterentropy.so.3 => /usr/lib64/libjitterentropy.so.3 (0x00007ffef60cf000)\n    libkrb5.so.3 => /usr/lib64/libkrb5.so.3 (0x00007ffef5df6000)\n    libk5crypto.so.3 => /usr/lib64/libk5crypto.so.3 (0x00007ffef5bde000)\n    libcom_err.so.2 => /lib64/libcom_err.so.2 (0x00007ffef59da000)\n    libkrb5support.so.0 => /usr/lib64/libkrb5support.so.0 (0x00007ffef57cb000)\n    libresolv.so.2 => /lib64/libresolv.so.2 (0x00007ffef55b3000)\n    libsasl2.so.3 => /usr/lib64/libsasl2.so.3 (0x00007ffef5396000)\n    libbrotlicommon.so.1 => /usr/lib64/libbrotlicommon.so.1 (0x00007ffef5175000)\n    libkeyutils.so.1 => /usr/lib64/libkeyutils.so.1 (0x00007ffef4f70000)\n    libselinux.so.1 => /lib64/libselinux.so.1 (0x00007ffef4d47000)\n    libpcre.so.1 => /usr/lib64/libpcre.so.1 (0x00007ffef4abe000)\n*************************************\n\nNotice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable.\nOnce libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.\n\nAlternative: SBCASTing a binary with all libraries\n\nAs mentioned above, you can alternatively use --exclude=NONE on sbcast to send all libraries along with the binary.\nUsing --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time.\nA job script for the previous example, modified for sending all libraries is shown below.\n\n#!/bin/bash\n#SBATCH -A <projid>\n#SBATCH -J sbcast_binary_to_nvme\n#SBATCH -o %x-%j.out\n#SBATCH -t 00:05:00\n#SBATCH -p batch\n#SBATCH -N 2\n#SBATCH -C nvme\n\ndate\n\n# Change directory to user scratch space (Orion)\ncd /lustre/orion/<projid>/scratch/<userid>\n\n# For this example, I use a HIP-enabled LAMMPS binary, with dependencies to MPI, HIP, and HWLOC\nexe=\"lmp\"\n\necho \"*****ldd ./${exe}*****\"\nldd ./${exe}\necho \"*************************\"\n\n# SBCAST executable from Orion to NVMe -- NOTE: ``-C nvme`` is needed in SBATCH headers to use the NVMe drive\n# NOTE: dlopen'd files will NOT be picked up by sbcast\nsbcast --send-libs --exclude=NONE -pf ${exe} /mnt/bb/$USER/${exe}\nif [ ! \"$?\" == \"0\" ]; then\n    # CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,\n    # your application may pick up partially complete shared library files, which would give you confusing errors.\n    echo \"SBCAST failed!\"\n    exit 1\nfi\n\n# Check to see if file exists\necho \"*****ls -lh /mnt/bb/$USER*****\"\nls -lh /mnt/bb/$USER/\necho \"*****ls -lh /mnt/bb/$USER/${exe}_libs*****\"\nls -lh /mnt/bb/$USER/${exe}_libs\n\n# SBCAST sends all libraries detected by `ld` (minus any excluded), and stores them in the same directory in each node's node-local storage\n# Any libraries opened by `dlopen` are NOT sent, since they are not known by the linker at run-time.\n\n# All required libraries now reside in /mnt/bb/$USER/${exe}_libs\nexport LD_LIBRARY_PATH=\"/mnt/bb/$USER/${exe}_libs\"\n\n# libfabric dlopen's several libraries:\nexport LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:$(pkg-config --variable=libdir libfabric)\"\n\n# cray-mpich dlopen's libhsa-runtime64.so and libamdhip64.so (non-versioned), so symlink on each node:\nsrun -N ${SLURM_NNODES} -n ${SLURM_NNODES} --ntasks-per-node=1 --label -D /mnt/bb/$USER/${exe}_libs \\\n    bash -c \"if [ -f libhsa-runtime64.so.1 ]; then ln -s libhsa-runtime64.so.1 libhsa-runtime64.so; fi;\n    if [ -f libamdhip64.so.5 ]; then ln -s libamdhip64.so.5 libamdhip64.so; fi\"\n\n# RocBLAS has over 1,000 device libraries that may be `dlopen`'d by RocBLAS during a run.\n# It's impractical to SBCAST all of these, so you can set this path instead, if you use RocBLAS:\n#export ROCBLAS_TENSILE_LIBPATH=${ROCM_PATH}/lib/rocblas/library\n\n# You may notice that some libraries are still linked from /sw/crusher, even after SBCASTing.\n# This is because the Spack-build modules use RPATH to find their dependencies. This behavior cannot be changed.\necho \"*****ldd /mnt/bb/$USER/${exe}*****\"\nldd /mnt/bb/$USER/${exe}\necho \"*************************************\"\n\nSome libraries still resolved to paths outside of /mnt/bb, and the reason for that is that the executable may have several paths in RPATH.\n\n\n\nSoftware\n\nVisualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the andes-viz-tools <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-viz-tools> section of the andes-user-guide <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-user-guide>.\n\nFor a full list of software availability and latest news at the OLCF, please reference the Software <https://docs.olcf.ornl.gov/software/software-news.html> section in OLCF's User Documentation.\n\nDebugging\n\nLinaro DDT\n\nLinaro DDT is an advanced debugging tool used for scalar, multi-threaded,\nand large-scale parallel applications. In addition to traditional\ndebugging features (setting breakpoints, stepping through code,\nexamining variables), DDT also supports attaching to already-running\nprocesses and memory debugging. In-depth details of DDT can be found in\nthe Official DDT User Guide, and\ninstructions for how to use it on OLCF systems can be found on the Debugging Software <https://docs.olcf.ornl.gov/software/debugging/index.html> page. DDT is the\nOLCF's recommended debugging software for large parallel applications.\n\nOne of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Frontier from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Frontier. For guidance in setting up the remote client see the Debugging Software <https://docs.olcf.ornl.gov/software/debugging/index.html> page.\n\nGDB\n\nGDB, the GNU Project Debugger,\nis a command-line debugger useful for traditional debugging and\ninvestigating code crashes. GDB lets you debug programs written in Ada,\nC, C++, Objective-C, Pascal (and many other languages).\n\nGDB is availableon Summit under all compiler families:\n\nmodule load gdb\n\nTo use GDB to debug your application run:\n\ngdb ./path_to_executable\n\nAdditional information about GDB usage can befound on the GDB Documentation Page.\n\nValgrind4hpc\n\nValgrind4hpc is a Valgrind-based debugging tool to aid in the detection of memory leaks\nand errors in parallel applications. Valgrind4hpc aggregates any duplicate\nmessages across ranks to help provide an understandable picture of\nprogram behavior. Valgrind4hpc manages starting and redirecting output from many\ncopies of Valgrind, as well as deduplicating and filtering Valgrind messages.\nIf your program can be debugged with Valgrind, it can be debugged with\nValgrind4hpc.\n\nValgrind4hpc is available on Frontier under all compiler families:\n\nmodule load valgrind4hpc\n\nAdditional information about Valgrind4hpc usage can be found on the HPE Cray Programming Environment User Guide Page.\n\nProfiling Applications\n\nGetting Started with the HPE Performance Analysis Tools (PAT)\n\nThe Performance Analysis Tools (PAT), formerly CrayPAT, are a suite of utilities that enable users to capture and analyze performance data generated during program execution. These tools provide an integrated infrastructure for measurement, analysis, and visualization of computation, communication, I/O, and memory utilization to help users optimize programs for faster execution and more efficient computing resource usage.\n\nThere are three programming interfaces available: (1) Perftools-lite, (2) Perftools, and (3) Perftools-preload.\n\nBelow are two examples that generate an instrumented executable using Perftools, which is an advanced interface that provides full-featured data collection and analysis capability, including full traces with timeline displays.\n\nThe first example generates an instrumented executable using a PrgEnv-amd build:\n\nmodule load PrgEnv-amd\nmodule load craype-accel-amd-gfx90a\nmodule load rocm\nmodule load perftools\n\nexport PATH=\"${PATH}:${ROCM_PATH}/llvm/bin\"\nexport CXX='CC -x hip'\nexport CXXFLAGS='-ggdb -O3 -std=c++17 –Wall'\nexport LD='CC'\nexport LDFLAGS=\"${CXXFLAGS} -L${ROCM_PATH}/lib\"\nexport LIBS='-lamdhip64'\n\nmake clean\nmake\n\npat_build -g hip,io,mpi -w -f <executable>\n\nThe second example generates an instrumened executable using a hipcc build:\n\nmodule load perftools\nmodule load craype-accel-amd-gfx90a\nmodule load rocm\n\nexport CXX='hipcc'\nexport CXXFLAGS=\"$(pat_opts include hipcc) \\\n  $(pat_opts pre_compile hipcc) -g -O3 -std=c++17 -Wall \\\n  --offload-arch=gfx90a -I${CRAY_MPICH_DIR}/include \\\n  $(pat_opts post_compile hipcc)\"\nexport LD='hipcc'\nexport LDFLAGS=\"$(pat_opts pre_link hipcc) ${CXXFLAGS} \\\n  -L${CRAY_MPICH_DIR}/lib ${PE_MPICH_GTL_DIR_amd_gfx908}\"\nexport LIBS=\"-lmpi ${PE_MPICH_GTL_LIBS_amd_gfx908} \\\n  $(pat_opts post_link hipcc)\"\n\nmake clean\nmake\n\npat_build -g hip,io,mpi -w -f <executable>\n\nThe pat_build command in the above examples generates an instrumented executable with +pat appended to the executable name (e.g., hello_jobstep+pat).\n\nWhen run, the instrumented executable will trace HIP, I/O, MPI, and all user functions and generate a folder of results (e.g., hello_jobstep+pat+39545-2t).\n\nTo analyze these results, use the pat_report command, e.g.:\n\npat_report hello_jobstep+pat+39545-2t\n\nThe resulting report includes profiles of functions, profiles of maximum function times, details on load imbalance, details on program energy and power usages, details on memory high water mark, and more.\n\nMore detailed information on the HPE Performance Analysis Tools can be found in the HPE Performance Analysis Tools User Guide.\n\nWhen using perftools-lite-gpu, there is a known issue causing ld.lld not to be found. A workaround this issue can be found here.\n\nGetting Started with HPCToolkit\n\nHPCToolkit is an integrated suite of tools for measurement and analysis of program performance on computers ranging from multicore desktop systems to the nation's largest supercomputers. HPCToolkit provides accurate measurements of a program's work, resource consumption, and inefficiency, correlates these metrics with the program's source code, works with multilingual, fully optimized binaries, has very low measurement overhead, and scales to large parallel systems. HPCToolkit's measurements provide support for analyzing a program execution cost, inefficiency, and scaling characteristics both within and across nodes of a parallel system.\n\nProgramming models supported by HPCToolkit include MPI, OpenMP, OpenACC, CUDA, OpenCL, DPC++, HIP, RAJA, Kokkos, and others.\n\nBelow is an example that generates a profile and loads the results in their GUI-based viewer.\n\nA full list of available HPCToolkit versions can be seen with the module spider hpctoolkit command.\n\nmodule load hpctoolkit/2022.05.15-rocm\n\n# 1. Profile and trace an application using CPU time and GPU performance counters\nsrun <srun_options> hpcrun -o <measurement_dir> -t -e CPUTIME -e gpu=amd <application>\n\n# 2. Analyze the binary of executables and its dependent libraries\nhpcstruct <measurement_dir>\n\n# 3. Combine measurements with program structure information and generate a database\nhpcprof -o <database_dir> <measurement_dir>\n\n# 4. Understand performance issues by analyzing profiles and traces with the GUI\nhpcviewer <database_dir>\n\nMore detailed information on HPCToolkit can be found in the HPCToolkit User's Manual.\n\nHPCToolkit does not require a recompile to profile the code. It is recommended to use the -g optimization flag for attribution to source lines.\n\nGetting Started with the ROCm Profiler\n\nrocprof gathers metrics on kernels run on AMD GPU architectures. The profiler works for HIP kernels, as well as offloaded kernels from OpenMP target offloading, OpenCL, and abstraction layers such as Kokkos.\nFor a simple view of kernels being run, rocprof --stats --timestamp on is a great place to start.\nWith the --stats option enabled, rocprof will generate a file that is named results.stats.csv by default, but named <output>.stats.csv if the -o flag is supplied.\nThis file will list all kernels being run, the number of times they are run, the total duration and the average duration (in nanoseconds) of the kernel, and the GPU usage percentage.\nMore detailed infromation on rocprof profiling modes can be found at ROCm Profiler documentation.\n\nIf you are using sbcast, you need to explicitly sbcast the AQL profiling library found in ${ROCM_PATH}/hsa-amd-aqlprofile/lib/libhsa-amd-aqlprofile64.so.\nA symbolic link to this library can also be found in ${ROCM_PATH}/lib.\nAlternatively, you may leave ${ROCM_PATH}/lib in your LD_LIBRARY_PATH.\n\nRoofline Profiling with the ROCm Profiler\n\nThe Roofline performance model is an increasingly popular way to demonstrate and understand application performance.\nThis section documents how to construct a simple roofline model for a single kernel using rocprof.\nThis roofline model is designed to be comparable to rooflines constructed by NVIDIA's NSight Compute.\nA roofline model plots the achieved performance (in floating-point operations per second, FLOPS/s) as a function of arithmetic (or operational) intensity (in FLOPS per Byte).\nThe model detailed here calculates the bytes moved as they move to and from the GPU's HBM.\n\nInteger instructions and cache levels are currently not documented here.\n\nTo get started, you will need to make an input file for rocprof, to be passed in through rocprof -i <input_file> --timestamp on -o my_output.csv <my_exe>.\nBelow is an example, and contains the information needed to roofline profile GPU 0, as seen by each rank:\n\npmc : TCC_EA_RDREQ_32B_sum TCC_EA_RDREQ_sum TCC_EA_WRREQ_sum TCC_EA_WRREQ_64B_sum SQ_INSTS_VALU_ADD_F16 SQ_INSTS_VALU_MUL_F16 SQ_INSTS_VALU_FMA_F16 SQ_INSTS_VALU_TRANS_F16 SQ_INSTS_VALU_ADD_F32 SQ_INSTS_VALU_MUL_F32 SQ_INSTS_VALU_FMA_F32 SQ_INSTS_VALU_TRANS_F32\npmc : SQ_INSTS_VALU_ADD_F64 SQ_INSTS_VALU_MUL_F64 SQ_INSTS_VALU_FMA_F64 SQ_INSTS_VALU_TRANS_F64 SQ_INSTS_VALU_MFMA_MOPS_F16 SQ_INSTS_VALU_MFMA_MOPS_BF16 SQ_INSTS_VALU_MFMA_MOPS_F32 SQ_INSTS_VALU_MFMA_MOPS_F64\ngpu: 0\n\nIn an application with more than one kernel, you should strongly consider filtering by kernel name by adding a line like: kernel: <kernel_name> to the rocprof input file.\n\nThis provides the minimum set of metrics used to construct a roofline model, in the minimum number of passes.\nEach line that begins with pmc indicates that the application will be re-run, and the metrics in that line will be collected.\nrocprof can collect up to 8 counters from each block (SQ, TCC) in each application re-run.\nTo gather metrics across multiple MPI ranks, you will need to use a command that redirects the output of rocprof to a unique file for each task.\nFor example:\n\nsrun -N 2 -n 16 --ntasks-per-node=8 --gpus-per-node=8 --gpu-bind=closest bash -c 'rocprof -o ${SLURM_JOBID}_${SLURM_PROCID}.csv -i <input_file> --timestamp on <exe>'\n\nThe gpu: filter in the rocprof input file identifies GPUs by the number the MPI rank would see them as. In the srun example above,\neach MPI rank only has 1 GPU, so each rank sees its GPU as GPU 0.\n\nTheoretical Roofline\n\nThe theoretical (not attainable) peak roofline constructs a theoretical maximum performance for each operational intensity.\n\ntheoretical peak is determined by the hardware specifications and is not attainable in practice. attaiable peak is the performance as measured by\nin-situ microbenchmarks designed to best utilize the hardware. achieved performance is what the profiled application actually achieves.\n\nThe theoretical roofline can be constructed as:\n\nFLOPS_{peak} = minimum(ArithmeticIntensity * BW_{HBM}, TheoreticalFLOPS)\n\nOn Frontier, the memory bandwidth for HBM is 1.6 TB/s, and the theoretical peak floating-point FLOPS/s when using vector registers is calculated by:\n\nTheoreticalFLOPS = 128 FLOP/cycle/CU * 110 CU * 1700000000 cycles/second = 23.9 TFLOP/s\n\nHowever, when using MFMA instructions, the theoretical peak floating-point FLOPS/s is calculated by:\n\nTheoreticalFLOPS = flop\\_per\\_cycle(precision) FLOP/cycle/CU * 110 CU * 1700000000 cycles/second\n\nwhere flop_per_cycle(precision) is the published floating-point operations per clock cycle, per compute unit.\nThose values are:\n\nTable-36-to-be-replaced\n\nAttainable peak rooflines are constructed using microbenchmarks, and are not currently discussed here.\nAttainable rooflines consider the limitations of cooling and power consumption and are more representative of what an application can achieve.\n\nAchieved FLOPS/s\n\nWe calculate the achieved performance at the desired level (here, double-precision floating point, FP64), by summing each metric count and weighting the FMA metric by 2, since a fused multiply-add is considered 2 floating point operations.\nAlso note that these SQ_INSTS_VALU_<ADD,MUL,TRANS> metrics are reported as per-simd, so we mutliply by the wavefront size as well.\nThe SQ_INSTS_VALU_MFMA_MOPS_* instructions should be multiplied by the Flops/Cycle/CU value listed above.\nWe use this equation to calculate the number of double-precision FLOPS:\n\nFP64\\_FLOPS =   64  *&(SQ\\_INSTS\\_VALU\\_ADD\\_F64         \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_MUL\\_F64       \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_TRANS\\_F64     \\\\\\\\\n                     &+ 2 * SQ\\_INSTS\\_VALU\\_FMA\\_F64)  \\\\\\\\\n              + 256 *&(SQ\\_INSTS\\_VALU\\_MFMA\\_MOPS\\_F64)\n\nWhen SQ_INSTS_VALU_MFMA_MOPS_*_F64 instructions are used, then 47.8 TF/s is considered the theoretical maximum FLOPS/s.\nIf only SQ_INSTS_VALU_<ADD,MUL,TRANS> are found, then 23.9 TF/s is the theoretical maximum FLOPS/s.\nThen, we divide the number of FLOPS by the elapsed time of the kernel to find FLOPS per second.\nThis is found from subtracting the rocprof metrics EndNs by BeginNs, provided by --timestamp on, then converting from nanoseconds to seconds by dividing by 1,000,000,000 (power(10,9)).\n\nFor ROCm/5.2.0 and earlier, there is a known issue with the timings provided by --timestamp on. See crusher-known-issues <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#crusher-known-issues>.\n\nCalculating for all precisions\n\nThe above formula can be adapted to compute the total FLOPS across all floating-point precisions (INT excluded).\n\nTOTAL\\_FLOPS =   64  *&(SQ\\_INSTS\\_VALU\\_ADD\\_F16         \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_MUL\\_F16       \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_TRANS\\_F16     \\\\\\\\\n                     &+ 2 * SQ\\_INSTS\\_VALU\\_FMA\\_F16)  \\\\\\\\\n              + 64  *&(SQ\\_INSTS\\_VALU\\_ADD\\_F32         \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_MUL\\_F32       \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_TRANS\\_F32     \\\\\\\\\n                     &+ 2 * SQ\\_INSTS\\_VALU\\_FMA\\_F32)  \\\\\\\\\n              + 64  *&(SQ\\_INSTS\\_VALU\\_ADD\\_F64         \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_MUL\\_F64       \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_TRANS\\_F64     \\\\\\\\\n                     &+ 2 * SQ\\_INSTS\\_VALU\\_FMA\\_F64)  \\\\\\\\\n              + 1024 &*(SQ\\_INSTS\\_VALU\\_MFMA\\_MOPS\\_F16) \\\\\\\\\n              + 1024 &*(SQ\\_INSTS\\_VALU\\_MFMA\\_MOPS\\_BF16) \\\\\\\\\n              + 256 *&(SQ\\_INSTS\\_VALU\\_MFMA\\_MOPS\\_F32) \\\\\\\\\n              + 256 *&(SQ\\_INSTS\\_VALU\\_MFMA\\_MOPS\\_F64) \\\\\\\\\n\nArithmetic Intensity\n\nArithmetic intensity calculates the ratio of FLOPS to bytes moved between HBM and L2 cache.\nWe calculated FLOPS above (FP64_FLOPS).\nWe can calculate the number of bytes moved using the rocprof metrics TCC_EA_WRREQ_64B, TCC_EA_WRREQ_sum, TCC_EA_RDREQ_32B, and TCC_EA_RDREQ_sum.\nTCC refers to the L2 cache, and EA is the interface between L2 and HBM.\nWRREQ and RDREQ are write-requests and read-requests, respectively.\nEach of these requests is either 32 bytes or 64 bytes.\nSo we calculate the number of bytes traveling over the EA interface as:\n\nBytesMoved = BytesWritten + BytesRead\n\nwhere\n\nBytesWritten = 64 * TCC\\_EA\\_WRREQ\\_64B\\_sum + 32 * (TCC\\_EA\\_WRREQ\\_sum - TCC\\_EA\\_WRREQ\\_64B\\_sum)\n\nBytesRead = 32 * TCC\\_EA\\_RDREQ\\_32B\\_sum + 64 * (TCC\\_EA\\_RDREQ\\_sum - TCC\\_EA\\_RDREQ\\_32B\\_sum)\n\n\n\n\n\nTips and Tricks\n\nThis section details 'tips and tricks' and information of interest to users when porting from Summit to Frontier.\n\n\n\nUsing reduced precision (FP16 and BF16 datatypes)\n\nUsers leveraging BF16 and FP16 datatypes for applications such as ML/AI training and low-precision matrix multiplication should be aware that the AMD MI250X GPU has different denormal handling than the V100 GPUs on Summit. On the MI250X, the V_DOT2 and the matrix instructions for FP16 and BF16 flush input and output denormal values to zero. FP32 and FP64 MFMA instructions do not flush input and output denormal values to zero.\n\nWhen training deep learning models using FP16 precision, some models may fail to converge with FP16 denorms flushed to zero. This occurs in operations encountering denormal values, and so is more likely to occur in FP16 because of a small dynamic range. BF16 numbers have a larger dynamic range than FP16 numbers and are less likely to encounter denormal values.\n\nAMD has provided a solution in ROCm 5.0 which modifies the behavior of Tensorflow, PyTorch, and rocBLAS. This modification starts with FP16 input values, casting the intermediate FP16 values to BF16, and then casting back to FP16 output after the accumulate FP32 operations. In this way, the input and output types are unchanged. The behavior is enabled by default in machine learning frameworks. This behavior requires user action in rocBLAS, via a special enum type. For more information, see the rocBLAS link below.\n\nIf you encounter significant differences when running using reduced precision, explore replacing non-converging models in FP16 with BF16, because of the greater dynamic range in BF16. We recommend using BF16 for ML models in general. If you have further questions or encounter issues, contact help@olcf.ornl.gov.\n\nAdditional information on MI250X reduced precision can be found at:\n\nThe MI250X ISA specification details the flush to zero denorm behavior at: https://developer.amd.com/wp-content/resources/CDNA2_Shader_ISA_18November2021.pdf (See page 41 and 46)\n\nAMD rocBLAS library reference guide details this behavior at: https://rocblas.readthedocs.io/en/master/API_Reference_Guide.html#mi200-gfx90a-considerations\n\n\n\nEnabling GPU Page Migration\n\nThe AMD MI250X and operating system on Frontier supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Frontier. In the following section we'll discuss how the heterogenous memory space on a Frontier node is surfaced within your application.\n\nThe accessibility of memory from GPU kernels and whether pages may migrate depends three factors: how the memory was allocated; the XNACK operating mode of the GPU; whether the kernel was compiled to support page migration. The latter two factors are intrinsically linked, as the MI250X GPU operating mode restricts the types of kernels which may run.\n\nXNACK (pronounced X-knack) refers to the AMD GPU's ability to retry memory accesses that fail due to a page fault. The XNACK mode of an MI250X can be changed by setting the environment variable HSA_XNACK before starting a process that uses the GPU. Valid values are 0 (disabled) and 1 (enabled), and all processes connected to a GPU must use the same XNACK setting. The default MI250X on Frontier is HSA_XNACK=0.\n\nIf HSA_XNACK=0, page faults in GPU kernels are not handled and will terminate the kernel. Therefore all memory locations accessed by the GPU must either be resident in the GPU HBM or mapped by the HIP runtime. Memory regions may be migrated between the host DDR4 and GPU HBM using explicit HIP library functions such as hipMemAdvise and hipPrefetchAsync, but memory will not be automatically migrated based on access patterns alone.\n\nIf HSA_XNACK=1, page faults in GPU kernels will trigger a page table lookup. If the memory location can be made accessible to the GPU, either by being migrated to GPU HBM or being mapped for remote access, the appropriate action will occur and the access will be replayed. Page migration  will happen between CPU DDR4 and GPU HBM according to page touch. The exceptions are if the programmer uses a HIP library call such as hipPrefetchAsync to request migration, or if a preferred location is set via hipMemAdvise, or if GPU HBM becomes full and the page must forcibly be evicted back to CPU DDR4 to make room for other data.\n\nIf ``HSA_XNACK=1``, page faults in GPU kernels will trigger a page table lookup. If the memory location can be made accessible to the GPU, either by being migrated to GPU HBM or being mapped for remote access, the appropriate action will occur and the access will be replayed. Once a memory region has been migrated to GPU HBM it typically stays there rather than migrating back to CPU DDR4. The exceptions are if the programmer uses a HIP library call such as ``hipPrefetchAsync`` to request migration, or if GPU HBM becomes full and the page must forcibly be evicted back to CPU DDR4 to make room for other data.\n\n\n\nMigration of Memory by Allocator and XNACK Mode\n\nMost applications that use \"managed\" or \"unified\" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Frontier. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.\n\nThe page migration behavior summarized by the following tables represents the current, observable behavior. Said behavior will likely change in the near future.\n\nCPU accesses to migratable memory may behave differently than other platforms you're used to. On Frontier, pages will not migrate from GPU HBM to CPU DDR4 based on access patterns alone. Once a page has migrated to GPU HBM it will remain there even if the CPU accesses it, and all accesses which do not resolve in the CPU cache will occur over the Infinity Fabric between the AMD \"Optimized 3rd Gen EPYC\" CPU and AMD MI250X GPU. Pages will only *automatically* migrate back to CPU DDR4 if they are forcibly evicted to free HBM capacity, although programmers may use HIP APIs to manually migrate memory regions.\n\nHSA_XNACK=1 Automatic Page Migration Enabled\n\n+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+\n| Allocator                                   | Initial Physical Location | CPU Access after GPU First Touch           | Default Behavior for GPU Access                    |\n+=============================================+===========================+============================================+====================================================+\n| System Allocator (malloc,new,allocate, etc) | Determined by first touch | Zero copy read/write                       | Migrate to GPU HBM on touch, then local read/write |\n+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+\n| hipMallocManaged                            | GPU HBM                   | Zero copy read/write                       | Populate in  GPU HBM, then local read/write        |\n+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+\n| hipHostMalloc                               | CPU DDR4                  | Local read/write                           | Zero copy read/write over Infinity Fabric          |\n+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+\n| hipMalloc                                   | GPU HBM                   | Zero copy read/write over Inifinity Fabric | Local read/write                                   |\n+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+\n\nTable-37-to-be-replaced\n\nDisabling XNACK will not necessarily result in an application failure, as most types of memory can still be accessed by the AMD \"Optimized 3rd Gen EPYC\" CPU and AMD MI250X GPU. In most cases, however, the access will occur in a zero-copy fashion over the Infinity Fabric. The exception is memory allocated through standard system allocators such as malloc, which cannot be accessed directly from GPU kernels without previously being registered via a HIP runtime call such as hipHostRegister. Access to malloc'ed and unregistered memory from GPU kernels will result in fatal unhandled page faults. The table below shows how common allocators behave with XNACK disabled.\n\nHSA_XNACK=0 Automatic Page Migration Disabled\n\nTable-38-to-be-replaced\n\n\n\nCompiling HIP kernels for specific XNACK modes\n\nAlthough XNACK is a capability of the MI250X GPU, it does require that kernels be able to recover from page faults. Both the ROCm and CCE HIP compilers will default to generating code that runs correctly with both XNACK enabled and disabled. Some applications may benefit from using the following compilation options to target specific XNACK modes.\n\nhipcc --amdgpu-target=gfx90a or CC --offload-arch=gfx90a -x hip\n\nKernels are compiled to a single \"xnack any\" binary, which will run correctly with both XNACK enabled and XNACK disabled.\n\nhipcc --amdgpu-target=gfx90a:xnack+ or CC --offload-arch=gfx90a:xnack+ -x hip\n\nKernels are compiled in \"xnack plus\" mode and will only be able to run on GPUs with HSA_XNACK=1 to enable XNACK. Performance may be better than \"xnack any\", but attempts to run with XNACK disabled will fail.\n\nhipcc --amdgpu-target=gfx90a:xnack- or CC --offload-arch=gfx90a:xnack- -x hip\n\nKernels are compiled in \"xnack minus\" mode and will only be able to run on GPUs with HSA_XNACK=0 and XNACK disabled. Performance may be better than \"xnack any\", but attempts to run with XNACK enabled will fail.\n\nhipcc --amdgpu-target=gfx90a:xnack- --amdgpu-target=gfx90a:xnack+ -x hip or CC --offload-arch=gfx90a:xnack- --offload-arch=gfx90a:xnack+ -x hip\n\nTwo versions of each kernel will be generated, one that runs with XNACK disabled and one that runs if XNACK is enabled. This is different from \"xnack any\" in that two versions of each kernel are compiled and HIP picks the appropriate one at runtime, rather than there being a single version compatible with both. A \"fat binary\" compiled in this way will have the same performance of \"xnack+\" with HSA_XNACK=1 and as \"xnack-\" with HSA_XNACK=0, but the final executable will be larger since it contains two copies of every kernel.\n\nIf the HIP runtime cannot find a kernel image that matches the XNACK mode of the device, it will fail with hipErrorNoBinaryForGpu.\n\n$ HSA_XNACK=0 srun -n 1 -N 1 -t 1 ./xnack_plus.exe\n\"hipErrorNoBinaryForGpu: Unable to find code object for all current devices!\"\nsrun: error: frontier002: task 0: Aborted\nsrun: launch/slurm: _step_signal: Terminating StepId=74100.0\n\nNOTE: This works in my shell because I used cpan to install the URI::Encode perl modules.\nThis won't work generically unless those get installed, so commenting out this block now.\n\nThe AMD tool `roc-obj-ls` will let you see what code objects are in a binary.\n\n.. code::\n    $ hipcc --amdgpu-target=gfx90a:xnack+ square.hipref.cpp -o xnack_plus.exe\n    $ roc-obj-ls -v xnack_plus.exe\n    Bundle# Entry ID:                                                              URI:\n    1       host-x86_64-unknown-linux                                           file://xnack_plus.exe#offset=8192&size=0\n    1       hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+                              file://xnack_plus.exe#offset=8192&size=9752\n\nIf no XNACK flag is specificed at compilation the default is \"xnack any\", and objects in `roc-obj-ls` with not have an XNACK mode specified.\n\n.. code::\n    $ hipcc --amdgpu-target=gfx90a square.hipref.cpp -o xnack_any.exe\n    $ roc-obj-ls -v xnack_any.exe\n    Bundle# Entry ID:                                                              URI:\n    1       host-x86_64-unknown-linux                                           file://xnack_any.exe#offset=8192&size=0\n    1       hipv4-amdgcn-amd-amdhsa--gfx90a                                     file://xnack_any.exe#offset=8192&size=9752\n\nOne way to diagnose hipErrorNoBinaryForGpu messages is to set the environment variable AMD_LOG_LEVEL to 1 or greater:\n\n$ AMD_LOG_LEVEL=1 HSA_XNACK=0 srun -n 1 -N 1 -t 1 ./xnack_plus.exe\n:1:rocdevice.cpp            :1573: 43966598070 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966598762 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966599392 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966599970 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966600550 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966601109 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966601673 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966602248 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:hip_code_object.cpp      :460 : 43966602806 us: hipErrorNoBinaryForGpu: Unable to find code object for all current devices!\n:1:hip_code_object.cpp      :461 : 43966602810 us:   Devices:\n:1:hip_code_object.cpp      :464 : 43966602811 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602811 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602812 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602813 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602813 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602814 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602814 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602815 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :468 : 43966602816 us:   Bundled Code Objects:\n:1:hip_code_object.cpp      :485 : 43966602817 us:     host-x86_64-unknown-linux - [Unsupported]\n:1:hip_code_object.cpp      :483 : 43966602818 us:     hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+ - [code object v4 is amdgcn-amd-amdhsa--gfx90a:xnack+]\n\"hipErrorNoBinaryForGpu: Unable to find code object for all current devices!\"\nsrun: error: frontier129: task 0: Aborted\nsrun: launch/slurm: _step_signal: Terminating StepId=74102.0\n\nThe above log messages indicate the type of image required by each device, given its current mode (amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack-) and the images found in the binary (hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+).\n\n\n\n\n\nFloating-Point (FP) Atomic Operations and Coarse/Fine Grained Memory Allocations\n\nThe Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities.\nThe AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs.\nThis new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.\n\nAMD GPUs can allocate two different types of memory locations: 1) Coarse grained and 2) Fine grained.\n\nCoarse grained memory is only guaranteed to be coherent outside of GPU kernels that modify it, enabling higher performance memory operations. Changes applied to coarse-grained memory by a GPU kernel are  only visible to the rest of the system (CPU or other GPUs) when the kernel has completed. A GPU kernel is only guaranteed to see changes applied to coarse grained memory by the rest of the system (CPU or other GPUs) if those changes were made before the kernel launched.\n\nFine grained memory allows CPUs and GPUs to synchronize (via atomics) and coherently communicate with each other while the GPU kernel is running, allowing more advanced programming patterns. The additional visibility impacts the performance of fine grained allocated memory.\n\nThe fast hardware-based Floating point (FP) atomic operations available on MI250X are assumed to be working on coarse grained memory regions; when these instructions are applied to a fine-grained memory region, they will silently produce a no-op. To avoid returning incorrect results, the compiler never emits hardware-based FP atomics instructions by default, even when applied to coarse grained memory regions. Currently, users can use the -munsafe-fp-atomics flag to force the compiler to emit hardware-based FP atomics.\nUsing hardware-based FP atomics translates in a substantial performance improvement over the default choice.\n\nUsers applying floating point atomic operations (e.g., atomicAdd) on memory regions allocated via regular hipMalloc() can safely apply the -munsafe-fp-atomics flags to their codes to get the best possible performance and leverage hardware supported floating point atomics.\nAtomic operations supported in hardware on non-FP datatypes  (e.g., INT32) will work correctly regardless of the nature of the memory region used.\n\nIn ROCm-5.1 and earlier versions, the flag -munsafe-fp-atomics is interpreted as a suggestion by the compiler, whereas from ROCm-5.2 the flag will always enforce the use of fast hardware-based FP atomics.\n\nThe following tables summarize the result granularity of various combinations of allocators, flags and arguments.\n\nFor hipHostMalloc(), the following table shows the nature of the memory returned based on the flag passed as argument.\n\nTable-39-to-be-replaced\n\nThe following table shows the nature of the memory returned based on the flag passed as argument to hipExtMallocWithFlags().\n\nTable-40-to-be-replaced\n\nFinally, the following table summarizes the nature of the memory returned based on the flag passed as argument to hipMallocManaged() and the use of CPU regular malloc() routine with the possible use of hipMemAdvise().\n\nTable-41-to-be-replaced\n\n\n\nPerformance considerations for LDS FP atomicAdd()\n\nHardware FP atomic operations performed in LDS memory are usually always faster than an equivalent CAS loop, in particular when contention on LDS memory locations is high.\nBecause of a hardware design choice, FP32 LDS atomicAdd() operations can be slower than equivalent FP64 LDS atomicAdd(), in particular when contention on memory locations is low (e.g. random access pattern).\nThe aforementioned behavior is only true for FP atomicAdd() operations. Hardware atomic operations for CAS/Min/Max on FP32 are usually faster than the FP64 counterparts.\nIn cases when contention is very low, a FP32 CAS loop implementing an atomicAdd() operation could be faster than an hardware FP32 LDS atomicAdd().\nApplications using single precision FP atomicAdd() are encouraged to experiment with the use of double precision to evaluate the trade-off between high atomicAdd() performance vs. potential lower occupancy due to higher LDS usage.\n\nLibrary considerations with atomic operations\n\nSome functionality provided by the rocBLAS and hipBLAS libraries use atomic operations to improve performance by default. This can cause results to not be bit-wise reproducible.\nLevel 2 functions that may use atomic operations include: gemv, hemv, and symv, which introduced atomic operations in ROCm 5.5. All of the Level 3 functions, along with Level 2 trsv, may use atomic operations where dependent\non gemm. Atomic operations are used for problem sizes where they are shown to improve performance.\nIf it is necessary to have bit-wise reproducible results from these libraries, it is recommended to turn the atomic operations off by setting the mode via the rocBLAS or hipBLAS handle:\n\n...\nrocblas_create_handle(handle);\nrocblas_set_atomics_mode(handle, rocblas_atomics_not_allowed);\n\nhipblasCreate(&handle);\nhipblasSetAtomicsMode(handle, HIPBLAS_ATOMICS_NOT_ALLOWED);\n\n\n\nSystem Updates\n\n2023-09-19\n\nOn Tuesday, September 19, 2023, Frontier's system software was upgraded. The following changes took place:\n\nThe system was upgraded to Slingshot Host Software 2.1.0.\n\nROCm 5.6.0 and 5.7.0 are now available via the rocm/5.6.0 and rocm/5.7.0 modulefiles, respectively.\n\nHPE/Cray Programming Environments (PE) 23.09 is now available via the cpe/23.09 modulefile.\n\nROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.\n\n2023-07-18\n\nOn Tuesday, July 18, 2023, Frontier was upgraded to a new version of the system software stack. During the upgrade, the following changes took place:\n\nThe system was upgraded to Cray OS 2.5, Slingshot Host Software 2.0.2-112, and the AMD GPU 6.0.5 device driver (ROCm 5.5.1 release).\n\nROCm 5.5.1 is now available via the rocm/5.5.1 modulefile.\n\nHPE/Cray Programming Environments (PE) 23.05 is now available via the cpe/23.05 modulefile.\n\nHPE/Cray PE 23.05 introduces support for ROCm 5.5.1. However, due to issues identified during testing, ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.\n\n2023-05-09\n\nOn Tuesday, May 9, 2023, the darshan-runtime modulefile was added to DefApps and is now loaded by default on Frontier. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>. Unloading darshan-runtime is recommended for users profiling their applications with other profilers to prevent conflicts.\n\n\n\nKnown Issues\n\nJIRA_CONTENT_HERE"}
{"doc":"getting_started","text":"Getting Started\n\n\n\nThis section will describe the project allocation process for Slate. This is\napplicable to both the Onyx and Marble OLCF OpenShift clusters.\n\nIn addition, we will go over the process to log into Onyx and Marble, and how\nnamespaces work within the OpenShift context.\n\nRequesting A Slate Project Allocation\n\nPlease fill out the Slate Request Form in order\nto use Slate resources. This must be done in order to proceed.\n\nIf you have any question please contact User Assistance, via a Help Ticket Submission or\nby emailing help@olcf.ornl.gov.\n\nRequired information:\n\n- Existing OLCF Project ID\n\n- Project PI\n\n- Enclave (i.e. Open or Moderate Enclave - Onyx or Marble respectively)\n\nNOTE: Summit is in Moderate\n\n- Description (i.e. How you will use Slate)\n\n- Resource Request (i.e. CPU/Memory/Storage requirements - Default is\n\n8CPU/16GB/50GB respectively)\n\nLogging in\n\nThe web UI for OpenShift is available from all of ORNL (you should be\nable to reach it from your laptop on ORNL WiFi as well as the VPN).\n\n\nThe table above provides information on two different clusters, Marble and Onyx, that are available for use. Marble is described as a moderate production cluster with access to Summit and Alpine, while Onyx is an open production cluster with access to Wolf. The table also includes the URL for each cluster's web console, which is where users can access and manage their resources on the cluster. The web console for Marble is https://marble.ccs.ornl.gov, while the web console for Onyx is https://onyx.ccs.ornl.gov. These URLs are important for users to know in order to access and utilize the resources on each cluster.  Overall, this table provides a brief overview of the two clusters and their corresponding web consoles, making it a useful reference for those looking to get started with these resources.\n\n| Cluster | URL |\n|---------|-----|\n| Marble (Moderate Production cluster with access to Summit/Alpine) | Marble Web Console - https://marble.ccs.ornl.gov https://marble.ccs.ornl.gov/ |\n| Onyx (Open Production Cluster with access to Wolf) | Onyx Web Console - https://onyx.ccs.ornl.gov https://onyx.ccs.ornl.gov/ |\n\n\n\nSlate Namespaces\n\nSlate Namespaces map directly to OLCF Project ID's.\n\nOnce your Slate Project Allocation Request is approved,\nyou can create your own namespaces and move your allocation\naround those namespaces via the quota dashboard located at https://quota.marble.ccs.ornl.gov\nand https://quota.onyx.ccs.ornl.gov. The terms\n\"namespace\" and \"project\" may get used interchangeably when referring to your\nproject's usable space within the requested resource boundaries\n(CPU/Memory/Storage).\n\n\n\nInstall the OC tool\n\nThe OC tool provides CLI access to the OpenShift cluster. It needs to be\ninstalled on your machine.\n\nIt is a single binary that can be downloaded from a number of places (the\nchoice is yours):\n\nDirect from the cluster (preferred):\n\nMarble Command Line Tools\n\nOnyx Command Line Tools\n\nHomebrew on MacOS (need Homebrew setup first):\n\nThe Homebrew package is not always kept up to date with the latest version\nof OpenShift so some client features may not be available\n\n$ brew install openshift-cli\n\nRHEL/CentOS (requires openshift-origin repo):\n\n$ yum install origin-clients\n\nFrom Source\n\nhttps://github.com/openshift/oc\n\nTest login with OC Tool\n\n\nThe table above provides a detailed overview of two different clusters, Marble and Onyx, that are available for use. The first cluster, Marble, is described as a moderate production cluster with access to Summit and Alpine. It has two corresponding URLs, both of which are listed as https://api.marble.ccs.ornl.gov. The second cluster, Onyx, is described as an open production cluster with access to Wolf. Similar to Marble, it also has two corresponding URLs, both of which are listed as https://api.onyx.ccs.ornl.gov. These clusters are likely used for data processing and analysis, and the listed URLs are most likely used to access the clusters remotely. It is important to note that these clusters may have different levels of access and capabilities, as indicated by the descriptions provided. Overall, this table provides a clear and concise summary of the available clusters and their corresponding URLs for users to get started with their data processing needs.\n\n| Cluster | URL |\n|---------|-----|\n| Marble (Moderate Production cluster with access to Summit/Alpine) | https://api.marble.ccs.ornl.gov https://api.marble.ccs.ornl.gov |\n| Onyx (Open Production Cluster with access to Wolf) | https://api.onyx.ccs.ornl.gov https://api.onyx.ccs.ornl.gov |\n\nReplace <URL> with the appropriate cluster link above (Onyx or Marble).\n\n$ oc login <URL>\n\nAfter entering the login command above, oc will ask you to obtain an API token and will provide a URL like the following: https://oauth-openshift.apps.<CLUSTER>.ccs.ornl.gov/oauth/token/request.\n\nYou will need to go to the given URL in your browser, log in with NCCS, click the Display Token link, copy the command under Log in with this token and enter it into your terminal.\n\n(NOTE: Marble authentication uses NCCS Usernames and RSA passcodes. Onyx\nuses XCAMS usernames and passwords).\n\nOnce you login, the output will tell you what projects/namespaces you have\naccess to.\n\nYou can view/select/switch between projects/namespaces with the oc tool:\n\n#List all projects/namespaces available to you\n$ oc get projects\n\n#Use or work within specific project/namespace\n$ oc project <project-name>"}
{"doc":"gitlab_runner","text":"GitLab Runners\n\n\n\nGitLab CI/CD jobs may be accomplished using the GitLab Runner application. An open-source application written in Go, a GitLab Runner\nmay be installed with a variety of executors. This documentation focuses on the installation and configuration for use of the\nGitLab Runner Kubernetes Executor on Slate.\n\nReferences\n\nhttps://docs.gitlab.com/runner/\n\nhttps://docs.gitlab.com/runner/executors/kubernetes.html\n\nhttps://docs.gitlab.com/runner/install/kubernetes.html\n\nRegistration Token\n\nPrior to installation of the GitLab Runner, a registration token for the runner is needed from the GitLab server. This token will\nallow a GitLab runner to register to the server in the needed location for running CI/CD jobs. Runners themselves may be registered\nto either a group as a shared runner or a project as a repository specific runner.\n\nIf the runner is to be a group shared runner, navigate to the group in GitLab and then go to Settings -> CI/CD. Expand the Runners\nsection of the CI/CD Settings panel. Ensure that the \"Enable shared runners for this group\" toggle is enabled. The\nregistration token should also be available for retrieval from \"Group Runners\" area.\n\nIf the runner is to be registered to a specific project, first ensure that the project is enabled for pipelines by navigating to\nthe project in GitLab. In the Settings for the project, select General. Expand the \"Visibility, project features, and permissions\"\nsection and locate the \"Pipelines\" option. If it is currently disabled, enable the \"Pipelines\" option and then \"Save Changes\".\nOnce saved, refresh the project General Settings page, and locate the newly available \"CI/CD Settings\" option. Select \"CI/CD\", and\nexpand the \"Runners\" section of the CI/CD settings. In the \"Specific Runners\" area, the registration token should be available for retrieval.\n\nInstallation and Configuration\n\nUpon login to the web UI for Marble or Onyx, you will be on a projects page. In the navigation bar in the upper left corner, you should see\neither the \"Developer\" or \"Administrator\" perspective indicated. These two perspectives present two different ways of viewing resources\ndeployed to the cluster. The Developer Perspective focuses on a Topology overview for the resources and will be used for the deployment\nof a GitLab Runner.\n\nTo deploy a GitLab Runner from the Developer Perspective, navigate to \"Topology\" -> \"Helm Chart\" and select\n\"GitLab Runner vX.X.X Provided by Slate Helm Charts\". From the new window, select \"Install Helm Chart\".\n\nOn the resulting screen, one can customize the installation of the GitLab Runner helm chart. The chart has been forked from upstream to\nallow for customized deployment to Slate. From the Form View, expand the \"GitLab Runner\" fields. Using the registration token retrieved\nin the prior section, enter token for adding the runner to the GitLab server in the empty field.\n\nIf the Registration Token is left blank, the runner installation will not succeed.\n\nIf the GitLab Runner needs to be able to use batch scheduler command, toggle the \"Enable Batch Scheduler\" option to true. Then, select the\nnecessary filesystem annotation to use for the cluster the chart is being deployed. For Marble, the \"olcf\" annotation would be used whereas\nwhereas for Onyx the \"ccsopen\" annotation would be used.\n\nThe Resources requests and limits are setup to match the namespace default limits. If more resources are anticipated for the deployment,\nmodify the requests and limits as needed.\n\nThis helm chart was forked from the the upstream GitLab Runner helm chart. As such, nearly all of the parameters documented in the\nupstream chart could be provided here as well.\n\nTo verify that the GitLab runner was properly deployed, one could explore the deployed resources from the Topology view from the\nSlate web console. Or, from the command line:\n\n$ helm ls\nNAME                 NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION\ngitlab-runner        myproject       1               2021-09-15 14:10:15.12187869 +0000 UTC  deployed        gitlab-runner-1.0.0     14.2.0\n$ oc get pods\nNAME                                          READY   STATUS    RESTARTS   AGE\ngitlab-runner-gitlab-runner-687486d94-lpmhs   1/1     Running   0          107s\n\nOnce the pod has a Status of \"Running\", navigate to the GitLab web interface and ensure that the runner has registered to either the group or repository per the token given, and that it is listed as available.\n\nFinally, if batch scheduler integration was enabled, one can verify functionality in the pod with:\n\n$ oc exec -it pods/gitlab-runner-gitlab-runner-687486d94-lpmhs -- bash\nbash-5.0$ squeue\n          JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           9951 ...\n\nRemoving a GitLab Runner\n\nTo remove a namespace installed GitLab runner, login to the web UI for the cluster and switch to the \"Developer\" perspective as before. In the\nnavigation panel on the left select \"Helm\". The deployment should be displayed as one of the \"Helm Releases\". Click on the vertical\nellipses located at the right side of the deployment, and select \"Uninstall Helm Release\". A confirmation dialog box will be displayed.\nType the information as requested and click \"Uninstall\".\n\nWhen clicking the uninstall, it may appear that the UI hangs and nothing is happening. It may take some time to remove all of the resources.\n\nOnce the installation is complete, the UI will refresh and the deployment will no longer be listed.\n\nVerify that the runner has been unregistered from the GitLab project (GitLab->Settings->CI/CD->Runners). One could also check\nto ensure that all the pods were deleted by changing over to the \"Administrator\" perspective and selecting Workloads -> Pods from the navigation."}
{"doc":"glossary","text":"Glossary\n\n\n\nClosed Source Software / Not Open Source Software\n  Software with source code which is not publicly available for general\n  distribution. For this type of software, the `Export Control\n  Classification Number<glossary-eccn-number> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Export Control\n  Classification Number<glossary-eccn-number>>` (ECCN) is requested. If the\n  code is subject to a different export control jurisdiction (e.g. Department\n  of State, ITAR) please indicate an appropriate categorization.\n\n\n\nExport Control Classification Number (ECCN)\n  US Department of Commerce Export Control Classification Number. If an\n  application software package is export controlled, list the applicable ECCN.\n  This is requested for all software that is `Not Open\n  Source<glossary-closed-source> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Not Open\n  Source<glossary-closed-source>>`. If the code is subject to a different\n  export control jurisdiction (e.g. Department of State ITAR, Nuclear\n  Regulatory Commission Controls) please indicate an appropriate\n  categorization.\n\n\n\nHealth Data\n  The term Health Data is interpreted rather broadly; it includes any part of\n  a patient's medical record or payment history. In particular, the US Health\n  Insurance Portability and Accountability Act (HIPAA) defines Protected\n  Health Information (PHI) that must be treated with great care.\n\n  For more information on HIPAA and PHI see `Health Information Privacy\n  <https://www.hhs.gov/hipaa/for-professionals/index.html>`_ from the U.S.\n  Department of Health & Human Services.\n\n\n\nNational Security Decision Directive (NSDD) 189\n  `National Security Decision Directive (NSDD) Number 189\n  <https://fas.org/irp/offdocs/nsdd/nsdd-189.htm>`_ is a federal directive\n  that establishes national policy for controlling the flow of science,\n  technology, and engineering information produced in federally-funded\n  fundamental research at colleges, universities, and laboratories. NSDD 189\n  defines \"Fundamental Research\" as:\n\n      *Basic and applied research in science and engineering, the results of which\n      ordinarily are published and shared broadly within the scientific community,\n      as distinguished from proprietary research and from industrial development,\n      design, production, and product utilization, the results of which ordinarily\n      are restricted for proprietary or national security reasons.*\n\n  \"Publicly Available Information\" is defined as information obtainable free\n  of charge (other than minor shipping or copying fees) and without\n  restriction; which is available via the internet, journal publications, text\n  books, articles, newspapers, magazines, etc.\n\n\n\nOpen Source Software\n  Open Source software is publicly available software (source code) which is\n  available for general distribution either for free or at a price that does\n  not exceed the cost of reproduction and distribution. Frequently, this Open\n  Source software is distributed under a license that grants the user the\n  rights to use, copy, modify, prepare derivative works, and distribute that\n  software without having to make royalty payments. Such distribution may\n  include original or modified source code, other formats, and any derivative\n  works thereof.\n\n\n\nProprietary Data\n  Proprietary Data are data which embody trade secrets developed at private\n  expense, where such data (a) are not generally known or available from other\n  sources without obligation concerning their confidentiality; (b) have not\n  been made available by the owner to others without obligation concerning\n  their confidentiality; and (c) are not already available to the Government\n  without obligation concerning their confidentiality.\n\n\n\nSensitive or Restricted Information\n  Principal Investigators are responsible for knowing whether their project\n  uses or generates sensitive or restricted information. Department of Energy\n  systems contain data only related to scientific research.\n\n  Sensitive Information: This includes, but is not limited to, personally-identifiable\n  information (PII). PII is information that can be used to distinguish or trace an\n  individual's identity, either alone or when combined with other information\n  that is linked or linkable to a specific individual.\n\n  Restricted Information: This includes, but is not limited to, classified information,\n  unclassified controlled nuclear information (UCNI), naval nuclear propulsion\n  information (NNPI), the design or development of nuclear, biological, or\n  chemical weapons or of any weapons of mass destruction. Use of OLCF resources to\n  store, manipulate, or remotely access classified information is prohibited.\n\n  For more information contact the DOE at:\n\n  | Office of Domestic and International Energy Policy\n  | US Department of Energy\n  | Washington DC 20585\n\n\n\nStrong Scaling\n  How time-to-solution of a computation varies with the number of processors\n  for a fixed *total* problem size. Use the examples below as a guide when\n  providing this kind of parallel performance data.\n\n  .. csv-table::\n     :header: \"nProc\", \"Time to Solution, Actual\", \"Time to Solution, Ideal\"\n     :widths: 10, 20, 20\n\n     64, 9600.00, 9600.00\n     128, 5333.33, 4800.00\n     256, 3000.00, 2400.00\n     512, 1714.29, 1200.00\n     1024, 1000.00, 600.00\n\n  .. image:: /images/accounts_glossary_strong_scaling_01.png\n     :width: 470 px\n     :alt: strong scaling example graph 1\n\n  |\n\n  .. csv-table::\n     :header: \"nProc\", \"Time to Solution, Actual\", \"Time to Solution, Ideal\"\n     :widths: 10, 20, 20\n\n     64, 64.00, 64.00\n     128, 115.20, 128.00\n     256, 204.80, 256.00\n     512, 358.40, 512.00\n     1024, 614.40, 1024.00\n\n  .. image:: /images/accounts_glossary_strong_scaling_02.png\n     :width: 470 px\n     :alt: strong scaling example graph 2\n\n\n\nWeak Scaling\n  How time-to-solution of a computation varies with the number of processors\n  for a fixed *per processor* problem size. Use the example below as a guide\n  when providing this kind of parallel performance data.\n\n  .. csv-table::\n     :header: \"nProc\", \"Time to Solution, Actual\", \"Time to Solution, Ideal\"\n     :widths: 10, 20, 20\n\n     64, 10.50, 10.50\n     128, 10.45, 10.50\n     256, 10.42, 10.50\n     512, 10.40, 10.50\n     1024, 10.43, 10.50\n\n  .. image:: /images/accounts_glossary_weak_scaling_01.png\n     :width: 470 px\n     :alt: weak scaling example graph"}
{"doc":"guided_tutorial_cli","text":"Guided Tutorial: CLI\n\n\n\nThis tutorial is a continuation of the slate_guided_tutorial <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_guided_tutorial> and you should start there.\n\nYou will be creating a single Pod in this tutorial which is not sufficient for a production service\n\nAdding a Pod to your Project\n\nBefore using the CLI it would be wise to read our Getting Started on the CLI<slate_getting_started_oc> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Getting Started on the CLI<slate_getting_started_oc>> doc.\n\nOnce the oc client has been installed and is logged into the cluster you need to switch to your Project. Switching to a Project allows the oc client to assume that the commands it is running should be executed inside of the  Project that you switch to. You could alternatively not switch to a project and append the -n flag to each command you run followed by the name of the project you wish to run your command in. That being said, switch to your project:\n\noc project <PROJECT_NAME>\n\nWhere <PROJECT_NAME> above is the name of your Project which will be the name of your RATS allocation. If you need to double check your Project name you can run:\n\noc get projects\n\nto get a list of projects that you have access to.\n\nNow, to get a list of pods that exist in the project run:\n\noc get pods\n\nThis should not return any Pods because there will not be any pods in the project yet.\n\nTo remedy this problem we will create a pod. Below is a basic definition of a pod in YAML. Copy this and save it to a file named pod.yaml.\n\napiVersion: v1\nkind: Pod\nmetadata:\n  # Pod name\n  name: test-pod\nspec:\n  containers:\n    # Container name\n    - name: test-container\n      # Using the base image\n      image: \"image-registry.openshift-image-registry.svc:5000/openshift/ccs-rhel7-base-amd64\"\n      # Starting a shell\n      command: [\"/bin/sh\",\"-c\"]\n      # Echoing a Hello World followed by an infinitely waiting cat\n      args: [\"echo 'Hello World!'; cat\"]\n      # Need a tty if we are to SSH. Need stdin for tty\n      tty: true\n      stdin: true\n\nWith the above YAML saved in a pod.yaml file we can now create the pod with the following command:\n\noc create -f pod.yaml\n\nNow if we run the command:\n\noc get pods\n\nWe should see our pod along with some status information about the pod.\n\nTo get useful metrics about the pod we can run:\n\noc describe pod <POD_NAME>\n\nwhere <POD_NAME> will be the name of the pod. In our case test-pod.\n\nTo get logs from the pod we can run the command:\n\noc logs -f <POD_NAME>\n\nNOTE the -f flag will follow the logs. You can run the logs command without the -f flag to get a snapshot of the logs.\n\noc get pod <POD_NAME> -o yaml\n\nWill allow you to view the YAML representation that exists in Openshift that defines your pod. You may notice that the YAML contains many more key/value pairs than the YAML that we have in our pod.yaml file. This is correct and is because extra YAML is added during the pod creation process.\n\nFinally, to get a remote shell in the pod we run the oc rsh <POD_NAME> command. This will default to using /bin/sh in the pod. If a different shell is required, we can provide the optional --shell=/path/to/shell flag. For example, if we wanted to open a bash shell in the pod we would run the following command:\n\noc rsh --shell='/bin/bash' <POD_NAME>\n\nIf you have multiple containers in your pod, the oc rsh <POD_NAME> command will default to the first container. If you would like to start a remote shell in one of the other containers, you can use the optional -c <CONTAINER_NAME> flag."}
{"doc":"guided_tutorial","text":"Guided Tutorial\n\n\n\nThis tutorial is meant to be followed step by step so you can get a basic understanding of Openshift and Openshift objects. This will only cover a very surface level knowledge of all the things you can accomplish with Openshift but will hopefully get you familiar with the foundational concepts.\n\nYou will be creating a single Pod in this tutorial which is not sufficient for a production service\n\nIf you have not already done so, you will need to get an allocation on either our Marble or Onyx  Cluster. To do this contact help@olcf.ornl.gov.\n\nBefore we dive in there are some terms that need to be understood. This will be a basic set of terms and a copy and paste from our slate_glossary <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_glossary>, so we recommend reading that document and even keeping it handy until you are familiar with all of the definitions there. On that note, another good piece of reference documentation the slate_examples <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_examples> document. There you can find basic YAML definitions for the most common objects in Kubernetes.\n\nCreating your project\n\nBefore you can do anything you need a Project to do your things in. Fortunately, when you get an allocation on one of our clusters a Project is automatically created for you with the same name as the allocation. By using our own distinct Project we are ensuring that we will not interfere with anyone else's work.\n\nNOTE: Everywhere that you see <cluster> replace that with the cluster that you will be running on (marble or onyx).\n\nNow that you have a project you can create objects inside that project. We will be doing this with the Openshift Web GUI and the oc CLI client so you can use whichever interface you are more comfortable with in this tutorial. If you are more comfortable using the command line than you are using a GUI you can now jump to the oc portion of this document<slate_guided_tutorial_cli> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#jump to the oc portion of this document<slate_guided_tutorial_cli>>. Otherwise, continue with the GUI based tutorial below.\n\nGuided Web GUI Tutorial\n\nGo to https://console-openshift-console.apps.<cluster>.ccs.ornl.gov/k8s/cluster/projects\n\nAdding a Pod to your Project\n\nOnce you have navigated to https://console-openshift-console.apps.<cluster>.ccs.ornl.gov/k8s/cluster/projects you should see a list of Projects or Namespaces, that you have access to. Scroll down or use the filter box in the upper right to select your project; your project will have the same name as your allocation in RATS. Once there your screen should look similar to the picture below:\n\n\n\nThe name of the pictured project is of the form RATS_ALLOCATION-CUSTOM_NAME. Your project will be only RATS_ALLOCATION.\n\nFrom here, in the left hand hamburger menu click on the 'Workloads' tab and then the 'pods' tab:\n\n\n\nHere you will be able to view all of the Pods in your Project. Since this is a new Project there will be no Pods in it. To create a  Pod click the 'Create Pod' button.\n\nThis will bring you to a screen of pre populated YAML that you can edit in the browser. This YAML is the basis of a podspec that will be sent to the API server once you click the 'Create' button in the lower left to create a  Pod in your Project. Here we will make a few slight modifications to the podspec.\n\nFirst, we will replace the openshift/hello-openshift value after the image tag with value image-registry.openshift-image-registry.svc:5000/openshift/ccs-rhel7-base-amd64. This is the  Image that the  Pod will be using. We will be using the ccs-base image; a bare-bones image provided by the platforms team that is usually used as the foundation to build more complex custom images on top of.\n\nSecondly, the  Pod needs something to do when it starts. For an nginx server this would be running nginx, for a flask app this would be running the app.py file etc. For illustrative purposes this  Pod is going to be starting a shell with the /bin/sh command, echoing a \"Hello World!\" prompt then running a cat command as a means to keep the pod running. Without the addition of the cat at the end the echo command would end causing the /bin/sh to end causing the  Pod to go from a status of Running to Completed.  To make these changes add the following lines below the image line:\n\ncommand: [\"/bin/sh\",\"-c\"]\n\nargs: [\"echo 'Hello World!'; cat\"]\n\nFinally, we need a tty. This will give us the ability to open a shell in our  Pod and get a better understanding of what is happing. To do this, add the following two lines under the command line that you just added:\n\ntty: true\n\nstdin: true\n\nYour page should now look as follows:\n\n\n\nYou can now click the 'Create' button in the lower left which will take you to the screen where the   Pod is created.\n\nYou should now be on the 'Pod' screen with the 'Overview' tab selected From here you can get a quick idea of the amount of resources (memory, CPU etc) that your  Pod is using.\n\nClick on the 'Logs' tab to get the logs from your pod. This will display \"Hello World!\" in our example because of our echo command. There will be a dropdown here that for our example will contain only one item named 'hello-openshift'. This is the name of the container that you are viewing the logs for inside your pod.\n\nThe 'Events' tab is for the events that took place to create your pod. This is for things that happen outside of the code that is running inside your pod such as pulling the pod image, scheduling the pod onto a node etc.\n\nThe 'Terminal' tab will give you a tty inside your pod. Here you can run most commands as you normally would on a RedHat machine."}
{"doc":"helm_example","text":"Deploy Packages with Helm\n\n\n\nHelm is the OLCF preferred package manager for Kubernetes. There are a variety of upstream\napplications that can be installed with helm, such as databases, web frontends, and monitoring tools. Helm has \"packages\" called \"charts\", which can essentially be thought of as kubernetes object templates. You can pass in values which helm uses to fill in these templates, and create the objects (pods, deployments, services, etc)\n\nFollow helm_prerequisite <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#helm_prerequisite> for installing Helm.\n\nOne nice feature of helm is that it uses the underlying authentication credentials to kubernetes, so once you login with\noc login, the helm client will authenticate automatically.\n\nBy default, helm doesn't have any chart repositories, so let's add the upstream stable repository.\n\nhelm repo add stable https://charts.helm.sh/stable\n\nNow you can install helm charts with helm install stable/<package_name>. You can think of this command as a parallel to running yum install on a RHEL/CentOS-based system, or apt install on a debian-based system.\n\nInstall MySQL with Helm\n\nFor an example, let's install a basic mysql database, with a release named mysql.\n\nSetting Values\n\nYou could simply run helm install mysql stable/mysql, and an basic mysql deployment would be created with default values. However, we probably want to customize this deployment a bit. Let's take a look at the documentation for the mysql helm chart.\n\nUnder the \"Configuration\" section, there is a large list of all the variables you can provide the chart, as well as their default values. Let's say we want to customize our mysql resource utilization as well as using a block volume instead of the default NFS volume for persistent storage. We'll also add a NodePort service in order to access our new database from outside the cluster.\n\nThere are two ways to set values for helm install:\n\nPass in individual values one-by-one with --set.\n\nPass in a group of values within a file with --values.\n\nIn this example, we'll be using the latter method. Let's create a file called values.yaml with the following values:\n\nYou may need to adjust the persistence > size value down depending on the storage quotas set for your project. Your project quotas can be found at https://quotas.CLUSTER.ccs.ornl.gov, where CLUSTER is replaced with the cluster you are running on (Marble/Onyx).\n\npersistence:\n  size: 20Gi\n  storageClass: \"netapp-block\"\ninitContainer:\n  resources:\n    requests:\n      cpu: 100m\n      memory: 10Mi\n    limits:\n      cpu: 300m\n      memory: 100Mi\nresources:\n  requests:\n    cpu: 2\n    memory: 2Gi\n  limits:\n    cpu: 4\n    memory:4Gi\nservice:\n  type: NodePort\n\nNow mysql can be installed with these values by running helm install mysql stable/mysql --values values.yaml --namespace <project_name>\n\nTo install quickly with --set instead of writing a values file, for example, if the default values are preferred except the persistent storage size and storageClass, this could be done by running helm install mysql stable/mysql --set persistence.size=\"20Gi\" --set persistence.storageClass=\"netapp-block\". Note that you can pass multiple values by passing --set multiple times.\n\nAfter Installation\n\nTo see all deployed helm charts in a namespace, along with their status, run helm list --namespace <project_name>\nIf following the above example, this command will show the new mysql deployment, with mysql under NAME.\n\nYou can run helm status <release_name> --namespace <project_name> in order to get information about the deployment. In this example, our release name is mysql. Running this command for our mysql installation will give us information on how to connect and authenticate to our newly created database.\n\nThe output of helm status will differ from chart to chart, as this output is customizable by the chart itself. If the output has kubectl commands to run, kubectl can be replaced with oc.\n\nFinding New Charts\n\nRunning helm search hub will search the Helm Hub, which has a wide variety of publicly available charts. For example, if a wordpress installation is desired, you could run:\n\n$ helm search hub wordpress\nURL                                                 CHART VERSION   APP VERSION DESCRIPTION\nhttps://hub.helm.sh/charts/bitnami/wordpress        7.6.7           5.2.4       Web publishing platform for building blogs and ...\nhttps://hub.helm.sh/charts/presslabs/wordpress-...  v0.6.3          v0.6.3      Presslabs WordPress Operator Helm Chart\nhttps://hub.helm.sh/charts/presslabs/wordpress-...  v0.7.1          v0.7.1\n\nNote that this searches much more than the stable repo we added above, so you may need to add another repo with helm repo add. Be sure to run helm repo update before installing new charts, to make sure the charts are up to date.\n\nYou can also search only the repos that you have added to your local client with helm search repo\n\nWriting Charts\n\nIt is also possible to write your own charts for helm, if you have an application that can be deployed to many namespaces or that could benefit from templating objects. How to write charts is outside the scope of this documentation, but the upstream docs are a great place to start."}
{"doc":"helm_prerequisite","text":"Helm Prerequisites\n\n\n\nThe following items need to be established before deploying applications on\nSlate systems (Marble or Onyx OpenShift clusters).\n\nEstablish a project/namespace on Onyx or Marble\n\nFollow slate_getting_started <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_getting_started> if you do not already have a\nproject/namespace established on Onyx or Marble.\n\nInstall Helm\n\nIt is recommended to use Helm version 3.\n\nHelm enables application deployment via Helm Charts. The high level Helm Architecture docs are also a great reference\nfor understanding Helm.\n\nLike oc, Helm is a single binary executable.\n\nThis can be installed on macOS with Homebrew :\n\n$ brew install helm\n\nOr can be pulled from the Helm Release Page. If downloading from the GitHub\nrelease page, you can copy this executable into /usr/local/bin to add it to\n$PATH.\n\nNOTE: One nice feature of Helm is that it uses the underlying\nauthentication credentials used with oc, so once you login with oc login,\nthe helm client will authenticate automatically.\n\nOnce oc and helm are setup and you are logged in with oc login, test Helm:\n\n$ helm ls"}
{"doc":"home_user_guide","text":"Home\n\n\n\n\n\nSystem Overview\n\nhome.ccs.ornl.gov (Home) is a general purpose system that can be used to\nlog into other OLCF systems that are not directly accessible from outside the\nOLCF network. For example, running the screen or tmux utility is one\ncommon use of Home. Compiling, data transfer, or executing long-running or\nmemory-intensive tasks should never be performed on Home.\n\n\n\nAccess & Connecting\n\nHome access is automatically granted to all enabled OLCF users.\n\nTo connect to Home, SSH to home.ccs.ornl.gov. For example:\n\nssh username@home.ccs.ornl.gov\n\nFor more information on connecting to OLCF resources, see\nconnecting-to-olcf <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#connecting-to-olcf>.\n\nUsage\n\nAcceptable Tasks\n\nThe Home system should only be used to access systems within the OLCF network.\nThe following are examples of appropriate uses of Home:\n\nRSA SecurID Token setup\n\nSSH\n\nVi and other non-GUI editors\n\nScreen or other terminal multiplexers\n\nUnacceptable Tasks\n\nThe following are examples of inappropriate uses of Home:\n\nCompiling\n\nData Transfers\n\nLong-running or memory-intensive processes"}
{"doc":"hpss_user_guide","text":"High Performance Storage System\n\nSystem Overview\n\n\n\n\n\nThe High Performance Storage System (HPSS) provides tape storage for large\namounts of data created on OLCF systems. The HPSS can be accessed from any OLCF\nsystem through the hsi utility. More information about using HPSS can be found\nin the data-hpss <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-hpss> section of the data-storage-and-transfers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers> page."}
{"doc":"hybrid_hpc","text":"Quantum Software on HPC Systems\n\n\n\nOverview\n\nThis page describes how a user can successfully install specific quantum\nsoftware tools to run on select OLCF HPC systems. This allows a user to utilize\nour systems to run a simulator, or even as a \"frontend\" to a vendor's quantum\nmachine (\"backend\").\n\nFor most of these instructions, we use conda environments. For more detailed\ninformation on how to use Python modules and conda environments on OLCF\nsystems, see our :doc:`Python on OLCF Systems page</software/python/index>`,\nand our Conda Basics guide <https://docs.olcf.ornl.gov/software/python/conda_basics.html>.\n\n<string>:15: (INFO/1) No role entry for \"doc\" in module \"docutils.parsers.rst.languages.en\".\nTrying \"doc\" as canonical role name.\n\n<string>:15: (ERROR/3) Unknown interpreted text role \"doc\".\n\nCurrently, Frontier does NOT have an Anaconda/Conda module.  To use conda,\nyou will have to download and install Miniconda on your own (see our\nInstalling Miniconda Guide <https://docs.olcf.ornl.gov/software/python/miniconda.html>).\nAlternatively, you can use Python's native virtual environments venv\nfeature with the cray-python module (as we will explore in the guides\nbelow).  For more details on venv, see Python's Official Documentation.  Contact help@olcf.ornl.gov\nif conda is required for your workflow, or if you have any issues.\n\nCurrently, the install steps listed below only work for our x86_64 based\nsystems (Andes, Frontier, etc.). The steps can be explored on Summit,\nbut -- due to Summit's Power architecture -- is not recommended or guaranteed\nto work.\n\nBoth Qiskit and pyQuil can live in the same Python environment if desired.\nHowever, as this may not always be the case, it is highly recommened to use\nseparate environments if possible or test if packages still function after\nmodifying your environment.\n\nQiskit\n\nQiskit is open-source software for\nworking with quantum computers at the level of circuits, pulses, and\nalgorithms.\n\nInstalling Qiskit provides access to the IBM backends. Due to the simple nature\nof installing Qiskit, most of this info was taken directly from their documentation.\nSee these links for more details:\n\nhttps://qiskit.org/documentation/getting_started.html\n\nhttps://qiskit.org/documentation/intro_tutorial1.html\n\nhttps://qiskit.org/ecosystem/ibm-provider/tutorials/Migration_Guide_from_qiskit-ibmq-provider.html\n\nhttps://quantum-computing.ibm.com/lab/docs/iql/manage/account/ibmq\n\nAndes\n\n.. code-block:: bash\n\n    $ module load python\n    $ source activate base\n    $ conda create -n ENV_NAME python=3.9 # works for python 3.7, 3.8, 3.9 (minimal support for 3.10)\n    $ conda activate ENV_NAME\n    $ pip install qiskit qiskit-ibm-runtime qiskit-aer --no-cache-dir\n\nFrontier\n\n.. code-block:: bash\n\n    $ module load cray-python\n    $ python3 -m venv ENV_NAME\n    $ source ENV_NAME/bin/activate\n    $ python3 -m pip install qiskit qiskit-ibm-runtime qiskit-aer --no-cache-dir\n\nQiskit - Code Example\n\nBelow is a simple code example to test if things installed properly.  Note that\nmethods for either using Qiskit Runtime or IBMProvider are provided.\n\nYour IBMQ API Token is listed on your IBM dashboard at https://quantum-computing.ibm.com/ .\n\nIBMProvider\n\n.. code-block:: python\n\n    import numpy as np\n    from qiskit import QuantumCircuit, transpile\n    from qiskit.providers.aer import QasmSimulator\n    from qiskit_ibm_provider import IBMProvider\n\n    #### IF YOU HAVE AN IBMQ ACCOUNT (using an actual backend) #####\n\n    # Save account credentials\n    #IBMProvider.save_account(TOKEN)\n\n    # Load default account credentials\n    provider = IBMProvider()\n\n    # Print instances (different hub/group/project options)\n    print( provider.instances() )\n\n    # Load a specific hub/group/project.\n    #provider = IBMProvider(instance=\"ibm-q-ornl/ornl/csc431\")\n\n    # Print available backends\n    print( provider.backends() )\n\n    ######################################\n\n    backend = QasmSimulator() #works with backend.run()\n\n    circuit = QuantumCircuit(2, 2)\n    circuit.h(0)\n    circuit.cx(0, 1)\n    circuit.measure([0,1], [0,1])\n    compiled_circuit = transpile(circuit, backend)\n\n    job = backend.run(compiled_circuit, shots=1000)\n\n    print(\"Job status is\", job.status() )\n    result = job.result()\n\n    counts = result.get_counts(compiled_circuit)\n    print(\"\\nTotal count for 00 and 11 are:\",counts)\n\n    # Draw the circuit\n    print(circuit.draw())\n\nRuntime\n\n.. code-block:: python\n\n    import numpy as np\n    from qiskit import QuantumCircuit, transpile\n    from qiskit_ibm_runtime import QiskitRuntimeService, Session, Sampler\n\n    #QiskitRuntimeService.save_account(channel=\"ibm_quantum\", token=\"API TOKEN GOES HERE\", overwrite=True)\n    service = QiskitRuntimeService(channel=\"ibm_quantum\", instance=\"ibm-q-ornl/ornl/csc431\")\n\n    backend = service.backend(\"ibmq_qasm_simulator\", instance=\"ibm-q-ornl/ornl/csc431\") #does not work with backend.run()\n\n    circuit = QuantumCircuit(2, 2)\n    circuit.h(0)\n    circuit.cx(0, 1)\n    circuit.measure([0,1], [0,1])\n    compiled_circuit = transpile(circuit, backend)\n\n    sampl = Sampler(backend)\n    job = sampl.run(compiled_circuit,shots=1000)\n\n    print(\"Job status is\", job.status() )\n    result = job.result()\n\n    probs = result.quasi_dists\n    print(\"\\nProbabilities for 00 and 11 are:\",probs)\n\n    # Draw the circuit\n    print(circuit.draw())\n\nAfter running the above script using your Qiskit environment, you should\nsee something like this:\n\nJob status is JobStatus.DONE\n\nTotal count for 00 and 11 are: {'11': 491, '00': 509}\n     ┌───┐     ┌─┐\nq_0: ┤ H ├──■──┤M├───\n     └───┘┌─┴─┐└╥┘┌─┐\nq_1: ─────┤ X ├─╫─┤M├\n          └───┘ ║ └╥┘\nc: 2/═══════════╩══╩═\n                0  1\n\nPyQuil/Forest SDK (Rigetti)\n\nQuil is the\nRigetti-developed quantum instruction/assembly language.\nPyQuil is a Python library for\nwriting and running quantum programs using Quil.\n\nInstalling pyQuil requires installing the Forest SDK. To quote Rigetti:\n\"pyQuil, along with quilc, the QVM, and other libraries, make up what is called\nthe Forest SDK\". Because we don't have Docker functionality and due to normal\nusers not having sudo privileges, this means that you will have to install the\nSDK via the \"bare-bones\" method. The general info below came from:\n\nhttps://pyquil-docs.rigetti.com/en/stable/start.html\n\nhttps://docs.rigetti.com/qcs/getting-started/installing-locally\n\nThe bare-bones installation only contains the executable binaries and manual\npages, and doesn’t contain any of the requisite dynamic libraries. As such,\ninstallation doesn’t require administrative or sudo privileges. This method of\ninstallation requires one, through whatever means, to install shared libraries\nfor BLAS, LAPACK, libffi, and libzmq3. Some download methods are listed here:\n\nLapack (with BLAS) download: http://www.netlib.org/lapack/\n\nlibffi download:\n\nOlder versions: https://sourceware.org/ftp/libffi/\n\nNewer: https://github.com/libffi/libffi/releases/\n\nZMQ download: https://github.com/zeromq/libzmq/releases\n\nForest SDK download: https://qcs.rigetti.com/sdk-downloads\n\nBelow are example instructions for installing the above packages into your $HOME directory.\nVersions may vary.\n\nNewer versions than those used in the install instructions below are\nknown to work on Andes; however, on Frontier, newer versions of libffi than\n3.2.1 are known to cause problems.\n\nAndes\n\n.. code-block:: bash\n\n    $ module load gcc cmake\n\nFrontier\n\n.. code-block:: bash\n\n    $ module swap PrgEnv-cray PrgEnv-gnu\n    $ module load cmake\n\n# INSTALLING LAPACK (also installs BLAS)\n$ cd\n$ mkdir pack_temp/\n$ cd pack_temp/\n$ cp ../lapack-3.10.0.tar.gz .\n$ tar -xvf lapack-3.10.0.tar.gz\n$ cd lapack-3.10.0/\n$ mkdir build\n$ cd build/\n$ cmake -DBUILD_SHARED_LIBS=ON -DCMAKE_INSTALL_LIBDIR=$HOME/lapackblas ..\n$ cmake --build . -j --target install\n\n# INSTALLING LIBFFI\n$ cd\n$ mkdir ffi_temp/\n$ cd ffi_temp/\n$ cp ../libffi-3.2.1.tar.gz .\n$ tar -xvf libffi-3.2.1.tar.gz\n$ cd libffi-3.2.1\n$ ./configure --prefix=$HOME/ffi/\n$ make\n$ make install\n# The lines below may not be necessary if the \"include\" directory already exists (required for libffi3.2.1)\n$ cd $HOME/ffi/\n$ mkdir include\n$ cd include\n$ cp $HOME/ffi_temp/libffi-3.2.1/include/ffi*.h .\n\n# INSTALLING ZMQ\n$ cd\n$ mkdir zmq_temp/\n$ cd zmq_temp/\n$ cp ../zeromq-4.1.4.tar.gz .\n$ tar -xvf zeromq-4.1.4.tar.gz\n$ cd zeromq-4.1.4/\n$ ./configure --prefix=$HOME/zmq/ --with-libsodium=no\n$ make\n$ make install\n\n# INSTALLING FOREST SDK (installs quilc and qvm)\n$ cd\n$ mkdir forest_temp/\n$ cd forest_temp/\n$ cp ../forest-sdk-2.23.0-linux-barebones.tar.bz2 .\n$ tar -xvf forest-sdk-2.23.0-linux-barebones.tar.bz2\n$ cd forest-sdk-2.23.0-linux-barebones/\n$ ./forest-sdk-2.23.0-linux-barebones.run # /ccs/home/YOUR_USERNAME/rigetti/ when prompted\n\n# EXPORT PATHS (can add to .bashrc / .bash_profile if desired)\n$ export LD_LIBRARY_PATH=\"/ccs/home/YOUR_USERNAME/lapackblas:$LD_LIBRARY_PATH\"\n$ export LD_LIBRARY_PATH=\"/ccs/home/YOUR_USERNAME/ffi/lib64:$LD_LIBRARY_PATH\"\n$ export LD_LIBRARY_PATH=\"/ccs/home/YOUR_USERNAME/zmq/lib:$LD_LIBRARY_PATH\"\n$ export PATH=\"/ccs/home/YOUR_USERNAME/rigetti/forest-sdk_2.23.0-linux-barebones:$PATH\"\n\n# VERIFY QUILC / QVM INSTALL\n\n$ quilc —-version\n1.23.0 [e6c0939]\n$ qvm —-version\n1.17.1 [cf3f91f]\n\n# If QUILC / QVM errors and is unable to find libffi.so.6 (e.g., you have libffi.so.8)\n# This workaround is NOT recommended, and should only be used as a LAST RESORT:\n#$ ln -s $HOME/ffi/lib64/libffi.so.8 $HOME/ffi/lib64/libffi.so.6\n\nFeel free to remove the [package name]_temp build directories once you\nverify that the libraries were installed correctly.\n\nFinally, you are ready to install pyQuil:\n\nAndes\n\n.. code-block:: bash\n\n    $ module load python\n    $ source activate base\n    $ conda create -n ENV_NAME python=3.9 # pyQuil requires Python version 3.7, 3.8, or 3.9\n    $ conda activate ENV_NAME\n    $ pip install pyquil --no-cache-dir\n\nFrontier\n\n.. code-block:: bash\n\n    $ module load cray-python\n    $ python3 -m venv ENV_NAME\n    $ source ENV_NAME/bin/activate\n    $ python3 -m pip install pyquil --no-cache-dir\n\nPyQuil - Setting up Servers\n\nNow that everything is installed properly, the rest of the instructions follow\nRigetti's Documentation .\n\nWith the way pyQuil works, you need to launch its compiler server, launch the\nvirtual machine / simulator QVM server, and then launch your pyQuil Python\nprogram on the same host. Running a Python script will ping and utilize both\nthe compiler and QVM servers. As a proof of concept, this has been done on a\nsingle login node and the steps are outlined below.\n\nUsing your already created ENV_NAME virtual environment (outlined above):\n\n(ENV_NAME)$ quilc -P -S > quilc.log 2>&1 & qvm -S > qvm.log 2>&1 & python3 script.py ; kill $(jobs -p)\n\nBefore trying to run the code example below, remember to set the relevant\nPATHs to your ffi, zmq, lapack, and forest-sdk installations if\nyou have not already exported them (outlined above).\n\nPyQuil - Code Example\n\nBelow is a simple code to test if packages installed properly.\nContext for this example: https://pyquil-docs.rigetti.com/en/latest/start.html#run-your-first-program\n\nfrom pyquil import get_qc, Program\nfrom pyquil.gates import H, CNOT, MEASURE\nfrom pyquil.quilbase import Declare\n\n# Set up your Quantum Quil Program (in this case, a \"Bell State\")\nprogram = Program(\n    Declare(\"ro\", \"BIT\", 2),\n    H(0),\n    CNOT(0, 1),\n    MEASURE(0, (\"ro\", 0)),\n    MEASURE(1, (\"ro\", 1)),\n).wrap_in_numshots_loop(10)\n\n# Set up your QVM\nqc = get_qc(\"2q-qvm\") # Ask for a QVM with two qubits and generic topology\n\n# Compile and Run (pings your Quilc and QVM servers)\nprint(qc.run(qc.compile(program)).readout_data.get(\"ro\"))\n\nAfter running the above script, you should see something similar to this:\n\n[[1 1]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [1 1]\n [0 0]]\n\nPennyLane\n\nPennyLane is a cross-platform Python\nlibrary for programming quantum computers.  Its differentiable programming\nparadigm enables the execution and training of quantum programs on various\nbackends.\n\nGeneral information of how to install and use PennyLane can be found here:\n\nhttps://docs.pennylane.ai/en/stable/introduction/pennylane.html\n\nhttps://pennylane.ai/qml/demos_getting-started.html\n\nhttps://pennylane.ai/install.html\n\nOn our systems, the install method is relatively simple:\n\nAndes\n\n.. code-block:: bash\n\n    $ module load python\n    $ source activate base\n    $ conda create -n ENV_NAME python=3.9 pennylane -c conda-forge\n    $ conda activate ENV_NAME\n\nFrontier\n\n.. code-block:: bash\n\n    $ module load cray-python\n    $ python3 -m venv ENV_NAME\n    $ source ENV_NAME/bin/activate\n    $ python3 -m pip install pennylane --upgrade --no-cache-dir\n\nPennyLane - Code Example\n\nimport pennylane as qml\nfrom pennylane import numpy as np\n\ndev1 = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev1)\ndef circuit(phi1, phi2):\n    qml.RX(phi1, wires=0)\n    qml.RY(phi2, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\ndef cost(x, y):\n    return np.sin(np.abs(circuit(x,y))) - 1\n\nprint(circuit(0.54, 0.12))\n\nAfter running the python script, if everything installed properly, you should get something like:\n\n0.8515405859048367\n\nBatch Jobs\n\nAlthough lightweight code can be run on the login nodes, more computationally\nintensive code should be run on the compute nodes through the use of a batch\njob.  See the relevant System Guide <https://docs.olcf.ornl.gov/systems/index.html> for more examples\nand best practices when running on the compute nodes for a given system.\n\nFor the compute nodes to be able to access external URLs (e.g., trying to\nconnect to IBM backends), you'll have to use proxy settings in your batch\nscript:\n\nexport all_proxy=socks://proxy.ccs.ornl.gov:3128/\nexport ftp_proxy=ftp://proxy.ccs.ornl.gov:3128/\nexport http_proxy=http://proxy.ccs.ornl.gov:3128/\nexport https_proxy=http://proxy.ccs.ornl.gov:3128/\nexport no_proxy='localhost,127.0.0.0/8,*.ccs.ornl.gov'\n\nThese settings currently do not work for pyQuil; thus, when running pyQuil\non the compute nodes, you are unable to connect to Rigetti's machines and can\nonly run local simulators. To be able to connect to Rigetti's machines, you'll\nhave to run on the login nodes instead.\n\nWhen using Python environments with SLURM, it is always recommended to submit a\nbatch script using the export=NONE flag to avoid $PATH issues and use\nunset SLURM_EXPORT_ENV in your job script (before calling srun);\nhowever, this means that previously set environment variables are NOT\npassed into the batch job, so you will have to set them again (and load modules\nagain) if they are required by your workflow. Alternatively, you can try\nsubmitting your batch script from a fresh login shell.\n\n$ sbatch --export=NONE submit.sl\n\nBelow are example batch scripts for running on Andes and Frontier:\n\nAndes\n\n.. code-block:: bash\n\n    #!/bin/bash\n    #SBATCH -A ABC123\n    #SBATCH -J job_name\n    #SBATCH -N 1\n    #SBATCH -t 0:05:00\n    #SBATCH -p batch\n\n    unset SLURM_EXPORT_ENV\n\n    cd $SLURM_SUBMIT_DIR\n    date\n\n    # Set proxy settings so compute nodes can reach internet (required when not using a simulator)\n    # Currently, does not work properly with pyQuil\n    export all_proxy=socks://proxy.ccs.ornl.gov:3128/\n    export ftp_proxy=ftp://proxy.ccs.ornl.gov:3128/\n    export http_proxy=http://proxy.ccs.ornl.gov:3128/\n    export https_proxy=http://proxy.ccs.ornl.gov:3128/\n    export no_proxy='localhost,127.0.0.0/8,*.ccs.ornl.gov'\n\n    # Load python module and virtual environment\n    module load python\n    source activate base\n    conda activate ENV_NAME\n\n    # For software like Qiskit and PennyLane\n    #python3 script.py\n\n    # For pyQuil\n    #export LD_LIBRARY_PATH=\"/ccs/home/YOUR_USERNAME/lapackblas:$LD_LIBRARY_PATH\"\n    #export LD_LIBRARY_PATH=\"/ccs/home/YOUR_USERNAME/ffi/lib64:$LD_LIBRARY_PATH\"\n    #export LD_LIBRARY_PATH=\"/ccs/home/YOUR_USERNAME/zmq/lib:$LD_LIBRARY_PATH\"\n    #export PATH=\"/ccs/home/YOUR_USERNAME/rigetti/forest-sdk_2.23.0-linux-barebones:$PATH\"\n    #quilc -P -S > quilc.log 2>&1 & qvm -S > qvm.log 2>&1 & python3 script.py ; kill $(jobs -p)\n\nFrontier\n\n.. code-block:: bash\n\n    #!/bin/bash\n    #SBATCH -A ABC123\n    #SBATCH -J job_name\n    #SBATCH -N 1\n    #SBATCH -t 0:05:00\n    #SBATCH -p batch\n\n    unset SLURM_EXPORT_ENV\n\n    cd $SLURM_SUBMIT_DIR\n    date\n\n    # Set proxy settings so compute nodes can reach internet (required when not using a simulator)\n    # Currently, does not work properly with pyQuil\n    export all_proxy=socks://proxy.ccs.ornl.gov:3128/\n    export ftp_proxy=ftp://proxy.ccs.ornl.gov:3128/\n    export http_proxy=http://proxy.ccs.ornl.gov:3128/\n    export https_proxy=http://proxy.ccs.ornl.gov:3128/\n    export no_proxy='localhost,127.0.0.0/8,*.ccs.ornl.gov'\n\n    # Load python module and virtual environment\n    module load cray-python\n    source $HOME/ENV_NAME/bin/activate\n\n    # For software like Qiskit and PennyLane\n    #python3 script.py\n\n    # For pyQuil\n    #export LD_LIBRARY_PATH=\"/ccs/home/YOUR_USERNAME/lapackblas:$LD_LIBRARY_PATH\"\n    #export LD_LIBRARY_PATH=\"/ccs/home/YOUR_USERNAME/ffi/lib64:$LD_LIBRARY_PATH\"\n    #export LD_LIBRARY_PATH=\"/ccs/home/YOUR_USERNAME/zmq/lib:$LD_LIBRARY_PATH\"\n    #export PATH=\"/ccs/home/YOUR_USERNAME/rigetti/forest-sdk_2.23.0-linux-barebones:$PATH\"\n    #quilc -P -S > quilc.log 2>&1 & qvm -S > qvm.log 2>&1 & python3 script.py ; kill $(jobs -p)\n\n\n.. note::\n\n    The above assumes you created your Python ``venv`` ``ENV_NAME`` in your ``$HOME`` directory."}
{"doc":"ibm_quantum","text":"IBM Quantum\n\n\n\nOverview\n\nIBM Quantum Services provides access to more than 20 currently available\nquantum systems (known as backends).  IBM's quantum processors are made up of\nsuperconducting transmon qubits, and users can utilize these systems via the\nuniversal, gate-based, circuit model of quantum computation.  Additionally,\nusers have access to 5 different types of simulators, simulating from 32 up to\n5000 qubits to represent different aspects of the quantum backends.\n\nThis guide describes how to use the system once you have access. For\ninstructions on how to gain access, see our Quantum Access <https://docs.olcf.ornl.gov/quantum/quantum_access.html> page instead.\n\n\n\nConnecting\n\nAccess to the IBM Quantum Computing queues, reservations, and simulators can be\nobtained via multiple methods -- either through the cloud <ibm-cloud> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#cloud <ibm-cloud>> or\nlocally <ibm-local> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#locally <ibm-local>>.\n\n\n\nCloud Access\n\nUsers can access information about IBM Quantum's systems, view queue\ninformation, and submit jobs on their cloud dashboard at\nhttps://quantum-computing.ibm.com. The cloud dashboard allows access to\nIBM Quantum's graphical circuit builder, Quantum Composer, IBM's Quantum Lab,\nand associated program examples.  A Jupyterlab server is provisioned with IBM\nQuantum's Qiskit programming framework for job submission.\n\n\n\nLocally via Qiskit\n\nIBM Quantum provides Qiskit (Quantum Information Software Kit for Quantum\nComputation) for working with OpenQASM and the IBM Q quantum processors.\nQiskit allows users to build quantum circuits, compile them for a particular\nbackend, and run the compiled circuits as jobs. Additional information on using\nQiskit is available at https://qiskit.org/learn/ and in our\nSoftware Section <ibm-soft> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Software Section <ibm-soft>> below.\n\nAs opposed to using IBM's JupyterLab server (described in Cloud Access <ibm-cloud> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Cloud Access <ibm-cloud>> above),\nusers are able to install IBM Quantum Qiskit locally via two methods:\n\nInstalling manually: https://qiskit.org/documentation/stable/0.24/install.html.\nThis option allows for building locally and executing jobs via a python virtual\nenvironment.\n\nDocker: https://www.ibm.com/cloud/learn/docker or https://hub.docker.com/u/ibmq\n\n\n\nRunning Jobs & Queue Policies\n\nUser can submit jobs to IBM Quantum backends both via a fair-sharing queue\nsystem as well as via priority reservation system.  As discussed below, the\ndynamic fair-sharing queue system determines the queuing order of jobs so as to\nfairly balance system time between access providers, of which the OLCF QCUP is\nonly one.  Because of this, the order of when a user's job in the fair-share\nqueue will run varies dynamically, and can't be predicted. In light of this,\nfor time-critical applications or iterative algorithms, IBM Quantum recommends\nusers consider making a priority reservation.\n\nFair-Share Queue Policy\n\nWhen jobs are submitted on IBM Quantum backends, the jobs enter into the\n\"fair-share\" queuing system, in which jobs run in a dynamically calculated\norder so as to provide fair sharing among all users of the device, to prevent\nindividual projects or users from monopolizing a given backend.\n\nAll OLCF users have access to the \"premium\" (>=20 qubits) and \"open\" (<20\nqubit) devices.  Since most of the open devices are shared with the public,\nqueue times will often be longer than the queues for the larger devices.\n\nSubmitting Jobs\n\nJobs are compiled and submitted via Qiskit in a Python virtual environment or\nJupyter notebook (see Cloud Access <ibm-cloud> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Cloud Access <ibm-cloud>> and\nLocal Access <ibm-local> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Local Access <ibm-local>> sections above).\n\nCircuit jobs comprise jobs of constructed quantum circuits and algorithms\nsubmitted to backends in IBM Quantum fair-share queue.\n\nProgram jobs utilize a pre-compiled quantum program utilizing the Qiskit\nRuntime framework.\n\nAllocations & Usage Limits\n\nBecause of the queuing method described above, users have no set allocation.\nJob throughput is only limited via the dynamic queue.\n\nThere is a time limit on program-wide usage of reservable systems (see below).\n\nReservations\n\nIn addition to the fair-share queue, users may request a backend reservation\nfor a certain period of time by contacting help@olcf.ornl.gov. If the\nreservation is granted, the reserved backend will be blocked from general use\nfor a specified period of time, and the user will have sole use of the\nbackend for that period.\n\nThere is a limited number of minutes per month that can be reserved on each\ndevice. Reservations are supported on these devices with these monthly\nallocations:\n\nibmq_kolkata, 2400 minutes per month\n\nibmq_jakarta, 480 minutes per month\n\nIn order to make the most efficient use of reservation allocations:\n\nReservations requests must be submitted to the project Principle Investigator\n(PI) to help@olcf.ornl.gov\n\nRequests for reservations must include technical justification.\n\nOnce submitted, requests will be sent to the Quantum Resource Utilization\nCouncil (QRUC) for consideration.\n\nChecking System Availability & Capability\n\nCurrent status listing, scheduled maintenance, and system capabilities for IBM\nQuantum's quantum resources can be found here:\nhttps://quantum-computing.ibm.com/services?services=systems\n\n\n\nSoftware\n\nQiskit documentation is available at https://qiskit.org/documentation/\n\nQiskit Terra is the foundational module set upon which the rest of Qiskit's\nfeatures are built; for more information, see:\nhttps://qiskit.org/documentation/apidoc/terra.html\n\nQiskit Aer is IBM Quantum's package for simulating quantum circuits, with\ndifferent backends for specific types of simulation\n\nSimulator backends currently available: https://quantum-computing.ibm.com/services?services=simulators\n\nAdditional Resources\n\nIBM's Documentation\n\nIBM Quantum Insider"}
{"doc":"ibm-wml-ce","text":"IBM Watson Machine Learning CE -> Open CE\n\nGetting Started\n\nIBM Watson Machine Learning Community Edition (ibm-wml-ce) has been replaced by\nOpen-CE. The Open-CE environment is provided on Summit through the module\nopen-ce, which is built based on the Open Cognitive Environment. Open-CE is a Python Anaconda\nenvironment that is pre-loaded with many popular machine learning frameworks\nand tuned to Summit's Power9+NVIDIA Volta hardware.\n\nTo access the latest analytics packages use the module load command:\n\nmodule load open-ce\n\nLoading a specific version of the module is recommended to future-proof scripts\nagainst software updates. The following commands can be used to find and load\nspecific module versions:\n\n[user@login2.summit ~]$ module avail open-ce\n\n---------------------------------- /sw/summit/modulefiles/core ---------------------------------\nopen-ce/1.2.0-py36-0        open-ce/1.4.0-py37-0    open-ce/1.5.0-py37-0    open-ce/1.5.2-py37-0\nopen-ce/1.2.0-py37-0        open-ce/1.4.0-py38-0    open-ce/1.5.0-py38-0    open-ce/1.5.2-py38-0\nopen-ce/1.2.0-py38-0 (D)    open-ce/1.4.0-py39-0    open-ce/1.5.0-py39-0    open-ce/1.5.2-py39-0\n\n[user@login2.summit ~]$ module load open-ce/1.5.0-py39-0\n\nAs seen above, there are also different Python versions of each Open-CE release\navailable on Summit (indicated by -pyXY- in the module name, where \"X\" and\n\"Y\" are the major and minor Python version numbers, respectively.)\n\nFor more information on loading modules, including loading specific verions,\nsee: environment-management-with-lmod <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#environment-management-with-lmod>\n\nLoading an Open-CE module will activate a conda environment which is pre-loaded\nwith the following packages, and their dependencies:\n\n\nThe table presents a comparison of different versions of IBM WML CE (Watson Machine Learning Community Edition) and the packages included in each version. The first column lists the different environments, namely open-ce/1.2.0, open-ce/1.4.0, open-ce/1.5.0, and open-ce/1.5.2. The second column lists the package included in the open-ce/1.2.0 environment, which is Tensorflow 2.4.1 from the GitHub repository open-ce/tensorflow-feedstock. Similarly, the third, fourth, and fifth columns list the packages included in the open-ce/1.4.0, open-ce/1.5.0, and open-ce/1.5.2 environments, respectively. These packages are updated versions of Tensorflow, namely 2.6.0, 2.7.0, and 2.7.1, all from the same GitHub repository. The next three rows follow the same pattern, listing the packages PyTorch and Horovod, along with their respective versions and GitHub repositories. The last row provides a link to the complete list of software packages included in each version of IBM WML CE, which can be found on the GitHub repository open-ce/open-ce/releases. This table serves as a helpful reference for users to compare the different versions of IBM WML CE and the packages included in each version. \n\n| Environment | open-ce/1.2.0 | open-ce/1.4.0 | open-ce/1.5.0 | open-ce/1.5.2 |\n| --- | --- | --- | --- | --- |\n| Package | Tensorflow 2.4.1 https://github.com/open-ce/tensorflow-feedstock | Tensorflow 2.6.0 https://github.com/open-ce/tensorflow-feedstock | Tensorflow 2.7.0 https://github.com/open-ce/tensorflow-feedstock | Tensorflow 2.7.1 https://github.com/open-ce/tensorflow-feedstock |\n| PyTorch | 1.7.1 https://github.com/open-ce/pytorch-feedstock | 1.9.0 https://github.com/open-ce/pytorch-feedstock | 1.10.0 https://github.com/open-ce/pytorch-feedstock | 1.10.2 https://github.com/open-ce/pytorch-feedstock |\n| Horovod | 0.21.0 (NCCL Backend) https://github.com/horovod/horovod | 0.22.1 (NCCL Backend) https://github.com/horovod/horovod | 0.23.0 (NCCL Backend) https://github.com/horovod/horovod | 0.23.0 (NCCL Backend) https://github.com/horovod/horovod |\n| Complete List | 1.2.0 Software Packages https://github.com/open-ce/open-ce/releases/tag/open-ce-v1.2.0 | 1.4.0 Software Packages https://github.com/open-ce/open-ce/releases/tag/open-ce-v1.4.0 | 1.5.0 Software Packages https://github.com/open-ce/open-ce/releases/tag/open-ce-v1.5.0 | 1.5.2 Software Packages https://github.com/open-ce/open-ce/releases/tag/open-ce-v1.5.2 |\n\n\n\nComparing to IBM WML CE, Open-CE no\nlonger has IBM DDL, Caffe(IBM-enhanced), IBM SnapML, Nvidia\nRapids, Apex packages, and TensorFlow and PyTorch are not compiled with IBM\nLarge Model Support (LMS). For standalone Keras users using Open-CE version\n1.2.0, please pip install keras after module load open-ce.\n\nWML-CE on Summit (slides | recording)\n\nScaling up deep learning application on Summit (slides | recording)\n\nML/DL on Summit (slides | recording)\n\nRunning Distributed Deep Learning Jobs\n\nThe IBM ddlrun tool has been deprecated. The recommended tool for\nlaunching distributed deep learning jobs on Summit is jsrun. When\nlaunching distributed deep learning jobs the primary concern for most\ndistribution methods is that each process needs to have access to\nall GPUs on the node it's running on. The following command should\ncorrectly launch most DDL scripts:\n\njsrun -r1 -g6 -a6 -c42 -bpacked:7  <SCRIPT>\n\n\nThe table presents a detailed breakdown of the resources available in IBM WML CE. The first column lists the different flags that can be used to specify the resources, while the second column provides a description of each flag. The \"-r1\" flag indicates that there is one resource set available per host. The \"-g6\" flag specifies that each resource set has access to 6 GPUs. The \"-a6\" flag denotes that there are 6 MPI tasks available per resource set. The \"-c42\" flag indicates that each resource set has 42 CPU cores available. Lastly, the \"-bpacked:7\" flag specifies that each task is bound to 7 contiguous CPU cores. This table provides a comprehensive overview of the resources available in IBM WML CE, allowing users to easily understand and utilize the available resources for their needs.\n\n| Flags | Description |\n|-------|-------------|\n| -r1   | 1 resource set per host |\n| -g6   | 6 GPUs per resource set |\n| -a6   | 6 MPI tasks per resource set |\n| -c42  | 42 CPU cores per resource set |\n| -bpacked:7 | Binds each task to 7 contiguous CPU cores |\n\nBasic Distributed Deep Learning BSUB Script\n\nThe following bsub script will run a distributed Tensorflow resnet50\ntraining job across 2 nodes.\n\n<string>:108: (ERROR/3) Error in \"code-block\" directive:\nmaximum 1 argument(s) allowed, 2 supplied.\n\n.. code-block:: bash\n     script.bash\n\n    #BSUB -P <PROJECT>\n    #BSUB -W 0:10\n    #BSUB -nnodes 2\n    #BSUB -q batch\n    #BSUB -J mldl_test_job\n    #BSUB -o /ccs/home/<user>/job%J.out\n    #BSUB -e /ccs/home/<user>/job%J.out\n\n    module load open-ce\n\n    jsrun -bpacked:7 -g6 -a6 -c42 -r1 python $CONDA_PREFIX/horovod/examples/tensorflow2_synthetic_benchmark.py\n\n\nbsub is used to launch the script as follows:\n\nbsub script.bash\n\nFor more information on bsub and job submission\nplease see: running-jobs <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#running-jobs>.\n\nFor more information on jsrun please see:\njob-launcher-jsrun <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#job-launcher-jsrun>.\n\nSetting up Custom Environments\n\nThe Open-CE conda environments are read-only. Therefore, users cannot install\nany additional packages that may be needed. If users need any additional conda\nor pip packages, they can clone the Open-CE conda environment into their home\ndirectory and then add any packages they need.\n\nThe conda environment includes a module revision number in its name, the\n'X' in open-ce-1.2.0-py38-X. The name of the active environment can be\nfound in the prompt string within the parentheses, or conda env list can be\nused to see what conda environments are available.\n\n$ module load open-ce\n(open-ce-1.2.0-py38-X) $ conda create --name cloned_env --clone open-ce-1.2.0-py38-X\n(open-ce-1.2.0-py38-X) $ conda activate cloned_env\n(cloned_env) $\n\nBy default this should create the cloned environment in\n/ccs/home/${USER}/.conda/envs/cloned_env (unless you changed it, as\noutlined in our Python on OLCF Systems <https://docs.olcf.ornl.gov/software/python/index.html> page).\n\nTo activate the new environment you should still load the module first. This\nwill ensure that all of the conda settings remain the same.\n\n$ module load open-ce\n(open-ce-1.2.0-py38-X) $ conda activate cloned_env\n(cloned_env) $\n\nBest Distributed Deep Learning Performance\n\nPerformance Profiling\n\nThere are several tools that can be used to profile the performance of a\ndeep learning job. Below are links to several tools that are available\nas part of the open-ce module.\n\nNVIDIA Profiling Tools\n\nThe open-ce module contains the nvprof profiling tool. It can be used to\nprofile work that is running on GPUs. It will give information about when\ndifferent CUDA kernels are being launched and how long they take to complete.\nFor more information on using the NVIDA profiling tools on Summit, please see\nthese slides.\n\nHorovod Timeline\n\nHorovod comes with a tool called Timeline which can help analyze the performance\nof Horovod. This is particularly useful when trying to scale a deep learning job\nto many nodes. The Timeline tool can help pick various options that can improve\nthe performance of distributed deep learning jobs that are using Horovod. For\nmore information, please see Horovod's documentation.\n\nPyTorch’s Autograd Profiler\n\nPyTorch provides a builtin profiler that can be used to find bottlenecks\nwithin a training job. It is most useful for profiling the performance of a job\nrunning on a single GPU. For more information on using PyTorch's profiler, see\nPyTorch's documentation.\n\nReserving Whole Racks\n\nMost users will get good performance using LSF basic job submission, and\nspecifying the node count with -nnodes N. However, users trying\nto squeeze out the final few percent of performance can use the following\ntechnique.\n\nWhen making node reservations for DDL jobs, it can sometimes improve\nperformance to reserve nodes in a rack-contiguous manner.\n\nIn order to instruct BSUB to reserve nodes in the same rack, expert mode must\nbe used (-csm y), and the user needs to explicitly specify the reservation\nstring. For more information on Expert mode see: easy_mode_v_expert_mode <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#easy_mode_v_expert_mode>\n\nThe following BSUB arguments and reservation string instruct bsub to\nreserve 2 compute nodes within the same rack:\n\n#BSUB -csm y\n#BSUB -n 85\n#BSUB -R 1*{select[((LN)&&(type==any))]order[r15s:pg]span[hosts=1]cu[type=rack:pref=config]}+84*{select[((CN)&&(type==any))]order[r15s:pg]span[ptile=42]cu[type=rack:maxcus=1]}\n\n-csm y enables 'expert mode'.\n\n-n 85 the total number of slots must be requested, as -nnodes is not\ncompatible with expert mode.\n\nWe can break the reservation string down to understand each piece.\n\nThe first term is needed to include a launch node in the reservation.\n\n1*{select[((LN)&&(type==any))]order[r15s:pg]span[hosts=1]cu[type=rack:pref=config]}\n\nThe second term specifies how many compute slots and how many racks.\n\n+84*{select[((CN)&&(type==any))]order[r15s:pg]span[ptile=42]cu[type=rack:maxcus=1]}\n\nHere the 84 slots represents 2 compute nodes. Each compute node has 42 compute slots.\n\nThe maxcus=1 specifies that the nodes can come from at most 1 rack.\n\nTroubleshooting Tips\n\nProblems Distributing Pytorch with Multiple Data Loader Workers\n\nProblem\n\nIt is common to encounter segmenation faults or deadlocks when running distributed\nPyTorch scripts that make use of a DataLoader with multiple workers. A typical\nsegfault may look something like the following:\n\nERROR: Unexpected segmentation fault encountered in worker.\nTraceback (most recent call last):\nFile \"/gpfs/anaconda3/envs/powerai/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 724, in _try_get_data\n    data = self._data_queue.get(timeout=timeout)\nFile \"/gpfs/anaconda3/envs/powerai/lib/python3.7/queue.py\", line 179, in get\n    self.not_empty.wait(remaining)\nFile \"/gpfs/anaconda3/envs/powerai/lib/python3.7/threading.py\", line 300, in wait\n    gotit = waiter.acquire(True, timeout)\nFile \"/gpfs/anaconda3/envs/powerai/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n    _error_if_any_worker_fails()\nRuntimeError: DataLoader worker (pid 150462) is killed by signal: Segmentation fault.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\nFile \"pytorch_imagenet_resnet50.py\", line 277, in <module>\n    train(epoch)\nFile \"pytorch_imagenet_resnet50.py\", line 169, in train\n    for batch_idx, (data, target) in enumerate(train_loader):\nFile \"/gpfs/anaconda3/envs/powerai/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 804, in __next__\n    idx, data = self._get_data()\nFile \"/gpfs/anaconda3/envs/powerai/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 761, in _get_data\n    success, data = self._try_get_data()\nFile \"/gpfs/anaconda3/envs/powerai/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 737, in _try_get_data\n    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str))\nRuntimeError: DataLoader worker (pid(s) 150462) exited unexpectedly\n\nSolution\n\nThe solution is to change the multiprocessing start method to forkserver (Python 3 only) or\nspawn. The forkserver method tends to give better performance. This Horovod PR\nhas examples of changing scripts to use the forkserver method.\n\nSee the PyTorch documentation\nfor more information."}
{"doc":"image_building","text":"Image Building\n\n\n\nBuilding an image in Openshift is the act of transferring a set of input parameters into an object. That object is\ntypically an image. Openshift contains all of the necessary components to build a Docker image or a piece of source\ncode an image that will run in a container.\n\nBuild Types\n\nDocker\n\nA Docker build is achieved by linking to a source repository that contains a docker image and all of the necessary\nDocker artifacts. A build is then triggered through the standard $ docker build command. More specific documentation\non docker builds can be found here.\n\nS2I\n\nSource to image is a tool to build docker formatted container images. This is accomplished by injecting source code\ninto a container image and then building a new image. The new image will contain the source code ready to run,\nvia the $ docker run command inside the newly built image.\n\nExamples\n\nBuilding From Git Repository in GUI\n\nUsing the S2I model of building images, it is possible to have your source hosted on a git repository and then pulled and\nbuilt by Openshift.\n\nClick on the newly created project and then on the blue Add to Project button on the center of the page.\n\nHere you will have the option to select from a number of container images. Select the one that matches the source code in the git repository that you will be using. In this example we'll be using Python.\n\nUsing the drop down menu select the version of the language that your source is written in and click Select.\n\nThen input the repo that contains the repository you wish to use.\n\nOpenshift will automatically pull and build your source. If you make a change and need an updated build simply navigate to the build option on the menu to the left and select your project and click Start Build in the upper right.\n\nCreating a BuildConfig from the CLI\n\nThe first step to creating the the build is to create a build config. This is done in one of three ways depending on\nhow your code is structured. If you have a git repository already configured and it is public then the command\n\noc new-build .\n\nwill create your build config from that repository.\n\nIf you have a public git repository that isn't already configured then you can create the build config by running\n\noc new-build <URL TO YOUR GIT REPOSITORY>\n\nIf you have a private git repository then your build config will be created by running the command:\n\n$ oc new-build <URL TO YOUR GIT REPOSITORY> --source-secret=yoursecret\n\nFollows is an example of creating a build for a public git repository:\n\n$ oc new-build https://github.com/sclorg/django-ex\n--> Found image 1ce91f1 (7 months old) in image stream \"openshift/python\" under tag \"3.5\" for \"python\"\n\n    Python 3.5\n    ----------\n    Python 3.5 available as docker container is a base platform for building and running various Python\n    3.5 applications and frameworks. Python is an easy to learn, powerful programming language. It has\n    efficient high-level data structures and a simple but effective approach to object-oriented\n    programming. Python's elegant syntax and dynamic typing, together with its interpreted nature, make\n    it an ideal language for scripting and rapid application development in many areas on most\n    platforms.\n\n    Tags: builder, python, python35, rh-python35\n\n    * The source repository appears to match: python\n    * A source build using source code from https://github.com/sclorg/django-ex will be created\n      * The resulting image will be pushed to image stream \"django-ex:latest\"\n      * Use 'start-build' to trigger a new build\n\n--> Creating resources with label build=django-ex ...\n    imagestream \"django-ex\" created\n    buildconfig \"django-ex\" created\n--> Success\n    Build configuration \"django-ex\" created and build triggered.\n    Run 'oc logs -f bc/django-ex' to stream the build progress.\n\nWhat happens is that oc pulls in the provided repository, in this example Django, and automatically configures\neverything needed to build the image. You should now be able to go to the Openshift web GUI and under the builds\ntab see your newly built build.\n\nNow, since everything has been configured, you can click the Start Build button in the upper right hand side of the\nWeb GUI anytime that you need to make another build. You can also start a another build from the command line with\neither:\n\noc start-build <buildconfig_name>\n\nOr, if you would like to receive logs from the build:\n\noc start-build <buildconfig_name> --follow\n\nIt is perfectly normal for a build to take a few minutes to complete.\n\nUsing A Dockerfile\n\nUsing a Dockerfile inside of Openshift works in the same way that $ docker build  works outside of Openshift. If\nall that is needed for your build is a Dockerfile. From within the directory containing the Dockerfile you can run:\n\n$ oc new-build . --name example\n--> Found image 224765a (3 months old) in image stream \"buildexample/openjdk\" under tag \"8-alpine\" for \"openjdk:8-alpine\"\n\n    * A Docker build using binary input will be created\n      * The resulting image will be pushed to image stream \"example:latest\"\n      * A binary build was created, use 'start-build --from-dir' to trigger a new build\n\n--> Creating resources with label build=example ...\n    imagestream \"example\" created\n    buildconfig \"example\" created\n--> Success\n\nThat will create a new build config, from that build config you can then use your app by running the start-build command\nwith the name of the newly created build config.\n\n$ oc start-build example --from-file=./Dockerfile\n  Uploading file \"Dockerfile\" as binary input for the build ...\n  build \"example-1\" started\n\nIn the above example example was the name of the build config.\n\nAdditionally, if there are artifacts that need to be included in your build, a directory containing those artifacts can\nbe used by passing the --from-dir flag to the start-build command like so:\n\n$ oc start-build example --from-dir=./sampledir\n  Uploading directory \"sampledir\" as binary input for the build ...\n  build \"django-5\" started\n\nUsing a local image\n\nThere might be an image built locally that you would like to have in your OpenShift project. It is possible to add this\nimage to your project by adding it to the Docker registry of the cluster that your project is on.\n\nFirst, copy your login token. We will need this for the next step.\n\noc login https://api.<cluster>.ccs.ornl.gov --token=<COPY THIS TOKEN>\n\nNext, log into the Docker registry. Use your copied token when prompted for your password. Upon succesful login, a message\nsaying so will appear.\n\ndocker login -u <NCCS USERNAME> registry.apps.<cluster>.ccs.ornl.gov\n\nNow, find the repository and tag information of the local image you want to add to the registry and tag it accordingly.\n\n$ docker images\nREPOSITORY                                TAG                 IMAGE ID            CREATED             SIZE\nexample:5000/streams                      v3.1.4              fd7673fdbe30        3 weeks ago         1.95GB\n\nThe command to tag your image is:\n\ndocker tag example:5000/streams:v3.1.4 registry.apps.<cluster>.ccs.ornl.gov/<namespace>/<image>:<tag>\n\nLastly, the image needs to be pushed to the registry.\n\ndocker push registry.apps.<cluster>.ccs.ornl.gov/<namespace>/<image>:<tag>\n\nOpenShift has an integrated container registry that can be accessed from outside the cluster to\npush and pull images as well as run containers.\n\nLogging into the registry externally\n\nThis assumes that you have Docker installed locally. Installing Docker is outside of the scope of this documentation.\n\nFirst you have to log into OpenShift\n\noc login https://api.<cluster>.ccs.ornl.gov\n\nNext you can use your token to log into the integrated registry.\n\ndocker login -u user -p $(oc whoami -t) registry.apps.<cluster>.ccs.ornl.gov\n\nThen you can push and pull from the integrated registry. In the following example we will pull\nbusybox:latest from Docker Hub and push it to our namespace in the integrate registry.\n\n$ docker pull busybox:latest\nlatest: Pulling from library/busybox\nee153a04d683: Pull complete\nDigest: sha256:9f1003c480699be56815db0f8146ad2e22efea85129b5b5983d0e0fb52d9ab70\nStatus: Downloaded newer image for busybox:latest\ndocker.io/library/busybox:latest\n\n$ docker tag busybox:latest registry.apps.marble.ccs.ornl.gov/stf002platform/busybox:latest\n\n$ docker push registry.apps.marble.ccs.ornl.gov/stf002platform/busybox:latest\nThe push refers to repository [registry.apps.marble.ccs.ornl.gov/stf002platform/busybox]\n0d315111b484: Pushed\nlatest: digest: sha256:895ab622e92e18d6b461d671081757af7dbaa3b00e3e28e12505af7817f73649 size: 527\n\n$ oc get is busybox\nNAME      DOCKER REPO                                               TAGS     UPDATED\nbusybox   image-registry.openshift-image-registry.svc:5000/stf002platform/busybox   latest   5 seconds ago\n\nWhen tagging an image, you must use the format registry.apps.<cluster>.ccs.ornl.gov/<namespace>/<image> where:\n\nCluster is the name of the OpenShift cluster\n\nNamespace is the name of the Kubernetes namespace you are using (Use oc status to see what\nOpenShift Project/Kubernetes Namespace you are currently in)\n\nImage is the name of the image you want to push\n\nOnce you push the image into the registry, a OpenShift ImageStream will be automatically created"}
{"doc":"index","text":"Scalable Protected Infrastructure (SPI)\n\nWhat is SPI\n\nThe OLCF's Scalable Protected Infrastructure (SPI) provides resources and protocols that enable researchers to process protected data at scale.  The SPI is built around a framework of security protocols that allows researchers to process large datasets containing private information.  Using this framework researchers can use the center's large HPC resources to compute data containing protected health information (PHI), personally identifiable information (PII), data protected under International Traffic in Arms Regulations, and other types of data that require privacy.\n\nThe SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.  This page lists the differences you may see when using OLCF resources to execute SPI workflows.  The page will also point to sections within the site where more information on standard non-SPI use can be found.\n\nNew User QuickStart\n\nIf you are new to the OLCF or the OLCF's SPI, this page can help get you started.  Below are some of the high-level steps needed to begin use of the OLCF's SPI:\n\nRequest an allocation (project)<spi-allocations-projects> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Request an allocation (project)<spi-allocations-projects>>.  All access and resource use occurs within an approved allocation.\n\nRequest a user account<spi-user-accounts> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Request a user account<spi-user-accounts>>.  Once an allocation (project) has been approved, each member of the project must request an account to use the project's allocated resources.\n\nWhitelist your IPs<spi-whitelisting-ip> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Whitelist your IPs<spi-whitelisting-ip>>.  Access to the SPI resources is limited to IPs that have been whitelisted by the OLCF.  The only exception is for projects using KDI resources.  If your project also uses KDI resources, you will use the KDI access procedures and do not need to provide your IP to the OLCF.\n\nTransfer needed data<spi-data-transfer> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Transfer needed data<spi-data-transfer>> to the SPI filesystems.  The SPI resources mount filesystems unique to the SPI.  Needed data, code, and libraries must be transferred into the SPI using the SPI's Data Transfer Nodes.\n\nBuild and run on Citadel<spi-compute-citadel> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Build and run on Citadel<spi-compute-citadel>>.  Citadel is front end for the OLCF's Summit resource.  Its resources and programming environment mirror Summit.  You can ssh into Citadel to build for and execute jobs on Summit's compute resources.\n\nNotable Differences\n\nIf you have a standard non-SPI account(s) on other OLCF resources, the differences between SPI and non-SPI can be found on this page.  Below are some of the notable differences:\n\nUserIDs are unique to each SPI project<spi-user-accounts> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#UserIDs are unique to each SPI project<spi-user-accounts>>.  Unlike non-SPI accounts, SPI accounts can not span multiple projects. SPI userIDs use the format: <userid>_<proj>_mde\n\nDirect access to SPI resources requires your IP to be whitelisted<spi-whitelisting-ip> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Direct access to SPI resources requires your IP to be whitelisted<spi-whitelisting-ip>> by the OLCF.  Before accessing SPI resources, you must contact help@olcf.ornl.gov and provide you system's IP.  If your project also uses KDI resources, you will use the KDI procedure and do not need to provide your IP to the OLCF.\n\nSPI resources mount SPI filesystems<spi-file-systems> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#SPI resources mount SPI filesystems<spi-file-systems>>.  The SPI resources do not mount the non-SPI's scratch filesystems, home areas, or mass storage.\n\nSPI compute resources cannot access external resources<spi-data-transfer> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#SPI compute resources cannot access external resources<spi-data-transfer>>.  Needed data must be transferred to the SPI resources through the SPI's DTN.\n\nThe Citadel login nodes<spi-compute-citadel> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#The Citadel login nodes<spi-compute-citadel>> and batch queues must be used to access Summit for SPI workflows.\n\nAllocations and User Accounts\n\n\n\nAllocations (Projects)\n\nSimilar to standard OLCF workflows, to run SPI workflows on OLCF resources, you will first need an approved project.  The project will be given an allocation(s) that will allow resource access and use.  The first step to requesting an allocation is to complete the project request form.  The form will initiate the process that includes peer review, export control review, agreements, and others.  Once submitted, members of the OLCF accounts team will help walk you through the process.\n\nRequesting a New Allocation (Project)\n\nPlease see the OLCF Accounts and Projects section of this site to request a new project.\n\nSPI project IDs may look similar to those used in the non-SPI moderate enclave but will always append _mde to the name. For example: abc123_mde.\n\nProjects cannot overlap non-SPI and SPI enclaves. SPI projects will only exist on SPI resources.\n\n<string>:3: (INFO/1) Duplicate explicit target name: \"olcf accounts and projects\".\n\nMore information on the OLCF account process can be found in the OLCF Accounts and Projects section of this site.\n\n\n\nUser Accounts\n\nOnce a project has been approved and created, the next step will be to request user accounts.  A user account will allow an individual to access the project's allocated resources.    This process to request an account is very similar to the process used for non-SPI projects.  One notable difference between SPI and non-SPI accounts: SPI usernames are unique to a project.  SPI usernames use the format: <userid>_<proj>_mde.  If you have access to three SPI projects, you will have three userIDs with three separate home areas.\n\nRequesting a New User Account\n\nPlease see the OLCF Applying for a User Account<applying-for-a-user-account> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#OLCF Applying for a User Account<applying-for-a-user-account>> section of this site to request a new account and join an existing project.  Once submitted, the OLCF Accounts team will help walk you through the process.\n\nIn order to help ensure data separation, each SPI user is given a unique userID for each project. SPI userIDs use the format: <userid>_<proj>_mde . For example: userx_abc123_mde. SPI usernames will not overlap with usernames used in the non-SPI enclaves. Unlike non-SPI usernames, SPI usernames only exist in a single project. Users on multiple SPI projects will have a unique username for each SPI project.\n\nMore information on the account process and a link to the request form can be found in the OLCF Applying for a User Account<applying-for-a-user-account> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#OLCF Applying for a User Account<applying-for-a-user-account>> section.\n\nAvailable Resources\n\nThe OLCF SPI provides compute, filesystem, and data transfer resources.\n\nCompute<spi-compute-citadel> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Compute<spi-compute-citadel>>\n\n<string>:82: (WARNING/2) Title underline too short.\n\n`Compute<spi-compute-citadel> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Compute<spi-compute-citadel>>`\n-------------------------------------\n\nThe SPI provides access to the OLCF's Summit resource for compute.  To safely separate SPI and non-SPI workflows, SPI workflows must use a separate login node named Citadel<spi-compute-citadel> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Citadel<spi-compute-citadel>>.  Citadel provides a login node specifically for SPI workflows.\n\nThe login node used by Citadel mirrors the Summit login nodes in hardare and software.  The login node also provides access to the same compute resources as are accessible from Summit.\n\nSimilar to Summit, accessing the compute nodes is accomplished through the batch system.  Separate batch queues must be used to access the compute resources and ensure the compute resourcs are configured for SPI workflows.\n\nFile Systems<spi-file-systems> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#File Systems<spi-file-systems>>\n\n<string>:90: (WARNING/2) Title underline too short.\n\n`File Systems<spi-file-systems> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#File Systems<spi-file-systems>>`\n-------------------------------------\n\nTo safely separate SPI and non-SPI workflows, the SPI resources only mount a GPFS resource named Arx<spi-file-systems> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Arx<spi-file-systems>>.  The Arx filesystem provides both the home and scratch filesystems for SPI resources.\n\nData Transfer<spi-data-transfer> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Data Transfer<spi-data-transfer>>\n\n<string>:94: (WARNING/2) Title underline too short.\n\n`Data Transfer<spi-data-transfer> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Data Transfer<spi-data-transfer>>`\n---------------------------------------\n\nThe SPI provides separate Data Transfer Nodes<spi-data-transfer> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Data Transfer Nodes<spi-data-transfer>> configured specifically for SPI workflows.  The nodes are not directly accessible for login but are accessible through the Globus tool.  The SPI DTNs mount the same Arx filesystem available on the SPI compute resources.  Globus is the prefered method to transfer data into and out of the SPI resources.\n\nIP Whitelisting\n\nAccess to the SPI resources is allowed to approved IP addresses only.\n\nDirect access to SPI resources require the connecting IP address to be whitelisted.  The OLCF must know your IP before you can directly connect to SPI resources.\n\nProject using ORNL's KDI must following KDI access procedures and cannot access SPI resources directly.  If your project uses both KDI and SPI, you do not need to provide an IP.\n\n\n\nWhitelisting an IP or range\n\nTo add an IP or range of IPs to your project’s whitelist, please contact help@olcf.ornl.gov\n\nFinding your IP\n\nAn easy way to locate your IP or range of IP addresses is to contact your local network administration team.  Your network administrator will be able to provide your individual IP or the ranges of IP addresses that you will use on the network.\n\nAnother way to find your IP is to use tools such as ‘whats my ip’. But please note, the tools may only return your internal IP. The IP you provide for the whitelist must be your external IP. The following are internal rages that cannot be used to whitelist your IP:\n\n10.0. 0.0 - 10.255. 255.255 (10.0. 0.0/8 prefix)\n\n172.16. 0.0 - 172.31. 255.255 (172.16. 0.0/12 prefix)\n\n192.168. 0.0 - 192.168. 255.255 (192.168. 0.0/16 prefix)\n\nThe tool may also return you current IP which may change if not static. For these reasons, reaching out to your IT department may be the best option. Your IT department can provide a range of externally facing IP addresses that can be whitelisted.\n\n\n\nCitadel\n\nThe SPI resource, Citadel, utilizes Summit's compute resources but adds measures to ensure separation of SPI and non-SPI workflows and data. The Summit User Guide<summit-documentation-resources> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Summit User Guide<summit-documentation-resources>> provides documentation on system details and use.  Because Citadel is largely a front end for Summit, you can use the Summit documentation when using Citadel.  This section describes some of the notable differences in using Summit directly and through the SPI's Citadel.  It should be used in combination with the Summit User Guide<summit-user-guide> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Summit User Guide<summit-user-guide>> .\n\nConnecting\n\nSimilar to the non-SPI resources, SPI resources reqiure two-factor authentication.  If you are new to the center, you will receive a SecurID fob during the account approval/creation process.  If you are an existing user of non-SPI resources, you can use the same SecurID fob and PIN used on your non-SPI account.\n\nAlso similar to non-SPI resources, you will connect directly to the SPI resources through ssh.\n\nORNL's KDI users are an exception and can not by policy log directly into SPI resources.  KDI users, please follow the KDI documented procedures:\n\nLogin to https://kdivdi.ornl.gov with your KDI issued credentials\n\nLaunch the Putty Application\n\nEnter the hostname \"citadel.ccs.ornl.gov\" and click Open\n\nYou will then be in an ssh terminal to authenticate with your OLCF credentials as detailed above.\n\nProjects using ORNL's KDI must following KDI access procedures and cannot access SPI resources directly.  If your project uses both KDI and SPI, you will not access the SPI resources directly.\n\nIn order to help ensure data separation, each SPI user is given a unique userid for each project. SPI user names use the format: <userid>_<proj>_mde . For example: usera_prj123_mde.\n\nSPI usernames will not overlap with usernames used in the non-SPI enclaves. Unlike non-SPI usernames, SPI usernames only exist in a single project. Users on multiple SPI projects will have a unique username for each SPI project.  You must specify your unique SPI username matching the target project when connecting.\n\nFor users with accounts on non-SPI resources, you will use the same SecurID fob and PIN, but you must specify your unique SPI userID when you connect.  The ID will be used to place you in the proper UNIX groups allowing access to the project specific data, directories, and allocation.\n\nLogin Nodes\n\nTo help separate data and processes, SPI use separate login nodes, citadel.ccs.ornl.gov, to reach Summit’s compute resources.\n\nThe Citadel login node must be used to submit SPI jobs to Summit’s compute resources and access the SPI specific filesystem.\n\nThe login node used by Citadel mirrors the Summit login nodes in hardware and software.  The login node also provides access to the same compute resources as are accessible from Summit.\n\nThe Citadel login nodes cannot access the external network and are only accessible from whitelisted IP addresses.\n\nBuilding Software\n\nThe user environment on the Citadel login nodes mirrors the Summit login nodes.  Code build for/on Summit, should also run on Citadel. Third party software, compilers, and libraries provided on Summit will also be available from Citadel. The Summit User Guide<summit-user-guide> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Summit User Guide<summit-user-guide>> can be used when building workflows for Citadel.\n\nThe Citadel login nodes can not access the internet.  This may impact build workflows that attempt to access external repositories.\n\nBecause the Citadel login nodes cannot reach repositories external to the system, you may need to alter your build workflows.  For these cases, you may be able to retrieve needed data. For cases where this is not possible, you can reach out to help@olcf.ornl.gov and request login access to Summit.  We can provide Summit login access by creating a sister project on Summit. You can then login to Summit to build your code and copy it to /sw/summit/mde/abc123_mde where abc123_mde is replaced by your Citadel project. This location is writable from Summit but only readable from Citadel. If the source code and data is small enough, you can also use the scp command from your whitelisted IP system to copy the data onto the Citadel login nodes directly.\n\nMore information on building codes for Citadel including programming environments, compilers, and available software can be found on Summit User Guide<summit-user-guide> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Summit User Guide<summit-user-guide>>.\n\nRunning Batch Jobs\n\nCitadel and Summit share compute resources.  However, compute resources are reconfigured for SPI workloads to protect data.  To access the compute resources, you must first log into Citadel.  From Citadel you can access the compute nodes through the batch system as you would from Summit.  The notable difference between Summit and Citadel batch submission is the requirement to use SPI specific batch queues. SPI batch jobs must specify one of the following SPI specific batch queues:\n\nbatch-spi\n\ndebug-spi\n\nbatch-hm-spi\n\nThe batch queues mirror the purpose of the similarly named Summit queues. Details on each queue can be found in the Summit User Guide<summit-user-guide> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Summit User Guide<summit-user-guide>>. The SPI queues must be used to launch batch jobs from the Citadel nodes and can not be used directly from the moderate enclave Summit login nodes.\n\nTo access Summit's compute resources for SPI workflows, you must first log into Citadel and then submit a batch job to one of the SPI specific batch queues.\n\nUse of the queues will trigger configuration changes to the Summit compute nodes to allow enhanced data protection. Compute nodes will be booted before and after each SPI batch job. Compute nodes will be booted into an image that mounts only the Arx filesystem. The image will also restrict connections. Please note: the reboot process may cause a slight delay in job startup.\n\nMore details on batch job submission through LSF and launching a parallel job through jsrun can be found on Summit User Guide<summit-user-guide> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Summit User Guide<summit-user-guide>>.\n\n\n\nFile Systems\n\nThe SPI resources use filesystems visible only from SPI resources. The SPI resources do not mount filesystems mounted on non-SPI resources. The GPFS filesystem named Arx provides home, scratch, and shared project areas for SPI resources.\n\nAvailable filesystems:\n\n\nThe table above provides a detailed overview of the different locations and purposes within the index. The first location listed is the Home directory, which is located at /gpfs/arx/<proj>/home/<userid>. This directory serves as the user's login and home directory, where they can store small scripts and source files. The next location is the Project Shared directory, which is located at /gpfs/arx/<proj>/proj-shared. This directory is used for sharing data with other members of the user's project. Finally, the table includes the Scratch directory, which is located at /gpfs/arx/<proj>/scratch/<userid>. This directory is specifically designed for storing compute job input/output, making it a useful location for managing large amounts of data during computational tasks. Overall, this table provides a clear and concise breakdown of the different locations within the index and their respective purposes.\n\n| Name | Location | Purpose |\n|------|----------|---------|\n| Home | /gpfs/arx/<proj>/home/<userid> | Your login/home directory. Used to store small scripts and source. |\n| Project Shared | /gpfs/arx/<proj>/proj-shared | Location to share data with others in your project. |\n| Scratch | /gpfs/arx/<proj>/scratch/<userid> | Location to store compute job I/O. |\n\nSPI resources do not mount filesystems accessible from non-SPI resources.  SPI resources only mount the GPFS Arx filesytem.\n\n\n\nData Transfer\n\nGlobus is the best option to transfer data into and out of the SPI resources.\n\nThe SPI Data Transfer Nodes are not directly accessible, but can be used through Globus to transfer data.\n\nA simple example using the CLI:\n\nmyproxy-logon -T -b -l usera_prj123_mde\nglobus-url-copy -cred /gpfs/arx/prj123_mde/home/usera_prj123_mde/dataA -dcpriv -list"}
{"doc":"job_submit","text":"Batch Job Submission\n\nBatch job submission from containers is designed to work exactly like on a login\nnode for the cluster. The workload will need to be annotated in order to get the\nnecessary configuration injected at runtime.\n\n\nThe table above displays information related to job submission for two clusters, Marble and Onyx. The first column, \"Cluster,\" lists the names of the clusters. The second column, \"Annotation,\" provides the URL for the batch scheduler used for job submission on each cluster, which is ccs.ornl.gov/batchScheduler for both Marble and Onyx. The third column, \"Value,\" indicates that job submission is enabled on both clusters, as it is set to \"true\" for both. The last column, \"Schedulers,\" lists the specific batch schedulers used on each cluster, with Slurm and LSF being used on Marble, and LSF being the sole scheduler for Onyx. This table provides a clear overview of the job submission process for these two clusters, including the relevant URLs and schedulers used. \n\n| Cluster | Annotation | Value | Schedulers |\n|---------|------------|-------|------------|\n| Marble  | ccs.ornl.gov/batchScheduler | true | Slurm, LSF |\n| Onyx    | ccs.ornl.gov/batchScheduler | true | LSF |\n\nYou can add the required annotations to any workload object such as a Pod, Deployment,\nor a DeploymentConfig. Submitting a batch job from a container requires access to\nthe OLCF shared filesystems so that annotation is also included.\n\nmetadata:\n  annotations:\n    ccs.ornl.gov/batchScheduler: \"true\"\n    ccs.ornl.gov/fs: olcf\n\nFull example of a deployment using a base image provided by OLCF.\n\nBatch job submission from containers uses SSH to access the submission host. If you\nuse your own image you must install the openssh client package in your image.\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-jobsubmit\n  annotations:\n    ccs.ornl.gov/batchScheduler: \"true\"\n    ccs.ornl.gov/fs: olcf\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: test-jobsubmit\n  template:\n    metadata:\n      labels:\n        app: test-jobsubmit\n    spec:\n      containers:\n      - name: test-jobsubmit\n        image: \"image-registry.openshift-image-registry.svc:5000/openshift/ccs-rhel7-base-amd64:latest\"\n        args:\n        - cat\n        stdin: true\n        stdinOnce: true\n\nThe annotation will install wrappers into /usr/bin:\n\nSlurm\n\nsbatch\n\nsqueue\n\nLSF\n\nbsub\n\nbjobs"}
{"doc":"libensemble","text":"libEnsemble\n\n\n\nOverview\n\nlibEnsemble is a complete Python<py-index> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Python<py-index>> toolkit for\nsteering dynamic ensembles of calculations. Workflows are highly portable and detect/integrate heterogeneous\nresources with little effort. For instance, libEnsemble can automatically detect, assign, and reassign allocated\nprocessors and GPUs to ensemble members.\n\nUsers select or supply generator and simulator functions to express their ensembles; the generator\ntypically steers the ensemble based on prior simulator results. Such functions can also launch and monitor\nexternal executables at any scale.\n\nInstallation\n\nBegin by loading the python module:\n\n$ module load cray-python\n\nlibEnsemble is available on PyPI, conda-forge,\nthe xSDK, and E4S. Most users install libEnsemble\nvia pip:\n\n$ pip install libensemble\n\nInstalling libEnsemble in a virtual environment is highly recommended. See the Python on OCLF Systems<py-index> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Python on OCLF Systems<py-index>> page\nfor more information.\n\nExamples\n\nFor a very simple example of using libEsemble\nsee the Simple Sine Tutorial\non libEnsemble's documentation.\n\nFor an example that runs a small ensemble with an application that offloads work to a GPU, see\nthis GPU App Tutorial.\n\nAdditional information on compiling/running the above sample GPU app is available here.\n\nSee this video for an example workflow on Spock<spock-quick-start-guide> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Spock<spock-quick-start-guide>>.\nThe channel will soon publish a Frontier-specific guide.\n\nExample Code\n\nimport numpy as np\nfrom tutorial_gen import gen_random_sample\nfrom tutorial_sim import sim_find_sine\n\nfrom libensemble.libE import libE\nfrom libensemble.tools import add_unique_random_streams\n\nlibE_specs = {\"nworkers\": 4, \"comms\": \"local\"}\n\ngen_specs = {\n    \"gen_f\": gen_random_sample,  # Our generator function\n    \"out\": [(\"x\", float, (1,))],  # gen_f output (name, type, size).\n    \"user\": {\n        \"lower\": np.array([-3]),  # random sampling lower bound\n        \"upper\": np.array([3]),  # random sampling upper bound\n        \"gen_batch_size\": 5,  # number of values gen_f will generate per call\n    },\n}\n\nsim_specs = {\n    \"sim_f\": sim_find_sine,  # Our simulator function\n    \"in\": [\"x\"],  # Input field names. 'x' from gen_f output\n    \"out\": [(\"y\", float)],  # sim_f output. 'y' = sine('x')\n}\n\npersis_info = add_unique_random_streams({}, 5)  # Initialize manager/workers random streams\n\nexit_criteria = {\"sim_max\": 80}  # Stop libEnsemble after 80 simulations\n\nH, persis_info, flag = libE(sim_specs, gen_specs, exit_criteria, persis_info, libE_specs=libE_specs)\n\nJob Submission\n\nUpon initialization, libEnsemble will detect available nodes and GPUs from the Slurm environment, and\nallocate those resources towards application-launches.\n\nStart an interactive session:\n\n$ salloc --nodes=2 -A <project_id> --time=00:10:00\n\nWithin the session (multiprocessing comms, all processes on first node):\n\n$ python my_libensemble_script.py --comms local --nworkers 8"}
{"doc":"miniconda","text":"Installing Miniconda\n\nCurrently, Crusher and Frontier do NOT have Anaconda/Conda modules.\nIf your workflow better suits conda environments, you can install your own Miniconda on Frontier.\n\nThe install process is rather simple (with a few notable warnings, see Cautionary Notes <miniconda-notes> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Cautionary Notes <miniconda-notes>> further below):\n\nmkdir miniconda_frontier/\ncd miniconda_frontier/\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nchmod u+x Miniconda3-latest-Linux-x86_64.sh\n./Miniconda3-latest-Linux-x86_64.sh -u -p ~/miniconda_frontier\n\nThe -p flag specifies the prefix path for where to install miniconda.\n\nThe -u updates any current installations at the -p location (not necessary if you didn't do a \"mkdir\" beforehand).\n\n\n\nCautionary Notes\n\nWhile running the installer, you will be prompted with something like this:\n\nDo you wish the installer to initialize Miniconda3 by running conda init? [yes|no]\n\nIt is MUCH SAFER to answer \"no\" and to just export the PATH manually when on Frontier to avoid clashing:\n\nexport PATH=\"/path/to/your/miniconda/bin:$PATH\"\n\nIf you answer \"yes\", your .bashrc (or equivalent shell configuration file) will be updated with something like this:\n\n# >>> conda initialize >>>\n# !! Contents within this block are managed by 'conda init’ !! .\n.\n.\n.\n.\n#unset __conda_setup\n# <<< conda initialize <<<\n\nBy default, this will always initialize conda upon login, which clashes with other Python installations (e.g., if you use the anaconda modules on other OLCF systems).\n\nIf your .bashrc already has a similar block of code (e.g., from other OLCF modules), then it will NOT modify your bashrc\n\nAn additional recommendation is to set things to not activate your base environment by default (to help with the potential clashing):\n\n# Only needs to be run once after exporting conda into your PATH\nconda config --set auto_activate_base false\n\nAs always, if you encounter issues, don't hesitate to contact help@olcf.ornl.gov"}
{"doc":"minio","text":"MinIO Object Store (On an NCCS Filesystem)\n\n\n\nMinIO is a high-performance software-defined object\nstorage suite that enables the ability to easily deploy cloud-native data\ninfrastructure for various data workloads. In this example we are deploying a\nsimple, standalone implementation of MinIO on our cloud-native platform, Slate\n(slate_overview <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_overview>).\n\nThis service will only be accessible from inside of ORNL's network.\n\nIf your project requires an externally facing service available to the\nInternet, please contact User Assistance by submitting a help ticket. There is\na process to get such approval.\n\nWe hope this provides a starting point for more robust implementations of\nMinIO, if your workload/project benefits from that. In addition, it gives\ninsight into some of the core building blocks for establishing your own,\ndifferent, applications on Slate.\n\nThis example deployment of MinIO enables two possible configurations (which we\nconfigure in the following sections):\n\nMinIO running on a NCCS filesystem (GPFS or NFS - exposing filesystem project\nspace through the MinIO GUI).\n\nMinIO running on a dedicated volume, allocated automatically from the NetApp\nstorage server, isolated to the MinIO server.\n\nIt is important to note that we are also launching MinIO in standalone mode,\nwhich is a single MinIO server instance. MinIO also supports distributed mode\nfor more robust implementations, but we are not setting that up in this\nexample.\n\n<string>:5: (INFO/1) Duplicate explicit target name: \"user assistance\".\n\nFor this example to work, it is required to have a project \"automation user\"\nsetup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you\nare unsure about the automation user setup for your project.\n\nThis is not meant to be a production deployment, but a way for users to gain\nfamiliarity with building an application targeting Slate.\n\nGetting Started\n\nIt is assumed you have already gone through slate_getting_started <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_getting_started> and\nestablished the helm_prerequisite <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#helm_prerequisite>. Please do that before attempting to\ndeploy anything on Slate's clusters.\n\nThis example uses Helm version 3 to deploy a MinIO standalone Helm chart on\nSlate's Marble\nCluster. This is the cluster in OLCF's Moderate enclave, the same enclave as\nSummit.\n\nTo start, clone the slate helm examples repository , containing the MinIO\nstandalone Helm chart, and navigate into the charts directory:\n\n$ git clone https://code.ornl.gov/ryu/slate_helm_examples.git && cd slate_helm_examples/charts\n\nIf you are interested in the details of this Helm chart, please look at the\nminio-standalone chart's README.\n\nNext, log into Marble with the OC CLI tool by running this command on your\nlocal machine:\n\n$ oc login https://api.marble.ccs.ornl.gov\n\nYou should see output confirming your login. It will also name your available\nproject spaces on Marble.\n\nTo list your available project spaces run this command:\n\n$ oc projects\n\nFinally, confirm Helm works by running this command on your local machine:\n\n$ helm ls\n\nYou should get some output similar to this (although, you may not have any applications listed, if you have not deployed any):\n\nNAME                                  NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION\ngitlab-runner-for-slate-examples      stf007          9               2020-05-18 11:31:08.3245 -0400 EDT      deployed        gitlab-runner-0.16.1    12.10.2\nrprout-minio-standalone               stf007          1               2020-05-20 10:52:34.353245 -0400 EDT    deployed        minio-standalone-1.0.0\n\nConfigure Your Deployment\n\nWhere you cloned the slate_helm_examples repository, in the\n'slate_helm_examples/charts/minio-standalone` directory, you will see a\nvalues.yaml file. This file containes variables for the Helm chart\ndeployment.\n\nThis is how we configure your instance of the MinIO application. All of these\nchanges will be to your local copy of values.yaml.\n\nHere is what it looks like:\n\n# This can be used to provide variables to your chart.\n# Below are the current configurable variables.\nminio:\n  resources:\n    requests:\n      cpu: 2\n      memory: 1Gi\n    limits:\n      cpu: 2\n      memory: 1Gi\n  # Change this to reflect <your_uid>, this must be unique: <your_uid>-minio-standalone.apps.marble.ccs.ornl.gov\n  host: rprout-minio-standalone.apps.marble.ccs.ornl.gov\n  # Change this to create unique app name\n  name: rprout-minio-standalone\n  # Set this to \"disbled\" to not use OLCF fileystem. If \"disabled\" it will use a volume isolated to the MinIO Pod.\n  use_olcf_fs: enabled\n  # This is the OLCF file system path MinIO will server out of, if \"enabled\" above.\n  olcf_mount: /ccs/proj/stf007/minio-test\n  # Amount of storage to use, if use_olcf_fs is \"disabled\"\n  pvc_storage: 3Gi\n  # Change this to reflect <your-project-namespace>, this will be the output of the `oc project` command.\n  network_policy_namespace: <your-project-namespace>\n\nWhat do you need to consider?\n\nWhat should I name my host value? (This will be the URL in which you access\nyour MinIO instance)\n\nWhat should I name my application? (This is the name value and should be\nunique to you or your project)\n\nDo I want MinIO to run on an OLCF filesytem? (It can run on NFS or GPFS\nproject spaces. If you do not run it on an OLCF filesystem it uses an\nisolated volume dedicated to the MinIO server)\n\nWhat do you need to configure?\n\nhost (Set the URL of your application)\n\nname (Set the name of your application)\n\nuse_olcf_fs (Controls if NCCS filesystems are used or not - enabled or disabled)\n\nolcf_mount (Set the mount path to your project directory (i.e /ccs/proj/<projectID>/minio/))\n\npvc_storage (Set the quota for your dedicated storage if use_olcf_fs is disabled)\n\nnetwork_policy_namespace (Set the network policy's namespace to your project name, this will be the output of the oc project command)\n\nCreate the MinIO Application's Secret Tokens\n\nThe below is not provided in the above configuration, but it must be done for\nthe MinIO application to start properly.\n\nThese are the root credentials referenced here.\n\nTo establish these credentials in our Marble project, allowing our MinIO\ndeployment to use them, we need to create a secret-token.yaml file and\napply it to our project.\n\nCreate this example secret-token.yaml file locally:\n\napiVersion: v1\nkind: List\nmetadata: {}\nitems:\n- apiVersion: v1\n  kind: Secret\n  metadata:\n    # The <name-of-your-app> piece can be found in the values.yaml file at 'minio.name'. You can set the name of your app.\n    # Keep the \"-access-key\" part appended to the name.\n    name: <name-of-your-app>-access-key\n  stringData:\n    SECRET_TOKEN: <your_choice>\n- apiVersion: v1\n  kind: Secret\n  metadata:\n    # Keep the '-secret-key' part appended to the name.\n    # Note: <your_choice> below must be a string\n    # Ex: SECRET_TOKEN: \"your_choice_string\"\n    name: <name-of-your-app>-secret-key\n  stringData:\n    SECRET_TOKEN: <your_choice>\n\nReplace <name-of-your-app> with the name value you put in your\nvalues.yaml file.\n\nReplace <your-choice> with strings of your choice (the access-key length\nshould be at least 3, and the secret-key must be at least 8 characters). These\nwill be the SECRET_TOKEN values.\n\nOnce your secret-token.yaml file is set, you can apply it to your Marble\nproject/namespace with this command (assumes you are logged into Marble's CLI):\n\n$ oc apply -f secret-token.yaml\n\nYou should get output similar to this:\n\nsecret \"rprout-test-minio-access-key\" created\nsecret \"rprout-test-minio-secret-key\" created\n\nThese values are picked up as environment variables from the\ntemplates/minio-standalone-deployment.yaml file.\n\nIt is recommended to keep the secret-token.yaml file safe, locally, and not\nin a repository if unencrypted.\n\nInstalling the MinIO Standalone Application\n\nAt this point we are ready to install our minio-standalone chart in our Marble\nproject namespace.\n\nTo list your available project spaces run this command:\n\n$ oc projects\n\nCheck list:\n\nYou have the OC CLI Tool\n\nYou have Helm version 3\n\nYou are logged into Marble, with the OC CLI Tool, and in the correct Marble project.\n\nYou have configured your values.yaml file.\n\nYou have created your MinIO Application's Secret Tokens and applied them to the Marble project you are logged into.\n\n<string>:5: (INFO/1) Duplicate explicit target name: \"slate helm examples repository\".\n\nYou are in the slate_helm_examples/charts directory, within your local copy of the slate helm examples repository.\n\nIf you checked the above off, you can install the MinIO chart, into your Marble project, with this command:\n\n$ helm install <your application name> minio-standalone/ --namespace <your marble project namespace>\n\nReplace <your application name> with the name value in your\nvalues.yaml file.\n\nReplace <your marble project> with your proper Marble project space. This\nis from the output of the oc projects command.\n\nThe output, if successful, should be something similar to this:\n\nNAME: rprout-minio-standalone\nLAST DEPLOYED: Wed May 20 10:35:43 2020\nNAMESPACE: stf007\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n\nThis is also a good time to log into the Marble GUI. You can see\nthe Pod/Deployment/Route/Service/Secrets we created with the chart.\n\nPaths to each in the GUI panel:\n\nWorkloads->Pods\n\nWorkloads->Deployments\n\nWorkloads->Secrets\n\nNetworking->Services\n\nNetworking->Routes\n\nStorage->Persistent Volume Claims (only applicable if you disabled\nuse_olcf_fs in values.yaml)\n\nUse the MinIO Standalone Application\n\nAfter a few minutes, the URL to your MinIO server will become available.\n\nYou can reach it by going to the URL you put for the host value in your\nvalues.yaml file.\n\n<string>:5: (INFO/1) Duplicate explicit target name: \"marble gui\".\n\nYou can also go to it by logging into the Marble GUI. Once logged\nin, go to Networking->Routes and click the URL in the \"Location\" column of your\nMinIO applications row.\n\nYou will be greeted with the NCCS SSO page. Continue through that with your\nnormal NCCS login credentials.\n\nAfter the NCCS login, you will be greeted with MinIO's login page. Here you\nwill enter the access-key and secret-key you created with the\nsecret-token.yaml file.\n\nAt this point, you should be inside the MinIO Browser.\n\nDepending on you how configured your deployment, this could be your NFS or GPFS\nproject space or an isolated volume dedicated/isolated to this MinIO server.\n\nWithin the GUI you can create buckets and upload/download data. If you are\nrunning this on NFS or GPFS the bucket will map to a directory.\n\nNOTE: This application runs as the automation user ID, setup for your\nproject. Anyone who logs into the MinIO app, runs as that user. If you are\nintegrated with an NCCS filesystem, any file uploaded through MinIO will be\nowned by that user. If you plan to run something like this for your OLCF\nproject, it is recommended to create a directory in the $PROJWORK\nspace.\n\nDeleting the MinIO Standalone Application\n\nTo delete this installation, just run this Helm command:\n\n$ helm delete <your-application-name>\n\nYou can get your deployed applications with this Helm command:\n\n$ helm ls"}
{"doc":"mlflow","text":"MLflow\n\n\n\nOverview\n\nMLflow is an open source platform to manage the Machine Learning (ML) lifecycle, including\nexperimentation, reproducibility, deployment, and a central model registry. To\nlearn more about MLflow, please refer to its\ndocumentation.\n\nPrerequisites\n\nIn order to use MLflow on Summit, load the module as shown below:\n\n$ module load workflows\n$ module load mlflow/1.22.0\n\nRun the following command to verify that MLflow is available:\n\n$ mlflow --version\nmlflow, version 1.22.0\n\nHello world!\n\nTo run this MLflow demo on Summit, you will create a directory with two files and then\nsubmit a batch job to LSF from a Summit login node.\n\nFirst, create a directory mlflow-example to contain two files. The first will be\nnamed MLproject:\n\nname: demo\n\nentry_points:\n  main:\n    command: \"python3 demo.py\"\n\nThe second will be named demo.py:\n\nimport mlflow\n\nprint(\"MLflow Version:\", mlflow.version.VERSION)\nprint(\"Tracking URI:\", mlflow.tracking.get_tracking_uri())\n\nwith mlflow.start_run() as run:\n    print(\"Run ID:\", run.info.run_id)\n    print(\"Artifact URI:\", mlflow.get_artifact_uri())\n    with open(\"hello.txt\", \"w\") as f:\n        f.write(\"Hello world!\")\n        mlflow.log_artifact(\"hello.txt\")\n\nFinally, create an LSF batch script called mlflow_demo.lsf, and\nchange abc123 to match your own project identifier:\n\n#BSUB -P abc123\n#BSUB -W 10\n#BSUB -nnodes 1\n\n#BSUB -J mlflow_demo\n#BSUB -o mlflow_demo.o%J\n#BSUB -e mlflow_demo.e%J\n\nmodule load git\nmodule load workflows\nmodule load mlflow/1.22.0\n\njsrun -n 1 mlflow run ./mlflow-example --no-conda\n\nFinally, submit the batch job to LSF by executing the following command from a\nSummit login node:\n\n$ bsub mlflow_demo.lsf\n\nCongratulations! Once the job completes, you will be able to check the standard\noutput files to find the tracking and artifact directories."}
{"doc":"mongodb_service","text":"Deploy MongoDB\n\nMongoDB is a common \"NoSQL\" database. We will be creating a Deployment to run the MongoDB service\nand expose it external to the cluster after setting up authentication. We will also be deploying a\nmanagement Web UI for viewing queries.\n\nPrerequisites\n\nAccess to an allocation in Slate, the NCCS Kubernetes service\n\noc client installed\n\nCLI client is logged into the cluster (oc login https://api.<cluster>.ccs.ornl.gov)\n\nDeploy MongoDB\n\n<string>:18: (INFO/1) Duplicate implicit target name: \"deploy mongodb\".\n\nWe will be deploying MongoDB with a\nStatefulSet. This is a\nspecial kind of deployment controller that is different from a normal Deployment in a few distinct\nways and is primarily meant for for applications that rely on well known names for each pod.\n\nWe will create a Service with the StatefulSet because the StatefulSet controller requires a headless\nservice in order to provide well-known DNS identifiers.\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: mongo\n  labels:\n    app: mongo\nspec:\n  ports:\n  - port: 27017\n    name: mongo\n  selector:\n    app: mongo\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  serviceName: \"mongo\"\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo # has to match .spec.template.metadata.labels\n  template:\n    metadata:\n      labels:\n        app: mongo # has to match .spec.selector.matchLabels\n    spec:\n      terminationGracePeriodSeconds: 10\n      containers:\n      - name: mongo\n        image: mongo\n        env:\n        - name: MONGO_INITDB_ROOT_USERNAME\n          value: admin\n        - name: MONGO_INITDB_ROOT_PASSWORD\n          value: password\n        ports:\n        - containerPort: 27017\n          name: mongo\n        volumeMounts:\n        - name: mongo-store\n          mountPath: /data/db\n  volumeClaimTemplates:\n  - metadata:\n      name: mongo-store\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      resources:\n        requests:\n          storage: 1Gi\n\nYou should replace the value under MONGO_INITDB_ROOT_PASSWORD with something other than password, such as with a randomly generated password for this purpose (just be sure to save this file and the password). While the service will not be accessible outside NCCS, it is good practice to secure your data with a secure password.\n\nCreate a file with the above contents and instantiate the objects in Kubernetes\n\noc apply -f statefulset.yaml\n\nRun some commands to check on the StatefulSet\n\noc describe statefulset mongo\n\noc logs statefulset/mongo\n\noc describe service mongo\n\noc get pods -l app=mongo -o wide\n\noc exec -it mongo-0 /bin/bash\n\nDeploy Mongoku (Management UI)\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mongoku\n  name: mongoku\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongoku\n  template:\n    metadata:\n      labels:\n        app: mongoku\n    spec:\n      containers:\n      - image: huggingface/mongoku\n        name: mongoku\n\nCreate a file with the above contents and instantiate the objects in Kubernetes\n\noc apply -f deployment.yaml\n\nSnippet created with oc create deployment mongoku --image huggingface/mongoku --dry-run -o yaml\n\nRun some commands to check on the Deployment\n\noc describe deployment mongoku\n\noc logs deployment/mongoku\n\noc port-forward deployment/mongoku 3100:3100\n\nThe oc port-forward command runs in the foreground. To test connectivity, one would need to\nuse the MongoDB CLI from a second terminal.\n\nSince we created the mongo service with the StatefulSet, all pods in our namespace will be able\nto resolve that ClusterIP so we can add a server to mongoku with just the service name.\n\nSteps to configure mongoku\n\nNavigate to http://localhost:3100\n\nAdd Server -> \"admin:password@mongo:27017\"\n\nClick \"mongo\"\n\nExpose MongoDB outside of the cluster\n\nWe could use the port forwarding technique but that uses a connection that goes through the API\nserver for the cluster which is not very performant. We will change the Service/mongo object so\nthat it creates a NodePort that we can access from outside of the cluster.\n\n$ oc patch service mongo -p '{\"spec\":{\"type\":\"NodePort\"}}'\n$ oc get service mongo\nNAME    TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE\nmongo   NodePort   172.25.233.185   <none>        27017:32093/TCP   13s\n\nIn this example, the NodePort that was automatically assigned was 32093 which is routing traffic to 27017 on the Service.\n\nWe will also need to add a network rule to allow ingress traffic.\n\nkind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: mongo-allow-external\nspec:\n  podSelector:\n    matchLabels:\n      app: mongo\n  ingress:\n    - {}\n  policyTypes:\n    - Ingress\n\nCreate a file with the above contents and apply the network policy:\n\noc apply -f networkpolicy.yaml\n\nWe can now connect to the db from another host inside of NCCS:\n\nmongo -u admin -p password apps.<cluster>.ccs.ornl.gov:32093\n\ncluster is the Slate cluster (marble, onyx)\n\nThe port number should be the one listed from the service command listed above. It may differ from the example, so be sure to update accordingly.\n\nChange the password to the randomly generated one you created during set up.\n\nTeardown\n\nOnce we have finished, we should remove the resources we created.\n\nWe have to remove the PVC that was created by the StatefulSet\n\noc delete service mongo\noc delete statefulset mongo\noc delete persistentvolumeclaim mongo-store-mongo-0\noc delete deployment mongoku"}
{"doc":"mount_fs","text":"Mount OLCF Filesystems\n\nOLCF shared filesystems can be mounted into a container running in Slate. The mountpoints\nwill be the same as a cluster node. The Kubernetes object will need to be annotated in order\nto get the necessary configuration injected into the container at runtime.\n\n\nThe table provides information on the mount file system (fs) for two clusters, Marble and Onyx, at the Oak Ridge National Laboratory's Center for Computational Sciences (CCS). The first column lists the names of the clusters, while the second column provides the annotation for the fs, which is ccs.ornl.gov/fs for both clusters. The third column displays the specific value for each cluster, which is olcf for Marble and ccsopen for Onyx. The last column lists the different mount points for each cluster, including /ccs/sw, /ccs/home, /ccs/sys, /ccs/proj, and /gpfs/alpine for Marble, and /ccsopen/sw, /ccsopen/home, /ccsopen/proj, and /gpfs/wolf for Onyx. These mount points represent the different directories and file systems that are accessible for users on each cluster. This table provides a clear and organized overview of the mount fs for both clusters, making it easier for users to navigate and access the necessary files and directories. \n\n| Cluster | Annotation | Value | Mounts |\n|---------|------------|-------|--------|\n| Marble  | ccs.ornl.gov/fs | olcf | /ccs/sw, /ccs/home, /ccs/sys, /ccs/proj, /gpfs/alpine |\n| Onyx    | ccs.ornl.gov/fs | ccsopen | /ccsopen/sw, /ccsopen/home, /ccsopen/proj, /gpfs/wolf |\n\nIf you already have a Deployment running you can add the annotation with the client\n\noc annotate deployment web ccs.ornl.gov/fs=olcf\n\nYou cannot annotate an existing pod because the injection happens at pod creation time\n\nAnnotating a pod not managed by a deployment with oc annotate pod test ccs.ornl.gov/fs=olcf\nwill not work. Instead delete the pod and add the annotation to the metadata and recreate it.\n\nYou can also add the annotations to any workload object's YAML such as a Pod, Deployment,\nor DeploymentConfig.\n\nkind: Deployment\nmetadata:\n  annotations:\n    ccs.ornl.gov/fs: olcf\n---\nkind: DeploymentConfig\nmetadata:\n  annotations:\n    ccs.ornl.gov/fs: olcf\n---\nkind: Pod\nmetadata:\n  annotations:\n    ccs.ornl.gov/fs: olcf\n\nFull example of a deployment mounting the OLCF shared filesystems:\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-fs\n  annotations:\n    ccs.ornl.gov/fs: olcf\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: test-fs\n  template:\n    metadata:\n      labels:\n        app: test-fs\n    spec:\n      containers:\n      - name: test-fs\n        image: busybox:latest\n        args:\n        - cat\n        stdin: true\n        stdinOnce: true\n\nThere are no requirements in the container image in order to mount OLCF filesystems"}
{"doc":"networkpolicy","text":"Network Policies\n\n\n\nNetwork policies are specifications of how groups of pods are allowed to communicate with each other and other network endpoints. A pod is selected in a Network Policy based on a label so the Network Policy can define rules to specify traffic to that pod.\n\nIsolation Explanation\n\nWhen you create a namespace in Slate with the Quota Dashboard some Network Policies are added to your newly created namespace. This means that pods inside your project are isolated from connections not defined in these Network Policies.\n\nNetwork Policies do not conflict, they are additive. This means that if two policies match a pod the pod will be restricted to what is allowed by the union of those policies' ingress/egress rules.\n\nCreating a Network Policy\n\nIn the GUI\n\nTo create a Network policy using the GUI click the Networking tab followed by the Network Policy tab:\n\nCreating Network Policies\n\nThis will place you in an editor with some boiler plate YAML. From here you can define the network policy that you need for your namespace. Below is an example Network Policy that you would use to allow all external traffic to access a nodePort in your namespace.\n\nkind: NetworkPolicy\napiVersion: networking.k8s.io/v1\nmetadata:\n  name: web-allow-external\nspec:\n  podSelector:\n    matchLabels:\n      key: value\n  ingress:\n    - {}\n  policyTypes:\n    - Ingress\n\nA blank field matches all. The above example matches on all from/port combinations. Similarly, a blank pod selector would match on all pods in the namespace.\n\nThe key value pair, or label, under spec.podSelector.matchLabels will need to match exactly to the pod in your namespace that\nthe policy is for example the above NetworkPolicy would match pods with these labels set:\n\napiVersion: v1\nKind: Pod\nmetadata:\n  labels:\n    key: value\n...\n\nUsing the CLI\n\nTo view the Network policies in your namespace you can run:\n\noc get networkpolicy -n YOUR_NAMESPACE\n\nto get the name of the network policy and then:\n\noc get networkpolicy NETWORKPOLICY_NAME -o yaml\n\nto view object's YAML.\n\nTo create a Network Policy, define one in YAML similar to the output of the previous command and run:\n\noc create -f FILENAME\n\nFor a more complex example of a Network Policy please see the Kubernetes doc.\n\nA full reference of Network Policies can be found here."}
{"doc":"nginx_hello_world","text":"Deploy NGINX with Hello World\n\nOne of the simplest use cases for Kubernetes is running a web server. We will walk through the\nsteps needed to set up an NGINX web server on OpenShift that serves a static html file. This\nexample assumes that you have an allocation on the cluster.\n\nFirst make sure that you are in the correct project:\n\noc project <YOUR_PROJECT>\n\nIn the next part of this we will be creating a few objects needed to run NGINX. The objects will be\nsaved into a file and then added to the cluster with the command:\n\noc create -f <FILENAME>\n\nThe first object we wish to create is our BuildConfig. This is the object that defines how we build\nour NGINX image.\n\nBefore we create the BuildConfig we should give it a way to access two files before they are\npulled into the build pod. The files are my index.html and my nginx.conf file. You can get them\ninto your build pod however you wish, for simplicity I chose to add them to a public git repository\nand wget them. The index.html and nginx.conf file are defined respectively as:\n\n<h1>Hello, World!</h1>\n\nuser nginx;\nworker_processes auto;\nerror_log /tmp/error.log;\npid /tmp/nginx.pid;\n\n# Load dynamic modules. See /usr/share/nginx/README.dynamic.\ninclude /usr/share/nginx/modules/*.conf;\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                      '$status $body_bytes_sent \"$http_referer\" '\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    access_log  /tmp/access.log  main;\n\n    client_body_temp_path /tmp/nginx 1 2;\n    proxy_temp_path /tmp/nginx-proxy;\n    fastcgi_temp_path /tmp/nginx-fastcgi;\n    uwsgi_temp_path /tmp/nginx-uwsgi;\n    scgi_temp_path /tmp/nginx-scgi;\n\n    sendfile            on;\n    tcp_nopush          on;\n    tcp_nodelay         on;\n    keepalive_timeout   65;\n    types_hash_max_size 2048;\n\n    include             /etc/nginx/mime.types;\n    default_type        application/octet-stream;\n\n    # Load modular configuration files from the /etc/nginx/conf.d directory.\n    # See http://nginx.org/en/docs/ngx_core_module.html#include\n    # for more information.\n    include /etc/nginx/conf.d/*.conf;\n\n    server {\n        listen       8080 default_server;\n        listen       [::]:8080 default_server;\n        server_name  _;\n        root         /usr/share/nginx/html;\n\n        # Load configuration files for the default server block.\n        include /etc/nginx/default.d/*.conf;\n\n        location / {\n        }\n\n        error_page 404 /404.html;\n            location = /40x.html {\n        }\n\n        error_page 500 502 503 504 /50x.html;\n            location = /50x.html {\n        }\n    }\n}\n\nThe NGINX configuration file is completely standard except I changed the listen port to be\nfrom 80 to 8080 since the server will be running as a non-root user. The Route that we will add\nlater on will redirect traffic coming in on port 80 to our server running on port 8080.\n\nThe BuildConfig, the following should be placed inside a buildconfig.yaml file:\n\napiVersion: build.openshift.io/v1\nkind: BuildConfig\nmetadata:\n    name: nginx-hello-world\nspec:\n  runPolicy: Serial\n  source:\n    dockerfile: |\n      FROM rockylinux:latest\n      RUN yum install -y epel-release && \\\n          yum install -y nginx\n\n      COPY index.html /usr/share/nginx/html\n      COPY nginx.conf /etc/nginx/nginx.conf\n\n      CMD /usr/sbin/nginx -g 'daemon off;'\n\n  strategy:\n    type: Docker\n    dockerStrategy:\n      noCache: false\n  output:\n    to:\n      kind: ImageStreamTag\n      name: \"nginx-hello-world:latest\"\n\nWe create the BuildConfig object with:\n\noc create -f buildconfig.yaml\n\nWe now need to create an ImageStream with the same name as our build. This will create a place for\nour BuildConfig to push the image to and our Deployment to pull the image from during a deployment.\n\noc create imagestream nginx-hello-world\n\nWe can now start a build to get our NGINX image:\n\noc start-build nginx-hello-world --from-dir=./ --follow\n\nThis should spin up a build pod that produces a nginx-hello-world image while also tailing the\nlogs. Once the build completes then we should have an image pushed to our ImageStream:\n\noc get imagestream nginx-hello-world\nNAME                DOCKER REPO                                                         TAGS      UPDATED\nnginx-hello-world   image-registry.openshift-image-registry.svc:5000/YOUR_NAMESPACE/nginx-hello-world   latest    3 minutes ago\n\nIf all goes well it is time to create the Deployment. The following should be placed inside a deployment.yaml file:\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-hello-world\n  labels:\n    app: nginx-hello-world\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-hello-world\n  template:\n    metadata:\n      labels:\n        app: nginx-hello-world\n    spec:\n      containers:\n        - name: nginx\n          image: \"image-registry.openshift-image-registry:5000/YOUR_NAMESPACE/nginx-hello-world\"\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          tty: true\n          stdin: true\n          serviceAccount: default\n      terminationGracePeriodSeconds: 5\n\nIn the Deployment make sure to change the YOUR_NAMESPACE string.\n\nCreate the Deployment object:\n\noc create -f deployment.yaml\n\nView the deployment:\n\noc get deployment nginx-hello-world\nNAME                DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx-hello-world   3         3         3            3           9s\n\nYou should see Desired: 3 and Current: 3\n\nAfter the deployment has been created it will spin up a pod running NGINX but we need to get\ntraffic from outside the cluster to the pod so that we can display the hello world.\n\nThe Service object will create a Cluster IP address that will direct traffic to any pod in our\ndeployment that is considered by the cluster to be ready. The following should be placed inside a service.yaml file:\n\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: nginx-hello-world\n  name: nginx-hello-world\nspec:\n  ports:\n  - name: nginx\n    port: 80\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: nginx-hello-world\n  sessionAffinity: None\n  type: ClusterIP\n\nCreate the Service object:\n\noc create -f service.yaml\n\nThe Route object will set up the cluster load balancers to accept traffic for a specified hostname\nand direct the traffic to the service which will in turn direct the traffic to any pod into our\ndeployment that is considered by the cluster to be ready.\n\nIf you do not set a hostname on the route, one will be automatically chosen. We will use this\nmechanism for this demo but you can choose any hostname as long as it ends with\napps.CLUSTER.ccs.ornl.gov where CLUSTER is one marble or onyx.\n\nThe following should be placed inside a route.yaml file:\n\napiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: nginx-hello-world\nspec:\n  # hostname: foo.apps.CLUSTER.ccs.ornl.gov\n  port:\n    targetPort: nginx\n  tls:\n    insecureEdgeTerminationPolicy: Redirect\n    termination: edge\n  to:\n    kind: Service\n    name: nginx-hello-world\n    weight: 100\n  wildcardPolicy: None\n\nCreate the Route object:\n\noc create -f route.yaml\n\nWe need to get the route so that we can see the generated hostname.\n\noc get route nginx-hello-world\nNAME                HOST/PORT                                             PATH      SERVICES            PORT      TERMINATION     WILDCARD\nnginx-hello-world   nginx-hello-world-test.apps.granite.ccs.ornl.gov                nginx-hello-world   nginx     edge/Redirect   None\n\nNow if you access the hostname that you set up with the route from a browser you should see the\ntext \"Hello World\".\n\nOnce you are finished you can remove the resources that were created for this demo:\n\noc delete buildconfig nginx-hello-world\noc delete imagestream nginx-hello-world\noc delete deployment nginx-hello-world\noc delete service nginx-hello-world\noc delete route nginx-hello-world"}
{"doc":"nodeport","text":"NodePorts\n\n\n\nA NodePort reserves a port across all nodes of the cluster. This port routes traffic to a service, which points to the pods that match the service's label selector.\n\nNodePorts are given in the 30000-32767 range. These are ports you can use from outside the cluster to access resources inside of OpenShift.\n\nFor the Openshift clusters you will additionally need to create a network policy <slate_network_policies> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#network policy <slate_network_policies>> file to allow external traffic into your namespace.\n\nConfiguring a Service NodePort\n\nCLI\n\nFor example, let's look at service that was created in the slate_services <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_services> document. This document assumes that it was deployed to the my-project project.\n\nIf you run oc get services, you should see your service in the list.\n\n$ oc get services\nNAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)     AGE\nmy-service            ClusterIP   172.25.170.246   <none>        8080/TCP    8s\n\nThen, get some information about the service with oc describe service my-service.\n\n$ oc describe services my-service\nName:              my-service\nNamespace:         my-project\nLabels:            <none>\nAnnotations:       <none>\nSelector:          name=my-app\nType:              ClusterIP\nIP:                172.25.170.246\nPort:              <unset>  8080/TCP\nTargetPort:        8080/TCP\nEndpoints:         <none>\nSession Affinity:  None\nEvents:            <none>\n\nNote the Type under the service is ClusterIP. This is the default value for services, but we want to expose it with a NodePort.\n\nIn order to expose the service with a NodePort, we need to change spec.type from ClusterIP to NodePort.\nThis can be done with oc patch.\n\noc patch service my-service -p '{\"spec\": {\"type\": \"NodePort\"}}'\n\nThen, you can run oc describe service my-service again to see if your change has been made.\n\n$ oc describe services my-service\nName:                     my-service\nNamespace:                my-project\nLabels:                   <none>\nAnnotations:              <none>\nSelector:                 name=my-app\nType:                     NodePort\nIP:                       172.25.170.246\nPort:                     <unset>  8080/TCP\nTargetPort:               8080/TCP\nNodePort:                 <unset>  30298/TCP\nEndpoints:                <none>\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   <none>\n\nNote that a NodePort value will automatically be given by the service controller.\n\nYour service can then be accessed by the scheme apps.{cluster}.ccs.ornl.gov:{nodePort}.\n\nIn this example, if the service was running on marble, I could access it with apps.marble.ccs.ornl.gov:30298"}
{"doc":"nvidia-rapids","text":"NVIDIA RAPIDS\n\nOverview\n\nRAPIDS is a suite of libraries to execute end-to-end data science and analytics pipelines on GPUs. RAPIDS utilizes NVIDIA CUDA primitives for low-level compute optimization through user-friendly Python interfaces. An overview of the RAPIDS libraries available at OLCF is given next.\n\ncuDF\n\ncuDF is a Python GPU DataFrame library (built on the Apache Arrow columnar memory format) for loading, joining, aggregating, filtering, and otherwise manipulating data.\n\ncuML\n\ncuML is a suite of libraries that implement machine learning algorithms and mathematical primitives functions that share compatible APIs with other RAPIDS projects.\n\ncuGraph\n\ncuGraph is a GPU accelerated graph analytics library, with functionality like NetworkX, which is seamlessly integrated into the RAPIDS data science platform.\n\ndask-cuda\n\ndask-cuda extends Dask where it is necessary to scale up and scale out RAPIDS workflows.\n\ncuCIM\n\ncuCIM is a GPU accelerated n-dimensional image processing and image I/O library. It has a similar API to scikit-image.\n\nIn addition to NVIDIA supported libraries, the RAPIDS modules also includes:\n\nCuPy\n\nPreferred Networks' CuPy is a NumPy-compatible, open source mathematical library. While CuPy is not a library under the RAPIDS framework, it is compatible with RAPIDS and dask-cuda for memory management and multi-GPU, multi-node workload distribution.\n\nComplete documentation is available at the official RAPIDS documentation and CuPy's documentation websites.\n\nBlazingSQL\n\n<string>:3: (INFO/1) Duplicate implicit target name: \"blazingsql\".\n\nBlazingSQL is an open source community effort that provides a GPU accelerated and distributed SQL engine in Python. No database needed, BlazingSQL can operate directly on tabular data. Full documentation is available at the official BlazingSQL documentation website.\n\nGetting Started\n\nRAPIDS is available at OLCF via Jupyter and via module load command in Summit.\n\nWe recommend the use of Jupyter in example situations like:\n\nPython script preparation.\n\nWorkload fits comfortably on a single GPU (NVIDIA V100 16GB).\n\nInteractive capabilities needed.\n\nwhereas Summit is recommended in example situations like:\n\nLarge workloads.\n\nLong runtimes on Summit's high memory nodes.\n\nYour Python script has support for multi-gpu/multi-node execution via dask-cuda.\n\nYour Python script is single GPU but requires simultaneous job steps.\n\nRAPIDS on Jupyter\n\nRAPIDS is provided in Jupyter following  these instructions.\n\nNote that Python scripts prepared on Jupyter can be deployed on Summit if they use the same RAPIDS version. Use !jupyter nbconvert --to script my_notebook.ipynb to convert notebook files to Python scripts.\n\nRAPIDS on Summit\n\nRAPIDS is provided on Summit through the module load command:\n\nmodule load ums\nmodule load ums-gen119\nmodule load nvidia-rapids/21.08\n\nDue different dependecies, cuCIM is available on Summit as a separate module using the next commands:\n\nmodule load ums\nmodule load ums-gen119\nmodule load nvidia-rapids/cucim_21.08\n\nThe RAPIDS and cuCIM modules loads gcc/9.3.0 and cuda/11.0.3 modules. For a complete list of available packages, use conda list command.\n\nAfter Summit's OS upgrade on August 7th, 2021. Older RAPIDS modules were deprecated.\n\nRAPIDS basic execution\n\nAs an example, the following LSF script will run a single-GPU RAPIDS script in one Summit node:\n\n#BSUB -P <PROJECT>\n#BSUB -W 0:05\n#BSUB -nnodes 1\n#BSUB -q batch\n#BSUB -J rapids_test\n#BSUB -o rapids_test_%J.out\n#BSUB -e rapids_test_%J.out\n\nmodule load ums\nmodule load ums-gen119\nmodule load nvidia-rapids/21.08\n\njsrun --nrs 1 --tasks_per_rs 1 --cpu_per_rs 1 --gpu_per_rs 1 --smpiargs=\"-disable_gpu_hooks\" \\\n      python $CONDA_PREFIX/examples/cudf/cudf_test.py\n\nFrom the jsrun options, note the --smpiargs=\"-disable_gpu_hooks\" flag is being used. Disabling gpu hooks allows non Spectrum MPI codes run with CUDA.\n\nNote the \"RAPIDS basic execution\" option is for illustrative purposes and not recommended to run RAPIDS on Summit since it underutilizes resources. If your RAPIDS code is single GPU, consider Jupyter or the concurrent job steps option.\n\nSimultaneous job steps with RAPIDS\n\n<string>:3: (INFO/1) Duplicate explicit target name: \"simultaneous job steps\".\n\nIn cases when a set of time steps need to be processed by single-GPU RAPIDS codes and each time step fits comfortably in GPU memory, it is recommended to execute simultaneous job steps.\n\nThe following script provides a general pattern to run job steps simultaneously with RAPIDS:\n\n#BSUB -P <PROJECT>\n#BSUB -W 0:05\n#BSUB -nnodes 1\n#BSUB -q batch\n#BSUB -J rapids_test\n#BSUB -o rapids_test_%J.out\n#BSUB -e rapids_test_%J.out\n\nmodule load ums\nmodule load ums-gen119\nmodule load nvidia-rapids/21.08\n\njsrun --nrs 1 --tasks_per_rs 1 --cpu_per_rs 1 --gpu_per_rs 1 --smpiargs=\"-disable_gpu_hooks\" \\\n      python /my_path/my_rapids_script.py dataset_part01 &\njsrun --nrs 1 --tasks_per_rs 1 --cpu_per_rs 1 --gpu_per_rs 1 --smpiargs=\"-disable_gpu_hooks\" \\\n      python /my_path/my_rapids_script.py dataset_part02 &\njsrun --nrs 1 --tasks_per_rs 1 --cpu_per_rs 1 --gpu_per_rs 1 --smpiargs=\"-disable_gpu_hooks\" \\\n      python /my_path/my_rapids_script.py dataset_part03 &\n...\nwait\n\nBe aware of different OLCF's queues and scheduling policies to make best use of regular and high memory Summit nodes.\n\nDistributed RAPIDS execution\n\nPreliminaries\n\nRunning RAPIDS multi-gpu/multi-node workloads requires a dask-cuda cluster. Setting up a dask-cuda cluster on Summit requires two components:\n\ndask-scheduler.\n\ndask-cuda-workers.\n\nOnce the dask-cluster is running, the RAPIDS script should perform four main tasks. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.\n\nReference of multi-gpu/multi-node operation with cuDF, cuML, cuGraph is available in the next links:\n\n10 Minutes to cuDF and Dask-cuDF.\n\ncuML's Multi-Node, Multi-GPU Algorithms.\n\nMulti-GPU with cuGraph.\n\nLaunching the dask-scheduler and dask-cuda-workers\n\nThe following script will run a dask-cuda cluster on two compute nodes, then it executes a Python script.\n\n#BSUB -P <PROJECT>\n#BSUB -W 0:05\n#BSUB -alloc_flags \"gpumps smt4 NVME\"\n#BSUB -nnodes 2\n#BSUB -J rapids_dask_test_tcp\n#BSUB -o rapids_dask_test_tcp_%J.out\n#BSUB -e rapids_dask_test_tcp_%J.out\n\nPROJ_ID=<project>\n\nmodule load ums\nmodule load ums-gen119\nmodule load nvidia-rapids/21.08\n\nSCHEDULER_DIR=$MEMBERWORK/$PROJ_ID/dask\nWORKER_DIR=/mnt/bb/$USER\n\nif [ ! -d \"$SCHEDULER_DIR\" ]\nthen\n    mkdir $SCHEDULER_DIR\nfi\n\nSCHEDULER_FILE=$SCHEDULER_DIR/my-scheduler.json\n\necho 'Running scheduler'\njsrun --nrs 1 --tasks_per_rs 1 --cpu_per_rs 1 --smpiargs=\"-disable_gpu_hooks\" \\\n      dask-scheduler --interface ib0 \\\n                     --scheduler-file $SCHEDULER_FILE \\\n                     --no-dashboard --no-show &\n\n#Wait for the dask-scheduler to start\nsleep 10\n\njsrun --rs_per_host 6 --tasks_per_rs 1 --cpu_per_rs 2 --gpu_per_rs 1 --smpiargs=\"-disable_gpu_hooks\" \\\n      dask-cuda-worker --nthreads 1 --memory-limit 82GB --device-memory-limit 16GB --rmm-pool-size=15GB \\\n                       --interface ib0 --scheduler-file $SCHEDULER_FILE --local-directory $WORKER_DIR \\\n                       --no-dashboard &\n\n#Wait for WORKERS\nsleep 10\n\nWORKERS=12\n\npython -u $CONDA_PREFIX/examples/dask-cuda/verify_dask_cuda_cluster.py $SCHEDULER_FILE $WORKERS\n\nwait\n\n#clean DASK files\nrm -fr $SCHEDULER_DIR\n\necho \"Done!\"\n\nNote twelve dask-cuda-workers are executed, one per each available GPU, --memory-limit is set to 82 GB and  --device-memory-limit is set to 16 GB. If using Summit's high-memory nodes --memory-limit can be increased and setting --device-memory-limit to 32 GB  and --rmm-pool-size to 30 GB or so is recommended. Also note it is recommeded to wait some seconds for the dask-scheduler and dask-cuda-workers to start.\n\nAs mentioned earlier, the RAPIDS code should perform four main tasks as shown in the following script. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.\n\nimport sys\nfrom dask.distributed import Client\n\ndef disconnect(client, workers_list):\n    client.retire_workers(workers_list, close_workers=True)\n    client.shutdown()\n\nif __name__ == '__main__':\n\n    sched_file = str(sys.argv[1]) #scheduler file\n    num_workers = int(sys.argv[2]) # number of workers to wait for\n\n    # 1. Connects to the dask-cuda-cluster\n    client = Client(scheduler_file=sched_file)\n    print(\"client information \",client)\n\n    # 2. Blocks until num_workers are ready\n    print(\"Waiting for \" + str(num_workers) + \" workers...\")\n    client.wait_for_workers(n_workers=num_workers)\n\n\n    workers_info=client.scheduler_info()['workers']\n    connected_workers = len(workers_info)\n    print(str(connected_workers) + \" workers connected\")\n\n    # 3. Do computation\n    # ...\n    # ...\n\n    # 4. Shutting down the dask-cuda-cluster\n    print(\"Shutting down the cluster\")\n    workers_list = list(workers_info)\n    disconnect (client, workers_list)\n\nSetting up Custom Environments\n\nThe RAPIDS environment is read-only. Therefore, users cannot install any additional packages that may be needed. If users need any additional conda or pip packages, they can clone the RAPIDS environment into their preferred directory and then add any packages they need.\n\nCloning the RAPIDS environment can be done with the next commands:\n\nmodule load ums\nmodule load ums-gen119\nmodule load nvidia-rapids/21.08\n\nconda create --clone nvrapids_21.08_gcc_9.3.0 -p <my_environment_path>\n\nTo activate the new environment you should still load the RAPIDS module first. This will ensure that all of the conda settings remain the same.\n\nmodule load ums\nmodule load ums-gen119\nmodule load nvidia-rapids/21.08\n\nconda activate <my_environment_path>\n\nBlazingSQL Distributed Execution\n\nRunning BlazingSQL multi-gpu/multi-node workloads requires a dask-cuda cluster as explained earlier.\n\nThe following script will run a dask-cuda cluster on two compute nodes, then it executes a Python script running BlazingSQL.\n\n#BSUB -P ABC123\n#BSUB -W 0:05\n#BSUB -alloc_flags \"gpumps smt4 NVME\"\n#BSUB -nnodes 2\n#BSUB -q batch\n#BSUB -J bsql_dask\n#BSUB -o bsql_dask_%J.out\n#BSUB -e bsql_dask_%J.out\n\nPROJ_ID=abc123\n\nmodule load ums\nmodule load ums-gen119\nmodule load nvidia-rapids/21.08\n\nSCHEDULER_DIR=$MEMBERWORK/$PROJ_ID/dask\nBSQL_LOG_DIR=$MEMBERWORK/$PROJ_ID/bsql\nWORKER_DIR=/mnt/bb/$USER\n\nmkdir -p $SCHEDULER_DIR\nmkdir -p $BSQL_LOG_DIR\n\nSCHEDULER_FILE=$SCHEDULER_DIR/my-scheduler.json\n\necho 'Running scheduler'\njsrun --nrs 1 --tasks_per_rs 1 --cpu_per_rs 2 --smpiargs=\"-disable_gpu_hooks\" \\\n      dask-scheduler --interface ib0 --scheduler-file $SCHEDULER_FILE \\\n                     --no-dashboard --no-show &\n\n#Wait for the dask-scheduler to start\nsleep 10\n\njsrun --rs_per_host 6 --tasks_per_rs 1 --cpu_per_rs 2 --gpu_per_rs 1 --smpiargs=\"-disable_gpu_hooks\" \\\n      dask-cuda-worker --nthreads 1 --memory-limit 82GB --device-memory-limit 16GB --rmm-pool-size=15GB \\\n                       --death-timeout 60  --interface ib0 --scheduler-file $SCHEDULER_FILE --local-directory $WORKER_DIR \\\n                       --no-dashboard &\n\n#Wait for WORKERS\nsleep 10\n\nexport BSQL_BLAZING_LOGGING_DIRECTORY=$BSQL_LOG_DIR\nexport BSQL_BLAZING_LOCAL_LOGGING_DIRECTORY=$BSQL_LOG_DIR\n\npython -u $CONDA_PREFIX/examples/blazingsql/bsql_test_multi.py $SCHEDULER_FILE\n\nwait\n\n#clean LOG files\nrm -fr $SCHEDULER_DIR\nrm -fr $BSQL_LOG_DIR\n\nBSQL_* environment variables defines the behavior of BlazingContext. Refer to BlazingContext options for a full description.\n\nOnce the dask-cluster is running, the BlazingSQL script should perform five main tasks:\n\nCreate a dask client to connect to the dask-scheduler.\n\nCreate a BlazingContext that takes in the dask client.\n\nCreate some tables.\n\nRun queries.\n\nShutting down the dask-cuda-cluster.\n\nThis is exemplified in the next script:\n\nimport sys\nimport cudf\nfrom dask.distributed import Client\nfrom blazingsql import BlazingContext\n\n\ndef disconnect(client, workers_list):\n    client.retire_workers(workers_list, close_workers=True)\n    client.shutdown()\n\nif __name__ == '__main__':\n\n    sched_file = str(sys.argv[1]) #scheduler file\n\n    # 1. Create a dask client to connect to the dask-scheduler\n    client = Client(scheduler_file=sched_file)\n    print(\"client information \",client)\n\n    workers_info=client.scheduler_info()['workers']\n    connected_workers = len(workers_info)\n    print(str(connected_workers) + \" workers connected\")\n\n    # 2. Create a BlazingContext that takes in the dask client\n    # you want to set `allocator='existing'` if you are launching the dask-cuda-worker with an rmm memory pool\n    bc = BlazingContext(dask_client = client, network_interface='ib0', allocator='existing')\n\n    # 3. Create some tables\n    bc.create_table('my_table','/data/file*.parquet')\n\n    # 4. Run queries\n    ddf = bc.sql('select count(*) from my_table')\n    print(ddf.head())\n\n    # 5. Shutting down the dask-cuda-cluster\n    print(\"Shutting down the cluster\")\n    workers_list = list(workers_info)\n    disconnect (client, workers_list)\n\nConsult this example for single gpu usage. Then, follow RAPIDS' basic or simultaneous execution LFS scripts."}
{"doc":"olcf_gpu_hackathons","text":"I used html for the section headings to avoid individual entries in the associated menu (TP)\n\n\n\nOLCF GPU Hackathons\n\nEach year, the Oak Ridge Leadership Computing Facility (OLCF) works with our vendor partners to organize a series of GPU hackathons at a number of host\nlocations around the world.\n\n<p style=\"font-size:20px\"><b>What is a GPU hackathon?</b></p>\n\nA GPU hackathon is a multi-day coding event in which teams of developers port their own applications to run on GPUs, or optimize their applications that already\nrun on GPUs. Each team consists of 3 or more developers who are intimately familiar with (some part of) their application, and they work alongside 1 or\nmore mentors with GPU programming expertise. Our mentors come from universities, national laboratories, supercomputing centers, and industry partners.\n\nThere are a variety of programming models available to program GPUs (e.g. CUDA, OpenACC, OpenMP offloading, etc.) and you are welcome to use any of them at these events.\n\n<p style=\"font-size:20px\"><b>Why participate?</b></p>\n\nIf you want/need to get your code running (or optimized) on a GPU-accelerated system, these hackathons offer a unique opportunity to set aside the time, surround yourself with experts in the field, and push toward your development goals. By the end of the event, each team should have part of their code running (or more optimized) on GPUs, or at least have a clear roadmap of how to get there.\n\n<p style=\"font-size:20px\"><b>Target audience</b></p>\n\nWe are looking for teams of 3-6 developers with a scalable** application to port (or optimize on) GPUs. Collectively, the team should know the application\nintimately.\n\n** We say scalable here because we're typically looking for codes intended to run on multiple nodes (e.g. MPI-enabled), although porting/optimizing such codes on a single node during the events is encouraged whenever possible.\n\n<p style=\"font-size:20px\"><b>Virtual hackathon format</b></p>\n\nTypically, these hackathons are in-person events, where each team (app developers + mentors) sits at their own round table in a single large conference room. This structure allows teams to hack away on their own codes, but also to interact (ask questions, give advice, etc.) with members/mentors from other teams when needed.\n\nTo recreate this environment in a virtual setting, we will be using Zoom + Slack. Zoom will be used as the main online tool due to its breakout room capabilities; there will be a single Zoom session, where the main room will be used for presentations, and breakout rooms will be used by individual teams (for screen sharing and verbal communication). We will also set up a Slack workspace for communication between all participants, and individual team channels for communication within teams (chat, sharing code snippets, etc.).\n\n<p style=\"font-size:20px\"><b>Ok, so how do I attend?</b></p>\n\nFirst, you must decide which event you'd like to attend (use link below to find a hackathon whose dates make sense for your team), and then submit a short proposal form describing your application and team. The organizing committee will then review all proposals after the call for that event closes and select the teams they believe are best suited for the event.\n\nPlease visit openhackathons.org/s/events-overview to see the current list of events (new ones added throughout the year) and their proposal deadlines. To submit a proposal, click on the event you'd like to attend and submit the form.\n\nThe OLCF-supported events are a subset of a larger number of Open hackathons organized around the world. Look for hackathons with the OLCF logo to find events supported by OLCF.\n\n<p style=\"font-size:20px\"><b>Want to be a mentor?</b></p>\n\nIf you would like more information about how you can volunteer to mentor a team at an upcoming Open/GPU hackathon, please visit our Become a Mentor page.\n\n<p style=\"font-size:20px\"><b>Who can I contact with questions?</b></p>\n\nIf you have any questions about the OLCF GPU Hackathon Series, please contact OLCF Help at (help@olcf.ornl.gov)."}
{"doc":"olcf_policy_guide","text":"OLCF Policy Guides\n\nOLCF Acknowledgement\n\nUsers should acknowledge the OLCF in all publications and presentations\nthat speak to work performed on OLCF resources:\n\nThis research used resources of the Oak Ridge Leadership Computing\nFacility at the Oak Ridge National Laboratory, which is supported by the\nOffice of Science of the U.S. Department of Energy under Contract No.\nDE-AC05-00OR22725.\n\n\n\nSoftware Requests\n\nTo request software installation, please contact help@olcf.ornl.gov with a description of the software, including the following details:\n\nSoftware name.\n\nDescription of the software and its purpose. Is it export controlled?\n\nHow the software is obtained.\n\nExplanation of why the software is needed. If OLCF provides equivalent software, what is different about this software? If requesting an upgrade, describe new features or bug fixes.\n\nWho will be using this software? Approximately how many people will be using it? If possible, list individual users of this software.\n\nSpecial Requests and Policy Exemptions\n\nTo request exemption to certain system policies, contact help@olcf.ornl.gov with an overview of the exemption you need. Example requests include:\n\nRelaxed queue limits for one or more jobs (longer walltime, higher priority)\n\nSystem reservation (a dedicated set of nodes at a specific date/time)\n\nIncreased disk quota\n\nPurge exemption for User/Group/World Work areas\n\nSpecial requests are reviewed weekly by the OLCF Resource Utilization\nCouncil. Please contact help@olcf.ornl.gov for more information.\n\n\n\nComputing Policy\n\nThis details an official policy of the OLCF, and must be\nagreed to by the following persons as a condition of access to and use\nof OLCF computational resources:\n\nPrincipal Investigators (Non-Profit)\n\nPrincipal Investigators (Industry)\n\nAll Users\n\nTitle: Computing Policy Version: 12.10\n\nComputer Use\n\nComputers, software, and communications systems provided by the OLCF are\nto be used for work associated with and within the scope of the approved\nproject. The use of OLCF resources for personal or non-work-related\nactivities is prohibited. All computers, networks, E-mail, and storage\nsystems are property of the United States Government. Any misuse or\nunauthorized access is prohibited, and is subject to criminal and civil\npenalties. OLCF systems are provided to our users without any warranty.\nOLCF will not be held liable in the event of any system failure or data\nloss or corruption for any reason including, but not limited to:\nnegligence, malicious action, accidental loss, software errors, hardware\nfailures, network losses, or inadequate configuration of any computing\nresource or ancillary system.\n\nData Use\n\nProhibited Data\n\nThe OLCF computer systems are operated as research systems and only\ncontain data related to scientific research and do not contain\npersonally identifiable information (data that falls under the Privacy\nAct of 1974 5U.S.C. 552a). Use of OLCF resources to store, manipulate,\nor remotely access any national security information is strictly\nprohibited. This includes, but is not limited to: classified\ninformation, unclassified controlled nuclear information (UCNI), naval\nnuclear propulsion information (NNPI), the design or development of\nnuclear, biological, or chemical weapons or any weapons of mass\ndestruction. Authors/generators/owners of information are responsible\nfor its correct categorization as sensitive or non-sensitive. Owners of\nsensitive information are responsible for its secure handling,\ntransmission, processing, storage, and disposal on OLCF systems.\nPrincipal investigators, users, or project delegates that use OLCF\nresources, or are responsible for overseeing projects that use OLCF\nresources, are strictly responsible for knowing whether their project\ngenerates any of these prohibited data types or information that falls\nunder Export Control. For questions, contact help@nccs.gov.\n\nConfidentiality, Integrity, and Availability\n\nThe OLCF systems provide protections to maintain the confidentiality,\nintegrity, and availability of user data. Measures include the\navailability of file permissions, archival systems with access control\nlists, and parity and CRC checks on data paths and files. It is the\nuser’s responsibility to set access controls appropriately for the data.\nIn the event of system failure or malicious actions, the OLCF makes no\nguarantee against loss of data or that a user’s data can be accessed,\nchanged, or deleted by another individual. It is the user’s\nresponsibility to insure the appropriate level of backup and integrity\nchecks on critical data and programs.\n\nData Modification/Destruction\n\nUsers are prohibited from taking unauthorized actions to intentionally\nmodify or delete information or programs.\n\nData Retention\n\nThe OLCF reserves the right to remove any data at any time and/or\ntransfer data to other users working on the same or similar project once\na user account is deleted or a person no longer has a business\nassociation with the OLCF. After a sensitive project has ended or has\nbeen terminated, all data related to the project must be purged from all\nOLCF computing resources within 30 days.\n\nSoftware Use\n\nAll software used on OLCF computers must be appropriately acquired and\nused according to the appropriate software license agreement.\nPossession, use, or transmission of illegally obtained software is\nprohibited. Likewise, users shall not copy, store, or transfer\ncopyrighted software, except as permitted by the owner of the copyright.\nOnly export-controlled codes approved by the Export Control Office may\nbe run by parties with sensitive data agreements.\n\nMalicious Software\n\nUsers must not intentionally introduce or use malicious software such as\ncomputer viruses, Trojan horses, or worms.\n\nReconstruction of Information or Software\n\nUsers are not allowed to reconstruct information or software for which\nthey are not authorized. This includes but is not limited to any reverse\nengineering of copyrighted software or firmware present on OLCF\ncomputing resources.\n\nUser Accountability\n\nUsers are accountable for their actions and may be held accountable to\napplicable administrative or legal sanctions.\n\nMonitoring and Privacy\n\nUsers are advised that there is no expectation of privacy of your\nactivities on any system that is owned by, leased or operated by\nUT-Battelle on behalf of the U.S. Department of Energy (DOE). The\nCompany retains the right to monitor all activities on these systems, to\naccess any computer files or electronic mail messages, and to disclose\nall or part of information gained to authorized individuals or\ninvestigative agencies, all without prior notice to, or consent from,\nany user, sender, or addressee. This access to information or a system\nby an authorized individual or investigative agency is in effect during\nthe period of your access to information on a DOE computer and for a\nperiod of three years thereafter. OLCF personnel and users are required\nto address, safeguard against, and report misuse, abuse and criminal\nactivities. Misuse of OLCF resources can lead to temporary or permanent\ndisabling of accounts, loss of DOE allocations, and administrative or\nlegal actions. Users who have not accessed a OLCF computing resource in\nat least 6 months will be disabled. They will need to reapply to regain\naccess to their account. All users must reapply annually.\n\nAuthentication and Authorization\n\nAll users are required to use a one-time password for authentication.\nTokens will be distributed to OLCF users. Users will be required to\ncreate a Personal Identification Number (PIN). This is used in\nconjunction with a generated token code as part of a two-factor\nauthentication implementation. Accounts on the OLCF machines are for the\nexclusive use of the individual user named in the account application.\nUsers should not share accounts or tokens with anyone. If evidence is\nfound that more than one person is using an account, that account will\nbe disabled immediately. Users are not to attempt to receive unintended\nmessages or access information by some unauthorized means, such as\nimitating another system, impersonating another user or other person,\nmisuse of legal user credentials (usernames, tokens, etc.), or by\ncausing some system component to function incorrectly. Users are\nprohibited from changing or circumventing access controls to allow\nthemselves or others to perform actions outside their authorized\nprivileges. Users must notify the OLCF immediately when they become\naware that any of the accounts used to access OLCF have been\ncompromised. Users should inform the OLCF promptly of any changes in\ntheir contact information (E-mail, phone, affiliation, etc.) Updates\nshould be sent to accounts@ccs.ornl.gov.\n\nForeign National Access\n\nApplicants who appear on a restricted foreign country listing in section\n15 CFR 740.7 License Exceptions for Computers are denied access based on\nUS Foreign Policy. The countries cited are Cuba, Iran, North Korea,\nSudan, and Syria. Additionally, no work may be performed on OLCF\ncomputers on behalf of foreign nationals from these countries.\n\nDenial of Service\n\nUsers may not deliberately interfere with other users accessing system\nresources.\n\n\n\nData Management Policy\n\nThis details an official policy of the OLCF, and must be\nagreed to by the following persons as a condition of access to or use of\nOLCF computational resources:\n\nPrincipal Investigators (Non-Profit)\n\nPrincipal Investigators (Industry)\n\nAll Users\n\nTitle: Data Management Policy Version: 20.02\n\nIntroduction\n\nThe OLCF provides a comprehensive suite of hardware and software\nresources for the creation, manipulation, and retention of scientific\ndata. This document comprises guidelines for acceptable use of those\nresources. It is an official policy of the OLCF, and as such, must be\nagreed to by relevant parties as a condition of access to and use of\nOLCF computational resources.\n\nData Storage Resources\n\nThe OLCF provides an array of data storage platforms, each designed with\na particular purpose in mind. Storage areas are broadly divided into two\ncategories: those intended for user data and those intended for project\ndata. Within each of the two categories, we provide different sub-areas,\neach with an intended purpose:\n\n\nThe table presented is a comprehensive guide to the storage areas and paths available for data storage at the Oak Ridge Leadership Computing Facility (OLCF). The purpose column outlines the specific use case for each storage area, while the storage area column specifies the name of the storage location. The path column provides the exact path to access each storage area. \n\nThe first row describes the storage area for long-term data that is routinely accessed and is not related to a specific project. This data is stored in the User Home directory, with the path being /ccs/home/[userid]. The second row is for long-term data that is archived and not related to a project, which is stored in the User Archive directory with the path /home/[userid]. \n\nThe next three rows pertain to short-term project data that is accessed for fast, batch job processing. The Member Work directory is for data that is not shared with other project members and has the path /gpfs/alpine/[projid]/scratch/[userid]. The Project Work directory is for data that is shared with other project members and has the path /gpfs/alpine/[projid]/proj-shared. The World Work directory is for data that is shared with those outside of the project and has the path /gpfs/alpine/[projid]/world-shared. \n\nThe final three rows are for long-term project data that is archived. The Member Archive directory is for data that is not shared with other project members and has the path /hpss/prod/[projid]/users/$USER. The Project Archive directory is for data that is shared with other project members and has the path /hpss/prod/[projid]/proj-shared. The World Archive directory is for data that is shared with those outside of the project and has the path /hpss/prod/[projid]/world-shared. \n\nThis table serves as a useful reference for users of the OLCF, providing clear and specific information on the available storage areas and paths for different types of data. It ensures efficient and organized data management for projects and users at the facility. \n\n| Purpose | Storage Area | Path |\n| --- | --- | --- |\n| Long-term data for routine access that is unrelated to a project | User Home | /ccs/home/[userid] |\n| Long-term data for archival access that is unrelated to a project | User Archive | /home/[userid] |\n| Long-term project data for routine access that's shared with other project members | Project Home | /ccs/proj/[projid] |\n| Short-term project data for fast, batch job access that you don't want to share | Member Work | /gpfs/alpine/[projid]/scratch/[userid] |\n| Short-term project data for fast, batch job access that's shared with other project members | Project Work | /gpfs/alpine/[projid]/proj-shared |\n| Short-term project data for fast, batch job access that's shared with those outside your project | World Work | /gpfs/alpine/[projid]/world-shared |\n| Long-term project data for archival access that you don't want to share | Member Archive | /hpss/prod/[projid]/users/$USER |\n| Long-term project data for archival access that's shared with other project members | Project Archive | /hpss/prod/[projid]/proj-shared |\n| Long-term project data for archival access that's shared with those outside your project | World Archive | /hpss/prod/[projid]/world-shared |\n\n\n\nFor more information about using the data storage archiving\nsystems, please refer to the pages on data-storage-and-transfers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers>.\n\nUser Home\n\nHome directories for each user are NFS-mounted on all OLCF systems and\nare intended to store long-term, frequently-accessed user data. User\nHome areas are backed up on a daily basis. This file system does not\ngenerally provide the input/output (I/O) performance required by most\ncompute jobs, and is not available to compute jobs on most systems. See\nthe section retention-policy <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#retention-policy> for more details on\napplicable quotas, backups, purge, and retention timeframes.\n\nUser Archive\n\nThe High Performance Storage System (HPSS) is the tape-archive storage\nsystem at the OLCF and is the storage technology that supports the User\nArchive areas. HPSS is intended for data that do not require day-to-day\naccess.\n\nUse of this directory for data storage is deprecated in favor of storing\ndata in the User, Project, and World Archive directories. For new users,\nthis directory is a \"link farm\" with symlinks to that user's /hpss/prod\ndirectories. Data for existing users remains in this directory but should\nbe moved into a User/Project/World Archive directory, at which time this\ndirectory will automatically convert to a link farm.\n\nProject Home\n\nProject Home directories are NFS-mounted on selected OLCF systems and\nare intended to store long-term, frequently-accessed data that is needed\nby all collaborating members of a project. Project Home areas are backed\nup on a daily basis. This file system does not generally provide the\ninput/output (I/O) performance required by most compute jobs, and is not\navailable to compute jobs on most systems. See the section\nretention-policy <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#retention-policy> for more details on applicable\nquotas, backups, purge, and retention timeframes.\n\nMember Work\n\nProject members get an individual Member Work directory for each associated\nproject; these reside in the center-wide, high-capacity Spectrum Scale file\nsystem on large, fast disk areas intended for global (parallel) access to\ntemporary/scratch storage. Member Work areas are not shared with other\nusers of the system and are intended for project data that the user does\nnot want to make available to other users. Member Work directories are\nprovided commonly across all systems. Because of the scratch nature of the\nfile system, it is not backed up and files are automatically purged on a\nregular basis. Files should not be retained in this file system for long,\nbut rather should be migrated to Project Home or Project Archive space as\nsoon as the files are not actively being used. If a file system associated\nwith your Member Work directory is nearing capacity, the OLCF may contact\nyou to request that you reduce the size of your Member Work directory. See\nthe section retention-policy <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#retention-policy> for more details on applicable quotas,\nbackups, purge, and retention timeframes.\n\nProject Work\n\nEach project is granted a Project Work directory; these reside in the\ncenter-wide, high-capacity Spectrum Scale file system on large, fast disk\nareas intended for global (parallel) access to temporary/scratch storage.\nProject Work directories can be accessed by all members of a project and\nare intended for sharing data within a project. Project Work directories\nare provided commonly across most systems. Because of the scratch nature of\nthe file system, it is not backed up and files are automatically purged on\na regular bases. Files should not be retained in this file system for long,\nbut rather should be migrated to Project Home or Project Archive space as\nsoon as the files are not actively being used. If a file system associated\nwith Project Work storage is nearing capacity, the OLCF may contact the PI\nof the project to request that he or she reduce the size of the Project\nWork directory. See the section retention-policy <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#retention-policy> for more details on\napplicable quotas, backups, purge, and retention timeframes.\n\nWorld Work\n\nEach project has a World Work directory that resides in the center-wide,\nhigh-capacity Spectrum Scale file system on large, fast disk areas intended\nfor global (parallel) access to temporary/scratch storage. World Work areas\ncan be accessed by all users of the system and are intended for sharing of\ndata between projects. World Work directories are provided commonly across\nmost systems. Because of the scratch nature of the file system, it is not\nbacked up and files are automatically purged on a regular bases. Files\nshould not be retained in this file system for long, but rather should be\nmigrated to Project Home or Project Archive space as soon as the files are\nnot actively being used. If a file system associated with World Work\nstorage is nearing capacity, the OLCF may contact the PI of the project to\nrequest that he or she reduce the size of the World Work directory. See the\nsection retention-policy <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#retention-policy> for more details on applicable quotas,\nbackups, purge, and retention timeframes.\n\nMember Archive\n\nProject members get an individual Member Archive directory for each\nassociated project; these reside on the High Performance Storage System\n(HPSS), OLCF's tape-archive storage system. Member Archive areas are not\nshared with other users of the system and are intended for project data\nthat the user does not want to make available to other users.  HPSS is\nintended for data that do not require day-to-day access. Users should not\nstore data unrelated to OLCF projects on HPSS. Users should periodically\nreview files and remove unneeded ones. See the section\nretention-policy <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#retention-policy> for more details on applicable quotas, backups,\npurge, and retention timeframes.\n\nProject Archive\n\nEach project is granted a Project Archive directory; these reside on the\nHigh Performance Storage System (HPSS), OLCF's tape-archive storage system.\nProject Archive directories are shared among all members of a project and\nare intended for sharing data within a project.  HPSS is intended for data\nthat do not require day-to-day access. Users should not store data\nunrelated to OLCF projects on HPSS. Project members should also\nperiodically review files and remove unneeded ones. See the section\nretention-policy <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#retention-policy> for more details on applicable quotas, backups,\npurge, and retention timeframes.\n\nWorld Archive\n\nEach project is granted a World Archive directory; these reside on the High\nPerformance Storage System (HPSS), OLCF's tape-archive storage system.\nWorld Archive areas are shared among all users of the system and are\nintended for sharing data between projects. HPSS is intended for data that\ndo not require day-to-day access. Users should not store data unrelated to\nOLCF projects on HPSS. Users should periodically review files and remove\nunneeded ones. See the section retention-policy <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#retention-policy> for more details on\napplicable quotas, backups, purge, and retention timeframes.\n\n\n\nData Retention, Purge, & Quotas\n\nSummary\n\nThe following table details quota, backup, purge, and retention\ninformation for each user-centric and project-centric storage area\navailable at the OLCF.\n\nUser-Centric Storage Areas\n\n\nThe table presented is a part of the OLCF (Oak Ridge Leadership Computing Facility) Policy Guide and provides detailed information about the various areas, paths, types, permissions, quotas, backups, purged data, retention period, and accessibility on compute nodes for user home and archive directories. The first row of the table pertains to the user home directory, which is located at /ccs/home/[userid]. The type of storage used for this directory is NFS (Network File System) and the permissions for this directory can be set by the user. The quota for this directory is limited to 50 GB and backups are enabled, meaning that data can be recovered in case of accidental deletion or loss. However, purged data cannot be recovered as it is permanently deleted. The retention period for data in this directory is 90 days, after which it becomes read-only. This means that users can only view and access the data, but cannot make any changes to it. \n\nThe next two rows of the table pertain to the user archive directories, which are located at /home/[userid]. These directories use HPSS (High Performance Storage System) as the type of storage and the permissions can also be set by the user. However, the quota for User Archive 1 is much larger at 2TB, while User Archive 2 has a quota of 700. This indicates that users have different storage limits for their archive directories. Backups are not enabled for these directories, meaning that data cannot be recovered in case of accidental deletion or loss. Additionally, data in these directories is not purged, but it is also not backed up, so it is important for users to regularly back up their data themselves. The retention period for data in these directories is also 90 days, after which it is not accessible on compute nodes. This means that users cannot access this data while running their jobs on the compute nodes. \n\nIn summary, this table provides important information about the storage and accessibility of user home and archive directories at OLCF. It outlines the different types of storage used, permissions, quotas, backups, purged data, retention period, and accessibility on compute nodes for each directory. This information is crucial for users to understand and follow in order to effectively manage their data and ensure its safety and accessibility. \n\n| Area           | Path                 | Type | Permissions | Quota | Backups | Purged | Retention | On Compute Nodes |\n|----------------|----------------------|------|-------------|-------|---------|--------|-----------|------------------|\n| User Home      | /ccs/home/[userid]   | NFS  | User set    | 50 GB | Yes     | No     | 90 days   | Read-only        |\n| User Archive 1 | /home/[userid]       | HPSS | User set    | 2TB   | No      | No     | 90 days   | No               |\n| User Archive 2 | /home/[userid]       | HPSS | 700         | N/A   | N/A     | N/A    | N/A       | No               |\n\n\n\nProject-Centric Storage Areas\n\n\nThe table presented is a comprehensive guide to the policies and permissions for the Oak Ridge Leadership Computing Facility (OLCF). The first column, \"Area\", outlines the different types of storage areas available for users. The \"Path\" column specifies the specific path or location for each area. The \"Type\" column indicates the type of storage system used for each area, such as NFS, Spectrum Scale, or HPSS. The \"Permissions\" column shows the level of access granted to users for each area, with numbers representing different levels of read and write permissions. The \"Quota\" column displays the maximum amount of storage space allowed for each area. The \"Backups\" column indicates whether or not backups are performed for each area. The \"Purged\" column shows if data is automatically deleted after a certain period of time. The \"Retention\" column specifies the length of time data is kept before being purged. The final column, \"On Compute Nodes\", indicates whether or not the storage area is accessible on compute nodes. This table serves as a useful reference for users to understand the policies and permissions for different storage areas at OLCF.\n\n| Area           | Path                                      | Type          | Permissions | Quota   | Backups | Purged | Retention | On Compute Nodes |\n|----------------|-------------------------------------------|---------------|-------------|---------|---------|--------|-----------|------------------|\n| Project Home   | /ccs/proj/[projid]                        | NFS           | 770         | 50 GB   | Yes     | No     | 90 days   | Read-only        |\n| Member Work    | /gpfs/alpine/[projid]/scratch/[userid]    | Spectrum Scale| 700 3       | 50 TB   | No      | 90 days| N/A 4     | Yes              |\n| Project Work   | /gpfs/alpine/[projid]/proj-shared         | Spectrum Scale| 770         | 50 TB   | No      | 90 days| N/A 4     | Yes              |\n| World Work     | /gpfs/alpine/[projid]/world-shared        | Spectrum Scale| 775         | 50 TB   | No      | 90 days| N/A 4     | Yes              |\n| Member Archive | /hpss/prod/[projid]/users/$USER           | HPSS          | 700         | 100 TB  | No      | No     | 90 days   | No               |\n| Project Archive| /hpss/prod/[projid]/proj-shared           | HPSS          | 770         | 100 TB  | No      | No     | 90 days   | No               |\n| World Archive  | /hpss/prod/[projid]/world-shared          | HPSS          | 775         | 100 TB  | No      | No     | 90 days   | No               |\n\n\n\nArea - The general name of storage area.\n\nPath - The path (symlink) to the storage area's directory.\n\nType - The underlying software technology supporting the storage area.\n\nPermissions - UNIX Permissions enforced on the storage area's top-level directory.\n\nQuota - The limits placed on total number of bytes and/or files in the storage area.\n\nBackups - States if the data is automatically duplicated for disaster recovery purposes.\n\nPurged - Period of time, post-file-access, after which a file will be marked as eligible for permanent deletion.\n\nRetention - Period of time, post-account-deactivation or post-project-end, after which data will be marked as eligible for permanent deletion.\n\nImportant! Files within \"Work\" directories (i.e., Member Work,\nProject Work, World Work) are not backed up and are purged on a\nregular basis according to the timeframes listed above.\n\nFootnotes\n\n1\n\nThis entry is for legacy User Archive directories which contained user data on January 14, 2020. There is also a quota/limit of 2,000 files on this directory.\n\n2\n\nUser Archive directories that were created (or had no user data) after January 14, 2020. Settings other than permissions are not applicable because directories are root-owned and contain no user files.\n\n3\n\nPermissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by \"UNIX-others\" and cannot be changed (security measures).\n\n4\n\nRetention is not applicable as files will follow purge cycle.\n\nOn Summit, Rhea and the DTNs, additional paths to the various project-centric work areas are available\nvia the following symbolic links and/or environment variables:\n\nMember Work Directory:  /gpfs/alpine/scratch/[userid]/[projid] or $MEMBERWORK/[projid]\n\nProject Work Directory: /gpfs/alpine/proj-shared/[projid] or $PROJWORK/[projid]\n\nWorld Work Directory: /gpfs/alpine/world-shared/[projid] or $WORLDWORK/[projid]\n\nData Retention Overview\n\nBy default, there is no lifetime retention for any data on OLCF\nresources. The OLCF specifies a limited post-deactivation timeframe\nduring which user and project data will be retained. When the retention\ntimeframe expires, the OLCF retains the right to delete data. If you\nhave data retention needs outside of the default policy, please notify\nthe OLCF.\n\nUser Data Retention\n\nThe user data retention policy exists to reclaim storage space after a\nuser account is deactivated, e.g., after the user’s involvement on all\nOLCF projects concludes. By default, the OLCF will retain data in\nuser-centric storage areas only for a designated amount of time after\nthe user’s account is deactivated. During this time, a user can request\na temporary user account extension for data access. See the section\nretention-policy <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#retention-policy> for details on retention\ntimeframes for each user-centric storage area.\n\nProject Data Retention\n\nThe project data retention policy exists to reclaim storage space after\na project ends. By default, the OLCF will retain data in project-centric\nstorage areas only for a designated amount of time after the project end\ndate. During this time, a project member can request a temporary user\naccount extension for data access. See the section retention-policy <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#retention-policy>\nfor details on purge and retention timeframes\nfor each project-centric storage area.\n\nSensitive Project Data Retention\n\nFor sensitive projects only, all data related to the project must be\npurged from all OLCF computing resources within 30 days of the project’s\nend or termination date.\n\nTransfer of Member Work and Member Archive Data\n\nAlthough the Member Work and Member Archive directories are for storage\nof data a user does not want to make available to other users on the\nsystem, files in these directories are still considered project data\nand can be reassigned to another user at the PI's request.\n\nData Purges\n\nData purge mechanisms are enabled on some OLCF file system directories\nin order to maintain sufficient disk space availability for job\nexecution. Files in these scratch areas are automatically purged on a\nregular purge timeframe. If a file system with an active purge policy is\nnearing capacity, the OLCF may contact you to request that you reduce\nthe size of a directory within that file system, even if the purge\ntimeframe has not been exceeded. See the section retention-policy <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#retention-policy>\nfor details on purge timeframes for each storage area, if applicable.\n\nStorage Space Quotas\n\nEach user-centric and project-centric storage area has an associated\nquota, which could be a hard (systematically-enforceable) quota or a\nsoft (policy-enforceable) quota. Storage usage will be monitored\ncontinually. When a user or project exceeds a soft quota for a storage\narea, the user or project PI will be contacted and will be asked if at\nall possible to purge data from the offending area. See the section\nretention-policy <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#retention-policy> for details on quotas for each storage area.\n\nData Prohibitions & Safeguards\n\nProhibited Data\n\n<string>:564: (INFO/1) Duplicate implicit target name: \"prohibited data\".\n\nThe OLCF computer systems are operated as research systems and only\ncontain data related to scientific research and do not contain\npersonally identifiable information (data that falls under the Privacy\nAct of 1974 5U.S.C. 552a). Use of OLCF resources to store, manipulate,\nor remotely access any national security information is strictly\nprohibited. This includes, but is not limited to: classified\ninformation, unclassified controlled nuclear information (UCNI), naval\nnuclear propulsion information (NNPI), the design or development of\nnuclear, biological, or chemical weapons or any weapons of mass\ndestruction. Authors/generators/owners of information are responsible\nfor its correct categorization as sensitive or non-sensitive. Owners of\nsensitive information are responsible for its secure handling,\ntransmission, processing, storage, and disposal on OLCF systems.\nPrincipal investigators, users, or project delegates that use OLCF\nresources, or are responsible for overseeing projects that use OLCF\nresources, are strictly responsible for knowing whether their project\ngenerates any of these prohibited data types or information that falls\nunder Export Control. For questions, contact help@olcf.ornl.gov.\n\nUnauthorized Data Modification\n\nUsers are prohibited from taking unauthorized actions to intentionally\nmodify or delete information or programs.\n\nData Confidentiality, Integrity, & Availability\n\nThe OLCF systems provide protections to maintain the confidentiality,\nintegrity, and availability of user data. Measures include: the\navailability of file permissions, archival systems with access control\nlists, and parity/CRC checks on data paths/files. It is the user’s\nresponsibility to set access controls appropriately for data. In the\nevent of system failure or malicious actions, the OLCF makes no\nguarantee against loss of data nor makes a guarantee that a user’s data\ncould not be potentially accessed, changed, or deleted by another\nindividual. It is the user’s responsibility to insure the appropriate\nlevel of backup and integrity checks on critical data and programs.\n\nAdministrator Access to Data\n\nOLCF resources are federal computer systems, and as such, users should\nhave no explicit or implicit expectation of privacy. OLCF employees and\nauthorized vendor personnel with “root” privileges have access to all\ndata on OLCF systems. Such employees can also login to OLCF systems as\nother users. As a general rule, OLCF employees will not discuss your\ndata with any unauthorized entities nor grant access to data files to\nany person other than the UNIX “owner” of the data file, except in the\nfollowing situations:\n\nWhen the owner of the data requests a change of ownership for any\nreason, e.g., the owner is leaving the project and grants the PI\nownership of the data.\n\nIn situations of suspected abuse/misuse computational resources,\ncriminal activity, or cyber-security violations.\n\nNote that the above applies even to project PIs. In general, the OLCF\nwill not overwrite existing UNIX permissions on data files owned by\nproject members for the purpose of granting access to the project PI.\nProject PIs should work closely with project members throughout the\nduration of the project to ensure UNIX permissions are set\nappropriately.\n\nSoftware\n\nSoftware Licensing\n\nAll software used on OLCF computers must be appropriately acquired and\nused according to the appropriate software license agreement.\nPossession, use, or transmission of illegally obtained software is\nprohibited. Likewise, users shall not copy, store, or transfer\ncopyrighted software, except as permitted by the owner of the copyright.\nOnly export-controlled codes approved by the Export Control Office may\nbe run by parties with sensitive data agreements.\n\nMalicious Software\n\n<string>:645: (INFO/1) Duplicate implicit target name: \"malicious software\".\n\nUsers must not intentionally introduce or use malicious software,\nincluding but not limited to, computer viruses, Trojan horses, or\ncomputer worms.\n\nReconstruction of Information or Software\n\n<string>:652: (INFO/1) Duplicate implicit target name: \"reconstruction of information or software\".\n\nUsers are not permitted to reconstruct information or software for which\nthey are not authorized. This includes but is not limited to any reverse\nengineering of copyrighted software or firmware present on OLCF\ncomputing resources.\n\n\n\nSecurity Policy\n\nThis details an official policy of the OLCF, and must be\nagreed to by the following persons as a condition of access to or use of\nOLCF computational resources:\n\nPrincipal Investigators (Non-Profit)\n\nPrincipal Investigators (Industry)\n\nAll Users\n\nTitle:Security Policy Version: 12.10\n\nThe Oak Ridge Leadership Computing Facility (OLCF) computing resources\nare provided to users for research purposes. All users must agree to\nabide by all security measures described in this document. Failure to\ncomply with security procedures will result in termination of access to\nOLCF computing resources and possible legal actions.\n\nScope\n\nThe requirements outlined in this document apply to all individuals who\nhave an OLCF account. It is your responsibility to ensure that all\nindividuals have the proper need-to-know before allowing them access to\nthe information on OLCF computing resources. This document will outline\nthe main security concerns.\n\nPersonal Use\n\nOLCF computing resources are for business use only. Installation or use\nof software for personal use is not allowed. Incidents of abuse will\nresult in account termination. Inappropriate uses include, but are not\nlimited to:\n\nSexually oriented information\n\nDownloading, copying, or distributing copyrighted materials without\nprior permission from the owner\n\nDownloading or storing large files or utilizing streaming media for\npersonal use (e.g., music files, graphic files, internet radio, video\nstreams, etc.)\n\nAdvertising, soliciting, or selling\n\nAccessing OLCF Computational Resources\n\nAccess to systems is provided via Secure Shell version 2 (sshv2). You\nwill need to ensure that your ssh client supports keyboard-interactive\nauthentication. The method of setting up this authentication varies from\nclient to client, so you may need to contact your local administrator\nfor assistance. Most new implementations support this authentication\ntype, and many ssh clients are available on the web. Login sessions will\nbe automatically terminated after a period of inactivity. When you apply\nfor an account, you will be mailed an RSA SecurID token. You will also\nbe sent a request to complete identity verification. When your account is\napproved, your RSA SecurID token will also be enabled. Please refer to our\nsystem-user-guides <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#system-user-guides> for more information on host access. DO NOT share your\nPIN or RSA SecurID token with anyone. Sharing of accounts will result in\ntermination. If your SecurID token is stolen or misplaced, contact the OLCF\nimmediately and report the missing token. Upon termination of your account\naccess, return the token to the OLCF in person or via mail.\n\nData Management\n\nThe OLCF uses a standard file system structure to assist users with data\norganization on OLCF systems. Complete details about all file systems\navailable to OLCF users can be found in the Data Management Policy\nsection.\n\nSensitive Data\n\nAdditional file systems and file protections may be employed for\nsensitive data. If you are a user on a project producing sensitive data,\nfurther instructions will be given by the OLCF. The following guidelines\napply to sensitive data:\n\nOnly store sensitive data in designated locations. Do not store\nsensitive data in your User Home directory.\n\nNever allow access to your sensitive data to anyone outside of your\ngroup.\n\nTransfer of sensitive data must be through the use encrypted methods\n(scp, sftp, etc).\n\nAll sensitive data must be removed from all OLCF resources when your\nproject has concluded.\n\nData Transfer\n\nThe OLCF offers a number of dedicated data transfer nodes to users. The nodes have been\ntuned specifically for wide area data transfers, and also perform well on the\nlocal area. There are also several utilities that the OLCF recommends for data\ntransfer. Please refer to our system-user-guides <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#system-user-guides> for information about\nthe DTNs and available utilities.\n\nTitan Scheduling Policy\n\n=======================\n\n\n\n.. note::\n\nThis details an official policy of the OLCF, and must be\n\nagreed to by the following persons as a condition of access to or use of\n\nOLCF computational resources:\n\n\n\n-  Principal Investigators (Non-Profit)\n\n-  Principal Investigators (Industry)\n\n-  All Users\n\n\n\n**Title:** Titan Scheduling Policy **Version:** 13.02\n\n\n\nIn a simple batch queue system, jobs run in a first-in, first-out (FIFO)\n\norder. This often does not make effective use of the system. A large job\n\nmay be next in line to run. If the system is using a strict FIFO queue,\n\nmany processors sit idle while the large job waits to run. *Backfilling*\n\nwould allow smaller, shorter jobs to use those otherwise idle resources,\n\nand with the proper algorithm, the start time of the large job would not\n\nbe delayed. While this does make more effective use of the system, it\n\nindirectly encourages the submission of smaller jobs.\n\n\n\nThe DOE Leadership-Class Job Mandate\n\n------------------------------------\n\n\n\nAs a DOE Leadership Computing Facility, the OLCF has a mandate that a\n\nlarge portion of Titan's usage come from large, *leadership-class* (aka\n\n*capability*) jobs. To ensure the OLCF complies with DOE directives, we\n\nstrongly encourage users to run jobs on Titan that are as large as their\n\ncode will warrant. To that end, the OLCF implements queue policies that\n\nenable large jobs to run in a timely fashion.\n\n\n\n.. note::\n\nThe OLCF implements queue policies that encourage the\n\nsubmission and timely execution of large, leadership-class jobs on\n\nTitan.\n\n\n\nThe basic priority-setting mechanism for jobs waiting in the queue is\n\nthe time a job has been waiting relative to other jobs in the queue.\n\nHowever, several factors are applied by the batch system to modify the\n\n*apparent* time a job has been waiting. These factors include:\n\n\n\n-  The number of nodes requested by the job.\n\n-  The queue to which the job is submitted.\n\n-  The 8-week history of usage for the project associated with the job.\n\n-  The 8-week history of usage for the user associated with the job.\n\n\n\nIf your jobs require resources outside these queue policies, please complete the\n\nrelevant request form on the `Special Requests\n\n<https://www.olcf.ornl.gov/support/getting-started/special-request-form/>`__\n\npage. If you have any questions or comments on the queue policies below, please\n\ndirect them to the User Assistance Center.\n\n\n\nJob Priority by Processor Count\n\n-------------------------------\n\n\n\nJobs are *aged* according to the job's requested processor count (older\n\nage equals higher queue priority). Each job's requested processor count\n\nplaces it into a specific *bin*. Each bin has a different aging\n\nparameter, which all jobs in the bin receive.\n\n\n\n+-------+-------------+-------------+------------------------+----------------------+\n\n| Bin   | Min Nodes   | Max Nodes   | Max Walltime (Hours)   | Aging Boost (Days)   |\n\n+=======+=============+=============+========================+======================+\n\n| 1     | 11,250      | --          | 24.0                   | 15                   |\n\n+-------+-------------+-------------+------------------------+----------------------+\n\n| 2     | 3,750       | 11,249      | 24.0                   | 5                    |\n\n+-------+-------------+-------------+------------------------+----------------------+\n\n| 3     | 313         | 3,749       | 12.0                   | 0                    |\n\n+-------+-------------+-------------+------------------------+----------------------+\n\n| 4     | 126         | 312         | 6.0                    | 0                    |\n\n+-------+-------------+-------------+------------------------+----------------------+\n\n| 5     | 1           | 125         | 2.0                    | 0                    |\n\n+-------+-------------+-------------+------------------------+----------------------+\n\n\n\nFairShare Scheduling Policy\n\n---------------------------\n\n\n\nFairShare, as its name suggests, tries to push each user and project\n\ntowards their fair share of the system's utilization: in this case, 5%\n\nof the system's utilization per user and 10% of the system's utilization\n\nper project. To do this, the job scheduler adds (30) minutes priority\n\naging per user and (1) hour of priority aging per project for every (1)\n\npercent the user or project is under its fair share value for the prior\n\n(8) weeks. Similarly, the job scheduler subtracts priority in the same\n\nway for users or projects that are over their fair share. For instance,\n\na user who has personally used 0.0% of the system's utilization over the\n\npast (8) weeks who is on a project that has also used 0.0% of the\n\nsystem's utilization will get a (12.5) hour bonus (5 \\* 30 min for the\n\nuser + 10 \\* 1 hour for the project). In contrast, a user who has\n\npersonally used 0.0% of the system's utilization on a project that has\n\nused 12.5% of the system's utilization would get no bonus (5 \\* 30 min\n\nfor the user - 2.5 \\* 1 hour for the project).\n\n\n\n``batch`` Queue Policy\n\n----------------------\n\n\n\nThe ``batch`` queue is the default queue for production work on Titan.\n\nMost work on Titan is handled through this queue. It enforces the\n\nfollowing policies:\n\n\n\n-  Limit of (4) *eligible-to-run* jobs per user.\n\n-  Jobs in excess of the per user limit above will be placed into a\n\n*held* state, but will change to eligible-to-run at the appropriate\n\ntime.\n\n-  Users may have only (2) jobs in bin 5 *running* at any time. Any\n\nadditional jobs will be blocked until one of the running jobs\n\ncompletes.\n\n\n\n.. note::\n\nThe *eligible-to-run* state is not the *running* state.\n\nEligible-to-run jobs have not started and are waiting for resources.\n\nRunning jobs are actually executing.\n\n\n\n``killable`` Queue Policy\n\n-------------------------\n\n\n\nAt the start of a scheduled system outage, a *queue reservation* is used\n\nto ensure that no jobs are running. In the ``batch`` queue, the\n\nscheduler will not start a job if it expects that the job would not\n\ncomplete (based on the job's user-specified max walltime) before the\n\nreservation's start time. In constrast, the ``killable`` queue allows\n\nthe scheduler to start a job even if it will *not* complete before a\n\nscheduled reservation. It enforces the following policies:\n\n\n\n-  Jobs will be killed if still running when a system outage begins.\n\n-  The scheduler will stop scheduling jobs in the ``killable`` queue (1)\n\nhour before a scheduled outage.\n\n-  Maximum-job-per-user limits are the same (i.e., in conjunction with)\n\nthe ``batch`` queue.\n\n-  Any killed jobs will be automatically re-queued after a system outage\n\ncompletes.\n\n\n\n``debug`` Queue Policy\n\n----------------------\n\n\n\nThe ``debug`` queue is intended to provide faster turnaround times for\n\nthe code development, testing, and debugging cycle. For example,\n\ninteractive parallel work is an ideal use for the debug queue. It\n\nenforces the following policies:\n\n\n\n-  Production jobs are not allowed.\n\n-  Maximum job walltime of (1) hour.\n\n-  Limit of (1) job per user *regardless of the job's state*.\n\n-  Jobs receive a (2)-day priority aging boost for scheduling.\n\n\n\n.. warning::\n\nUsers who misuse the ``debug`` queue may have further\n\naccess to the queue denied.\n\n\n\nAllocation Overuse Policy\n\n-------------------------\n\n\n\nProjects that overrun their allocation are still allowed to run on OLCF\n\nsystems, although at a reduced priority. Like the adjustment for the\n\nnumber of processors requested above, this is an adjustment to the\n\napparent submit time of the job. However, this adjustment has the effect\n\nof making jobs appear much younger than jobs submitted under projects\n\nthat have not exceeded their allocation. In addition to the priority\n\nchange, these jobs are also limited in the amount of wall time that can\n\nbe used. For example, consider that ``job1`` is submitted at the same\n\ntime as ``job2``. The project associated with ``job1`` is over its\n\nallocation, while the project for ``job2`` is not. The batch system will\n\nconsider ``job2`` to have been waiting for a longer time than ``job1``.\n\nAlso projects that are at 125% of their allocated time will be limited\n\nto only one running job at a time. The adjustment to the apparent submit\n\ntime depends upon the percentage that the project is over its\n\nallocation, as shown in the table below:\n\n\n\n+------------------------+----------------------+--------------------------+------------------+\n\n| % Of Allocation Used   | Priority Reduction   | Number eligible-to-run   | Number running   |\n\n+========================+======================+==========================+==================+\n\n| < 100%                 | 0 days               | 4 jobs                   | unlimited jobs   |\n\n+------------------------+----------------------+--------------------------+------------------+\n\n| 100% to 125%           | 30 days              | 4 jobs                   | unlimited jobs   |\n\n+------------------------+----------------------+--------------------------+------------------+\n\n| > 125%                 | 365 days             | 4 jobs                   | 1 job            |\n\n+------------------------+----------------------+--------------------------+------------------+\n\n\n\nSystem Reservation Policy\n\n-------------------------\n\n\n\nProjects may request to reserve a set of processors for a period of time\n\nthrough the reservation request form, which can be found on the `Special\n\nRequests <https://www.olcf.ornl.gov/support/getting-started/special-request-form/>`__\n\npage. If the reservation is granted, the reserved processors will be\n\nblocked from general use for a given period of time. Only users that\n\nhave been authorized to use the reservation can utilize those resources.\n\nSince no other users can access the reserved resources, it is crucial\n\nthat groups given reservations take care to ensure the utilization on\n\nthose resources remains high. To prevent reserved resources from\n\nremaining idle for an extended period of time, reservations are\n\nmonitored for inactivity. If activity falls below 50% of the reserved\n\nresources for more than (30) minutes, the reservation will be canceled\n\nand the system will be returned to normal scheduling. A new reservation\n\nmust be requested if this occurs. Since a reservation makes resources\n\nunavailable to the general user population, projects that are granted\n\nreservations will be charged (regardless of their actual utilization) a\n\nCPU-time equivalent to\n\n``(# of cores reserved) * (length of reservation in hours)``.\n\n\n\nINCITE Allocation Under-utilization Policy\n\nThis details an official policy of the OLCF, and must be\nagreed to by the following persons as a condition of access to and use\nof OLCF computational resources:\n\nINCITE Principal Investigators\n\nTitle: INCITE Allocation Under-utilization Policy Version: 12.10\n\nThe OLCF has a pull-back policy for under-utilization of INCITE\nallocations. Under-utilized INCITE project allocations will have\ncore-hours removed from their outstanding core-hour project balance at\nspecific times during the INCITE calendar year. The following table\nsummarizes the current under-utilization policy:\n\n\nThe table presented is a part of the OLCF (Oak Ridge Leadership Computing Facility) Policy Guide. It outlines the utilization and forfeiture amounts for allocations on a specific date. The first column represents the date, with May 1 and September 1 being the two dates mentioned. The second column, \"Utilization to-Date\", indicates the percentage of the allocated resources that have been utilized by the user. The third column, \"Forfeited Amount\", specifies the amount that will be forfeited by the user if their utilization falls below a certain threshold. For example, on May 1, if the utilization is less than 10%, the user may forfeit up to 30% of their remaining allocation. Similarly, on September 1, if the utilization is less than 10%, the user may forfeit up to 75% of their remaining allocation. The forfeited amount decreases as the utilization increases, with the lowest forfeited amount being up to 33% of the remaining allocation if the utilization is less than 50%. This table serves as a guideline for users to effectively manage their allocated resources and avoid forfeiting a significant portion of their allocation. \n\n| Date      | Utilization to-Date | Forfeited Amount               |\n|-----------|---------------------|--------------------------------|\n| May 1     | < 10%               | Up to 30% of remaining allocation |\n|           | < 15%               | Up to 15% of remaining allocation |\n| September 1 | < 10%               | Up to 75% of remaining allocation |\n|           | < 33%               | Up to 50% of remaining allocation |\n|           | < 50%               | Up to 33% of remaining allocation |\n\n\n\nFor example, a 1,000,000 core-hour INCITE project that has utilized only\n50,000 core-hours (5% of the allocation) on May 1st would forfeit (0.30\n* 950,000) = 285,000 core-hours from their remaining allocation.\n\n\n\nProject Reporting Policy\n\nThis details an official policy of the OLCF, and must be\nagreed to by the following persons as a condition of access to and use\nof OLCF computational resources:\n\nPrincipal Investigators (Non-Profit)\n\nPrincipal Investigators (Industry)\n\nTitle: Project Reporting Policy Version: 12.10\n\nPrincipal Investigators of current OLCF projects must submit a quarterly\nprogress report. The quarterly reports are essential as the OLCF must\ndiligently track the use of the center's resources. In keeping with\nthis, the OLCF (and DOE Leadership Computing Facilities in general)\nimposes the following penalties for late submission:\n\n\nThe table presented is a part of the OLCF (Oak Ridge Leadership Computing Facility) policy guide and outlines the penalties for late submissions of jobs and projects. The timeframe column specifies the duration of the delay, with the first row indicating a delay of 1 month and the second row indicating a delay of 3 months. The penalty column states the consequences for such delays. For a delay of 1 month, job submissions against the offending project will be suspended. This means that the project will not be able to submit any new jobs during this period. For a delay of 3 months, the consequences are more severe as login privileges will be suspended for all OLCF resources for all users associated with the offending project. This means that not only will the project be unable to submit new jobs, but all users associated with the project will also lose access to OLCF resources. This table serves as a reminder for users to adhere to the submission deadlines and highlights the importance of timely project completion in the OLCF community.\n\n| Timeframe   | Penalty                                                                 |\n|-------------|-------------------------------------------------------------------------|\n| 1 Month Late| Job submissions against offending project will be suspended.            |\n| 3 Months Late| Login privileges will be suspended for all OLCF resources for all users associated with offending project. |\n\n\n\n\n\nNon-proprietary Institutional User Agreement Policy\n\nThis details an official policy of the OLCF, and must be\nagreed to by the following persons as a condition of access to and use\nof OLCF computational resources:\n\nPrincipal Investigators (Non-Profit)\n\nAll Users\n\nTitle: Non-proprietary Institutional User Agreement Policy\nVersion: 12.10\n\nUsers of DOE-designated User Facilities must understand and agree to the\nfollowing Institutional User Agreement clause: I understand that my\ninstitution has entered into a User Agreement with UT-Battelle, the\nmanagement and operating contractor for the U.S. Department of Energy’s\n(DOE) Oak Ridge National Laboratory (ORNL), that governs my research\nORNL’s DOE-designated User Facilities. I have read and understand my\nobligations under that Agreement, including the provisions summarized\nbelow. You may check with your institution or contact\naccounts@ccs.ornl.gov if you require a copy of your User Agreement.\n\nAccess\n\nI understand that my access is limited to certain designated areas\nand/or systems, and my access may be revoked if I pose a security,\nsafety, or operational risk.\n\nRules and Regulations\n\nI will follow the applicable ORNL rules, regulations and requirements,\nincluding those requirements of the ORNL User Facility. I will follow\nthe requirements set forth in training if assigned to me by the ORNL\nUser Facility.\n\nSafety and Health\n\nI will take all reasonable precautions to protect the safety and health\nof others and comply with all applicable safety and health requirements.\n\nIntent to Publish\n\nI will use best efforts to publish the results from my use of the ORNL\nUser Facility in an open scientific journal or significant industry\ntechnical journal or conference proceedings. I will acknowledge use of\nthe ORNL User Facility in the publication and\nnotify the ORNL User Facility of any publications that result from my\nuse of the facility.\n\nExport Control\n\nI will comply with all U.S. Export Control laws and regulations and be\nresponsible for the appropriate handling and transfer of any export\ncontrolled information, which may require advance U.S. Government\nauthorization.\n\nIntellectual Property\n\nI will disclose any invention conceived as a part of the work at a\nORNL User Facility and will protect the invention until a patent\napplication can be filed. I understand that my institution may elect\ntitle to the invention and the U.S. Government retains rights to the\ninvention.\n\n\n\nHIPAA/ITAR Project Rules of Behavior Policy\n\nThis details an official policy of the OLCF, and must be\nagreed to by the following persons as a condition of access to and use\nof OLCF computational resources:\n\nPrincipal Investigators of HIPAA/ITAR Projects\n\nUsers on HIPAA/ITAR Projects\n\nTitle: HIPAA/ITAR Project Rules of Behavior Policy Version: 21.01\n\nPortions of data and/or software used in your project require extra\nprotections due to requirements for protecting HIPAA/ITAR or other sensitive\nor controlled information. There are countries from which citizens are\nrestricted from accessing sensitive/controlled information and therefore\ncannot be a part of your project. When you request users to be added to\nyour project, our user assistance center will check the nationality of\nthose users for conflict.\n\nIn addition, to protect sensitive data and code identified by these export\ncontrol restrictions, data and code must be handled in accordance with the following rules:\n\nHIPAA/ITAR regulated data is only allowed within the following directory:\n\n\nThe OLCF Policy Guide provides a comprehensive overview of the policies and guidelines that govern the use of the Oak Ridge Leadership Computing Facility (OLCF). The table in the guide outlines the specific details related to project scratch storage, including the designated path and type of storage. The path for project scratch storage is located in the /gpfs/arx/ directory and the type of storage is GPFS (General Parallel File System). This information is important for users of the OLCF to understand as it outlines the specific location and type of storage that is available for their projects. This table serves as a quick reference guide for users to easily access and understand the policies and guidelines related to project scratch storage at the OLCF. \n\n| Area          | Path           | Type  |\n| ------------- |:--------------:| -----:|\n| Project scratch | /gpfs/arx/[projid] | GPFS |\n\nThis is your project's scratch space on the \"arx\" filesystem. Only members of your project will have access.\n\n2. Filenames, application names, job names, environment variables, batch job scripts, or\nany other unencrypted text must never contain sensitive or controlled information.\nTransfer of sensitive or controlled information must only take place through designated\nData Transfer Nodes (DTNs) over an encrypted transport protocol.\n\n3. Prior to your project being initialized, we must have source IP addresses for devices\nin your organization that are authorized to transfer sensitive/controlled information\nto/from your organization, or for use in accessing that information. Encryption is\nnecessary for transferring sensitive and/or controlled information to and from the OLCF.\n\n4. For codes being used, please make sure that the proper number of licenses have been\nobtained from the vendors of the respective software. It is also the responsibility of\nthe PI to ensure that all project members have the appropriate authorization to access\nany sensitive data and/or codes used as required by relevant data use agreements.\n\n5. The Principal Investigator (PI) has the responsibility to make sure that other project members\nfollow the sensitive controls outlined in this policy and protect sensitive/controlled\ninformation. It is also the responsibility of the PI to alert us of any personnel\nchanges on the project.\n\nIf you have security-related questions, contact us via email at: security-admins@ccs.ornl.gov.\nOther questions can be sent to help@olcf.ornl.gov.\n\nUser-Managed Software (UMS) Policy\n\n<string>:3: (INFO/1) Duplicate implicit target name: \"software\".\n\nMore information about the UMS program can be found in the Software\nsection.\n\nThis details an official policy of the OLCF, and must be\nagreed to by the following persons as a condition of access to and use\nof OLCF computational resources:\n\nPrincipal Investigators of UMS Projects\n\nPoints of Contact for UMS Projects\n\nTitle: UMS Project Policies Version: 21.08\n\nPurpose\n\nThis document is intended to describe the agreement between the OLCF and the providers of user-managed\nsoftware installations. User-managed software is built, maintained, and supported by OLCF users\nrather than as official offerings of the OLCF, but is exposed to all users through the module system.\n\nPolicies\n\nOrder is for convenience and no implication of priority is implied.\n\nProducts installed should be limited to those explicitly listed in the project application\nand approved by the OLCF.\n\nThe project application is reviewed by the Export Control Office. If you would like to\ninstall additional packages not listed in your original application, the Project PI must\ncontact the OLCF at help@olcf.ornl.gov before making changes.\n\nProducts must provide appropriate modules for their software.\n\nProducts must provide a statement of support, to be displayed via the module system and in other\nappropriate contexts/locations.\n\nThe statement should clearly indicate that the product is not supported or maintained by the OLCF,\nbut is supported by the UMS project applicant and/or the UMS project team.\n\nThe statement should clearly indicate the organization that is providing support and maintenance,\nand clearly indicate the preferred method(s) of reporting issues or requesting support.\n\nProduct modules will be grouped under project-level modules.\n\nUsers will be advised to do a module load <UMS project>, which will expose modules for the\nindividual products associated with that project, accessed by a second module load <product>.\n\nProject PI must ensure that support is provided for users of the product, as documented in the statement of support.\n\nProject PI must ensure that the product is updated in response to changes in the system software environment\n(e.g. updates to OS, compiler, library, or other key tools) and in a timely fashion.  If this commitment is\nnot met, the OLCF may remove the software from UMS.\n\nProject PI must ensure that installations are tested to ensure basic functionality before being released to users.\nThese are expected to be at minimum basic function/unit tests to ensure that the build/install was successful.\n\nThe resources provided by the OLCF for UMS shall not be used for software development or for routine\ntesting purposes beyond the installation testing as described above.\n\nProducts may be removed from UMS at the request of the Project PI by notifying the OLCF (help@olcf.ornl.gov) of their intent and\ncleaning up their directory space.\n\nIf a product is judged to be problematic for the OLCF, it may be removed by the OLCF staff, who will also notify the Project PI.\n\nRationale: This is intended to be a measure of last resort.  The UMS concept is designed so that use of the software is opt-in and there should be no problems for users who do not opt in, or for the facility at large.  However in the rare case where something is really wrong, the facility needs a way out. Presumably, everything possible will be done to avoid this situation\n\nWill undergo Export Control Review.  Need enough information to pass ECR in the application.\nWill be given \"modest\" allocation of hours.  Monitored but not explicit.\nAdded to RUC for approval.\nQuota in RATS?"}
{"doc":"openshift_gitops","text":"OpenShift GitOps\n\n\n\nFrom the release notes:\n\nRed Hat OpenShift GitOps is a declarative way to implement continuous deployment for cloud native\napplications. Red Hat OpenShift GitOps ensures consistency in applications when you deploy them\nto different clusters in different environments, such as: development, staging, and production.\n\nIn addition to multiple operators, Red Hat OpenShift GitOps provides RBAC roles and bindings, default resource request limits,\nintegration with Red Hat SSO, integration with OpenShift cluster logging as well as cluster metrics, ability to manage resources\nacross multiple OpenShift clusters with a single OpenShift GitOps instance, automatic remediation if resource configuration changes\nfrom desired configuration, and promotion of configurations from dev to test/staging to production.\n\nContents\n\nResources\n\nInstallation\n\nConfiguration\n\nMultiple Project Management\n\nApplication Deployment\n\nDirectory Structure\n\nkustomization.yaml\n\nArgoCD and kustomize\n\nResources\n\nRed Hat OpenShift GitOps documentation\n\nArgoCD documentation\n\nGitOps Guide to the Galaxy\n\nIntroduction to Kustomize\n\nDeclarative Management of Kubernetes Objects using Kustomize\n\nKustomize Examples\n\nRed Hat Blog post: Continuous Delivery with Helm and ArgoCD\n\nTo foster collaboration, discussion, and knowledge sharing, the CNCF GitOps Working Group held\nGitOpsCon North America 2021\nwith sessions concerning GitOps in general practice as well as specific tools. Additionally, there are\ntwo awesome lists where one may find more information concerning GitOps and tools:\n\nAwesome Argo\n\nAwesome GitOps\n\nThe former is curated by one of the committers to the ArgoCD project while the latter is curated by Weaveworks.\nThe remainder of this document is focused solely on the use of the Red Hat OpenShift GitOps operator.\n\nInstallation\n\nThe Red Hat OpenShift GitOps operator is already installed onto each of the Slate clusters. However, to use ArgoCD an instance will\nneed to be added into a project namespace. If one is to use ArgoCD to manage resources in a single project, then the ArgoCD instance\ncould be located in the same project as the resources being managed. However, if the application being managed is resource\n(CPU and memory) sensitive or if resources in multiple projects are to be managed, it may be better to have the ArgoCD instance\ndeployed into a separate project to allow for better control of resources allocated to ArgoCD.\n\nIf a sub project is being setup under the main project for deployment of ArgoCD, allocate at least 6 CPU, 5 Gi memory, and 5 Gi storage resources. Once deployed, these may need to be adjusted higher depending on observed performance.\n\nOnce a project is identified for deployment of ArgoCD, navigate to the project using the Administrator view of the console, Select\nOperators -> Installed Operators -> Red Hat OpenShift GitOps -> \"Argo CD\" tab\n\nImage of OpenShift UI for creating an ArgoCD instance.\n\nIf the project already has an ArgoCD instance, do not install another into the same project.\n\nNext Click the \"Create ArgoCD\" button. You will be presented with a form view similar to:\n\nImage of the form view for ArgoCD instance creation.\n\nStarting with the form view of the process, make the following changes allowing for access to the ArgoCD web UI via a route:\n\nServer -> Insecure -> true\n\nServer -> Route -> Enabled -> true\n\nServer -> Route -> Tls -> Termination -> edge\n\nImage of the form view for ArgoCD instance creation with the route settings configured.\n\nThis enables access to the ArgoCD instance once deployed via the web browser more easily. In the above images, notice that the\ninstance name is argocd. By default, the route name to the web UI will be <<instanceName>>-server-<<projectName>>.apps.<<clusterName>>.ccs.ornl.gov.\nIf a different host name is desired to access the instance, enter the desired name in the Host parameter while\nmaintaining the pattern new-name.apps.<cluster>.ccs.ornl.gov. For example,\n\nImage of the form view with a custom host name set.\n\nOnce these changes are made, changing over to the YAML view will expose some other parameters set for the ArgoCD instance, such as resource\nrequests for each component:\n\nImage of the yaml view for ArgoCD instance creation.\n\nAll of the prior mentioned settings should be visible in the spec.server parameters block. In addition to the server component\nof ArgoCD, there are other components:\n\n\nThe table presents a breakdown of the components involved in openshift gitops. The server component is responsible for providing the web UI and API for managing the gitops process. The repo component acts as a repository server, caching the git repository and generating Kubernetes manifests. The dex component provides a Dex OATH instance, which is not deployed by default on OpenShift. The ha component deploys ArgoCD in a high availability configuration, including an HA Redis instance. The redis component serves as a cache for the other components. Finally, the controller component is a Kubernetes controller that continuously monitors applications, compares their state to the manifests generated from the repository, and takes corrective actions to add, change, or remove application resources. This table provides a comprehensive overview of the various components involved in the openshift gitops process and their respective roles in ensuring efficient and effective deployment and management of applications. \n\n| Component | Description |\n|-----------|-------------|\n| server    | Provides the web UI and API |\n| repo      | Acts as a repository server, caching the git repository and generating Kubernetes manifests |\n| dex       | Provides a Dex OATH instance, not deployed by default on OpenShift |\n| ha        | Deploys ArgoCD in an HA configuration, including an HA Redis instance |\n| redis     | Serves as a cache for the other components |\n| controller| Kubernetes controller that continuously monitors applications, compares their state to the manifests generated from the repository, and takes corrective actions |\n\nThe initial resources set by OpenShift GitOps should be sufficient to start working with ArgoCD. If needed, these may be\nincreased should performance issues occur. On the right hand side is the schema for the ArgoCD custom resource listing all of the\navailable parameters that could be used. At this point, no other parameters are needed to create an instance. However, if there are\nquestions over a parameter or capability, please contact the Platforms Group.\n\nWhen ready to deploy, click the Create button in the lower left. The view returns back to the Argo CD tab, and the Status will\nbe listed as Phase: Pending:\n\nImage of pending ArgoCD installation.\n\nThe OpenShift GitOps operator is working in the background to deploy multiple custom resources. Once complete, the\nstatus will change to Phase: Available. The oc get all command will reveal the resources deployed:\n\n$ oc get all\nNAME                                      READY   STATUS    RESTARTS   AGE\npod/argocd-application-controller-0       1/1     Running   0          2m52s\npod/argocd-redis-6b9cd5d47-7dpwh          1/1     Running   0          2m52s\npod/argocd-repo-server-5c4dbb5556-sm2bt   1/1     Running   0          2m52s\npod/argocd-server-5bc4646756-2zkr5        1/1     Running   0          2m52s\n\nNAME                            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE\nservice/argocd-metrics          ClusterIP   172.25.155.151   <none>        8082/TCP            2m52s\nservice/argocd-redis            ClusterIP   172.25.178.123   <none>        6379/TCP            2m52s\nservice/argocd-repo-server      ClusterIP   172.25.220.108   <none>        8081/TCP,8084/TCP   2m52s\nservice/argocd-server           ClusterIP   172.25.254.6     <none>        80/TCP,443/TCP      2m52s\nservice/argocd-server-metrics   ClusterIP   172.25.239.4     <none>        8083/TCP            2m52s\n\nNAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/argocd-redis         1/1     1            1           2m52s\ndeployment.apps/argocd-repo-server   1/1     1            1           2m52s\ndeployment.apps/argocd-server        1/1     1            1           2m52s\n\nNAME                                            DESIRED   CURRENT   READY   AGE\nreplicaset.apps/argocd-redis-6b9cd5d47          1         1         1       2m52s\nreplicaset.apps/argocd-repo-server-5c4dbb5556   1         1         1       2m52s\nreplicaset.apps/argocd-server-5bc4646756        1         1         1       2m52s\n\nNAME                                             READY   AGE\nstatefulset.apps/argocd-application-controller   1/1     2m52s\n\nNAME                                     HOST/PORT                                PATH   SERVICES        PORT   TERMINATION   WILDCARD\nroute.route.openshift.io/argocd-server   argocd-stf042.apps.marble.ccs.ornl.gov          argocd-server   http   edge          None\n\nWhen one navigates to the route in a web browser, the ArgoCD login screen will be presented:\n\nImage  of the ArgoCD login screen.\n\nFor ArgoCD authentication, the default user is admin with the password stored in the <<instanceName>>-cluster secret in the\nproject. Following login, the instance is ready for configuration:\n\nImage of the ArgoCD applications tab.\n\nConfiguration\n\nPrior to using ArgoCD, a git repository containing Kubernetes custom resources needs to be added for use.\n\nThe git repository containing Kubernetes custom resources is typically not the same git repository used to build the application.\n\nThere are three ways for ArgoCD to connect to a git repository:\n\nconnect using ssh\n\nconnect using https\n\nconnect using GitHub App\n\nEach of these methods are described in the ArgoCD Private Repositories\ndocument. For example, to connect to an OLCF or NCCS GitLab instance, create a deploy token per the GitLab documentation for use by\nArgoCD copying the username and token value. Then, in ArgoCD, navigate to \"Manage your repositories, projects, settings\" tab and\nselect \"Repositories\".\n\nImage of the Manage your repositories, project, settings tab.\n\nOnce into the \"Repositories\" area, select \"CONNECT REPO USING HTTPS\":\n\nImage of the repositories area.\n\nand then add the \"Repository URL\", \"Username\" for the deploy token, and the deploy token itself as the password. If Git-LFS support is\nneeded, click the \"Enable LFS support\" at the bottom of the page. Once entries look correct:\n\nImage of the connect to repo using https parameters.\n\nclick the \"CONNECT\" button in the upper left. Once entered and ArgoCD is able to access the server, the connection should have a\nstatus of \"Successful\" with a green check mark:\n\nImage of a successful git repository configuration.\n\nAt this point, it should now be possible to use this git repository for deployment of resources into a project.\n\nMultiple Project Management\n\nBy default, OpenShift GitOps will automatically configure the project and add the necessary roles to allow for the deployment of\nKubernetes resources to the same project that contains the ArgoCD deployment. If it is desired to manage resources in a project\nother than where ArgoCD is deployed, please contact the Platforms Group to assistance in configuring the additional projects.\n\nApplication Deployment\n\nArgoCD supports multiple methods to deploy Kubernetes resource manifests:\n\ndirectory of YAML or JSON files\n\nkustomize applications\n\nhelm charts\n\nThis section will focus on the deployment of Kubernetes resources using kustomize. If the use of helm is\npreferred, refer to the\nContinuous Delivery with Helm and ArgoCD\nblog post as well as the App of Apps Pattern discussed on the\nArgoCD Cluster Bootstrapping\npage.\n\nReferences to ksonnet for deployment of Kubernetes resources may be mentioned in some documentation. However, the use if ksonnet is no longer supported by ArgoCD.\n\nIn order to deploy resources, one should have the following to start with:\n\nbase directory of YAML files that specify one or multiple kubernetes resources\n\nkustomization.yaml file\n\none or more overlay directories\n\nUnlike helm which is a template framework for deployment of kubernetes resources, kustomize is a patching framework. Once the base\ndirectory of YAML files is in place, kustomize patches those files to modify kubernetes resources for deployment with custom\nconfigurations for one or multiple environments such as dev, test, and prod.\n\nDirectory Structure\n\nBefore going into how ArgoCD will use a kustomize configuration setup, a word about organizing the code repository. Prior to starting\nwork with kustomize, take some time to consider what makes sense for setting up the directory of repository. Looking at the GitHub\nrepository for kustomize, there is a\nkustomize Hello World\ndocument illustrating the basic layout to start with:\n\n├── base\n│   ├── configMap.yaml\n│   ├── deployment.yaml\n│   ├── kustomization.yaml\n│   └── service.yaml\n└── overlays\n    ├── production\n    │   ├── deployment.yaml\n    │   └── kustomization.yaml\n    └── staging\n        ├── kustomization.yaml\n        └── map.yaml\n\nAs seen above, in the simplest form a git repository contains two directories at the root: base and overlays. The\nbase directory contains a set resources which deploy the application. These could be YAML files generated from the output of existing\nOpenShift project resources, new YAML files for resources that have not been deployed prior, YAML files generated by the helm template\ncommand, or even an existing git repository of YAML files located in GitHub or GitLab maintained by someone else. The overlays directory\nwas added to contain patches for distinct deployments. In this case, two deployments (production and staging), often referred to as\nenvironments, have been specified. The production environment has a patch file for the deployment.yaml resource file and the\nstaging environment contains the map.yaml resource file which is a patch file for configMap.yaml.\n\nIf using a remote repository as a base set of resources, make sure to pin the version at a specific tag or hash to prevent unexpected changes in your project should upstream change.\n\nFor more advanced use cases, Gerald Nunn provides a helpful directory layout in his\ngnunn-gitops repository which considers not only kustomize\ncode organization but also resources for bootstrapping ArgoCD instances, TekTon pipeline resources, Jenkins pipeline resources,\napplication management on multiple clusters, and applications that consist of multiple component applications. Pieces represented\nhere could be incorporated into the above initial directory structure as makes sense for how the group operates.\n\nOther potential information of use:\n\nkustomize.io\n\n<string>:5: (INFO/1) Duplicate explicit target name: \"introduction to kustomize\".\n\nIntroduction to Kustomize\n\n<string>:5: (INFO/1) Duplicate explicit target name: \"declarative management of kubernetes objects using kustomize\".\n\nDeclarative Management of Kubernetes Objects Using kustomize\n\nkustomize glossary\n\nAn Introduction to Kustomize by Scott Lowe\n\n<string>:5: (INFO/1) Duplicate explicit target name: \"kustomize examples\".\n\nkustomize Examples\n\nkustomization.yaml\n\nThe kustomization.yaml file declares what resource files kustomize should use when generating kubernetes resources. Additionally,\nthe kustomization.yaml file will specify how resources should be modified, if needed. A kustomization.yaml file will contain\ninformation that falls typically into four categories:\n\nresources: what existing resource files should be used.\n\ngenerators: what new resources should be created dynamically.\n\ntransformers: what resources should be changed and how to change them.\n\nmeta: fields that may influence all of the above.\n\nStarting with the Hello World example in the prior section, the kustomization.yaml file located in the base directory would look\nsimilar to:\n\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nmetadata:\n  name: arbitrary\ncommonLabels:\n  app: hello\nresources:\n- deployment.yaml\n- service.yaml\n- configMap.yaml\n\nThe structure of the kustomization.yaml file starts off similar to the structure of a kubernetes object: apiVersion, kind, and\nmetadata.name. From there, the file contains resource information and meta information. The resource information is\nspecified in the resources block and lists files that should be included for use by kustomize. In this case, three\nfiles are specified: deployment.yaml, service.yaml, and configMap.yaml. Each of these files define a kubernetes resource\nof the type indicated by the filename. Resource file names are arbitrary, but they must match the name of the file in the directory. Files\nthat exist in a directory but are not included as resources, consumed by generators, or applied by transformers are ignored.\n\nThe meta information in the kustomization.yaml illustrates what is referred to as a cross-cutting field. In this case, the\ncommonLabels block adds a label app: hello which will be included in all of the resources specified in the resource files.\nCross-cutting fields could also be used to set the namespace (namespace) for the resources to be created in, add a prefix (namePrefix)\nor suffix (nameSuffix) to all resource names, or add a set of annotations (commonAnnotations).\n\nTo see the results of the commonLabels field, the kustomize build command will display the output for inspection:\n\n$ kustomize build base/\napiVersion: v1\ndata:\n  altGreeting: Good Morning!\n  enableRisky: \"false\"\nkind: ConfigMap\nmetadata:\n  labels:\n    app: hello\n  name: the-map\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: hello\n  name: the-service\nspec:\n  ports:\n  - port: 8666\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: hello\n    deployment: hello\n  type: LoadBalancer\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hello\n  name: the-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: hello\n      deployment: hello\n  template:\n    metadata:\n      labels:\n        app: hello\n        deployment: hello\n    spec:\n      containers:\n      - command:\n        - /hello\n        - --port=8080\n        - --enableRiskyFeature=$(ENABLE_RISKY)\n        env:\n        - name: ALT_GREETING\n          valueFrom:\n            configMapKeyRef:\n              key: altGreeting\n              name: the-map\n        - name: ENABLE_RISKY\n          valueFrom:\n            configMapKeyRef:\n              key: enableRisky\n              name: the-map\n        image: monopole/hello:1\n        name: the-container\n        ports:\n        - containerPort: 8080\n\nThe app label now appears in each of the generated resources- configMap, Deployment, and service. Looking at the\n`kustomization.yaml file for the staging environement:\n\n$ cat overlays/staging/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nmetadata:\n  name: staging-arbitrary\nnamePrefix: staging-\ncommonLabels:\n  variant: staging\n  org: acmeCorporation\ncommonAnnotations:\n  note: Hello, I am staging!\nresources:\n- ../../base\npatchesStrategicMerge:\n- map.yaml\n\nthere are a few meta information blocks present: namePrefix, commonLabels, and commonAnnotations. Additionally, we\nsee that there is a patch specified with the patchesStrategicMerge block where a patch file to be merged is specified:\n\n$ cat overlays/staging/map.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: the-map\ndata:\n  altGreeting: \"Have a pineapple!\"\n  enableRisky: \"true\"\n\nIn this case, the patch will use a merge strategy to change the data entries for the specified apiVersion/kind/metadata.name object.\nRunning kustomize build on the staging environment shows the result of the patch as well as the added meta:\n\n$ kustomize build overlays/staging\napiVersion: v1\ndata:\n  altGreeting: Have a pineapple!\n  enableRisky: \"true\"\nkind: ConfigMap\nmetadata:\n  annotations:\n    note: Hello, I am staging!\n  labels:\n    app: hello\n    org: acmeCorporation\n    variant: staging\n  name: staging-the-map\n---\napiVersion: v1\nkind: Service\n...\n\nwhen compared to the output from the build command ran against the base directory.\n\nThe apiVersion/kind/metadata.name must match exactly the object to modify. If the patch does not match an object, an error similar to: Error: no matches for Id ~G_v1_ConfigMap|~X|themap; failed to find unique target for patch ~G_v1_ConfigMap|themap will be generated will instead pointing to the problem. In the case, the metadata.name field was themap instead of the-map.\n\nIf more advanced patching is needed of a resources or field does not support the strategic merge process, use patchesJson6902 instead\nof patchesStrategicMerge as this provides for more operations and control over the merge process. Additionally, one may also be\nable to use a configuration transformation to\nmodify the resulting resources. While not utilized the the helloWorld kustomize application, these are illustrated in some of the\nother examples it the same repository.\n\nReview of the production environment is left as an exercise for the reader.\n\nArgoCD and kustomize\n\nWith a git repository defined in the ArgoCD Repositories settings that has kustomize environments ready for use, one can start using\nArgoCD to deploy and manage kubernetes resources.\n\nFrom the ArgoCD Applications screen, click the Create Application button. In the General application settings, give the\napplication deployment a name for ArgoCD to refer to in the display.\nFor Project usually the ArgoCD default project created during the ArgoCD instance installation is\nsufficient. However, your workload may benefit from a different logical grouping by using multiple\nArgoCD projects. If it is\ndesired to have ArgoCD automatically deploy resources to an OpenShift namespace, change the Sync Policy to Automatic. Check the\nPrune Resources to automatically remove objects when they are removed from the repository and Self Heal to automatically restore\nconfiguration of objects when their configuration gets of sync with the specified files in the git repository.\n\nImage of ArgoCD new application general settings.\n\nNext, specify the application source and destination settings. The source settings specify the git repository, revision, and directory\npath for ArgoCD to use for resource deployment. ArgoCD likely automatically detected the possible kustomize environment choices in\nthe repository when clicking in the Path entry. If so, select the appropriate environment. If not, enter the path to the directory\nwithin the git repository to use. The destination settings refer to where ArgoCD will deploy resources. The Cluster URL refers to\nwhich kubernetes cluster to deploy. This will likely be https://kubernetes.default.svc- the same cluster the ArgoCD instance is\ninstalled. The Namespace setting should be the OpenShift namespace that ArgoCD will deploy resources. This may or may not be the\nsame namespace that ArgoCD is installed (see prior discussion on multiple namespace management in this document).\n\nImage of ArgoCD new application source and destination settings.\n\nThe last section entitled Directory most likely will be left at the defaults.\n\nImage of ArgoCD new application directory settings.\n\nOnce everything is set, scroll to the top and click the Create button. An application tile should be created on the ArgoCD\nApplications page:\n\nImage of ArgoCD new application tile.\n\nClicking on the tile in this case revealed that there was an error on deployment:\n\nImage of ArgoCD namespace error message.\n\nThe Namespace XXXX for XXXX is not managed. indicates that the namespace has not yet been setup for ArgoCD to deploy resources. Please\ncontact the Platforms Group for assistance in changing the configuration of the OpenShift namespace.\n\nIn this case, the namespace to deploy resources to was incorrect. The application was editted to change the namespace to deploy resources,\nand the application tile was reviewed:\n\nImage of ArgoCD application tile with corrected namespace.\n\nArgoCD has successfully accessed the namespace, realized that the state of the resources in the namespace are OutOfSync with the resource\nrequirements of the code repository, and started the Syncing process to create and/or modify resources in the namespace to match the\ndesired configuration. Clicking on the application tile will reveal more detailed information on the process:\n\nImage of ArgoCD application tile detailed information.\n\nWhen the ArgoCD has completed the sync process, the application tile will indicate a green check mark next to Synced indicating that the\nprocess completed and a green heart next to Healthy indicating that all resources are properly configured.\n\nImage of ArgoCD application tile in the healthy state."}
{"doc":"openshift_pipelines","text":"OpenShift Pipelines\n\n\n\nComing soon."}
{"doc":"other_resources","text":"Schedule Other Slate Resources\n\n\n\nBeyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF\nsupport first to make sure that your project can schedule these resources.\n\nGPUs\n\nGPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.\n\nGPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case\n\nWe are targeting use cases that need GPUs for long running services. For batch access to GPUs we recommend using the standard\nHPC clusters in NCCS\n\nThe Slate Marble cluster has nodes with three NVIDIA Tesla V100 GPUs per node available for scheduling so a single pod could\nrequest from 1 to 3 GPUs\n\nPod Example\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: cuda-vector-add\nspec:\n  restartPolicy: OnFailure\n  containers:\n    - name: cuda-vector-add\n      # https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile\n      image: \"k8s.gcr.io/cuda-vector-add:v0.1\"\n      resources:\n        limits:\n          nvidia.com/gpu: 1 # requesting 1 GPU\n\nIn the above example we are requesting one GPU using the pod.spec.resources.limits\n\nCaveats\n\nGPUs can only be specified in the .limits section\n\nContainers and Pods do not share GPUs, the allocation is for exclusive use of the requested GPUs\n\nContainers can request one or more GPUs\n\nUpstream documentation\n\nhttps://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/"}
{"doc":"overview","text":"Workflows Overview\n\nThe question \"what is a workflow system\" usually starts a spirited debate. Here, we will be referring to software\nor sets of tools used to automate processes and tasks on Slate. In addition to the Slate platform, these processes\nand tasks may utilize other NCCS compute and storage systems.\n\nCI/CD Workflows\n\nTaking the GitLab CI/CD concepts documentation as a start point,\nContinuous Integration (CI) completes tasks necessary to test and build software resulting in a container image.\nExample tasks performed could be code linting, test coverage, unit testing, functional testing, code compiling\nor integration testing. Tasks would be triggered whenever code is pushed into a repository.\n\nCD could be either Continuous Delivery or Continuous Deployment. Both take an application following CI\nand make it available for use. In Continuous Delivery, an application deployment is triggered\nmanually whereas in Continuous Deployment the process occurs automatically without the involvement of a person.\n\nA more in depth discussion may be found with Martin Fowler's\nContinuous Integration article.\n\nOn Slate, there are three primary CI/CD style tools in use:\n\nGitLab Runners\n\nJenkins\n\nOpenShift Pipelines\n\nGitLab Runners\n\nDeploying a GitLab Runner into a Slate project may be found in the slate_gitlab_runners <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_gitlab_runners> document\nwhich leverages a localized Slate Helm Chart. The\nGitLab Pipelines documentation as well as\nGitLab CI/CD Examples provide more details on GitLab Runner capabilities\nand usage.\n\nJenkins\n\nFor Jenkins deployment, a Slate Helm Chart is planned to be released. Documentation concerning\nJenkins Pipelines\nas well as Jenkins Pipeline Tutorials are available.\n\nOpenShift Pipelines\n\nA more recent offering on Slate, Red Hat OpenShift Pipelines is based on the open source Tekton\nproject and provides a cloud native test, build and deployment framework fully integrated into the OpenShift Console.\nFor details, see slate_openshift_pipelines <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_openshift_pipelines>.\n\nGitOps Workflows\n\nUnlike the CI/CD tools mentioned above, ArgoCD is not used for testing and creating container images. Rather, ArgoCD\nmanages Kubernetes application deployments in an automated and consistent manner using custom resource files versioned\nin a git repository. Additionally, individual development, test and production deployments across multiple projects\nmay be accomplished using a singular git repository. Whenever a change occurs in the git repository, ArgoCD will\nmake the necessary changes to a project by adding, reconfiguring, or removing resources. In other words, the CD in\nArgoCD is for continuous delivery of the application(s).\n\nWith GitOps potentially meaning different ideas to different groups, the mission of the Cloud Native Computing\nFoundation (CNCF) GitOps Working Group (WG) is to define vendor neutral, principle-led meaning for GitOps practices.\nWith the KubeCon NA conference in October, 2021, the GitOps WG released a set of four core\nGitOps Principles where the desired state of a GitOps managed system must be:\n\nDeclarative: A system managed by GitOps must have its desired state expressed declaratively.\n\nVersioned and Immutable: Desired state is stored in a way that enforces immutability, versioning and retains a complete version history.\n\nPulled Automatically: Software agents automatically pull the desired declarations from the source.\n\nContinuously Reconciled: Software agents continuously observe actual system state and attempt to apply the desired state.\n\nOn Slate, Red Hat OpenShift GitOps is based on the open source ArgoCD project.\nFor more information as well as how to install and use ArgoCD on Slate, see: slate_openshift_gitops <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_openshift_gitops>.\n\nScientific Workflows\n\nScientific workflows at OLCF can be found in the workflows <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#workflows> page under the Software section."}
{"doc":"parallel_h5py","text":"Installing mpi4py and h5py\n\nThis guide has been adapted for Frontier only for a conda\nworkflow. Using the default cray-python module on Frontier does not\nwork with parallel h5py (because Python 3.9 is incompatible). Thus,\nthis guide assumes that you are using a personal\nMiniconda distribution on Frontier <https://docs.olcf.ornl.gov/software/python/miniconda.html>.\n\nFor venv users only interested in installing mpi4py, the pip\ncommand in this guide is still accurate.\n\nThis guide has been adapted from a challenge in OLCF's Hands-On with Summit GitHub repository (Python: Parallel HDF5).\n\nThe guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.\n\nOverview\n\nThis guide teaches you how to build a personal, parallel-enabled version of h5py and how to write an HDF5 file in parallel using mpi4py and h5py.\n\nIn this guide, you will:\n\nLearn how to install mpi4py\n\nLearn how to install parallel h5py\n\nTest your build with Python scripts\n\nOLCF Systems this guide applies to:\n\nSummit\n\nAndes\n\nFrontier\n\nParallel HDF5\n\nScientific simulations generate large amounts of data on Summit (about 100 Terabytes per day for some applications).\nBecause of how large some datafiles may be, it is important that writing and reading these files is done as fast as possible.\nLess time spent doing input/output (I/O) leaves more time for advancing a simulation or analyzing data.\n\nOne of the most utilized file types is the Hierarchical Data Format (HDF), specifically the HDF5 format.\nHDF5 is designed to manage large amounts of data and is built for fast I/O processing and storage.\nAn HDF5 file is a container for two kinds of objects: \"datasets\", which are array-like collections of data, and \"groups\", which are folder-like containers that hold datasets and other groups.\n\nThere are various tools that allow users to interact with HDF5 data, but we will be focusing on h5py -- a Python interface to the HDF5 library.\nh5py provides a simple interface to exploring and manipulating HDF5 data as if they were Python dictionaries or NumPy arrays.\nFor example, you can extract specific variables through slicing, manipulate the shapes of datasets, and even write completely new datasets from external NumPy arrays.\n\nBoth HDF5 and h5py can be compiled with MPI support, which allows you to optimize your HDF5 I/O in parallel.\nMPI support in Python is accomplished through the mpi4py package, which provides complete Python bindings for MPI.\nBuilding h5py against mpi4py allows you to write to an HDF5 file using multiple parallel processes, which can be helpful for users handling large datasets in Python.\nh5Py is available after loading the default Python module on either Summit or Andes, but it has not been built with parallel support.\n\nSetting up the environment\n\nBefore setting up your environment, you must exit and log back in so that you have a fresh login shell.\nThis is to ensure that no previously activated environments exist in your $PATH environment variable.\nAdditionally, you should execute module reset.\n\nBuilding h5py from source is highly sensitive to the current environment variables set in your profile.\nBecause of this, it is extremely important that all the modules and environments you plan to load are done in the correct order, so that all the environment variables are set correctly.\n\nFirst, load the gnu compiler module (most Python packages assume GCC), hdf5 module (necessary for h5py), and the python module (allows you to create a new environment):\n\nSummit\n\n.. code-block:: bash\n\n   $ module load gcc\n   $ module load hdf5\n   $ module load python\n\nAndes\n\n.. code-block:: bash\n\n   $ module load gcc\n   $ module load hdf5\n   $ module load python\n\nFrontier\n\n.. code-block:: bash\n\n   $ module load PrgEnv-gnu\n   $ module load hdf5\n\n   # Make sure your personal miniconda installation is in your path\n   $ export PATH=\"/path/to/your/miniconda/bin:$PATH\"\n\nLoading a python module puts you in a \"base\" environment, but you need to create a new environment using the conda create command:\n\nSummit\n\n.. code-block:: bash\n\n   $ conda create -p /ccs/proj/<project_id>/<user_id>/envs/summit/h5pympi-summit python=3.8 numpy\n\nAndes\n\n.. code-block:: bash\n\n   $ conda create -p /ccs/proj/<project_id>/<user_id>/envs/andes/h5pympi-andes python=3.8 numpy\n\nFrontier\n\n.. code-block:: bash\n\n   $ conda create -p /ccs/proj/<project_id>/<user_id>/envs/frontier/h5pympi-frontier python=3.8 libssh numpy -c conda-forge\n\nAs noted in the python <https://docs.olcf.ornl.gov/software/python/index.html> page, it is highly recommended to create new environments in the \"Project Home\" directory.\n\nNumPy is installed ahead of time because h5py depends on it.\n\nAfter following the prompts for creating your new environment, you can now activate it:\n\nSummit\n\n.. code-block:: bash\n\n   $ source activate /ccs/proj/<project_id>/<user_id>/envs/summit/h5pympi-summit\n\nAndes\n\n.. code-block:: bash\n\n   $ source activate /ccs/proj/<project_id>/<user_id>/envs/andes/h5pympi-andes\n\nFrontier\n\n.. code-block:: bash\n\n   $ source activate /ccs/proj/<project_id>/<user_id>/envs/frontier/h5pympi-frontier\n\nInstalling mpi4py\n\nNow that you have a fresh environment, you will next install mpi4py from source into your new environment.\nTo make sure that you are building from source, and not a pre-compiled binary, use pip:\n\nSummit\n\n.. code-block:: bash\n\n   $ MPICC=\"mpicc -shared\" pip install --no-cache-dir --no-binary=mpi4py mpi4py\n\nAndes\n\n.. code-block:: bash\n\n   $ MPICC=\"mpicc -shared\" pip install --no-cache-dir --no-binary=mpi4py mpi4py\n\nFrontier\n\n.. code-block:: bash\n\n   $ MPICC=\"cc -shared\" pip install --no-cache-dir --no-binary=mpi4py mpi4py\n\nThe MPICC flag ensures that you are using the correct C wrapper for MPI on the system.\nBuilding from source typically takes longer than a simple conda install, so the download and installation may take a couple minutes.\nIf everything goes well, you should see a \"Successfully installed mpi4py\" message.\n\nInstalling h5py\n\nNext, install h5py from source.\n\nSummit\n\n.. code-block:: bash\n\n   $ HDF5_MPI=\"ON\" CC=mpicc pip install --no-cache-dir --no-binary=h5py h5py\n\nAndes\n\n.. code-block:: bash\n\n   $ HDF5_MPI=\"ON\" CC=mpicc pip install --no-cache-dir --no-binary=h5py h5py\n\nFrontier\n\n.. code-block:: bash\n\n   $ HDF5_MPI=\"ON\" CC=cc HDF5_DIR=${OLCF_HDF5_ROOT} pip install --no-cache-dir --no-binary=h5py h5py\n\nThe HDF5_MPI flag is the key to telling pip to build h5py with parallel support, while the CC flag makes sure that you are using the correct C wrapper for MPI.\nThis installation will take much longer than both the mpi4py and NumPy installations (5+ minutes if the system is slow).\nWhen the installation finishes, you will see a \"Successfully installed h5py\" message.\n\nTesting parallel h5py\n\nTest your build by trying to write an HDF5 file in parallel using 42 MPI tasks.\n\nFirst, change directories to your GPFS scratch area:\n\n$ cd $MEMBERWORK/<YOUR_PROJECT_ID>\n$ mkdir h5py_test\n$ cd h5py_test\n\nLet's test that mpi4py is working properly first by executing the example Python script \"hello_mpi.py\":\n\n# hello_mpi.py\nfrom mpi4py import MPI\n\ncomm = MPI.COMM_WORLD      # Use the world communicator\nmpi_rank = comm.Get_rank() # The process ID (integer 0-41 for a 42-process job)\n\nprint('Hello from MPI rank %s !' %(mpi_rank))\n\nTo do so, submit a job to the batch queue:\n\nSummit\n\n.. code-block:: bash\n\n   $ bsub -L $SHELL submit_hello.lsf\n\nAndes\n\n.. code-block:: bash\n\n   $ sbatch --export=NONE submit_hello.sl\n\nFrontier\n\n.. code-block:: bash\n\n   $ sbatch --export=NONE submit_hello.sl\n\nExample \"submit_hello\" batch script:\n\nSummit\n\n.. code-block:: bash\n\n   #!/bin/bash\n   #BSUB -P <PROJECT_ID>\n   #BSUB -W 00:05\n   #BSUB -nnodes 1\n   #BSUB -J mpi4py\n   #BSUB -o mpi4py.%J.out\n   #BSUB -e mpi4py.%J.err\n\n   cd $LSB_OUTDIR\n   date\n\n   module load gcc\n   module load hdf5\n   module load python\n\n   source activate /ccs/proj/<project_id>/<user_id>/envs/summit/h5pympi-summit\n\n   jsrun -n1 -r1 -a42 -c42 python3 hello_mpi.py\n\nAndes\n\n.. code-block:: bash\n\n   #!/bin/bash\n   #SBATCH -A <PROJECT_ID>\n   #SBATCH -J mpi4py\n   #SBATCH -N 1\n   #SBATCH -p gpu\n   #SBATCH -t 0:05:00\n\n   unset SLURM_EXPORT_ENV\n\n   cd $SLURM_SUBMIT_DIR\n   date\n\n   module load gcc\n   module load hdf5\n   module load python\n\n   source activate /ccs/proj/<project_id>/<user_id>/envs/andes/h5pympi-andes\n\n   srun -n42 python3 hello_mpi.py\n\nFrontier\n\n.. code-block:: bash\n\n   #!/bin/bash\n   #SBATCH -A <PROJECT_ID>\n   #SBATCH -J mpi4py\n   #SBATCH -N 1\n   #SBATCH -p batch\n   #SBATCH -t 0:05:00\n\n   unset SLURM_EXPORT_ENV\n\n   cd $SLURM_SUBMIT_DIR\n   date\n\n   module load PrgEnv-gnu\n   module load hdf5\n   export PATH=\"/path/to/your/miniconda/bin:$PATH\"\n\n   source activate /ccs/proj/<project_id>/<user_id>/envs/frontier/h5pympi-frontier\n\n   srun -n42 python3 hello_mpi.py\n\nIf mpi4py is working properly, in mpi4py.<JOB_ID>.out you should see output similar to:\n\nHello from MPI rank 21 !\nHello from MPI rank 23 !\nHello from MPI rank 28 !\nHello from MPI rank 40 !\nHello from MPI rank 0 !\nHello from MPI rank 1 !\nHello from MPI rank 32 !\n.\n.\n.\n\nIf you see this, great, it means that mpi4py was built successfully in your environment.\n\nFinally, let's see if you can get these tasks to write to an HDF5 file in parallel using the \"hdf5_parallel.py\" script:\n\n# hdf5_parallel.py\nfrom mpi4py import MPI\nimport h5py\n\ncomm = MPI.COMM_WORLD      # Use the world communicator\nmpi_rank = comm.Get_rank() # The process ID (integer 0-41 for a 42-process job)\nmpi_size = comm.Get_size() # Total amount of ranks\n\nwith h5py.File('output.h5', 'w', driver='mpio', comm=MPI.COMM_WORLD) as f:\n    dset = f.create_dataset('test', (42,), dtype='i')\n    dset[mpi_rank] = mpi_rank\n\ncomm.Barrier()\n\nif (mpi_rank == 0):\n    print('42 MPI ranks have finished writing!')\n\nThe MPI tasks are going to write to a file named \"output.h5\", which contains a dataset called \"test\" that is of size 42 (assigned to the \"dset\" variable in Python).\nEach MPI task is going to assign their rank value to the \"dset\" array in Python, so you should end up with a dataset that contains 0-41 in ascending order.\n\nTime to execute \"hdf5_parallel.py\" by submitting \"submit_h5py\" to the batch queue:\n\nSummit\n\n.. code-block:: bash\n\n   $ bsub -L $SHELL submit_h5py.lsf\n\nAndes\n\n.. code-block:: bash\n\n   $ sbatch --export=NONE submit_h5py.sl\n\nFrontier\n\n.. code-block:: bash\n\n   $ sbatch --export=NONE submit_h5py.sl\n\nExample \"submit_h5py\" batch script:\n\nSummit\n\n.. code-block:: bash\n\n   #!/bin/bash\n   #BSUB -P <PROJECT_ID>\n   #BSUB -W 00:05\n   #BSUB -nnodes 1\n   #BSUB -J h5py\n   #BSUB -o h5py.%J.out\n   #BSUB -e h5py.%J.err\n\n   cd $LSB_OUTDIR\n   date\n\n   module load gcc\n   module load hdf5\n   module load python\n\n   source activate /ccs/proj/<project_id>/<user_id>/envs/summit/h5pympi-summit\n\n   jsrun -n1 -r1 -a42 -c42 python3 hdf5_parallel.py\n\nAndes\n\n.. code-block:: bash\n\n   #!/bin/bash\n   #SBATCH -A <PROJECT_ID>\n   #SBATCH -J h5py\n   #SBATCH -N 1\n   #SBATCH -p gpu\n   #SBATCH -t 0:05:00\n\n   unset SLURM_EXPORT_ENV\n\n   cd $SLURM_SUBMIT_DIR\n   date\n\n   module load gcc\n   module load hdf5\n   module load python\n\n   source activate /ccs/proj/<project_id>/<user_id>/envs/andes/h5pympi-andes\n\n   srun -n42 python3 hdf5_parallel.py\n\nFrontier\n\n.. code-block:: bash\n\n   #!/bin/bash\n   #SBATCH -A <PROJECT_ID>\n   #SBATCH -J h5py\n   #SBATCH -N 1\n   #SBATCH -p batch\n   #SBATCH -t 0:05:00\n\n   unset SLURM_EXPORT_ENV\n\n   cd $SLURM_SUBMIT_DIR\n   date\n\n   module load PrgEnv-gnu\n   module load hdf5\n   export PATH=\"/path/to/your/miniconda/bin:$PATH\"\n\n   source activate /ccs/proj/<project_id>/<user_id>/envs/frontier/h5pympi-frontier\n\n   srun -n42 python3 hdf5_parallel.py\n\nProvided there are no errors, you should see \"42 MPI ranks have finished writing!\" in your output file, and there should be a new file called \"output.h5\" in your directory.\nTo see explicitly that the MPI tasks did their job, you can use the h5dump command to view the dataset named \"test\" in output.h5:\n\n$ h5dump output.h5\n\nHDF5 \"output.h5\" {\nGROUP \"/\" {\n   DATASET \"test\" {\n      DATATYPE  H5T_STD_I32LE\n      DATASPACE  SIMPLE { ( 42 ) / ( 42 ) }\n      DATA {\n      (0): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n      (19): 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n      (35): 35, 36, 37, 38, 39, 40, 41\n      }\n   }\n}\n}\n\nIf you see the above output, then the build was a success!\n\nAdditional Resources\n\nh5py Documentation\n\nmpi4py Documentation\n\nHDF5 Support Page"}
{"doc":"paraview","text":"ParaView\n\nOverview\n\nParaView is an open-source, multi-platform data\nanalysis and visualization application. ParaView users can quickly build\nvisualizations to analyze their data using qualitative and quantitative\ntechniques. The data exploration can be done interactively in 3D or\nprogrammatically using ParaView’s batch processing capabilities. Further\ninformation regarding ParaView can be found at the links provided in the\nparaview-resources <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#paraview-resources> section.\n\nParaView was developed to analyze extremely large datasets using distributed\nmemory computing resources. The OLCF provides ParaView server installs on Andes\nand Summit to facilitate large scale distributed visualizations. The ParaView\nserver running on Andes and Summit may be used in a headless batch processing\nmode or be used to drive a ParaView GUI client running on your local machine.\n\nFor a tutorial of how to get started with ParaView on Andes, see our\nParaView at OLCF Tutorial.\n\n\n\nInstalling and Setting Up ParaView\n\nAlthough in a single machine setup both the ParaView client and server run on\nthe same host, this need not be the case. It is possible to run a local\nParaView client to display and interact with your data while the ParaView\nserver runs in an Andes or Summit batch job, allowing interactive analysis of\nvery large data sets.\n\nYou will obtain the best performance by running the ParaView client on your\nlocal computer and running the server on OLCF resources with the same version\nof ParaView. It is highly recommended to check the available ParaView versions\nusing module avail paraview on the system you plan to connect ParaView to.\nPrecompiled ParaView binaries for Windows, macOS, and Linux can be downloaded\nfrom Kitware.\n\nRecommended ParaView versions on our systems:\n\nSummit: ParaView 5.9.1, 5.10.0, 5.11.0\n\nAndes: ParaView 5.9.1, 5.10.0, 5.11.0\n\nUsing a different version than what is listed above is not guaranteed to work properly.\n\nWe offer two rendering modes of the ParaView API on our systems: OSMesa and\nEGL.  OSMesa is intended for use on regular compute nodes, whereas EGL is\nintended for use on GPU enabled nodes. When running interactively, you do not\nneed to download or install anything special to use the EGL or OSMesa versions,\nas you'll be able to choose between those options when connecting to the system\n(see paraview-gui <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#paraview-gui> below). If instead you're running in batch mode on the\ncommand line (see paraview-command-line <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#paraview-command-line> below), you can choose between\nthe rendering options by loading its corresponding module on the system you're\nconnected to. For example, to see these modules on Summit:\n\n[user@login4.summit ~]$ module -t avail paraview\n\n/sw/summit/modulefiles/core:\nparaview/5.9.1-egl\nparaview/5.9.1-osmesa\nparaview/5.10.0-egl\nparaview/5.10.0-osmesa\nparaview/5.11.0-egl\nparaview/5.11.0-osmesa\n\n[user@login4.summit ~]$ module load paraview/5.9.1-egl\n\nIt is highly recommended to only use the modules located in\n/sw/andes/modulefiles/core or /sw/summit/modulefiles/core.\n\nThe EGL mode seems to work better with larger datasets and is generally\nrecommended over OSMesa on our systems. However, we encourage users to try both\noptions and see which version works best for their data.\n\nAfter installing, you must give ParaView the relevant server information to be\nable to connect to OLCF systems (comparable to VisIt's system of host\nprofiles). The following provides an example of doing so. Although several\nmethods may be used, the one described should work in most cases.\n\nFor macOS clients, it is necessary to install XQuartz\n(X11) to get a command prompt\nin which you will securely enter your OLCF credentials.\n\nFor Windows clients, it is necessary to install PuTTY to\ncreate an ssh connection in step 2.\n\nStep 1: Save the following servers.pvsc file to your local computer\n\nFor Andes:\n\n<Servers>\n  <Server name=\"ORNL andes\" resource=\"csrc://localhost\">\n    <CommandStartup>\n      <Options>\n        <Option name=\"HOST\" label=\"Server host\" save=\"true\">\n          <String default=\"andes.olcf.ornl.gov\"/>\n        </Option>\n        <Option name=\"HEADLESS_API\" label=\"Server headless API\" save=\"true\">\n          <Enumeration default=\"osmesa\">\n            <Entry value=\"osmesa\" label= \"OSMesa\" />\n            <Entry value=\"egl\" label= \"EGL\" />\n          </Enumeration>\n        </Option>\n        <Option name=\"USER\" label=\"Server username\" save=\"true\">\n          <String default=\"YOURUSERNAME\"/>\n        </Option>\n        <Switch name=\"PV_CLIENT_PLATFORM\">\n          <Case value=\"Apple\">\n            <Set name=\"TERM_PATH\" value=\"/opt/X11/bin/xterm\" />\n            <Set name=\"TERM_ARG1\" value=\"-T\" />\n            <Set name=\"TERM_ARG2\" value=\"ParaView\" />\n            <Set name=\"TERM_ARG3\" value=\"-e\" />\n            <Set name=\"SSH_PATH\" value=\"ssh\" />\n          </Case>\n          <Case value=\"Linux\">\n            <Set name=\"TERM_PATH\" value=\"xterm\" />\n            <Set name=\"TERM_ARG1\" value=\"-T\" />\n            <Set name=\"TERM_ARG2\" value=\"ParaView\" />\n            <Set name=\"TERM_ARG3\" value=\"-e\" />\n            <Set name=\"SSH_PATH\" value=\"ssh\" />\n          </Case>\n          <Case value=\"Windows\">\n            <Set name=\"TERM_PATH\" value=\"cmd\" />\n            <Set name=\"TERM_ARG1\" value=\"/C\" />\n            <Set name=\"TERM_ARG2\" value=\"start\" />\n            <Set name=\"TERM_ARG3\" value=\"\" />\n            <Set name=\"SSH_PATH\" value=\"plink.exe\" />\n          </Case>\n          <Case value=\"Unix\">\n            <Set name=\"TERM_PATH\" value=\"xterm\" />\n            <Set name=\"TERM_ARG1\" value=\"-T\" />\n            <Set name=\"TERM_ARG2\" value=\"ParaView\" />\n            <Set name=\"TERM_ARG3\" value=\"-e\" />\n            <Set name=\"SSH_PATH\" value=\"ssh\" />\n          </Case>\n        </Switch>\n        <Option name=\"PV_SERVER_PORT\" label=\"Server port \">\n          <Range type=\"int\" min=\"1025\" max=\"65535\" step=\"1\" default=\"random\"/>\n        </Option>\n        <Option name=\"NUM_NODES\" label=\"Number of compute nodes\" save=\"true\">\n          <Range type=\"int\" min=\"1\" max=\"512\" step=\"1\" default=\"2\"/>\n        </Option>\n        <Option name=\"NUM_MPI_TASKS\" label=\"Total number of MPI tasks\" save=\"true\">\n          <Range type=\"int\" min=\"1\" max=\"16384\" step=\"1\" default=\"2\"/>\n        </Option>\n        <Option name=\"NUM_CORES_PER_MPI_TASK\" label=\"Number of cores per MPI task\" save=\"true\">\n          <Range type=\"int\" min=\"1\" max=\"28\" step=\"1\" default=\"1\"/>\n        </Option>\n        <Option name=\"PROJECT\" label=\"Project to charge\" save=\"true\">\n          <String default=\"cscXXX\"/>\n        </Option>\n        <Option name=\"MINUTES\" label=\"Number of minutes to reserve\" save=\"true\">\n          <Range type=\"int\" min=\"1\" max=\"240\" step=\"1\" default=\"30\"/>\n        </Option>\n      </Options>\n      <Command exec=\"$TERM_PATH$\" delay=\"5\">\n        <Arguments>\n          <Argument value=\"$TERM_ARG1$\"/>\n          <Argument value=\"$TERM_ARG2$\"/>\n          <Argument value=\"$TERM_ARG3$\"/>\n          <Argument value=\"$SSH_PATH$\"/>\n          <Argument value=\"-t\"/>\n          <Argument value=\"-R\"/>\n          <Argument value=\"$PV_SERVER_PORT$:localhost:$PV_SERVER_PORT$\"/>\n          <Argument value=\"$USER$@$HOST$\"/>\n          <Argument value=\"/sw/andes/paraview/pvsc/ORNL/login_node.sh\"/>\n          <Argument value=\"$NUM_NODES$\"/>\n          <Argument value=\"$MINUTES$\"/>\n          <Argument value=\"$PV_SERVER_PORT$\"/>\n          <Argument value=\"$PV_VERSION_FULL$\"/>\n          <Argument value=\"$HEADLESS_API$\"/>\n          <Argument value=\"/sw/andes/paraview/pvsc/ORNL/andes.cfg\"/>\n          <Argument value=\"PROJECT=$PROJECT$\"/>\n          <Argument value=\"NUM_MPI_TASKS=$NUM_MPI_TASKS$\"/>\n          <Argument value=\"NUM_CORES_PER_MPI_TASK=$NUM_CORES_PER_MPI_TASK$\"/>\n        </Arguments>\n      </Command>\n    </CommandStartup>\n  </Server>\n</Servers>\n\nFor Summit:\n\n<Servers>\n  <Server name=\"ORNL summit\" resource=\"csrc://localhost\">\n    <CommandStartup>\n      <Options>\n        <Option name=\"HOST\" label=\"Server host\" save=\"true\">\n          <String default=\"summit.olcf.ornl.gov\"/>\n        </Option>\n        <Option name=\"HEADLESS_API\" label=\"Server headless API\" save=\"true\">\n          <Enumeration default=\"osmesa\">\n            <Entry value=\"osmesa\" label= \"OSMesa\" />\n            <Entry value=\"egl\" label= \"EGL\" />\n          </Enumeration>\n        </Option>\n        <Option name=\"USER\" label=\"Server username\" save=\"true\">\n          <String default=\"YOURUSERNAME\"/>\n        </Option>\n        <Switch name=\"PV_CLIENT_PLATFORM\">\n          <Case value=\"Apple\">\n            <Set name=\"TERM_PATH\" value=\"/opt/X11/bin/xterm\" />\n            <Set name=\"TERM_ARG1\" value=\"-T\" />\n            <Set name=\"TERM_ARG2\" value=\"ParaView\" />\n            <Set name=\"TERM_ARG3\" value=\"-e\" />\n            <Set name=\"SSH_PATH\" value=\"ssh\" />\n          </Case>\n          <Case value=\"Linux\">\n            <Set name=\"TERM_PATH\" value=\"xterm\" />\n            <Set name=\"TERM_ARG1\" value=\"-T\" />\n            <Set name=\"TERM_ARG2\" value=\"ParaView\" />\n            <Set name=\"TERM_ARG3\" value=\"-e\" />\n            <Set name=\"SSH_PATH\" value=\"ssh\" />\n          </Case>\n          <Case value=\"Windows\">\n            <Set name=\"TERM_PATH\" value=\"cmd\" />\n            <Set name=\"TERM_ARG1\" value=\"/C\" />\n            <Set name=\"TERM_ARG2\" value=\"start\" />\n            <Set name=\"TERM_ARG3\" value=\"\" />\n            <Set name=\"SSH_PATH\" value=\"plink.exe\" />\n          </Case>\n          <Case value=\"Unix\">\n            <Set name=\"TERM_PATH\" value=\"xterm\" />\n            <Set name=\"TERM_ARG1\" value=\"-T\" />\n            <Set name=\"TERM_ARG2\" value=\"ParaView\" />\n            <Set name=\"TERM_ARG3\" value=\"-e\" />\n            <Set name=\"SSH_PATH\" value=\"ssh\" />\n          </Case>\n        </Switch>\n        <Option name=\"PV_SERVER_PORT\" label=\"Server port \">\n          <Range type=\"int\" min=\"1025\" max=\"65535\" step=\"1\" default=\"random\"/>\n        </Option>\n        <Option name=\"NUM_NODES\" label=\"Number of compute nodes\" save=\"true\">\n          <Range type=\"int\" min=\"1\" max=\"100\" step=\"1\" default=\"1\"/>\n        </Option>\n        <Option name=\"NRS\" label=\"Number of resource sets (RS)\" save=\"true\">\n          <Range type=\"int\" min=\"1\" max=\"202400\" step=\"1\" default=\"1\"/>\n        </Option>\n        <Option name=\"TASKS_PER_RS\" label=\"Number of MPI tasks (ranks) per RS\" save=\"true\">\n          <Range type=\"int\" min=\"1\" max=\"42\" step=\"1\" default=\"1\"/>\n        </Option>\n        <Option name=\"CPU_PER_RS\" label=\"Number of CPUs (cores) per RS\" save=\"true\">\n          <Range type=\"int\" min=\"1\" max=\"42\" step=\"1\" default=\"1\"/>\n        </Option>\n        <Option name=\"GPU_PER_RS\" label=\"Number of GPUs per RS\" save=\"true\">\n          <Range type=\"int\" min=\"0\" max=\"6\" step=\"1\" default=\"0\"/>\n        </Option>\n        <Option name=\"PROJECT\" label=\"Project to charge\" save=\"true\">\n          <String default=\"cscXXX\"/>\n        </Option>\n        <Option name=\"MINUTES\" label=\"Number of minutes to reserve\" save=\"true\">\n          <Range type=\"int\" min=\"1\" max=\"240\" step=\"1\" default=\"30\"/>\n        </Option>\n      </Options>\n      <Command exec=\"$TERM_PATH$\" delay=\"5\">\n        <Arguments>\n          <Argument value=\"$TERM_ARG1$\"/>\n          <Argument value=\"$TERM_ARG2$\"/>\n          <Argument value=\"$TERM_ARG3$\"/>\n          <Argument value=\"$SSH_PATH$\"/>\n          <Argument value=\"-t\"/>\n          <Argument value=\"-R\"/>\n          <Argument value=\"$PV_SERVER_PORT$:localhost:$PV_SERVER_PORT$\"/>\n          <Argument value=\"$USER$@$HOST$\"/>\n          <Argument value=\"/sw/summit/paraview/pvsc/ORNL/login_node.sh\"/>\n          <Argument value=\"$NUM_NODES$\"/>\n          <Argument value=\"$MINUTES$\"/>\n          <Argument value=\"$PV_SERVER_PORT$\"/>\n          <Argument value=\"$PV_VERSION_FULL$\"/>\n          <Argument value=\"$HEADLESS_API$\"/>\n          <Argument value=\"/sw/summit/paraview/pvsc/ORNL/summit.cfg\"/>\n          <Argument value=\"PROJECT=$PROJECT$\"/>\n          <Argument value=\"NRS=$NRS$\"/>\n          <Argument value=\"TASKS_PER_RS=$TASKS_PER_RS$\"/>\n          <Argument value=\"CPU_PER_RS=$CPU_PER_RS$\"/>\n          <Argument value=\"GPU_PER_RS=$GPU_PER_RS$\"/>\n        </Arguments>\n      </Command>\n    </CommandStartup>\n  </Server>\n</Servers>\n\nAlthough they can be separate files, both Andes and Summit server\nconfigurations can be combined and saved into one file following the hierarchy\n<Servers><Server name= >...<\\Server><Server name= >...<\\Server><\\Servers>.\n\nStep 2: Launch ParaView on your Desktop and Click on File -> Connect\n\nStart ParaView and then select File/Connect to begin.\n\n\n\nStep 3: Import Servers\n\nClick Load Servers button and find the servers.pvsc file\n\n\n\nThe Fetch Servers button fetches\nOfficial Kitware Server Configurations.\nSummit and Andes configurations can be imported through this method, but are\nnot guaranteed to be supported in future updates. Users may use these\nat their own risk.\n\nAfter successfully completing the above steps, you should now be able to\nconnect to either Andes or Summit.\n\n\n\nRemote GUI Usage\n\nAfter setting up and installing ParaView, you can connect to OLCF systems\nremotely to visualize your data interactively through ParaView's GUI. To do so,\ngo to File→Connect and select either ORNL Andes or ORNL Summit (provided they\nwere successfully imported -- as outlined in paraview-install-setup <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#paraview-install-setup>).\nNext, click on Connect and change the values in the Connection Options box.\n\n\n\nA dialog box follows, in which you must enter in your username and project\nallocation, the number of nodes to reserve and a duration to reserve them for.\nThis is also where you can choose between the OSMesa and EGL rendering options\n(via the \"Server headless API\" box).\n\n\n\nWhen you click OK, a windows command prompt or xterm pops up. In this\nwindow enter your credentials at the OLCF login prompt.\n\n\n\nWhen your job reaches the top of the queue, the main window will be returned to\nyour control. At this point you are connected and can open files that reside\nthere and visualize them interactively.\n\nCreating a Python Trace\n\nOne of the most convenient tools available in the GUI is the ability to convert\n(or \"trace\") interactive actions in ParaView to Python code. Users that repeat\na sequence of actions in ParaView to visualize their data may find the Trace\ntool useful. The Trace tool creates a Python script that reflects most actions\ntaken in ParaView, which then can be used by either PvPython or PvBatch\n(ParaView's Python interfaces) to accomplish the same actions. See section\nparaview-command-line <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#paraview-command-line> for an example of how to run a Python script using\nPvBatch on Andes and Summit.\n\nTo start tracing from the GUI, click on Tools→Start Trace. An options window\nwill pop up and prompt for specific Trace settings other than the default. Upon\nstarting the trace, any time you modify properties, create filters, open files,\nand hit Apply, etc., your actions will be translated into Python syntax. Once\nyou are finished tracing the actions you want to script, click Tools→Stop\nTrace. A Python script should then be displayed to you and can be saved.\n\n\n\nCommand Line Example\n\nUsing ParaView via the command line should always be done through a\nbatch job, and should always be executed on a compute node -- never the\nlogin or launch nodes.\n\nParaView can be controlled through Python without opening the ParaView GUI. To\ndo this on OLCF systems, one must use a batch script in combination with\nPvBatch (one of the Python interfaces available in ParaView). PvBatch accepts\ncommands from Python scripts and will run in parallel using MPI. Example\nbatch scripts, along with a working Python example, are provided below.\n\nAndes\n\n.. code-block:: bash\n\n\n   #!/bin/bash\n   #SBATCH -A XXXYYY\n   #SBATCH -J para_test\n   #SBATCH -N 1\n   #SBATCH -p batch\n   #SBATCH -t 0:05:00\n\n   cd $SLURM_SUBMIT_DIR\n   date\n\n   module load paraview/5.11.0-osmesa\n\n   srun -n 28 pvbatch para_example.py\n\nSummit\n\n.. code-block:: bash\n\n\n   #!/bin/bash\n   #BSUB -P XXXYYY\n   #BSUB -W 00:05\n   #BSUB -nnodes 1\n   #BSUB -J para_test\n   #BSUB -o para_test.%J.out\n   #BSUB -e para_test.%J.err\n\n   cd $LSB_OUTDIR\n   date\n\n   module load paraview/5.11.0-osmesa\n\n   # Set up flags for jsrun\n   export NNODES=$(($(cat $LSB_DJOB_HOSTFILE | uniq | wc -l)-1))\n   export NCORES_PER_NODE=28\n   export NGPU_PER_NODE=0\n   export NRS_PER_NODE=1\n   export NMPI_PER_RS=28\n   export NCORES_PER_RS=$(($NCORES_PER_NODE/$NRS_PER_NODE))\n   export NGPU_PER_RS=$(($NGPU_PER_NODE/$NRS_PER_NODE))\n   export NRS=$(($NNODES*$NRS_PER_NODE))\n\n   jsrun -n ${NRS} -r ${NRS_PER_NODE} -a ${NMPI_PER_RS} -g ${NGPU_PER_RS} -c ${NCORES_PER_RS} pvbatch para_example.py\n\nIf you plan on using the EGL version of the ParaView module (e.g.,\nparaview/5.11.0-egl), then you must be connected to the GPUs. On Andes,\nthis is done by using the gpu partition via #SBATCH -p gpu, while\non Summit the -g flag in the jsrun command must be greater\nthan zero.\n\nSubmitting one of the above scripts will submit a job to the batch partition\nfor five minutes using 28 MPI tasks across 1 node. As rendering speeds and\nmemory issues widely vary for different datasets and MPI tasks, users are\nencouraged to find the optimal amount of MPI tasks to use for their data. Users\nwith large datasets may also find a slight increase in performance by using the\ngpu partition on Andes, or by utilizing the GPUs on Summit. Once the batch job\nmakes its way through the queue, the script will launch the loaded ParaView\nmodule (specified with module load) and execute a python script called\npara_example.py using PvBatch. The example python script is detailed below,\nand users are highly encouraged to use this script (especially after version\nupgrades) for testing purposes.\n\nThe following script renders a 3D sphere colored by the ID (rank) of each MPI task:\n\n# para_example.py:\nfrom paraview.simple import *\n\n# Add a polygonal sphere to the 3D scene\ns = Sphere()\ns.ThetaResolution = 128                        # Number of theta divisions (longitude lines)\ns.PhiResolution = 128                          # Number of phi divisions (latitude lines)\n\n# Convert Proc IDs to scalar values\np = ProcessIdScalars()                         # Apply the ProcessIdScalars filter to the sphere\n\ndisplay = Show(p)                              # Show data\ncurr_view = GetActiveView()                    # Retrieve current view\n\n# Generate a colormap for Proc Id's\ncmap = GetColorTransferFunction(\"ProcessId\")   # Generate a function based on Proc ID\ncmap.ApplyPreset('Viridis (matplotlib)')       # Apply the Viridis preset colors\n#print(GetLookupTableNames())                  # Print a list of preset color schemes\n\n# Set Colorbar Properties\ndisplay.SetScalarBarVisibility(curr_view,True) # Show bar\nscalarBar = GetScalarBar(cmap, curr_view)      # Get bar's properties\nscalarBar.WindowLocation = 'Any Location'       # Allows free movement\nscalarBar.Orientation = 'Horizontal'           # Switch from Vertical to Horizontal\nscalarBar.Position = [0.15,0.80]               # Bar Position in [x,y]\nscalarBar.LabelFormat = '%.0f'                 # Format of tick labels\nscalarBar.RangeLabelFormat = '%.0f'            # Format of min/max tick labels\nscalarBar.ScalarBarLength = 0.7                # Set length of bar\n\n# Render scene and save resulting image\nRender()\nSaveScreenshot('pvbatch-test.png',ImageResolution=[1080, 1080])\n\nFor older versions of ParaView (e.g., 5.9.1), line 23 should be 'AnyLocation' (no space).\n\n\n\nIf everything is working properly, the above image should be generated after\nthe batch job is complete.\n\nAll of the above can also be achieved in an interactive batch job through the\nuse of the salloc command on Andes or the bsub -Is command on Summit.\nRecall that login nodes should not be used for memory- or compute-intensive\ntasks, including ParaView.\n\nTroubleshooting\n\nProcess failed to start connection issue (or DISPLAY not set)\n\nIf ParaView is unable to connect to our systems after trying to initiate a\nconnection via the GUI and you see a \"The process failed to start. Either the\ninvoked program is missing, or you may have insufficient permissions to invoke\nthe program\" error, make sure that you have XQuartz (X11) installed.\n\nFor macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will\nsecurely enter your OLCF credentials.\n\nAfter installing, if you see a \"Can't open display\" or a \"DISPLAY is not set\"\nerror, try restarting your computer. Sometimes XQuartz doesn't function\nproperly if the computer was never restarted after installing.\n\nParaView crashes when using the EGL API module via command line\n\nIf ParaView crashes when using the EGL version of the ParaView module via the\ncommand line and raises errors about OpenGL drivers or features, this is most\nlikely due to not being connected to any GPUs.\n\nDouble check that you are either running on the GPU partition on Andes (i.e.,\n-p gpu), or that you have -g set to a value greater than zero in your\njsrun command on Summit.\n\nIf problems persist and you do not need EGL, try using the OSMesa version of\nthe module instead (e.g., paraview/5.9.1-osmesa instead of paraview/5.9.1-egl).\n\nDefault Andes module not working with PvBatch or PvPython (Aug. 31, 2021)\n\nA command not found error occurs when trying to execute either PvBatch or\nPvPython after loading the default ParaView module on Andes. To fix this, you\nmust load the equivalent ParaView module ending in \"pyapi\" instead (i.e.,\nmodule load paraview/5.9.1-py3-pyapi instead of module load\nparaview/5.9.1-py3).\n\nAlternatively, the ParaView installations in /sw/andes/paraview (i.e., the\nparaview/5.9.1-egl and paraview/5.9.1-osmesa modules) can also be loaded to\navoid this issue.\n\n\n\nAdditional Resources\n\nThe ParaView at OLCF Tutorial highlights\nhow to get started on Andes with example datasets.\n\nThe Official ParaView User's Guide\nand the Python API Documentation\ncontain all information regarding the GUI and Python interfaces.\n\nA full list of ParaView Documentation\ncan be found on ParaView's website.\n\nThe ParaView Wiki\ncontains extensive information about all things ParaView.\n\nTutorials can be found on the ParaView Wiki at\nThe ParaView Tutorial and\nSNL ParaView Tutorials.\n\nSample Data not pre-packaged with\nParaView can be found on the ParaView download page under the Data section.\n\nSpecific ParaView Versions and their\nRelease Notes\ncan be found on the ParaView website and ParaView Wiki, respectively.\n\nNon-ORNL related bugs and issues in ParaView can be found and reported on\nDiscourse."}
{"doc":"parsl","text":"Parsl\n\n\n\nOverview\n\nParsl is a flexible and scalable parallel programming library for Python which\nis being developed at the University of Chicago. It augments Python with simple\nconstructs for encoding parallelism. For more information about Parsl, please\nrefer to its documentation.\n\nPrerequisites\n\nParsl can be installed with Conda for use on Summit by running the following\nfrom a login node:\n\n$ module load workflows\n$ module load parsl/1.1.0\n\nHello world!\n\nThe following instructions illustrate how to run a \"Hello world\" program with\nParsl on Summit.\n\nParsl needs to be able to write to the working directory from compute nodes,\nso we will work from within the member work directory and assume a project ID\nABC123:\n\n$ mkdir -p ${MEMBERWORK}/abc123/parsl-demo/\n$ cd ${MEMBERWORK}/abc123/parsl-demo/\n\nTo run an example \"Hello world\" program with Parsl on Summit, create a\nfile called hello-parsl.py with the following contents, but with your own\nproject ID in the line specified:\n\nfrom parsl.addresses import address_by_interface\nfrom parsl.config import Config\nfrom parsl.executors import HighThroughputExecutor\nfrom parsl.launchers import JsrunLauncher\nfrom parsl.providers import LSFProvider\n\nfrom parsl import python_app\n\nimport parsl\n\nparsl.set_stream_logger()\n\nconfig = Config(\n    executors = [\n        HighThroughputExecutor(\n            label = 'Summit_HTEX',\n            address = address_by_interface('ib0'),\n            worker_port_range = (50000, 55000),\n            provider = LSFProvider(\n                launcher = JsrunLauncher(),\n                walltime = '00:10:00',\n                nodes_per_block = 1,\n                init_blocks = 1,\n                max_blocks = 1,\n                worker_init = 'source activate parsl-py36',\n                project = 'abc123', # replace this line\n                cmd_timeout = 30\n            )\n        )\n    ]\n)\n\n@python_app\ndef hello ():\n    import platform\n    return 'Hello from {}'.format(platform.uname())\n\nparsl.load(config)\nprint(hello().result())\nparsl.clear()\n\nNow, run the program from a shell or script:\n\n$ python3 hello-parsl.py\n\nThere will be a flood of output to stdout, but the lines that indicate\nsuccessful execution will look something like the following:\n\n2021-06-28 16:10:46 parsl.dataflow.dflow:431 [INFO]  Task 0 completed (launched -> exec_done)\nHello from uname_result(system='Linux', node='a01n14', release='4.14.0-115.21.2.el7a.ppc64le', version='#1 SMP Thu May 7 22:22:31 UTC 2020', machine='ppc64le', processor='ppc64le')\n\nCongratulations! You have now run a Parsl job on Summit."}
{"doc":"pbdR","text":"R and pbdR on Summit\n\nLoading R\n\nSeveral versions of R are available on Summit. You can see which by entering\nthe command module spider r. Throughout this example, we will be using R\nversion 3.6.1.\n\nIf you have logged in with the default modules, then you need to swap\nxl for gcc and the load R:\n\nmodule swap xl gcc/6.4.0\nmodule load r/3.6.1\n\nIf we do that and launch R, then we see:\n\nversion\n## platform       powerpc64le-unknown-linux-gnu\n## arch           powerpc64le\n## os             linux-gnu\n## system         powerpc64le, linux-gnu\n## status\n## major          3\n## minor          6.1\n## year           2019\n## month          07\n## day            05\n## svn rev        76782\n## language       R\n## version.string R version 3.6.1 (2019-07-05)\n## nickname       Action of the Toes\n\nsessionInfo()\n## R version 3.6.1 (2019-07-05)\n## Platform: powerpc64le-unknown-linux-gnu (64-bit)\n## Running under: Red Hat Enterprise Linux Server 7.6 (Maipo)\n##\n## Matrix products: default\n## BLAS/LAPACK: /autofs/nccs-svm1_sw/summit/r/3.6.1/rhel7.6_gnu6.4.0/lib64/R/lib/libRblas.so\n##\n## locale:\n## [1] C\n##\n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base\n##\n## loaded via a namespace (and not attached):\n## [1] compiler_3.6.1\n\nHow to Run an R Script\n\nSummit has a node hierarchy that can be very confusing for the\nuninitiated. The Summit User\nGuide\nexplains this in depth. This has some consequences that may be unusual\nfor R programmers. A few important ones to note are:\n\nYou must have a script that you can run in batch, e.g. with\nRscript or R CMD BATCH\n\nAll data that needs to be visible to the R process (including the\nscript and your packages) must be on gpfs (not your home directory!).\n\nYou must launch your script from the launch nodes with jsrun.\n\nR Hello World Example\n\nWe'll start with something very simple. The script is:\n\n\"hello world\"\n\nSave this in a file hw.r, somewhere on gpfs. So say your project\nis abc123. You might have hw.r in\n/gpfs/alpine/abc123/proj-shared/my_hw_path/.\n\nThere are two ways we can run this. One is with an interactive job, and\none is with a batch job. The interactive job doesn’t provide us with the\nability to run interactive R jobs on the compute nodes (using R interactively\non the compute node is complicated, so we do not discuss that here.). However,\nit does allow us to interactively submit tasks to the compute nodes (launched\nvia jsrun). This can be useful if you are trying to debug a script\nthat unexpectedly dies, without having to continuously submit jobs to\nthe batch queue (more on that in a moment).\n\nWe can start an interactive job with 1 node that will run for no more\nthan 10 minutes via:\n\nbsub -P $PROJECT -nnodes 1 -W 10 -Is $SHELL\n\nNote that you need to either set the shell variable PROJECT to your\nproject identifier, or replace $PROJECT above with the identifier.\n\nOnce your job begins, you will again be at a shell prompt, but the host\nshould have changed from something like login1 to batch1\n(numbering may differ). From here, you can launch the hello world\nscript.\n\nWith our interactive job running, we can run our script on the compute node\nvia:\n\n$ jsrun -n 1 Rscript hw.r\n## [1] \"hello world\"\n\nOur task run, we can enter exit into the terminal.\n\nOf course, this involves no parallelism, since it is a single R session.\nWe will show how to run a basic MPI example with pbdR next.\n\npbdR Hello World Example\n\nThe R code is:\n\nsuppressMessages(library(pbdMPI))\n\nmsg = paste0(\"Hello from rank \", comm.rank(), \" (local rank \", comm.localrank(), \") of \", comm.size())\ncomm.print(msg, all.rank=TRUE, quiet=TRUE)\n\nfinalize()\n\nAs before, save this file as, say, pbdr_hw.r somewhere on gpfs. This\ntime, we will get an interactive node with 2 nodes:\n\nbsub -P $PROJECT -nnodes 2 -W 10 -Is $SHELL\n\nWe will use 2 MPI ranks per node, giving 4 total ranks across the 2 nodes:\n\n$ jsrun -n4 -r2 Rscript hw.r\n## [1] \"Hello from rank 0 (local rank 0) of 4\"\n## [1] \"Hello from rank 1 (local rank 1) of 4\"\n## [1] \"Hello from rank 2 (local rank 0) of 4\"\n## [1] \"Hello from rank 3 (local rank 1) of 4\"\n\nAt this point, we are still running the job and can submit more tasks to\nthe compute nodes if we like. If not, we can end the job by entering\nexit to the terminal.\n\nSo far we have seen how to launch jobs interactively. The other way to run our\nscript is to submit a batch job. To do that, we need to create a batch script:\n\n#!/bin/bash\n#BSUB -P ABC123\n#BSUB -W 10\n#BSUB -nnodes 2\n#BSUB -J rhw\n\nmodule load gcc/6.4.0\nmodule load r/3.6.1\n\ncd /gpfs/alpine/abc123/proj-shared/my_hw_path/\n\njsrun -n4 -r2 Rscript hw.r\n\nBefore continuing, a few comments. First you need to replace the example\nproject identifiers (ABC123 above) with your project. Second, load\nthe appropriate modules (here we just need R). Third, make sure that you\nchange directory to the appropriate place on gpfs. Fourth, add your\njsrun call. Finally, you should also change the name of your job from\nrhw to something else by modifying the #BSUB -J line.\n\nWe need to save this to a file, say job.bs. We submit the job to the\nqueue via bsub job.bs. Once we do, we have to wait for the job to\nstart, then to run. After however long that takes, I get the output file\nrhw.679095. If I cat that file, I see:\n\n[1] \"Hello from rank 0 (local rank 0) of 4\"\n[1] \"Hello from rank 1 (local rank 1) of 4\"\n[1] \"Hello from rank 2 (local rank 0) of 4\"\n[1] \"Hello from rank 3 (local rank 1) of 4\"\n\n------------------------------------------------------------\n(additional output excluded for brevity's sake)\n\nThe information below the dashes which we omitted can be occasionally\nhelpful for debugging, say if there is some kind of hardware problem..\n\nCommon R Packages for Parallelism\n\nThere are many R packages for parallel computing. Some popular ones include\nthe core parallel package, as well as high-level interface packages like\nfuture and foreach. Many of these will use the OS fork mechanism to launch\nadditional R processes. This mechanism generally does not behave well with\nMPI, which you must use (in the form of jsrun) to push your task out\nto the compute node(s). We highly recommend you avoid these packages if at\nall possible, unless they are a frontend to Rmpi.\n\nFor parallelism, you should use pbdR packages, Rmpi directly, or an interface\nwhich can use Rmpi as a backend. We address GPUs specifically next.\n\nGPU Computing with R\n\nThere are some R packages which can use GPUs, such as\nxgboost.\nThere is also the gpuR series of packages.\nSeveral pbdR packages support GPU\ncomputing. It is also possible to offload some linear algebra\ncomputations (specifically matrix-matrix products, and methods which are\ncomputationally dominated by them) to the GPU using NVIDIA’s\nNVBLAS.\n\nIf you want to do GPU computing on Summit with R, we would love to collaborate\nwith you (see contact details at the bottom of this document).\n\nMore Information\n\nFor more information about using R and pbdR effectively in an HPC environment\nsuch as Summit, please see the R and pbdR\narticles.\nThese are long-form articles that introduce HPC concepts like MPI programming\nin much more detail than we do here.\n\nAlso, if you want to use R and/or pbdR on Summit, please feel free to\ncontact us directly:\n\nMike Matheson - mathesonma AT ornl DOT gov\n\nGeorge Ostrouchov - ostrouchovg AT ornl DOT gov\n\nDrew Schmidt - schmidtda AT ornl DOT gov\n\nWe are happy to provide support and collaboration for R and pbdR users on\nSummit."}
{"doc":"pmake","text":"pmake\n\n\n\nOverview\n\n<string>:5: (INFO/1) Duplicate implicit target name: \"pmake\".\n\npmake is a parallel\nmake developed for use within batch jobs.  A rules.yaml\nfile specifies extended make-rules with:\n\nmultiple input and multiple output files\n\na resource-set specification\n\na multi-line shell script that can use variable\nsubstitution (e.g. {mpirun} expands to\n{jsrun -g -c ...} on summit).\n\nFull documentation and examples are available\nin https://code.ornl.gov/99R/pmake.\n\nPrerequisites\n\npmake is a standard python package.  It is recommended\nto install it in a virtual environment.  One easy\nway to create a virtual environment is to load an available\npython module, and then put a new environment into /ccs/proj/<projid>/<systemname>.  This way, the project can share environments, and each system\ngets its own install location.\n\n$ module load python/3.8-anaconda3\n$ python -m venv /path/to/new-venv\n$ source /path/to/new-venv/bin/activate\n\nOn subsequent logins, remember to load the same python module\nand run the source /path/to/new-venv/bin/activate command\nagain.\n\nOnce you have entered the virtual environment, pmake can be installed\nwith:\n\n$ python -m pip install git+https://code.ornl.gov/99R/pmake.git@latest\n\nRun the following command to verify that pmake is available:\n\n$ pmake --help\n\nHello world!\n\nTo run a pmake demo on Summit, you will create a pmake-example\ndirectory with its preferred file layout, then submit\na batch job to LSF from a Summit login node.\n\nFirst, create the directories,\n\n$ mkdir -p pmake-example/simulation\n\nNext, create pmake's two configuration files, rules.yaml\nand targets.yaml:\n\n# pmake-example/targets.yaml\n\nsimulation:\n  dirname: simulation # request simulation/run.log to be created\n  out:\n    - run.log\n\n#... additional directories here\n\n# pmake-example/rules.yaml\n\nsimulate:\n   resource:\n      cpu: 1 # number of CPUs per resource set\n      gpu: 0 # number of GPUs per resource set\n      nrs: 1 # number of resource sets to request\n      time: 3 # minutes\n   inp: [] # empty list of input files - no inputs required\n   # Declare a list of output files, (same as \"out: [run.log]\").\n   out:\n     - run.log\n   # The | character here creates a multi-line string.\n   script: |\n     {mpirun} seq 4 >run.log\n\n#... additional rules here\n\nTo check the syntax of your files, cd into the pmake-example\ndirectory and run pmake --test.\nIt should show the commands that would\nbe run if pmake were being executed inside a job-script.\n\nFinally, create an LSF batch script called pmake.lsf,\nfix the python module and virtual environment path to match\nyour installation above, and change\nabc123 to match your own project identifier:\n\n#BSUB -P abc123\n#BSUB -W 10\n#BSUB -nnodes 1\n\n#BSUB -J pmake_demo\n#BSUB -o pmake.o%J\n#BSUB -e pmake.e%J\n\nmodule load python/3.8-anaconda3\nsource /path/to/new-venv/bin/activate\n\npmake rules.yaml targets.yaml 8\n# Note the 8 here is a time-limit on launching new rules\n# to 8 minutes.  This prevents launching job steps\n# that are not likely to complete before the job time-limit.\n# (Note: this example -W 10 requests 10 minutes total)\n\nFinally, submit the batch job to LSF by executing the\nfollowing command from a login node:\n\n$ bsub pmake.lsf\n\nWhen the job completes, you will\nsee pmake explain what rules it launched, completed, or errored\nin pmake.oNNN, where NNN is your job ID.\n\nInside the simulation directory, you should see 3 new files,\nsimulate.sh, which contains the shell script pmake built\nfrom the simulate rule, simulate.log, containing the\nlog output from running simulate.sh, and run.log,\nthe file written during rule execution.\n\nExtending pmake using your own rules is straightforward.\npmake acts like make, running rules to create output\nfiles (that do not yet exist) from input files\n(that must exist before the rule is run).\n\nUnlike make, pmake does not run a rule unless its\noutput is requested by some target."}
{"doc":"pod","text":"Pods\n\n\n\nA pod is the smallest unit in Kubernetes, it is a grouping of containers that will be scheduled\ntogether onto a node in the cluster. usually it will just be one container but it could be a group\nof processes that make up an application.\n\nPods have a lifecycle: they are defined, scheduled onto a node, and then they run until their\ncontainers exit or the pod is removed from the node for some reason. Pods are immutable and changes\nto a pod are not persisted between restarts.\n\nA pod does not:\n\nhave state (data should be stored in persistent volumes)\n\nmove nodes once scheduled onto a node\n\nreschedule itself (we will use higher level controllers to manage pods)\n\nRunning a Container in a Pod with the CLI\n\nLets get started:\n\noc run --restart=Never hello-world-pod --image openshift/hello-openshift:latest\n\nUnder the hood, the oc run command is taking options and creating a specification\nthat it is then passing to Kubernetes to run. To see the spec it is creating append\n-o yaml to the oc run command and you will get an output of what it is sending\nto Kubernetes.\n\nThe --restart=Never tells oc run to generate only a Pod spec. By default it would\ngenerate something more advanced we will talk about later.\n\nThe openshift/hello-openshift is just a simple\nDocker image\n\nList current running pods:\n\noc get pods\n\nGet more information on our pod:\n\noc describe pod hello-world-pod\n\nOnce we see Status: Running (near the top of the output, not the bottom) we can interact with the container by first setting up port forwarding:\n\noc port-forward hello-world-pod 8080:8080\n\nThen, since oc port-forward stays in the foreground, we run curl http://localhost:8080 in a second terminal.\n\nThe initial port in the port pair references a non-allocated port on our local system similar to how SSH\nport forwarding works.\n\nPods also have logs.\nAnd we can see the logs for the pod: (whatever was printed to stdout from within the container, not kubernetes).\n\noc logs hello-world-pod\n\nNow lets delete our pod:\n\noc delete pod hello-world-pod\n\nDeleting the pod will remove our ability to inspect the log output from oc logs so if you are debugging an issue\nyou will want to keep the pod until the issue is resolved.\n\nRunning a Container in a Pod with the Web Console\n\nTo create a single pod using the web console we will create from YAML\n\nFirst, in the upper right-hand corner, click the + symbol. This can be used to add any YAML object from the web UI.\n\nAdd to Project\n\nMake sure the project in the upper left-hand dropdown is set to the project in which you wish to deploy.\nThen paste this YAML into the box.\n\napiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    run: hello-world-pod\n  name: hello-world-pod\nspec:\n  containers:\n  - image: openshift/hello-openshift:latest\n    imagePullPolicy: IfNotPresent\n    name: hello-world-pod\n    resources: {}\n  dnsPolicy: ClusterFirst\n  restartPolicy: Never\nstatus: {}\n\nImport YAML\n\nClick \"Create\" at the bottom of the screen, and you will be redirected to the pod object page.\nWait for this to say \"Running\" on the right side of the screen in the Status field.\n\nHello World Pod\n\nFinally we can delete the pod by clicking Actions -> Delete Pod\n\nPod Actions\n\nMore Information\n\nKubernetes Pod Overview"}
{"doc":"policies","text":"This page has been deprecated, and relevant information is now available on data-storage-and-transfers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers>. Please update any bookmarks to use that page.\n\nPolicy\n\nA brief description of each area and basic guidelines to follow are provided in\nthe table below:\n\n\nThe table provided contains information on various policies related to user and project data storage and access. The first column, \"Area\", specifies the different types of storage areas available, such as User Home, User Archive, Project Home, Member Work, Project Work, World Work, Open Member Work, Open Project Work, Open World Work, Member Archive, Project Archive, World Archive, Moderate Enhanced User Home, Moderate Enhanced Member Work, and Moderate Enhanced Project Work. The \"Path\" column indicates the specific path or location for each storage area. The \"Enclave\" column specifies the security level for each area, with options including O (Open), M1, M2, and ME (Moderate Enhanced). The \"Type\" column indicates the type of storage used, such as NFS, HPSS, or Spectrum Scale. The \"Permissions\" column specifies the level of access granted to users, with options for user-set permissions or preset permissions such as 770 or 775. The \"Quota\" column indicates the maximum storage limit for each area, with units in GB or TB. The \"Backups\" column specifies whether backups are available for each area, with options for Yes or No. The \"Purged\" column indicates whether data is automatically purged after a certain period of time, with options for Yes or No. The \"Retention\" column specifies the length of time data is retained before being purged, with a default of 90 days. The last column, \"On Compute Nodes\", indicates whether data can be accessed and modified on compute nodes, with options for Read-only or Read/Write. Overall, this table provides a comprehensive overview of the different policies and settings for data storage and access within the system.\n\n| Area | Path | Enclave | Type | Permissions | Quota | Backups | Purged | Retention | On Compute Nodes |\n|------|------|---------|------|-------------|--------|---------|--------|-----------|------------------|\n| User Home | /ccs/home/[userid] | O, M1,2 | NFS | User set | 50 GB | Yes | No | 90 days | Read-only |\n| User Archive 1 | /home/[userid] | O, M1 | HPSS | User set | 2TB | No | No | 90 days | No |\n| User Archive 2 | /home/[userid] | O, M1 | HPSS | 700 | N/A | N/A | N/A | N/A | No |\n| Project Home | /ccs/proj/[projid] | O, M1,2 | NFS | 770 | 50 GB | Yes | No | 90 days | Read-only |\n| Member Work | /gpfs/alpine/[projid]/scratch/[userid] | M1,2 | Spectrum Scale | 700 3 | 50 TB | No | 90 days | N/A 4 | Read/Write |\n| Project Work | /gpfs/alpine/[projid]/proj-shared | M1,2 | Spectrum Scale | 770 | 50 TB | No | 90 days | N/A 4 | Read/Write |\n| World Work | /gpfs/alpine/[projid]/world-shared | M1 | Spectrum Scale | 775 | 50 TB | No | 90 days | N/A 4 | Read/Write |\n| Open Member Work | /gpfs/wolf/[projid]/scratch/[userid] | Open | Spectrum Scale | 700 3 | 50 TB | No | 90 days | N/A 4 | Read/Write |\n| Open Project Work | /gpfs/wolf/[projid]/proj-shared | Open | Spectrum Scale | 770 | 50 TB | No | 90 days | N/A 4 | Read/Write |\n| Open World Work | /gpfs/wolf/[projid]/world-shared | Open | Spectrum Scale | 775 | 50 TB | No | 90 days | N/A 4 | Read/Write |\n| Member Archive | /hpss/prod/[projid]/users/$USER | M1 | HPSS | 700 | 100 TB | No | No | 90 days | No |\n| Project Archive | /hpss/prod/[projid]/proj-shared | M1 | HPSS | 770 | 100 TB | No | No | 90 days | No |\n| World Archive | /hpss/prod/[projid]/world-shared | M1 | HPSS | 775 | 100 TB | No | No | 90 days | No |\n| Moderate Enhanced User Home | /gpfs/arx/[projid]/home/[userid] | ME | Spectrum Scale | 700 | 50 TB | No | 90 days | N/A 4 | Read/Write |\n| Moderate Enhanced Member Work | /gpfs/arx/[projid]/scratch/[userid] | ME | Spectrum Scale | 700 | 50 TB | No | 90 days | N/A 4 | Read/Write |\n| Moderate Enhanced Project Work | /gpfs/arx/[projid]/proj-shared/[userid] | ME | Spectrum Scale | 770 | 50 TB | No | 90 days | N/A 4 | Read/Write |\n\nArea - The general name of the storage area.\n\nPath - The path (symlink) to the storage area's directory.\n\nEnclave - The security enclave where the path is available. There are several security enclaves:\n\n- Open (O) - Ascent and other OLCF machines accessible with a username/password\n\n- Moderate Projects not subject to export control (M1) - These are projects on machines such as Summit or Andes that require 2-factor authentication but are not subject to export controll restrictions.\n\n- Moderate Projects subject to export control (M2) - Same as M1, but projects that are subject to export control restrictions.\n\n- Moderated Enhanced (ME) - These are projects that might involve HIPAA or ITAR regulations. These projects utilize Summit compute resources but have extra security precautions and separate file systems.\n\nType - The underlying software technology supporting the storage area.\n\nPermissions - UNIX Permissions enforced on the storage area's top-level directory.\n\nQuota - The limits placed on total number of bytes and/or files in the storage area.\n\nBackups - States if the data is automatically duplicated for disaster recovery purposes.\n\nPurged - Period of time, post-file-access, after which a file will be marked as eligible for permanent deletion.\n\nRetention - Period of time, post-account-deactivation or post-project-end, after which data will be marked as eligible for permanent deletion.\n\nOn Compute Nodes - Is this filesystem available on compute nodes (no, available but read-only, and available read/write)\n\nFiles within \"Work\" directories (i.e., Member Work, Project Work, World Work) are not backed up and are purged on a regular basis according to the timeframes listed above.\n\nIf your home directory reaches its quota, your batch jobs might fail with the error cat: write error: Disk quota exceeded. This error may not be intuitive, especially if your job exclusively uses work areas that are well under quota. The error is actually related to your home directory quota. Sometimes, batch systems write temporary files to the home directory (for example, on Summit LSF writes temporary data in ~/.lsbatch), so if the home directory is over quota and that file creation fails, the job will fail with the quota error.\n\nYou can check your home directory quota with the quota command. If it is over quota, you need to bring usage under the quota and then your jobs should run without encountering the Disk quota exceeded error.\n\nFootnotes\n\n1\n\nThis entry is for legacy User Archive directories which contained user data on January 14, 2020.\n\n2\n\nUser Archive directories that were created (or had no user data) after January 14, 2020. Settings other than permissions are not applicable because directories are root-owned and contain no user files.\n\n3\n\nPermissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by \"UNIX-others\" and cannot be changed (security measures).\n\n4\n\nRetention is not applicable as files will follow purge cycle.\n\nOn Summit, Andes, and the DTNs, additional paths to the various project-centric work areas are available\nvia the following symbolic links and/or environment variables:\n\nMember Work Directory:  /gpfs/alpine/scratch/[userid]/[projid] or $MEMBERWORK/[projid]\n\nProject Work Directory: /gpfs/alpine/proj-shared/[projid] or $PROJWORK/[projid]\n\nWorld Work Directory: /gpfs/alpine/world-shared/[projid] or $WORLDWORK/[projid]\n\nInformation\n\nAlthough there are no hard quota limits for the project storage, an upper\nstorage limit should be reported in the project request. The available space\nof a project can be modified upon request.\n\nPurge\n\nTo keep the Spectrum Scale file system exceptionally performant, files that have\nnot been accessed in the project and user areas are purged at the intervals\nshown in the table above. Please make sure that valuable data is moved off of\nthese systems regularly. See data-hpss <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-hpss>. for information about using the HSI\nand HTAR utilities to archive data on HPSS. Just to note that when you read a\nfile, then the 90 days counter restarts.\n\nSpecial Requests\n\nIf you need an exception to the limits listed in the table above, such as a higher quota in your User/Project Home or a purge exemption in a Member/Project/World Work area, contact help@olcf.ornl.gov with a summary of the exception that you need.\n\nData Retention\n\nBy default, the OLCF does not guarantee lifetime data retention on any OLCF\nresources. Following a user account deactivation or project end, user and\nproject data in non-purged areas will be retained for 90 days. After this\ntimeframe, the OLCF retains the right to delete data. Data in purged areas\nremains subject to normal purge policies."}
{"doc":"port_forwarding","text":"Quick Access from Outside Slate\n\nAccess to the Web UI and CLI\n\nBoth the web UI and the API endpoint for the oc client are exposed outside of ORNL. However, you must log in with NCCS USERNAME AND PASSWORD rather than NCCS Single Sign On on the Web UI.\n\nAccess to Internal Resources\n\nFor production workloads, it is recommended to learn about services <slate_services> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#services <slate_services>> and routes <slate_routes> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#routes <slate_routes>> in order to gain access to your internal resources.\n\nHowever, for testing and development, oc port-forward can be a powerful tool for quick access to internal cluster resources.\n\nThis tool will forward a local port on your system to a pod inside the cluster.\n\nFor example, if you have an nginx deployment running on port 8080 inside the container, you can view this nginx instance locally by running:\n\noc port-forward ${pod_name} 7777:8080\n\nThe first port is the local port you want forwarded, and the second port is the port exposed by the pod. After running this command, you can go into your browser (or use curl in a second terminal) and connect to http://localhost:7777.\n\nAdditionally, oc port-forward doesn't have to be given a pod name. This tool is aware of services and deployments as well. If you had a service called nginx-svc and a deployment called nginx, for example, the following commands would achieve the same result:\n\noc port-forward deployment/nginx 7777:8080\noc port-forward svc/nginx-svc 7777:8080\n\nYou will be forwarded to any of the pods matched by the service or deployment.\n\nFurthermore, this doesn't only work for http traffic. You could also access other exposed services such as databases.\n\nFor instance, if you have a mongoDB instance running on port 27017 with a deployment named mongodb, you could run oc port-forward deployment/mongodb 7777:27017. Now you can simply run mongo --port 7777 (assuming you have the mongo client installed on your local machine) and have access to your mongodb instance in the cluster, as if it were running on your local machine."}
{"doc":"processing_membership_requests","text":"Processing Project Membership Requests\n\n\n\nAs a principal investigator of a project at the OLCF, you must approve (or reject) every potential\nuser that requests membership on your project. myOLCF provides a mechanism for processing these\nrequests via the \"For My Approval\" page.\n\nClick the \"For My Approval\" link in the \"My Account\" top navigation dropdown:\n\nfor my approval link\n\nYou'll see a list of all pending requests that need your response:\n\n<string>:6: (INFO/1) Enumerated list start value not ordinal-1: \"2\" (ordinal 2)\n\nfor my approval link\n\nClick the \"More Information\" button more info button to view the details of an individual request:\n\n<string>:6: (INFO/1) Enumerated list start value not ordinal-1: \"3\" (ordinal 3)\n\nfor my approval link\n\n4. Click the \"thumbs up\" thumbs up approve button button to approve the applicant, or click\nthe \"thumbs down\" thumbs down reject button button to reject the applicant. In either case, you\nwill see a confirmation window:\n\nrequest confirmation window\n\nmore info button\n\nthumbs up approve button\n\nthumbs down reject button\n\nOptionally enter a comment, and click \"Ok\" to submit your response.\n\n<string>:6: (INFO/1) Enumerated list start value not ordinal-1: \"5\" (ordinal 5)"}
{"doc":"project_centric","text":"Project-Centric Data Storage\n\nThis page has been deprecated, and relevant information is now available on data-storage-and-transfers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers>. Please update any bookmarks to use that page.\n\nProject directories provide members of a project with a common place to\nstore code, data, and other files related to their project.\n\nProject Home Directories (NFS)\n\n\nThe table provides a detailed breakdown of the various aspects of project centric, including the area, path, type, permissions, quota, backups, purged, retention, and whether it is accessible on compute nodes. The area is listed as \"Project Home\" and the path is specified as \"/ccs/proj/[projid]\", indicating the location of the project within the system. The type of storage used for this project is NFS (Network File System), which allows for remote file access. The permissions for this project are set at 770, indicating that the project is accessible to the owner, group members, and other users. The quota for this project is limited to 50 GB, meaning that the total amount of storage space available for this project is capped at 50 GB. Backups are enabled for this project, ensuring that any data stored within it is regularly backed up for safekeeping. The project is not set to be purged, meaning that the data will not be automatically deleted after a certain period of time. However, there is a retention period of 90 days, after which the data may be deleted if deemed necessary. Finally, the project is set to be read-only on compute nodes, meaning that users can view and access the data, but cannot make any changes to it. Overall, this table provides a comprehensive overview of the various settings and features of project centric, allowing for efficient and organized management of project data.\n\n| Area         | Path              | Type | Permissions | Quota | Backups | Purged | Retention | On Compute Nodes |\n|--------------|-------------------|------|-------------|-------|---------|--------|-----------|------------------|\n| Project Home | /ccs/proj/[projid] | NFS  | 770         | 50 GB | Yes     | No     | 90 days   | Read-only        |\n\n\n\nOpen and Moderate Projects are provided with a Project Home storage area in the\nNFS-mounted filesystem. This area is intended for storage of data, code,\nand other files that are of interest to all members of a project. Since\nProject Home is an NFS-mounted filesystem, its performance will not be\nas high as other filesystems.\n\nModerate Enhanced projects are not provided with Project Home spaces, just Project Work spaces.\n\nProject Home Path\n\nProject Home area is accessible at /ccs/proj/abc123 (where\nabc123 is your project ID).\n\nProject Home Quotas\n\nTo check your project’s current usage, run df -h /ccs/proj/abc123\n(where abc123 is your project ID). Quotas are enforced on project\nhome directories. The current limit is shown in the table above.\n\nProject Home Permissions\n\nThe default permissions for project home directories are 0770 (full\naccess to the user and group). The directory is owned by root and the\ngroup includes the project’s group members. All members of a project\nshould also be members of that group-specific project. For example, all\nmembers of project “ABC123” should be members of the “abc123” UNIX\ngroup.\n\nProject Home Backups\n\nIf you accidentally delete files from your project home directory\n(/ccs/proj/[projid]), you may be able to retrieve them. Online backups\nare performed at regular intervals.  Hourly backups for the past 24 hours,\ndaily backups for the last 7 days, and once-weekly backups are available. It is\npossible that the deleted files are available in one of those backups. The\nbackup directories are named hourly.*, daily.*, and weekly.* where\n* is the date/time stamp of backup creation. For example,\nhourly.2020-01-01-0905 is an hourly backup made on January 1st, 2020 at\n9:05 AM.\n\nThe backups are accessed via the .snapshot subdirectory. Note that ls\nalone (or even ls -a) will not show the .snapshot subdirectory exists,\nthough ls .snapshot will show its contents. The .snapshot feature is\navailable in any subdirectory of your project home directory and will show the\nonline backups available for that subdirectory.\n\nTo retrieve a backup, simply copy it into your desired destination with the\ncp command.\n\nProject Work Areas\n\nThree Project Work Areas to Facilitate Collaboration\n\nTo facilitate collaboration among researchers, the OLCF provides (3)\ndistinct types of project-centric work storage areas: Member Work\ndirectories, Project Work directories, and World Work directories.\nEach directory should be used for storing files generated by\ncomputationally-intensive HPC jobs related to a project.\n\nModerate enhanced projects do not have World Work directories and the filesystem is called \"arx\" rather than \"alpine\"\n\n\nThe table provides a detailed breakdown of the different types of work areas within a project-centric system. The first column, \"Area\", lists the different types of work areas, including Member Work, Project Work, and World Work. The second column, \"Path\", specifies the location of each work area within the system. The third column, \"Type\", indicates that all work areas use Spectrum Scale as the file system. The fourth column, \"Permissions\", shows the default permissions for each work area, with Member Work having a permission level of 700 and Project Work and World Work having a permission level of 770. The fifth column, \"Quota\", displays the maximum storage capacity for each work area, which is set at 50 TB for all areas. The sixth column, \"Backups\", indicates whether or not backups are enabled for each work area, with all areas having backups disabled. The seventh column, \"Purged\", shows the time period after which data will be automatically purged from each work area, with a default of 90 days for all areas. The eighth column, \"Retention\", specifies the retention policy for each work area, with a default of N/A for all areas. Finally, the last column, \"On Compute Nodes\", indicates whether or not the work area is accessible on compute nodes, with all areas being accessible. Overall, this table provides a comprehensive overview of the different work areas within a project-centric system, including their locations, permissions, storage capacity, and retention policies.\n\n| Area | Path | Type | Permissions | Quota | Backups | Purged | Retention | On Compute Nodes |\n|------|------|------|-------------|--------|---------|---------|-----------|------------------|\n| Member Work | /gpfs/alpine/[projid]/scratch/[userid] | Spectrum Scale | 700 | 50 TB | No | 90 days | N/A | Yes |\n| Member Work, Moderate Enhanced | /gpfs/arx/[projid]/scratch/[userid] | Spectrum Scale | 700 | 50 TB | No | 90 days | N/A | Yes |\n| Project Work | /gpfs/alpine/[projid]/proj-shared | Spectrum Scale | 770 | 50 TB | No | 90 days | N/A | Yes |\n| Project Work, Moderate Enhanced | /gpfs/arx/[projid]/proj-shared | Spectrum Scale | 770 | 50 TB | No | 90 days | N/A | Yes |\n| World Work | /gpfs/alpine/[projid]/world-shared | Spectrum Scale | 775 | 50 TB | No | 90 days | N/A | Yes |\n\n\n\nFootnotes\n\n1\n\nPermissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by \"UNIX-others\" and cannot be changed (security measures).\n\n2\n\nRetention is not applicable as files will follow purge cycle.\n\nOn Summit, Rhea and the DTNs, additional paths to the various project-centric work areas are available\nvia the following symbolic links and/or environment variables:\n\nMember Work Directory:  /gpfs/alpine/scratch/[userid]/[projid] or $MEMBERWORK/[projid]\n\nProject Work Directory: /gpfs/alpine/proj-shared/[projid] or $PROJWORK/[projid]\n\nWorld Work Directory: /gpfs/alpine/world-shared/[projid] or $WORLDWORK/[projid]\n\nThe difference between the three lies in the accessibility of the data\nto project members and to researchers outside of the project. Member\nWork directories are accessible only by an individual project member by\ndefault. Project Work directories are accessible by all project members.\nWorld Work directories are readable by any user on the system.\n\nPermissions\n\nUNIX Permissions on each project-centric work storage area differ\naccording to the area’s intended collaborative use. Under this setup,\nthe process of sharing data with other researchers amounts to simply\nensuring that the data resides in the proper work directory.\n\nMember Work Directory: 700\n\nProject Work Directory: 770\n\nWorld Work Directory: 775\n\nFor example, if you have data that must be restricted only to yourself,\nkeep them in your Member Work directory for that project (and leave the\ndefault permissions unchanged). If you have data that you intend to\nshare with researchers within your project, keep them in the project’s\nProject Work directory. If you have data that you intend to share with\nresearchers outside of a project, keep them in the project’s World Work\ndirectory.\n\nBackups\n\nMember Work, Project Work, and World Work directories are not backed\nup. Project members are responsible for backing up these files, either\nto Project Archive areas (HPSS) or to an off-site location.\n\nProject Archive Directories\n\nModerate projects without export control restrictions are also allocated project-specific archival space on the High\nPerformance Storage System (HPSS). The default quota is shown on the\ntable below. If a higher quota is needed, contact the User Assistance\nCenter.\n\nThere is no HPSS storage for Moderate Enhanced Projects, Moderate Projects subject to export control, or Open projects.\n\nThree Project Archive Areas Facilitae Collaboration on Archival Data\n\nTo facilitate collaboration among researchers, the OLCF provides (3)\ndistinct types of project-centric archival storage areas: Member Archive\ndirectories, Project Archive directories, and World Archive directories.\nThese directories should be used for storage of data not immediately needed\nin either the Project Home (NFS) areas or Project Work (Alpine) areas and\nto serve as a location to store backup copies of project-related files.\n\n\nThe table above provides a detailed breakdown of the content presented in the project centric system. This system is used for archiving and storing data related to specific projects. The first column, \"Area\", specifies the different types of archives available, including Member Archive, Project Archive, and World Archive. The \"Path\" column shows the location of each archive, which is within the /hpss/prod/[projid] directory. The \"Type\" column indicates that all archives are stored in HPSS (High Performance Storage System). The \"Permissions\" column shows the access permissions for each archive, with the Member Archive having the most restricted access (700) and the World Archive having the least restricted access (775). The \"Quota\" column specifies the maximum storage capacity for each archive, which is set at 100 TB for all three types. The \"Backups\" and \"Purged\" columns indicate whether backups are performed and if data is automatically purged after a certain period of time, with both being set to \"No\" for all three types. The \"Retention\" column shows the retention period for each archive, with data being kept for 90 days before being purged. Finally, the \"On Compute Nodes\" column specifies whether the archives are accessible on compute nodes, with all three types being set to \"No\". This table provides a comprehensive overview of the project centric system and its various features and limitations.\n\n| Area           | Path                                      | Type | Permissions | Quota  | Backups | Purged | Retention | On Compute Nodes |\n|----------------|-------------------------------------------|------|-------------|--------|---------|--------|-----------|------------------|\n| Member Archive | /hpss/prod/[projid]/users/$USER           | HPSS | 700         | 100 TB | No      | No     | 90 days   | No               |\n| Project Archive| /hpss/prod/[projid]/proj-shared           | HPSS | 770         | 100 TB | No      | No     | 90 days   | No               |\n| World Archive  | /hpss/prod/[projid]/world-shared          | HPSS | 775         | 100 TB | No      | No     | 90 days   | No               |\n\nAs with the three project work areas, the difference between these three areas\nlies in the accessibility of data to project members and to researchers outside\nof the project. Member Archive directories are accessible only by an individual\nproject member by default, Project Archive directories are accessible by all\nproject members, and World Archive directories are readable by any user on the\nsystem.\n\nPermissions\n\n<string>:194: (INFO/1) Duplicate implicit target name: \"permissions\".\n\nUNIX Permissions on each project-centric archive storage area differ\naccording to the area’s intended collaborative use. Under this setup,\nthe process of sharing data with other researchers amounts to simply\nensuring that the data resides in the proper archive directory.\n\nMember Archive Directory: 700\n\nProject Archive Directory: 770\n\nWorld Archive Directory: 775\n\nFor example, if you have data that must be restricted only to yourself,\nkeep them in your Member Archive directory for that project (and leave the\ndefault permissions unchanged). If you have data that you intend to\nshare with researchers within your project, keep them in the project’s\nProject Archive directory. If you have data that you intend to share with\nresearchers outside of a project, keep them in the project’s World Archive\ndirectory.\n\nProject Archive Access\n\nProject Archive directories may only be accessed via utilities called\nHSI and HTAR. For more information on using HSI or HTAR, see the data-hpss <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-hpss> section."}
{"doc":"project_pages","text":"Project Pages\n\n\n\nAfter authenticating, you are redirected to the project pages area of myOLCF.\n\nProject Context\n\nEvery individual page in the project pages area should be interpreted within the context\nof a single, current project, which is displayed at the top of the left navigation\nmenu (e.g. \"ABC123\"):\n\nproject pages left navigation menu\n\nSwitching Project Contexts\n\nThe top navigation bar has a dropdown menu that can be used to switch the current project\ncontext to any of the projects of which you are a member.\n\nswitch projects dropdown menu\n\nAvailable Pages\n\nThe left navigation menu also includes a number of expandable items, each with links\nto project-centric pages for the current project context.\n\n\nThe table presents a comprehensive overview of the content available on project pages. The first column lists the different pages that can be accessed, while the second column describes the specific content that can be found on each page. The \"Project Profile\" page provides general information about the current project, such as its purpose, goals, and team members. The \"Renew This Project\" page allows users to request a renewal of the current project. Similarly, the \"Renew My Membership\" page allows users to request a renewal of their membership on the current project. The \"Current Users\" page displays a list of current project members, along with their contact information and role within the project. The \"Historical Users\" page displays a list of previous project members and their contact information. The \"Current Allocations\" page lists the current project allocations, while the \"Historical Allocations\" page displays past project allocations. The \"Allocation Usage\" page provides usage metrics and graphs for each allocation, allowing users to track their resource usage. Lastly, the \"Usage\" page displays usage metrics and graphs that can be queried by resource and timespan, providing a more detailed analysis of resource usage on the project. Overall, this table provides a comprehensive overview of the different types of content available on project pages, allowing users to easily access and manage information related to the current project. \n\n| Page              | Content                                                                                     |\n|-------------------|---------------------------------------------------------------------------------------------|\n| Project Profile   | General information about the current project                                              |\n| Renew This Project| Form to request renewal of the current project                                             |\n| Renew My Membership| Form to request renewal of your membership on the current project                          |\n| Current Users     | A list of current projects members with contact and application role information           |\n| Historical Users  | A list of previous project members with contact information                                |\n| Current Allocations| A list of current project allocations                                                     |\n| Historical Allocations| A list of current project allocations                                                  |\n| Allocation Usage  | Usage metrics and graphs, per-allocation                                                  |\n| Usage             | Usage metrics and graphs, queryable by resource and timespan                               |"}
{"doc":"quantinuum","text":"Quantinuum\n\nNew allocation policy in effect Oct. 1st 2022, see quantinuum-alloc <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#quantinuum-alloc> section.\n\nOverview\n\nQuantinuum offers access to trapped ion quantum computers and emulators,\naccessible via their API and User Portal. For the complete set of currently\navailable devices, qubit numbers, etc. see the Quantinuum Systems User Guide under the Examples tab on the Quantinuum User Portal.\n\nThis guide describes how to use the system once you have access. For\ninstructions on how to gain access, see our Quantum Access <https://docs.olcf.ornl.gov/quantum/quantum_access.html> page instead.\n\nFeatures\n\nThe complete set of Quantinuum System Model H1 and Model H2 hardware specifications and\noperations, can be found in the Quantinuum System Model H1 Product Data Sheet and the\nQuantinuum System Model H2 Product Data Sheet on the Quantinuum User Portal. Features include, but are not limited to:\n\nN ≤ 32 qubit trapped-ion based quantum computers\n\nAll-to-all connectivity\n\nLaser based quantum gates\n\nLinear trap Quantum Charge-Coupled Device (QCCD) architecture with three or more parallel gate zones\n\nMid-circuit measurement conditioned circuit branching\n\nQubit reuse after mid-circuit measurement\n\nNative gate set: single-qubit rotations, two-qubit ZZ-gates\n\nConnecting\n\n\n\nCloud Access\n\nUsers can access information about Quantinuum's systems, view submitted jobs,\nlook up machine availability, and update job notification preferences on the\ncloud dashboard on the Quantinuum User Portal.\n\nJupyter at OLCF: Access to the Quantinuum queues can also be obtained via OLCF JupyterHub, a web-based interactive computing\nenvironment.\n\n\n\nLocally via OpenQASM or pyket\n\nUsers are able to submit jobs that run remotely on Quantinuum's systems from a\nlocal python development environment. Directions for setting up the python\nenvironment and getting started in a notebook locally as well as additional\nexamples utilizing conditional logic and mid-circuit measurement are found\nunder the Examples tab on the Quantinuum User Portal.\n\n\n\nRunning Jobs & Queue Policies\n\nInformation on submitting jobs to Quantinuum systems, system availability,\nchecking job status, and tracking usage can be found in the Quantinuum Systems User Guide under the Examples tab on the Quantinuum User Portal.\n\nUsers have access to the API validator to check program syntax, and to the\nQuantinuum System Model H1 emulator, which returns actual results back as if\nusers submitted code to the real quantum hardware.\n\nA recommended workflow for running on Quantinuum's quantum computers is to\nutilize the syntax checker first, run on the emulator, then run on one of the\nquantum computers. This is highlighted in the examples.\n\n\n\nAllocations & Credit Usage\n\nRunning a job on the System Model H1 family and System Model H2 hardware requires Quantinuum\nCredits. Additional information on credit usage can be found in the Quantinuum Systems User Guide under the\nExamples tab on the Quantinuum User Portal.\nDue to increased demand and to make the most efficient use of credits, the following allocating policy will go into effect starting October 1st 2022:\n\nAny request for credits must be submitted by the project Principle Investigator (PI) to help@olcf.ornl.gov\n\nRequests for machine credits must be justified using results from the emulator to determine the appropriate amount needed. Requests without emulator-based justifications will be denied.\n\nRequests will be evaluated based on the provided technical justification, programmatic efficiency, and machine availability. The effective usage of prior allocations by the project will also be considered.\n\nAllocations will be granted on a monthly basis to maximize the availability of the H1 family and H2 machines. Please note that allocations do not carry over to the next month and must be consumed in the month granted.\n\nAllocation requests requiring 20 qubits and under will be considered for H1 family machines, and allocation requests requiring 21-32 qubits will be considered for H2.\n\nAllocation requests for the following month must be submitted no later than the 25th of the preceding month.  The uptime schedule is available on the Calendar tab of the Quantinuum User Portal.\n\nDue to hardware emulation complexity, jobs using 29-32 qubits are likely to experience significantly slowed execution times.\n\nSoftware\n\nThe TKET framework is a software platform for the development and execution of\ngate-level quantum computation, providing state-of-the-art performance in\ncircuit compilation. It was created and is maintained by Quantinuum. The\ntoolset is designed to extract the most out of the available NISQ devices of\ntoday and is platform-agnostic.\n\nIn python, the pytket package is available for python 3.8+. The pytket\nand pytket-quantinuum packages are included as part of the installation\ninstructions on Quantinuum's User Portal.\n\nFor more information on TKET, see the following links:\n\nTKET documentation is available at https://cqcl.github.io/pytket/manual/manual_intro.html\n\nAn introduction to quantum compilation with TKET is available at https://github.com/CalMacCQ/tket_blog/blob/main/blog1_intro_to_qc.ipynb\n\nFor a video introduction to TKET, see https://www.youtube.com/watch?v=yXKSpvgAtrk"}
{"doc":"quantum_access","text":"Quantum Computing User Program (QCUP) Access\n\nWelcome! The information below introduces how we structure projects and user\naccounts for access to the Quantum resources within the QCUP program. In\ngeneral, OLCF QCUP resources are granted to projects, which are in turn made\navailable to the approved users associated with each project.\n\nTo gain access, you must first submit a project proposal to the OLCF QCUP\n(see quantum-proj <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#quantum-proj>) or join an existing QCUP project (see quantum-user <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#quantum-user>).\nThe Quantum Resource Utilization Council (QRUC), as well as independent\nreferees, review and approve all QCUP project proposals.  Applications to QCUP\nare accepted year-round via the project application form found below. Once a\nproject is approved, then all of the users associated with the project will\nneed to apply for a User Account <quantum-user> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#User Account <quantum-user>>. After your user account\nis approved, you can then move on to accessing the quantum resources offered by\nour vendors (see quantum-vendors <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#quantum-vendors>).\n\nQCUP Priorities\n\nThere are several broad aims of the Quantum Computing User Program at OLCF, which are as follows:\n\nEnable Research\n\nThe QCUP aims to provide a broad spectrum of user access to the best\navailable quantum computing systems. Once a user’s intended research has been\nreviewed for merit and user agreements have been established, we seek to\nprovide users with the opportunity to become familiar with the unique aspects\nand challenges of quantum computing, as well as to implement and test quantum\nalgorithms on the available systems.\n\nEvaluate Technology\n\n<string>:3: (INFO/1) Enumerated list start value not ordinal-1: \"2\" (ordinal 2)\n\nThe QCUP aims to aid in the evaluation of technology by monitoring the\nbreadth and performance of early quantum computing applications. How users\nintegrate quantum computing with scientific computing is a question constrained\nby both application, infrastructure constraints, and the use cases expected for\nthe associated computational system. Through the QCUP program, users can\nexplore new potential computational research applications, and potentially\naccelerate existing scientific applications using quantum processors and\narchitectures. Research projects supported include advanced scientific\ncomputing, basic energy science, biological environmental research, high-energy\nphysics, fusion energy science, and nuclear physics, among others.\n\nEngage the Community\n\n<string>:3: (INFO/1) Enumerated list start value not ordinal-1: \"3\" (ordinal 3)\n\nThe QCUP aims to engage the quantum computing community and support the\ngrowth of the quantum information science ecosystems. Our quantum computing\nusers range in quantum computing experience from novice to expert; users are\nfrom US national labs, universities, government, and industry.  User groups\nutilize quantum computing expertise to investigate diverse application\ninterests, using multiple programming languages, quantum-classical programming,\nand multiple software environments. Most projects focus on proof-of-principle\ndemonstrations and/or new method development. Some projects focus on\napplication performance and/or benchmarking, and additionally some projects\nfocus on device characterization, verification, and validation.\n\n\n\nProject Allocations\n\nA QCUP proposal describes the nature, methodology, and merits of the project,\nexplains why it requires access to QCUP resources, and outlines any other\nessential information that might be needed for its consideration. Project\napplications are submitted using the Project Application Form. Select \"OLCF Quantum\nComputing User Program\" from the dropdown menu.\n\nFor QCUP Projects, all proposed work must be open, fundamental research and no\nExport Control, PHI, or other controlled data can be used.\n\nOnce submitted, you will receive email notification of successful proposal\nsubmission.  The proposal is then reviewed by the Quantum Resource Utilization\nCouncil (QRUC), as well as independent referees for merit and to ensure the\nfeasibility of project success using the resources available to the QCUP. You\nwill be notified of the QRUC decision via email.\n\nWhat happens after a project request is approved?\n\nOnce a project request is approved by the QRUC, an OLCF Accounts Manager will\ncommunicate with the project’s PI to finalize activation and request a signed\nPrincipal Investigator’s PI Agreement to be submitted.\n\nThe OLCF will then establish a QCUP project and notify the PI of its creation\nalong with the 6-character OLCF QCUP Project ID and resources allocation\ndetails. At this time project participants may proceed with applying for their\nindividual user accounts.\n\nProject Renewals\n\nQCUP Projects have a finite duration; when starting, projects get however many\nmonths are left in that allocation period and then must be renewed for\nsubsequent 6 month intervals. Projects can be renewed by filling out a renewal\nform (Accounts Renewal Form <https://docs.olcf.ornl.govQuantum-Renewal-Form.docx.html>) and\nemailing it to accounts@ccs.ornl.gov.\n\nCloseout and Quarterly Reports\n\nAll QCUP projects are required to submit\nquarterly reports and a\ncloseout report.\nThese forms may be emailed to accounts@ccs.ornl.gov.\n\n\n\nUser Accounts\n\nAny collaborator involved with an approved and activated OLCF project can apply\nfor a user account associated with that project to gain access.\n\nProject PIs do not receive a user account with project\ncreation, and must also apply.\n\nFirst-time users should apply for an account using the Account Request\nForm. You will need the correct\n6 character project ID from your PI.\n\nWhen our accounts team begins processing your application, you will receive an automated\nemail containing a unique 36-character confirmation code. Make note of it; you can use\nit to check the status of your application at any time.\n\nThe principal investigator (PI) of the project must approve your\naccount and system access. We will make the project PI aware of your request.\n\nOnce your application is evaluated and approved, you will be notified via email of your account\ncreation, and the quantum resource vendor will be contacted with instructions to grant you access.\n\nChecking the status of your application\n\nYou can check the general status of your application at any time using the\nmyOLCF self-service portal's account status page.\nFor more information, see our myOLCF Overview <https://docs.olcf.ornl.gov/services_and_applications/myolcf/overview.html> page.\nIf you need to make further inquiries about your application, you may email our\nAccounts Team at accounts@ccs.ornl.gov.\n\nWhen all of the above steps are completed, your user account will be created\nand you will be notified by email. Now that you have a user account and it has\nbeen associated with a project, you're ready to get to work.\n\n\n\nAccessing Quantum Resources\n\nAs opposed to setting up a personal account through each of the individual\nvendor websites, OLCF has purchased subscriptions to those vendor services and\nhandles setting up your access to each one once your Quantum User Application\nis approved. You will receive individual email invitations from Quantinuum and\nRigetti with further instructions, whereas for IBM you can proceed directly to\ntheir website to create an account. More details for gaining access to each\nvendor are listed below. For details on how to use a vendor's system once\naccess is gained, see our Quantum Systems Guides <https://docs.olcf.ornl.gov/quantum/quantum_systems/index.html>\ninstead.\n\nIBM Quantum Computing\n\nAfter submitting the OLCF quantum account application and receiving approval,\nproceed to https://quantum-computing.ibm.com/ and click on \"Create an IBMid\naccount\". Your IBM Quantum Hub account email will be the email associated with\nyour OLCF account. If sign-in fails, contact help@olcf.ornl.gov. Once logged\nin, users will have access to the IBM Quantum Hub, IBM’s online platform for\nQPU access, forums for quantum computing discussion, etc. From the IBM Quantum\nHub Dashboard, users can manage system reservations, view system (backend)\nstatuses, and view the results of your past jobs. More information about using\nthese IBM quantum resources can be found on the IBM's Documentation\nor our OLCF IBM Quantum Guide <https://docs.olcf.ornl.gov/quantum/quantum_systems/ibm_quantum.html>.\n\nQuantinuum\n\nAfter submitting the OLCF quantum account application and receiving approval,\nyou will receive an email from Quantinuum inviting you to create your quantum\naccount. Once logged in, users will have access to Quantinuum's User Interface,\nhttps://um.qapi.quantinuum.com, their online platform for managing jobs and\naccessing the available quantum systems, including the System Model H1, via the\ncloud. From the UI, users can view system status and upcoming system availability,\nas well as monitor batch submissions and job history. Information on using the\nquantum resources via Jupyter notebooks is available in the UI via the “Examples”\ntab. Quantinuum’s systems feature mid-circuit measurement and qubit reuse, and are\ncompatible with a variety of software frameworks.\n\nRigetti\n\nAfter submitting the OLCF quantum account application and receiving approval,\nyou will receive an email from support@rigetti.com inviting you to create your\nquantum account. If you did not receive this, proceed to\nhttps://qcs.rigetti.com/sign-in and click “Sign In”. It is necessary that the\nemail you use for sign in be associated with an affiliated subscribing\ninstitution, i.e. ORNL, ANL, etc. If sign in fails, contact help@olcf.ornl.gov.\nOnce logged in, users will have access to Quantum Cloud Services (QCS),\nRigetti’s online platform for accessing the hybrid infrastructure of available\nquantum processors and classical computational framework via the cloud. From\nthe QCS, users can view system status and availability, initiate and manage\nquantum infrastructure reservations (either executing programs manually or\nadding them to the queue). Information on using this resource is available on\nthe Rigetti's Documentation or our\nOLCF Rigetti Guide <https://docs.olcf.ornl.gov/quantum/quantum_systems/rigetti.html>.\n\nPublication Citations\n\nPublications using resources provided by the OLCF are requested to include the\nfollowing acknowledgment statement: “This research used resources of the Oak\nRidge Leadership Computing Facility, which is a DOE Office of Science User\nFacility supported under Contract DE-AC05-00OR22725.”"}
{"doc":"quantum_faq","text":"Frequently Asked Questions\n\nHow do quantum computers differ from classical computers?\n\nConventional/classical computing utilizes information storage based on digital\ndevices storing “bits”, which are in either of two distinct states at a given\ntime, i.e. 0 or 1. Quantum computers utilize properties of quantum mechanics,\nsuch as superposition and entanglement, in order to exceed certain capabilities\nof classical computers. Superposition means that the units of information\nstorage can be in multiple states at the same time, and entanglement means the\nstates can depend on each other.  In quantum computing systems, information is\nstored not using “bits”, but instead using “qubits”.\n\nWhat is a qubit?\n\nA qubit (pronounced “cue-bit”, a portmanteau of “quantum bit”) is the physical\nunit of quantum information in quantum computing. It is the quantum version of\na bit (itself a portmanteau of “binary digit”), consisting of a two-state\nquantum mechanical system that can (like a classical bit) exist in one state,\n|0⟩, or the other, |1⟩, but unlike the classical bit counterpart, a qubit\ncan also be in a quantum superposition of both states.\n\nHow do I access the OLCF quantum computing resources?\n\nApplications for both Quantum Computing projects and quantum user accounts can\nbe found on the access <https://docs.olcf.ornl.gov/quantum/quantum_access.html> page.\n\nWhat happens after I apply for access to QCUP?\n\nApplications are put through a merit review process, and you will be contacted\nregarding the status of your application. See the user documentation page for\nmore details.\n\nI formerly had access to quantum resources, but my backends/lattices/etc. have disappeared, what do I do?\n\nIf your account was established prior to July 5th, 2020, and was not through\nthe OLCF directly, your access to quantum resources has been removed, and you\nwill need to re-apply to an OLCF project.  Also, if your access to your OLCF\nproject or the project access itself has expired, you will also need to\nreapply.\n\nI applied to a quantum computing resource via the vendor website, but don’t have access; what do I do?\n\nMaking an account on the vendor website does not enable access to OLCF\nprojects; Access requires an account through an OLCF-affiliated website, and\napplying for an OLCF quantum account (see above)."}
{"doc":"rigetti","text":"Rigetti\n\n\n\nOverview\n\nRigetti currently offers access to their systems via their Quantum Cloud\nServices (QCS).  With QCS, Rigetti's quantum processors (QPUs) are tightly\nintegrated with classical computing infrastructure and made available to you\nover the cloud. Rigetti also provides users with quantum computing example\nalgorithms for optimization, quantum system profiling, and other applications.\n\nA list of available Rigetti systems/QPUs, along with their performance statistics,\ncan be found on the Rigetti Systems Page.\n\nIn addition to the rigetti-running <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#rigetti-running> section below, a general workflow\noverview of how programs are run with Rigetti's QCS can be found on Rigetti's\nHow Programs Are Built & Run Guide.\n\nThe guide below describes how to use the system(s) once you have access.\nFor instructions on how to gain access, see our Quantum Access <https://docs.olcf.ornl.gov/quantum/quantum_access.html> page instead.\n\nConnecting\n\nAccess to the Rigetti Quantum Computing queue and simulators can be obtained\nvia multiple methods -- either through the cloud <rigetti-cloud> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#cloud <rigetti-cloud>> or\nlocally <rigetti-local> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#locally <rigetti-local>>.\n\n\n\nCloud Access\n\nRigetti provides system access via a cloud-based JupyterLab development\nenvironment: https://docs.rigetti.com/qcs/getting-started/jupyterlab-ide.  From\nthere, users can access a JupyterLab server loaded with Rigetti's PyQuil\nprogramming framework, Rigetti's Forest Software Development Kit, and\nassociated program examples and tutorials.  This is the method that allows\naccess to Rigetti's QPU's directly, as opposed to simulators.\n\n\n\nLocally via Forest SDK\n\nUsers are able to install Rigetti software locally for the purpose of\ndevelopment using a provided Quantum Virtual Machine, or QVM, an implementation\nof a quantum computer simulator that can run Rigetti's Quil programs.  This can\nbe done via two methods:\n\nInstalling manually: https://docs.rigetti.com/qcs/getting-started/installing-locally\n\nDocker: https://hub.docker.com/r/rigetti/forest\n\n\n\nRunning Jobs\n\nReservations\n\nAll jobs run on Rigetti's systems are submitted via system reservation.  This\ncan be done either by using Rigetti's QCS dashboard to schedule the\nreservation, or via interacting with the QCS via the Command Line Interface\n(CLI).  Scheduled reservations can be viewed and/or cancelled via either\nmethod, either in the dashboard or from the CLI.\n\nTo submit a reservation via the QCS dashboard: https://docs.rigetti.com/qcs/guides/reserving-time-on-a-qpu#using-the-qcs-dashboard\n\nTo submit a reservation via the QCS CLI, and installation instructions: https://docs.rigetti.com/qcs/guides/using-the-qcs-cli\n\nSubmitting Jobs\n\nAccessing the Rigetti QPU systems can only be done during a user's reservation\nwindow.  To submit a job, users must have JupyterHub access to the system.  QVM\njobs can be run without network access in local environments.  Jobs are\ncompiled via Quilc and submitted via pyQuil (see Software Section <rigetti-soft> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Software Section <rigetti-soft>> below) in a python environment\nor Jupyter notebook.\n\nAllocations & Credit Usage\n\nRunning a job on the Aspen-M-1 and Aspen-11 systems requires Rigetti credits, which are exchanged for system reservation time.  Users are initially allocated credits equivalent to 60 minutes of reservation time at the beginning of a project, but requests for increased allocations should be submitted (with a brief explanation) to help@olcf.ornl.gov and will be reviewed by the QRUC.\n\nData Storage Policies\n\nAny work saved in your QCS JupyterLab will be saved and maintained.\n\n\n\nSoftware\n\nQuil: The Rigetti-developed quantum instruction/assembly language: https://pyquil-docs.rigetti.com/en/stable/compiler.html\n\nQuil-T: an extension of Quil with enhanced control of microwave input signals, gate definitions and pulse parameters: https://pyquil-docs.rigetti.com/en/stable/quilt.html\n\nForest SDK: Rigetti-provided software tools for writing quantum programs in Quil, compiling and running them.\n\nPyQuil: PyQuil is a Python library for writing and running quantum programs using Quil: https://pyquil-docs.rigetti.com/en/stable/\n\nQuilc: Quilc is an optional optimizing compiler for Rigetti QPU code deployment: https://pyquil-docs.rigetti.com/en/v2.1.1/quilc-man.html\n\nAdditional Resources\n\nRigetti's Documention\n\nRigetti System Performance Data"}
{"doc":"route","text":"Routes\n\n\n\nIn OpenShift, a Route exposes slate_services <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_services> with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.\n\nA Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).\n\nWhy Routes\n\nRoutes are best used when you have created a service which communicates over HTTP or HTTPS, and you\nwant this service to be accessible from outside the cluster with a FQDN.\n\nIf your application doesn't communicate over HTTP or HTTPS, you should use slate_nodeports <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_nodeports> instead.\n\nCreating Routes\n\nIf no hostname is provided when creating routes, the default will be\n{SERVICE_NAME}-{PROJECT_NAME}.apps.<cluster>.ccs.ornl.gov. Any hostname that follows the pattern\n*.apps.<cluster>.ccs.ornl.gov can be provided, as long as another service isn't using that hostname already.\n\nCLI\n\nSecured routes (over HTTPS) offer encryption to keep connections private. You can use oc create route to create a secured HTTPS route.\n\nSecured routes can use 3 different types of secure TLS termination.\n\nEdge Termination\n\nEdge Termination terminates TLS at the router, before sending traffic to the service. We have a wildcard certificate on the routers for each cluster. These will be used by default if no certificate is provided, and this is the preferred method for securing a route.\n\n$ oc create route edge --service=my-project \\\n  --hostname=my-project.apps.<cluster>.ccs.ornl.gov\n\nIf you would like to use your own keys with edge termination, this can be done with a command similar to this example.\n\n$ oc create route edge --service=my-project \\\n  --hostname=my-project.apps.<cluster>.ccs.ornl.gov \\\n  --cert=ca.crt \\\n  --key=ca.key \\\n  --ca-cert=ca.crt \\\n\nThen, oc get route my-project -o yaml will show the YAML:\n\napiVersion: v1\nkind: Route\nmetadata:\n  name: my-project\nspec:\n  host: my-project.apps.<cluster>.ccs.ornl.gov\n  to:\n    kind: Service\n    name: my-project\n  tls:\n    termination: edge\n    key: |-\n      -----BEGIN PRIVATE KEY-----\n      [...]\n      -----END PRIVATE KEY-----\n    certificate: |-\n      -----BEGIN CERTIFICATE-----\n      [...]\n      -----END CERTIFICATE-----\n    caCertificate: |-\n      -----BEGIN CERTIFICATE-----\n      [...]\n      -----END CERTIFICATE-----\n\nPassthrough Termination\n\nWith Passthrough Termination, the encrypted traffic goes straight to the pod with no TLS termination.\nThis is useful if you are running a service such as HTTPD that is handling TLS termination itself. Another use case example could be doing mutual TLS authentication from a pod.\n\nThe following command will create a secured route with passthrough termination.\n\noc create route passthrough --service=my-project \\\n  --hostname=my-project.apps.<cluster>.ccs.ornl.gov\n\nThe produced yaml will look like this:\n\napiVersion: v1\nkind: Route\nmetadata:\n  name: my-service\nspec:\n  host: my-project.apps.<cluster>.ccs.ornl.gov\n  to:\n    kind: Service\n    name: service-name\n  tls:\n    termination: passthrough\n\nNote that with passthrough termination, no keys are provided to the route.\n\nRe-encryption Termination\n\nRe-encryption termination combines edge termination and passthrough termination, in that the router terminates TLS, then re-encrypts its connection. The endpoint may have a different certificate. With re-encryption termination, both the internal and external network paths are encrypted.\n\nThe following command will create a secured route with re-encryption termination.\n\noc create route reencrypt --service=my-project \\\n  --hostname=my-project.apps.<cluster>.ccs.ornl.gov \\\n  --dest-ca-cert=ca.crt\n\nNote that the --dest-ca-cert flag for the destination CA certificate is required for re-encryption.\n\nThe outputted YAML will look like this example:\n\napiVersion: v1\nkind: Route\nmetadata:\n  name: my-service\nspec:\n  host: my-service.apps.<cluster>.ccs.ornl.gov\n  to:\n    kind: Service\n    name: my-service\n  tls:\n    termination: reencrypt\n    destinationCACertificate: |-\n      -----BEGIN CERTIFICATE-----\n      [...]\n      -----END CERTIFICATE-----\n\nAs with edge encryption, by default the wildcard certificate for the router is used. You can\nprovide your own keys if you like.\n\nHTTP\n\nOpenShift supports unsecured routes over HTTP, but it is not recommended for use. Use edge encryption if you are unsure.\n\nWeb Interface Configuration\n\nRoutes can also be created from the web interface. On the hamburger menu, click Networking, then Routes.\n\nRoute in Hamburger Menu\n\nIf no routes have been created for a project, you will be presented with a Create Route button.\n\nCreate Route\n\nOn the Create Route screen, fill out the form, select your service in the service dropdown.\n\nBe sure to check the Secure Route checkbox. See the CLI sections to determine which encryption type is best\nfor you. If you don't know, it's probably edge encryption. You can leave the certificate fields blank when using edge\nencryption on a wildcard *.apps.<cluster>.ccs.ornl.gov\n\nRoutes with NCCS Authentication\n\nIn order for us to maintain our existing security posture, only users who are on a project will be able\nto access to services that the project runs. This means that when a user accesses a route they will first be\nprompted to log in to OpenShift and once they are authenticated they will be able to access the service.\n\nlogin prompt\n\nRequirements\n\nAll routes require authentication\n\nHTTPS is required on routes for authentication so that sensitive cookie information is not leaked.\n\nThe authenticated user must use their NCCS Username and RSA PASSCODE to log in to OpenShift\n\nThe authenticated user must be on the project in order to use the application running in OpenShift\n\nOptional Application Authentication\n\nThe authentication will be handled by the cluster load balancers so that nothing is required by a user\napplication. If a user application needs to authenticate a user we set the X-Remote-User header which\nis the NCCS username of the authenticated user.\n\nAn example list of headers that are set by the loadbalancer:\n\nHost: nginx-echo-headers-stf002platform.bedrock-dev.ccs.ornl.gov\nX-Remote-User: kincljc\nX-Forwarded-Host: nginx-echo-headers-stf002platform.bedrock-dev.ccs.ornl.gov\nX-Forwarded-Port: 443\nX-Forwarded-Proto: https\nForwarded: for=160.91.195.36;host=nginx-echo-headers-stf002platform.bedrock-dev.ccs.ornl.gov;proto=https;proto-version=\nX-Forwarded-For: 160.91.195.36\n\nHow\n\nRoutes are secured by adding the annotation ccs.ornl.gov/requireAuth = \"true\" to the route.\n\nExemptions\n\nIf you have an application that should not require authentication reach out to NCCS Support.\n\nInternet-facing Services\n\nBy default, a route will only expose your slate_services <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_services>\nto NCCS networks. If you need your service exposed to the world outside ORNL, you will first need to get your project\napproved for external routes. To do this, submit a systems ticket. In the description, give us your project name\nand a brief reasoning for why exposing externally is needed.\n\nWe will let you know once your project is able to set up external routes.\n\nLabelling Routes\n\nOnce your project has been approved, you only need to give your route a label to tell the OpenShift router to expose\nthis service externally. You can do this in the CLI or in the web interface.\n\nCLI\n\n<string>:249: (INFO/1) Duplicate implicit target name: \"cli\".\n\nOn the CLI, run oc label route {ROUTE_NAME} ccs.ornl.gov/externalRoute=true.\n\nGUI\n\nIn the web interface, from the side menu, select Networking, then Routes.\n\nRoutes Menu\n\nThis will show a list of your routes. Click the route you want to expose, and click the YAML tab.\n\nUnder metadata, add a label for ccs.ornl.gov/externalRoute: 'true' as shown below and click the Save button at the bottom of the page.\n\nRoute After\n\nAfter saving, your route will be exposed on two routers, default and external. This means your service is now\naccessible from outside ORNL. Note that if your project has not yet been approved for external routing, this second\nrouter will not expose your route.\n\nRoute Exposed\n\nAdvanced Routes\n\nMultiple Services\n\nWhile a route usually points to one service through the to parameter in the configuration, it is\npossible to have as many as four services to load balance between. This is used with A/B deployments.\n\nHere is an example route which points to 3 services:\n\napiVersion: v1\nkind: Route\nmetadata:\n  name: route-alternate-service\n  annotations:\n    haproxy.router.openshift.io/balance: roundrobin\nspec:\n  host: www.example.com\n  to:\n    kind: Service\n    name: service-name\n    weight: 20\n  alternateBackends:\n  - kind: Service\n    name: service-name2\n    weight: 10\n  - kind: Service\n    name: service-name3\n    weight: 10\n\nNotice the weight parameter on each service. This weight must be in the range 0-256. The default\nis 1. If the weight is 0, no requests will be passed to the service. If all services have a 0 weight,\nthen all requests will return a 503 error.\n\nThe portion of requests sent to each service is determined by its weight divided by the sum of all\nweights. In the above example, service-name will get 20/40 or 1/2 of the requests, and service-name2\nand service-name3 will each get 10/40 or 1/4 of the requests.\n\nWhen using alternateBackends, be sure to set .metadata.annotations.haproxy.router.openshift.io/balance\nto roundrobin, like in the above example. This will ensure that HAProxy will use a round robin load balancing strategy."}
{"doc":"Scorep","text":"\n\n\n\n\n\n\n\n\n\n\n\n\n\nScore-P\n\nOverview\n\nThe Score-P measurement infrastructure is a highly\nscalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC\napplications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi\nprocessing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL,\nOpenACC) and combinations. It works in combination with Periscope, Scalasca, Vampir, and Tau.\n\nUsage\n\nSteps in a typical Score-P workflow to run on summit-user-guide <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#summit-user-guide>:\n\nLogin to Summit <connecting-to-olcf> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Summit <connecting-to-olcf>>: ssh <user_id>@summit.olcf.ornl.gov\n\nInstrument your code with Score-P\n\nPerform a measurement run with profiling enabled\n\nPerform a profile analysis with CUBE or cube_stat\n\nUse scorep-score to define a filter\n\nPerform a measurement run with tracing enabled and the filter applied\n\nPerform in-depth analysis on the trace data with Vampir\n\nInstrumentation\n\nTo instrument your code, you need to compile the code using the Score-P instrumentation command (scorep), which is added as a prefix to your compile statement.\nIn most cases the Score-P instrumentor is able to automatically detect the programming paradigm from the set of compile and link options given to the compiler.\nSome cases will, however, require some additional link options within the compile statement e.g. CUDA instrumentation.\n\nBelow are some basic examples of the different instrumentation scenarios:\n\nYou will need to unload the darshan-runtime module. In some instances you may need to unload the xalt and xl modules.\n\n$ module unload darshan-runtime\n\nSerial\n\n..  C\n\n    .. code-block:: bash\n\n        $ module unload darshan-runtime\n        $ module load scorep\n        $ module load gcc\n        $ scorep gcc -c test.c\n        $ scorep gcc -o test test.o\n\n..  C++\n\n    .. code-block:: bash\n\n        $ module unload darshan-runtime\n        $ module load scorep\n        $ module load gcc\n        $ scorep g++ -c test.cpp main.cpp\n        $ scorep g++ -o test test.o main.o\n\n..  Fortran\n\n    .. code-block:: bash\n\n        $ module unload darshan-runtime\n        $ module load scorep\n        $ module load gcc\n        $ scorep gfortran -c test_def.f90 test.f90 main.f90\n        $ scorep gfortran -o test test_def.o test.o main.o\n\nMPI\n\n..  C\n\n    .. code-block:: bash\n\n          $ module unload darshan-runtime\n          $ module load scorep\n          $ module load spectrum-mpi\n          $ module load gcc\n          $ scorep mpicc -c test.c main.c\n          $ scorep mpicc -o test test.o main.o\n\n..  C++\n\n    .. code-block:: bash\n\n          $ module unload darshan-runtime\n          $ module load scorep\n          $ module load spectrum-mpi\n          $ module load gcc\n          $ scorep mpiCC -c test.cpp main.cpp\n          $ scorep mpiCC -o test test.o main.o\n\n..  Fortran\n\n    .. code-block:: bash\n\n        $ module unload darshan-runtime\n        $ module load gcc\n        $ module load Scorep\n        $ scorep mpifort -c test.f90\n        $ scorep mpifort -o test test.o\n\nMPI + OpenMP\n\n..  C\n\n    .. code-block:: bash\n\n          $ module unload darshan-runtime\n          $ module load scorep\n          $ module load gcc\n          $ scorep mpicc -fopenmp -c test.c main.c\n          $ scorep mpicc -fopenmp -o test test.o main.o\n\n..  C++\n\n    .. code-block:: bash\n\n          $ module unload darshan-runtime\n          $ module load scorep\n          $ module load gcc\n          $ scorep mpiCC -fopenmp -c test.cpp main.cpp\n          $ scorep mpiCC -fopenmp -o test test.o main.o\n\n..  Fortran\n\n    .. code-block:: bash\n\n          $ module unload darshan-runtime\n          $ module load scorep\n          $ module load gcc\n          $ scorep mpifort -pthread -fopenmp -c test.f90\n          $ scorep mpifort -pthread -fopenmp -o test test.o\n\nCUDA\n\nIn some cases e.g. **CUDA** applications, Score-P needs to be made aware of the programming paradigm in order to do the correct instrumentation.\n\n.. code-block:: bash\n\n    $ module unload darshan-runtime xl\n    $ module load nvhpc\n    $ module load cuda\n    $ module load scorep/<version-number>-papi\n    $ scorep --cuda --user nvc++ -cuda -L${OLCF_CUDA_ROOT}/lib64 -c test.c\n    $ scorep --cuda --user nvc++ -cuda -L${OLCF_CUDA_ROOT}/lib64 -o test test.o\n\nMakefiles\n\nSetting PREP = scorep variable within a Makefile will allow for instrumentation control while using\nmake\n\nAdditionaly, one can add other Score-P options within the PREP variable e.g. --cuda\n\n##Sample Makefile:\n\nCCOMP  = nvc++\nCFLAGS =\nPREP = scorep --cuda\n\nINCLUDES  = -I<Path to Includes>/include ##If needed\nLIBRARIES = -L<Path to Libraries>/lib64 ##If needed\n\ntest: test.o\n   $(PREP) $(CCOMP) $(CFLAGS) $(LIBRARIES) test.o -o test\n\ntest.o: test.c\n   $(PREP) $(CCOMP) $(CFLAGS) $(INCLUDES) -c test.c\n\n.PHONY: clean\n\nclean:\n   rm -f test *.o\n\nCMake / Autotools\n\nFor CMake and Autotools based build systems, it is recommended to use the scorep-wrapper script\ninstances. The intended usage of the wrapper instances is to replace the application's compiler and\nlinker with the corresponding wrapper at configuration time so that they will be used at build time.\nAs the Score-P instrumentation during the CMake or configure steps is likely to fail, the wrapper script allows for disabling the instrumentation by setting the variable SCOREP_WRAPPER=off.\n\nFor CMake and Autotools based builds it is recommended to configure in the following way(s):\n\n#Example for CMake\n\n$ SCOREP_WRAPPER=off cmake .. \\\n     -DCMAKE_C_COMPILER=scorep-gcc \\\n     -DCMAKE_CXX_COMPILER=scorep-g++ \\\n     -DCMAKE_Fortran_COMPILER=scorep-ftn\n\n#Example for autotools\n\n$ SCOREP_WRAPPER=off  ../configure \\\n     CC=scorep-gcc \\\n     CXX=scorep-g++ \\\n     FC=scorep--ftn \\\n     --disable-dependency-tracking\n\nSCOREP_WRAPPER=off disables the instrumentation only in the environment of the configure or cmake command. Subsequent calls to make are not affected and will instrument the application as expected.\n\n<string>:16: (INFO/1) Duplicate implicit target name: \"score-p\".\n\nFor more detailed information on using Score-P with CMake or Autotools visit Score-P\n\nTo see all available options for instrumentation:\n\n$ scorep --help\n\n\n\nMeasurement\n\nOnce the code has been instrumented, it is time to begin the measurement run of the newly compiled code. The measurement calls will gather information during the runtime of the code where this information will be stored for later analysis.\n\nBy default Score-P is configured to run with profiling set to true and tracing set to false. Measurement types are configured via environment variables.\n\n##Environment variable setup examples\n\nexport SCOREP_ENABLE_TRACING=true\n\nYou can check what current Score-P environment variables are set:\n\n$ scorep-info config-vars --full\n\n#Output\n\nSCOREP_ENABLE_PROFILING\nDescription: Enable profiling\n      Type: Boolean\n      Default: true\n\nSCOREP_ENABLE_TRACING\nDescription: Enable tracing\n      Type: Boolean\n      Default: false\n\nSCOREP_VERBOSE\nDescription: Be verbose\n      Type: Boolean\n      Default: false\n\n .....\n\nProfiling\n\nTo generate a profile run of your instrumented code on Summit, you will first need to get a node allocation\nusing a batch script or an interactive job; Additionaly you will need to load modules otf2 and cubew:\n\n$ module load otf2\n$ module load cubew\n\nExample Batch Script\n\n#!/bin/bash\n# Begin LFS Directives\n#BSUB -P ABC123        #Project Account\n#BSUB -W 3:00          #Walltime\n#BSUB -nnodes 1        #Number of Nodes\n#BSUB -J RunSim123     #Job Name\n#BSUB -o RunSim123.%J  #Job System Out\n#BSUB -e RunSim123.%J  #Job System Error Out\n\ncd <path to instrumented code>\n\njsrun -n 1 ./<binary to run>\n\nFor more information on launching jobs on Summit, please see the Running Jobs <running-jobs> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Running Jobs <running-jobs>> section of the Summit User Guide.\n\nThe output files generated when the profile measurement runs are successful will be placed in a folder uniquely named:\n\n$ scorep-yyyymmdd_hhmm_<Unique ID created>\n\nA file will be placed within the above mentioned folder with the name profile.cubex. This type of file can be analyzed using a tool called Cube developed by Scalasca.\n\nFor a more detailed description of profiling measurements with Score-P, please visit the ScorepP_Profiling homepage.\n\nTracing\n\nTo run a tracing measurement, we will need to enable this through the environment variable SCOREP_ENABLE_TRACING:\n\n$ export SCOREP_ENABLE_TRACING=true\n\nSince tracing measurements acquire significantly more output data than profiling, we need to design a filter to remove some of the most visited calls within your instrumented code. There is a tool developed by Score-P that allows us to estimate the size of the trace file (OTF2) based on information attained from the profiling generated cube file.\n\nTo gather the needed information to design a filter file, first run scorep-score:\n\n$ scorep-score -r <profile cube dir>/profile.cubex\n\nOutput scorep-score generated Example:\n\nEstimated aggregate size of event trace:                   40GB\nEstimated requirements for largest trace buffer (max_buf): 10GB\nEstimated memory requirements (SCOREP_TOTAL_MEMORY):       10GB\n(warning: The memory requirements can not be satisfied by Score-P to avoid\nintermediate flushes when tracing. Set SCOREP_TOTAL_MEMORY=4G to get the\nmaximum supported memory or reduce requirements using USR regions filters.)\n\nFlt type      max_buf[B]         visits  time[s]  time[%]  time/visit[us]      region\n     ALL  10,690,196,070  1,634,070,493  1081.30    100.0            0.66         ALL\n     USR  10,666,890,182  1,631,138,069   470.23     43.5            0.29         USR\n     OMP      22,025,152      2,743,808   606.80     56.1          221.15         OMP\n     COM       1,178,450        181,300     2.36      0.2           13.04         COM\n     MPI         102,286          7,316     1.90      0.2          260.07         MPI\n\n     USR   3,421,305,420    522,844,416   144.46     13.4            0.28  matmul_sub\n     USR   3,421,305,420    522,844,416   102.40      9.5            0.20  matvec_sub\n\nThe first line of the output gives an estimation of the total size of the trace, aggregated over all processes. This information is useful for estimating the space required on disk. In the given example, the estimated total size of the event trace is 40GB. The second line prints an estimation of the memory space required by a single process for the trace. Since flushes heavily disturb measurements, the memory space that Score-P reserves on each process at application start must be large enough to hold the process’ trace in memory in order to avoid flushes during runtime.\n\nIn addition to the trace, Score-P requires some additional memory to maintain internal data structures. Thus, it provides also an estimation for the total amount of required memory on each process. The memory size per process that Score-P reserves is set via the environment variable SCOREP_TOTAL_MEMORY. In the given example the per process memory is about 10GB. When defining a filter, it is recommended to exclude short, frequently called functions from measurement since they require a lot of buffer space (represented by a high value under max_tbc) but incur a high measurement overhead. MPI functions and OpenMP constructs cannot be filtered. Thus, it is usually a good approach to exclude regions of type USR starting at the top of the list until you reduced the trace to your needs. The example below excludes the functions matmul_sub and matvec_sub from the trace:\n\n$ cat scorep.filter\nSCOREP_REGION_NAMES_BEGIN\n Exclude\n   matmul_sub\n   matvec_sub\nSCOREP_REGION_NAMES_END\n\nOne can check the effects of the filter by re-running the scorep-score command:\n\n$ scorep-score <profile cube dir>/profile.cubex -f scorep.filter\n\nTo apply the filter to your measurement run, you must specify this in an environment variable called\nSCOREP_FILTERING_FILE:\n\n$ export SCOREP_FILTERING_FILE=scorep.filter\n\nNow you are ready to submit your instrumented code to run with tracing enabled. This measurement will generate files of the form traces.otf.\nThe .otf2 file format can be analyzed by a tool called Vampir .\n\n<string>:16: (INFO/1) Duplicate explicit target name: \"vampir\".\n\nVampir provides a visual GUI to\nanalyze the .otf2 trace file generated with Score-P.\n\nSmall trace files can be viewed locally on your machine if you have the Vampir client downloaded,\notherwise they can be viewed locally on Summit. For large trace files, it is strongly recommended to run\nvampirserver reverse-connected to a local copy of the Vampir client. See the vamptunnel <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#vamptunnel> section for more details.\n\nManual Instrumentation\n\nIn addition to automatically profiling and tracing functions, there is also a way to manually instrument a specific region in the source code. To do this, you will need to add the --user flag to the scorep command when compiling:\n\n$ scorep --user gcc -c test.c\n$ scorep --user gcc -o test test.o\n\nNow you can manually instrument Score-P to the source code as seen below:\n\nC,C++\n\n.. code::\n\n   #include <scorep/SCOREP_User.h>\n\n   void foo() {\n      SCOREP_USER_REGION_DEFINE(my_region)\n      SCOREP_USER_REGION_BEGIN(my_region, \"foo\", SCOREP_USER_REGION_TYPE_COMMON)\n      // do something\n      SCOREP_USER_REGION_END(my_region)\n   }\n\nFortran\n\n.. code::\n\n   #include <scorep/SCOREP_User.inc>\n\n   subroutine foo\n      SCOREP_USER_REGION_DEFINE(my_region)\n      SCOREP_USER_REGION_BEGIN(my_region, \"foo\", SCOREP_USER_REGION_TYPE_COMMON)\n      ! do something\n      SCOREP_USER_REGION_END(my_region)\n   end subroutine foo\n\nIn this case, \"my_region\" is the handle name of the region which has to be defined with SCOREP_USER_REGION_DEFINE. Additionally, \"foo\" is the string containing the region's unique name (this is the name that will show up in Vampir) and SCOREP_USER_REGION_TYPE_COMMON identifies the type of the region. Make note of the header files seen in the above example that are needed to include the Score-P macros. See the Score-P User Adapter page for more user configuration options.\n\nBelow are some examples of manually instrumented regions using phase and loop types:\n\n#include <scorep/SCOREP_User.h>\n\nSCOREP_USER_REGION_DEFINE(sum_hdl)\nSCOREP_USER_REGION_BEGIN(sum_hdl, \"sum\", SCOREP_USER_REGION_TYPE_PHASE)\nif (x < 1){\n   //do calculation\n}\nelse{\n   //do other calculation\n}\nSCOREP_USER_REGION_END(sum_hdl)\n\n#include <scorep/SCOREP_User.h>\n\nSCOREP_USER_REGION_DEFINE(calculation_hdl)\nSCOREP_USER_REGION_BEGIN(calculation_hdl, \"my_calculations\", SCOREP_USER_REGION_TYPE_LOOP)\n#pragma omp parallel for ...\n   for (int i=0; i <num; i++){\n      //do calculation\n   }\nSCOREP_USER_REGION_END(calculation_hdl)\n\nThe regions \"sum\" and \"my_calculations\" in the above examples would then be included in the profiling and tracing runs and can be analysed with Vampir. For more details, refer to the Advanced Score-P training in the training-archive <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#training-archive>.\n\nScore-P Demo Video\n\nPlease see the provided video below to watch a brief demo of using Score-P provided by TU-Dresden and presented by Ronny Brendel.\n\n<div style=\"padding:56.25% 0 0 0;position:relative;\"><iframe src=\"https://player.vimeo.com/video/285908215?h=26f33f1775\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen></iframe></div><script src=\"https://player.vimeo.com/api/player.js\"></script>\n\n<p><a href=\"https://vimeo.com/285908215\">2018 Score-P / Vampir Workshop</a> from <a href=\"https://vimeo.com/olcf\">OLCF</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>\n\nThis recording is from the 2018 Score-P / Vampir workshop that took place at ORNL on August 17, 2018. In the video, Ronny Brendel gives an introduction to the Score-P and Vampir tools, which are often used together to collect performance profiles/traces from an application and visualize the results."}
{"doc":"services","text":"Services\n\n\n\nIn Kubernetes, a Service is an internal load balancer which identifies a set of pods and can proxy traffic to them.\nThis set of pods is determined by a label selector.\n\nA service is a stable way of accessing a set of pods, which are ephemeral.\n\nWhen a service is created, it is granted a ClusterIP, which is an IP address internal to the\nKubernetes cluster. Other pods can use this ClusterIP to access the service.\n\nHere is an example service definition:\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    name: my-app\n  ports:\n  - port: 8080\n    protocol: TCP\n    targetPort: 8080\n\nThis definition tells Kubernetes that all pods with the label \"my-app\" are associated with this service. Any traffic to the service should be distributed\namong these pods.\n\nThe port parameter contains what port the service listens on, and the targetPort parameter contains the port to which the service forwards connections.\n\nCreating Services\n\nCLI\n\nOn the command line, services can be created with the command oc create. Assuming our YAML file from above is in the file my-service.yaml, you can\ncreate the service with\n\n$ oc create -f my-service.yaml\n\nThen, you can run oc describe service my-service to see some information about it.\n\n$ oc describe service my-service\nName:                   my-service\nLabels:                 <none>\nSelector:               name=my-app\nType:                   ClusterIP\nIP:                     172.31.34.153\nPort:                   <unnamed>       8080/TCP\nEndpoints:              10.132.7.126:8080,10.132.7.127:8080,10.132.7.123:8080\nSession Affinity:       None\nNo events.\n\nIn this example, looking at Endpoints, we have 3 pods running with the my-app selector. This means that from inside the cluster\nif an application accesses the ClusterIP on port 8080 the traffic will be directed to one of the three pods.\n\nAccessing Services from Outside the Cluster\n\nA service of type ClusterIP will only ever be accessible from inside the cluster. If you need access to your service from outside\nof the cluster there are a few different options.\n\nRoute\n\nIn general, using FQDNs to access a service is more convenient. If your service communicates over HTTP or HTTPS, you can set up a\nslate_routes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_routes> to achieve this. If your service uses another protocol, you can use slate_nodeports <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#slate_nodeports>.\n\nNodePort\n\nNodePort is a type of Service that reserves a port across the cluster that can route traffic to your pods. This is more flexible\nthan a Route and can handle any TCP or UDP traffic."}
{"doc":"simple_website","text":"Build and Deploy Simple Website\n\nOpenShift has an integrated container image build service that users interact with through BuildConfig objects. BuildConfig's are very powerful, builds can be triggered by git repo or image tag pushes and connected into a pipeline to do automated deployments of newly built images. While powerful, these\nmechanisms can be cumbersome when starting out so we will be using a BuildConfig in a slightly simpler setup.\n\nCreating the BuildConfig\n\nWe will create a BuildConfig that will take a Binary (Local) source which will stream the contents of our local filesystem to the builder.\n\nFirst, we will log into the cluster using the oc CLI tool\n\noc login https://api.<cluster>.ccs.ornl.gov\n\nNext we will create the ImageStream that the BuildConfig will push the completed image to. The ImageStream is a direct mapping to the image stored in the OpenShift integrated registry.\n\noc create imagestream local-image\n\nNext, we will create the BuildConfig object\n\napiVersion: \"build.openshift.io/v1\"\nkind: \"BuildConfig\"\nmetadata:\n  name: \"local-image\"\nspec:\n  output:\n    to:\n      kind: \"ImageStreamTag\"\n      name: \"local-image:latest\"\n  source:\n    type: Binary\n  strategy:\n    type: dockerStrategy\n    dockerStrategy: {}\n\nCreate a file with the above contents and instantiate the objects in Kubernetes\n\noc apply -f buildconfig.yaml\n\nCreate the Image\n\nWe will create the directory along with our files:\n\nDockerfile Describes our docker build\n\nhttpd.conf Apache HTTPd does not work out of the box as non-root so we modify the default configuration file\n\nindex.html Simple index.html page we will serve\n\nmkdir local-image\ncd local-image\n\ncat <<EOF > index.html\nHello World!\nEOF\n\ncat <<EOF > httpd.conf\nServerRoot \"/etc/httpd\"\n\n# Minimum modules needed\nLoadModule mpm_event_module modules/mod_mpm_event.so\nLoadModule log_config_module modules/mod_log_config.so\nLoadModule mime_module modules/mod_mime.so\nLoadModule dir_module modules/mod_dir.so\nLoadModule authz_core_module modules/mod_authz_core.so\nLoadModule unixd_module modules/mod_unixd.so\n\nTypesConfig /etc/mime.types\n\nPidFile /tmp/httpd.pid\n\n# Port to Listen on\nListen *:8080\n\nDocumentRoot \"/var/www/html\"\n\n# Default file to serve\nDirectoryIndex index.html\n\n# Errors go to their own log\nErrorLog /dev/stderr\n\n# Access log\nLogFormat \"%h %l %u %t \\\"%r\\\" %>s %b\" common\nCustomLog /dev/stdout common\n\n# Never change this block\n<Directory />\n  AllowOverride None\n  Require all denied\n</Directory>\n\n# Allow documents to be served from the DocumentRoot\n<Directory \"/var/www/html\">\n    Options Indexes FollowSymLinks\n    AllowOverride None\n    Require all granted\n</Directory>\nEOF\n\ncat <<EOF > Dockerfile\nFROM rockylinux:latest\nRUN yum -y update\nRUN yum -y install httpd\nADD index.html /var/www/html\nADD httpd.conf /etc/httpd/conf/httpd.conf\nCMD [\"/usr/sbin/httpd\", \"-DFOREGROUND\"]\nEOF\n\nNow let's create a Build from the BuildConfig and upload our current directory as the source for the build.\n\n$ oc start-build local-image --from-dir=. --follow\nUploading directory \".\" as binary input for the build ...\n...\nSuccessfully built fe3e487fffe5\nPushing image image-registry.openshift-image-registry.svc:5000/stf002platform/local-image:latest ...\nPush successful\n\nOnce that is complete, we can see that the image was uploaded to the integrated registry by getting the ImageStream object\n\n$ oc get imagestream local-image\nNAME          DOCKER REPO                                                   TAGS     UPDATED\nlocal-image   image-registry.openshift-image-registry.svc:5000/stf002platform/local-image   latest   5 minutes ago\n\nDeploy the Image\n\nNow that we have built a container image we can deploy it with a Deployment object. Using the Docker Repo specified in the ImageStream we can create our deployment:\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  creationTimestamp: null\n  labels:\n    app: local-image\n  name: local-image\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: local-image\n  strategy: {}\n  template:\n    metadata:\n      creationTimestamp: null\n      labels:\n        app: local-image\n    spec:\n      containers:\n      - image: \"image-registry.openshift-image-registry.svc:5000/stf002platform/local-image:latest\"\n        imagePullPolicy: Always\n        name: local-image\n        resources: {}\nstatus: {}\n\nCreate a file with the above contents and instantiate the objects in Kubernetes\n\noc apply -f deployment.yaml\n\nSnippet created with oc create deployment local-image --image image-registry.openshift-image-registry.svc:5000/stf002platform/local-image --dry-run -o yaml\n\nNow once the Deployment controller creates a pod we should be able to do a port forward and test that our web server is serving our index.html file\n\n$ oc describe deployment local-image\n...\n$ oc get pods -l app=local-image\n...\n$ oc port-forward deployment/local-image 8080:8080\nForwarding from [::1]:8080 -> 8080\nForwarding from 127.0.0.1:8080 -> 8080\n\nSince this is running the foreground, in a second terminal use curl to run the test:\n\n$ curl localhost:8080\nHello World!\n\nor one could also use you web browser to verify the content.\n\nNext Steps\n\nCan you modify the index.html page to display \"Hello from (your name)\"?\n\n<details>\n<summary><a>Stuck?</a></summary>\n\nModify index.html in your current directory\n\nStart a new image build: oc start-build local-image --from-dir=. --follow\n\nGet a list of pods running in your namespace: oc get pods\n\nDelete the currently running pod: oc delete pod local-image-...\n\nAlternative to deleting the pod, update the Deployment to trigger a new rollout: oc patch deployment local-image -p '{\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"date\":\"'`date +'%s'`'\"}}}}}'\n\n</details>\n\nTeardown\n\nOnce we are finished testing, we can delete everything\n\noc delete deployment local-image\noc delete buildconfig local-image\noc delete imagestream local-image"}
{"doc":"software-news","text":"Software News\n\nThis page lists significant changes to software provided on OLCF systems. The\nmost recent changes are listed first.\n\n\n\n<p style=\"font-size:20px\"><b>Frontier and Crusher: System Software Upgrade (July 18, 2023)</b></p>\n\nThe Crusher TDS and Frontier systems were upgraded to a new version of the system software stack. This stack introduces ROCm 5.5.1 and HPE/Cray Programming Environment 23.05. For more information, please see:\n\nCrusher System Updates.\n\nFrontier System Updates.\n\nPlease contact help@olcf.ornl.gov with any issues or questions.\n\n\n\n<p style=\"font-size:20px\"><b>Frontier: Darshan Runtime 3.4.0 (May 10, 2023)</b></p>\n\nThe Darshan Runtime modulefile darshan-runtime/3.4.0 on Frontier is now loaded by default. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>.\n\nUnloading darshan-runtime modulefile is recommended for users profiling their applications with other profilers to prevent conflicts.\n\nPlease make a note of this change and contact help@olcf.ornl.gov with any issues or questions.\n\n\nThe table presents information on the latest version of the darshan-runtime package, which is version 3.4.0. This package is a software tool that collects and reports on I/O activity in high-performance computing (HPC) applications. It is designed to help users understand and optimize their I/O behavior, which is crucial for achieving optimal performance in HPC environments. The darshan-runtime package is widely used in the HPC community and is constantly updated to support new features and improve performance. Version 3.4.0 includes bug fixes and enhancements, such as improved support for Lustre file systems and better compatibility with newer versions of the Intel compiler. This table serves as a quick reference for users to check the latest version of the darshan-runtime package and stay updated on any changes or improvements. \n\n| Package        | Version |\n|----------------|---------|\n| darshan-runtime | 3.4.0   |\n\n\n\n\n\n<p style=\"font-size:20px\"><b>Summit: Darshan Runtime 3.4.0-lite (December 28, 2022)</b></p>\n\nThe default version of Darshan Runtime has been updated on Summit to version 3.4.0-lite and is available via the darshan-runtime/3.4.0-lite modulefile. In addition, the default version for the companion set of tools provided in Darshan Util has been updated to darshan-util/3.4.0. Please note that darshan-util/3.4.0 is required to properly parse logs generated with darshan-runtime/3.4.0-lite.\n\n\nThe table presents information on the latest version of the darshan-runtime package, which is version 3.4.0-lite. This package is a software tool used for collecting and analyzing I/O performance data from high-performance computing applications. The darshan-runtime package is designed to be lightweight and efficient, making it ideal for use in large-scale computing environments. This latest version, 3.4.0-lite, includes several updates and improvements, such as enhanced support for Lustre file systems and improved compatibility with different compilers and libraries. Additionally, this version also includes bug fixes and performance optimizations, ensuring a more reliable and efficient experience for users. Overall, the darshan-runtime package continues to be a valuable tool for researchers and developers in the high-performance computing community, and this latest version further solidifies its reputation as a top-performing software tool.\n\n| Package        | Version       |\n| -------------- | ------------- |\n| darshan-runtime | 3.4.0-lite    |\n\n\n\n\n\n<p style=\"font-size:20px\"><b>Summit: OpenCE 1.5.2 (March 17, 2022)</b></p>\n\nOpenCE 1.5.2 is now available on Summit. OpenCE 1.5.2 is available for python versions 3.9, 3.8, and 3.7. These builds can be accessed by\nloading the open-ce/1.5.2-py39-0, open-ce/1.5.2-py38-0, and open-ce/1.5.2-py37-0 modules, respectively.\n\nThe following packages are available in this release of OpenCE:\n\n\nThe table above presents a comprehensive list of software packages and their corresponding versions that are currently making waves in the tech industry. The first group of packages are all related to TensorFlow, a popular open-source software library for machine learning and deep learning. These packages include TensorFlow 2.7.1, TensorFlow Estimators 2.7.0, TensorFlow Probability 0.15.0, TensorBoard 2.7.0, TensorFlow Text 2.7.3, TensorFlow Model Optimizations 0.7.0, TensorFlow Addons 0.15.0, TensorFlow Datasets 4.4.0, TensorFlow Hub 0.12.0, and TensorFlow MetaData 1.5.0. These packages offer a wide range of functionalities such as building and training machine learning models, visualizing data, and optimizing models for better performance.\n\nThe next set of packages are related to PyTorch, another popular open-source machine learning library. These packages include PyTorch 1.10.2, TorchText 0.11.2, TorchVision 0.11.3, PyTorch Lightning 1.5.10, and PyTorch Lightning Bolts 0.5.0. These packages provide tools for building and training neural networks, natural language processing, computer vision, and distributed training.\n\nOther notable packages in the table include ONNX 1.10.2, a popular open format for representing deep learning models, Keras 2.7.0, a high-level neural networks API, Magma 2.5.4, a library for linear algebra computations on GPUs, XGBoost 1.5.2, a popular gradient boosting library, and Transformers 4.11.3, a library for natural language processing tasks. These packages are widely used in various industries for their powerful and efficient capabilities.\n\nThe last set of packages in the table includes Tokenizers 0.10.3, SentencePiece 0.1.96, Spacy 3.2.1, OpenCV 4.5.5, DALI 1.9.0, and Horovod 0.23.0. These packages offer tools for text processing, natural language understanding, computer vision, and distributed training, making them essential for many machine learning and deep learning projects.\n\nIn summary, this table provides a comprehensive overview of the latest versions of popular software packages used in the field of machine learning and deep learning. These packages offer a wide range of functionalities and are constantly updated to meet the ever-evolving needs of the tech industry.\n\n| Package | Version |\n|---------|---------|\n| Tensorflow | 2.7.1 |\n| TensorFlow Estimators | 2.7.0 |\n| TensorFlow Probability | 0.15.0 |\n| TensorBoard | 2.7.0 |\n| TensorFlow Text | 2.7.3 |\n| TensorFlow Model Optimizations | 0.7.0 |\n| TensorFlow Addons | 0.15.0 |\n| TensorFlow Datasets | 4.4.0 |\n| TensorFlow Hub | 0.12.0 |\n| TensorFlow MetaData | 1.5.0 |\n| PyTorch | 1.10.2 |\n| TorchText | 0.11.2 |\n| TorchVision | 0.11.3 |\n| PyTorch Lightning | 1.5.10 |\n| PyTorch Lightning Bolts | 0.5.0 |\n| ONNX | 1.10.2 |\n| Keras | 2.7.0 |\n| Magma | 2.5.4 |\n| XGBoost | 1.5.2 |\n| Transformers | 4.11.3 |\n| Tokenizers | 0.10.3 |\n| SentencePiece | 0.1.96 |\n| Spacy | 3.2.1 |\n| OpenCV | 4.5.5 |\n| DALI | 1.9.0 |\n| Horovod | 0.23.0 |\n\n\n\n\n\n<p style=\"font-size:20px\"><b>Ascent: Software Installation/Default Software Changes (February 7-11, 2022)</b></p>\n\nAscent's operating system will be upgraded to Red Hat Enterprise Linux 8 (RHEL 8) on February 7-11, 2022.\n\nCodes should be rebuilt prior to running following the upgrade due to the OS and software changes.\n\nAs a result of the upgrade, the following new packages will become available:\n\n\nThe table above presents the current and new default versions for various software packages related to CUDA, IBM Spectrum MPI, IBM XL, and IBM ESSL. The first package, CUDA Toolkit, currently has a default version of 10.1.243, but the new default version will be 11.0.3. This update may include new features, bug fixes, and performance improvements. The second package, IBM Spectrum MPI, has a current default version of 10.3.1.2-20200121, but the new default version will be 10.4.0.3-20210112. This update may also include new features and bug fixes for the MPI (Message Passing Interface) library used for parallel computing. The third package, IBM XL, has a current default version of 16.1.1-5, but the new default version will be 16.1.1-10. This update may include improvements to the XL C/C++ and Fortran compilers, as well as the XL runtime environment. Lastly, the IBM ESSL package has a current default version of 6.1.0-2, but the new default version will be 6.3.0. This update may include enhancements to the ESSL (Engineering and Scientific Subroutine Library) used for mathematical and scientific computing. Overall, these updates to the default versions of these software packages will provide users with improved functionality and performance. \n\n| Package        | Current Default | New Default |\n|----------------|-----------------|-------------|\n| CUDA Toolkit   | 10.1.243        | 11.0.3      |\n| IBM Spectrum MPI | 10.3.1.2-20200121 | 10.4.0.3-20210112 |\n| IBM XL         | 16.1.1-5        | 16.1.1-10   |\n| IBM ESSL       | 6.1.0-2         | 6.3.0       |\n\n\n\nThe OS-provided Python will no longer be accessible as python (including variations like /usr/bin/python or /usr/bin/env python); rather, you must specify it as python2 or python3. If you are using python from one of the modulefiles rather than the version in /usr/bin, this change should not affect how you invoke python in your scripts, although we encourage specifying python2 or python3 as a best practice.\n\n\n\n<p style=\"font-size:20px\"><b>Summit: OpenCE 1.5.0 (December 29, 2021)</b></p>\n\nOpenCE 1.5.0 is now available on Summit. OpenCE 1.5.0 is available for python versions 3.7, 3.8, and 3.9. These builds can be accessed by\nloading the open-ce/1.5.0-py37-0, open-ce/1.5.0-py38-0, and open-ce/1.5.0-py39-0 modules, respectively.\n\nThe following packages are available in this release of OpenCE:\n\n\nThe table above presents a comprehensive list of software packages and their corresponding versions that are currently making headlines in the world of technology. The first few entries are all related to TensorFlow, a popular open-source software library for machine learning and deep learning. The latest version of TensorFlow, 2.7.0, is included in the table, along with other related packages such as TensorFlow Estimators, TensorFlow Probability, and TensorFlow Text. These packages offer various functionalities and tools for developers to build and train machine learning models.\n\nMoving on, the table also includes the latest versions of PyTorch, another popular machine learning library, and its related packages such as TorchText, TorchVision, PyTorch Lightning, and PyTorch Lightning Bolts. These packages provide developers with a range of tools and utilities to simplify the process of building and training machine learning models.\n\nOther notable entries in the table include ONNX, a popular open-source format for representing deep learning models, and Keras, a high-level neural networks API. The table also includes Magma, a software library for linear algebra computations, and XGBoost, a popular gradient boosting library.\n\nIn addition to machine learning and deep learning packages, the table also includes software packages related to natural language processing (NLP) and computer vision. These include Transformers, Tokenizers, SentencePiece, Spacy, Thinc, OpenCV, and DALI. These packages offer various tools and utilities for processing and analyzing text and images.\n\nLastly, the table includes Horovod, a distributed training framework for deep learning models. This package enables developers to train their models on multiple GPUs or machines simultaneously, making the training process faster and more efficient.\n\nOverall, this table provides a comprehensive overview of the latest versions of various software packages that are currently making waves in the world of technology, particularly in the fields of machine learning, deep learning, NLP, and computer vision.\n\n| Package | Version |\n|---------|---------|\n| Tensorflow | 2.7.0 |\n| TensorFlow Estimators | 2.7.0 |\n| TensorFlow Probability | 0.15.0 |\n| TensorBoard | 2.7.0 |\n| TensorFlow Text | 2.7.0 |\n| TensorFlow Model Optimizations | 0.7.0 |\n| TensorFlow Addons | 0.15.0 |\n| TensorFlow Datasets | 4.4.0 |\n| TensorFlow Hub | 0.12.0 |\n| TensorFlow MetaData | 1.0.0 |\n| PyTorch | 1.10.0 |\n| TorchText | 0.11.0 |\n| TorchVision | 0.11.1 |\n| PyTorch Lightning | 1.5.4 |\n| PyTorch Lightning Bolts | 0.4.0 |\n| ONNX | 1.10.2 |\n| Keras | 2.7.0 |\n| Magma | 2.5.4 |\n| XGBoost | 1.5.1 |\n| Transformers | 4.11.3 |\n| Tokenizers | 0.10.3 |\n| SentencePiece | 0.1.96 |\n| Spacy | 3.2.0 |\n| Thinc | 8.0.13 |\n| OpenCV | 4.5.3 |\n| DALI | 1.9.0 |\n| Horovod | 0.23.0 |\n\n\n\nPlease note that Tensorflow Serving is currently unavailable. We are working with IBM to\nresolve the issue and will publish and update once available.\n\n\n\n<p style=\"font-size:20px\"><b>Andes: OS Upgrade (November 30, 2021)</b></p>\n\nOn November 30, 2021, the Andes cluster will be upgraded to a newer (minor) version of the operating system. The table below summarizes the main changes. While recompiling is not required, it is recommended.\n\n\nThe table presents a comparison of the old and new versions of various software components. The first component, Red Hat Enterprise Linux, has been updated from version 8.3 to 8.4. The second component, Mellanox InfiniBand Driver, has also been updated from version 5.3-1.0.0.1 to 5.4-1.0.3.0. The third component, NVIDIA driver, has undergone a significant update from version 450.36.06 to 460.106.00-1. This update may include bug fixes, performance improvements, and new features. The final component, Slurm, has been updated from version 20.02.6 to 20.02.7-1. This update may also include bug fixes and performance improvements for this popular job scheduler. These updates highlight the continuous development and improvement of software in the technology industry, ensuring that users have access to the latest and most efficient versions of these components. \n\n| Component              | Old Version | New Version     |\n| ---------------------- | ----------- | --------------- |\n| Red Hat Enterprise Linux | 8.3         | 8.4             |\n| Mellanox InfiniBand Driver | 5.3-1.0.0.1 | 5.4-1.0.3.0     |\n| NVIDIA driver          | 450.36.06   | 460.106.00-1    |\n| Slurm                  | 20.02.6     | 20.02.7-1       |\n\n\n\n\n\n<p style=\"font-size:20px\"><b>Summit: OpenCE 1.4.0 (October 13, 2021)</b></p>\n\nOpenCE 1.4.0 is now available on Summit. OpenCE 1.4.0 is available for python versions 3.7, 3.8, and 3.9. These builds can be accessed by\nloading the open-ce/1.4.0-py37-0, open-ce/1.4.0-py38-0, and open-ce/1.4.0-py39-0 modules, respectively.\n\nThe following packages are available in this release of OpenCE:\n\n\nThe table above presents a comprehensive list of the latest versions of various software packages related to machine learning and artificial intelligence. The first column lists the name of the package, while the second column displays the corresponding version number. The first few entries in the table are all related to TensorFlow, a popular open-source software library for dataflow and differentiable programming across a range of tasks. These include TensorFlow 2.6.0, TensorFlow Estimators 2.6.0, TensorFlow Probability 0.14.0, and TensorFlow Text 2.6.0. The next entry, TensorFlow Model Optimizations 0.6.0, is a package specifically designed for optimizing TensorFlow models. The table also includes other popular machine learning libraries such as PyTorch 1.9.0, TorchText 0.10.0, TorchVision 0.10.0, and PyTorch Lightning 1.4.4. These packages are widely used for deep learning and natural language processing tasks. Other notable entries in the table include ONNX 1.7.0, a popular open format for representing deep learning models, and XGBoost 1.4.2, a popular gradient boosting library. The table also includes various text processing packages such as Transformers 4.9.2, Tokenizers 0.10.3, SentencePiece 0.1.91, Spacy 3.1.2, and Thinc 8.0.8. These packages are commonly used for tasks such as text classification and language translation. Additionally, the table includes OpenCV 3.4.14, a popular computer vision library, and Horovod 0.22.1, a distributed training framework for deep learning models. Overall, this table provides a comprehensive overview of the latest versions of various software packages used in the field of machine learning and artificial intelligence.\n\n| Package | Version |\n|---------|---------|\n| Tensorflow | 2.6.0 |\n| TensorFlow Estimators | 2.6.0 |\n| TensorFlow Probability | 0.14.0 |\n| TensorBoard | 2.6.0 |\n| TensorFlow Text | 2.6.0 |\n| TensorFlow Model Optimizations | 0.6.0 |\n| TensorFlow Addons | 0.14.0 |\n| TensorFlow Datasets | 4.4.0 |\n| TensorFlow Hub | 0.12.0 |\n| TensorFlow MetaData | 1.0.0 |\n| PyTorch | 1.9.0 |\n| TorchText | 0.10.0 |\n| TorchVision | 0.10.0 |\n| PyTorch Lightning | 1.4.4 |\n| PyTorch Lightning Bolts | 0.3.4 |\n| ONNX | 1.7.0 |\n| Keras | 2.6.0 |\n| Magma | 2.5.4 |\n| XGBoost | 1.4.2 |\n| Transformers | 4.9.2 |\n| Tokenizers | 0.10.3 |\n| SentencePiece | 0.1.91 |\n| Spacy | 3.1.2 |\n| Thinc | 8.0.8 |\n| OpenCV | 3.4.14 |\n| Horovod | 0.22.1 |\n\n\n\nPlease note that DALI and Tensorflow Serving are currently unavailable on ppc64le. We are working with IBM to\nresolve the issue and will publish and update once available.\n\n\n\n<p style=\"font-size:20px\"><b>Summit: Software Installation/Default Software Changes (August 17-19, 2021)</b></p>\n\nSummit's operating system will be upgraded to Red Hat Enterprise Linux 8 (RHEL 8) on August 17-19, 2021.\n\nCodes should be rebuilt prior to running following the upgrade due to the OS and software changes.\n\nAs a result of the upgrade, the following new packages will become available:\n\n\nThe table above presents the latest updates and changes in various software packages. The first package, CUDA Toolkit, has been updated from version 10.1.243 to 11.0.3. This update brings new features and improvements to the toolkit, making it more efficient and user-friendly. The second package, IBM Spectrum MPI, has also been updated from version 10.3.1.2-20200121 to 10.4.0.3-20210112. This update includes bug fixes and performance enhancements, ensuring a smoother and more reliable experience for users. The third package, IBM XL, has been updated from version 16.1.1-5 to 16.1.1-10. This update includes new features and optimizations, making the compiler more powerful and efficient. Lastly, the IBM ESSL package has been updated from version 6.1.0-2 to 6.3.0. This update brings new functions and improvements to the library, making it easier for users to perform complex calculations and simulations. Overall, these updates showcase the continuous efforts of these software companies to provide the best and most up-to-date tools for their users.\n\n| Package         | Current Default   | New Default       |\n|-----------------|--------------------|-------------------|\n| CUDA Toolkit    | 10.1.243           | 11.0.3            |\n| IBM Spectrum MPI| 10.3.1.2-20200121  | 10.4.0.3-20210112 |\n| IBM XL          | 16.1.1-5           | 16.1.1-10         |\n| IBM ESSL        | 6.1.0-2            | 6.3.0             |\n\n\n\n\nThe OS-provided Python will no longer be accessible as python (including variations like /usr/bin/python or /usr/bin/env python); rather, you must specify it as python2 or python3. If you are using python from one of the modulefiles rather than the version in /usr/bin, this change should not affect how you invoke python in your scripts, although we encourage specifying python2 or python3 as a best practice.\n\nIn addition, the following packages will be upgraded to newer versions and the specific versions listed below will be removed from the system. If you need any of the specific versions scheduled to be removed, please contact help@olcf.ornl.gov.\n\n\nThe table presents the software updates on Summit.\n\nPackage|Versions Removed|Versions Available\nadios|1.11.1, 1.13.1|None\nadios2|2.2.0, 2.4.0, 2.5.0|2.6.0\namgx|2.0.0.130.0, 2.0.0.130.1, 2.0.0.130.2|2.1.0-1\napr|1.6.2|1.7.0\napr-util|1.6.0|1.6.1\nautomake|1.16.1|1.16.2\nbinutils|2.31.1|2.33.1\nbison|3.0.5|3.6.4\nboost|1.59.0, 1.61.0, 1.66.0, 1.70.0|1.62.0, 1.72.0, 1.74.0\nbzip2|1.0.6|1.0.8\nc-blosc|1.12.1|1.17.0\ncairo|1.14.12|1.16.0\nccache|3.7.9|3.7.11\ncmake|3.11.3, 3.12.2, 3.13.4, 3.14.2, 3.15.2, 3.17.3, 3.18.1, 3.18.2, 3.6.1|3.18.4\ncuda|9.1.85, 9.2.148, 10.1.105, 10.1.168, 10.1.243, 11.0.1, 11.0.2, 11.1.0|10.2.89, 11.0.3, 11.1.1\ncurl|7.60.0, 7.63.0|7.72.0\ndarshan-runtime|3.1.5-pre1, 3.1.6, 3.1.7|3.2.1\ndarshan-util|3.1.4, 3.1.5-pre1, 3.1.6, 3.1.7|3.2.1\nemacs|25.1|27.1\nessl|6.2.0-20190419|6.1.0-2, 6.2.1, 6.3.0\nexpat|2.2.5|2.2.10\nflex|2.6.3|2.6.4\nfont-util|1.3.1|1.3.2\nfontconfig|2.12.3|2.13.92\nfreetype|2.7.1, 2.9.1|2.10.1\ngcc|4.8.5, 5.4.0, 6.4.0, 7.4.0, 8.1.0, 8.1.1, 9.1.0, 9.2.0, 10.1.0|8.3.1 (OS), 9.3.0, 10.2.0, 11.1.0\ngdb|8.0, 8.2|9.2\ngdbm|1.14.1|1.18.1\ngdrcopy|2.0|2.1\ngettext|0.19.8.1|0.21\ngit|2.13.0, 2.20.1, 2.9.3|2.29.0\ngit-lfs|2.8.0|None\nglib|2.56.2, 2.56.3|2.66.2\ngnupg|2.2.3|2.2.19\ngo|1.11.5|1.15.2\ngo-bootstrap|1.7.1-bootstrap|None\ngobject-introspection|1.49.2|1.56.1\ngperf|3.0.4|3.1\ngromacs|2020, 2020.2|2020.4\nharfbuzz|1.4.6, 2.1.3|2.6.8\nhdf5|1.10.3, 1.10.4, 1.8.18|1.10.7\nhelp2man|1.47.4|1.47.11\nhpx|1.3.0, 1.4.1|1.5.1\nhtop|2.0.2|3.0.2\nhwloc|2.0.2|1.11.11, 2.2.0\nhypre|2.11.1, 2.13.0, 2.15.1, develop|2.20.0\nicu4c|58.2, 60.1|67.1\njulia|1.4.2|1.5.2\nkokkos|3.0.00|3.2.00\nkokkos-nvcc-wrapper|20200221|3.2.00\nlibassuan|2.4.5|2.5.3\nlibbsd|0.8.6, 0.9.1|0.10.0\nlibevent|2.0.21|2.1.8\nlibfabric|1.7.0|1.11.0\nlibffi|3.2.1|3.3\nlibgcrypt|1.8.1|1.8.5\nlibgpg-error|1.27|1.37\nlibiconv|1.15|1.16\nlibjpeg-turbo|1.5.90|2.0.4\nlibksba|1.3.5|1.4.0\nlibpciaccess|0.13.5|0.16\nlibpng|1.6.34|1.6.37\nlibsigsegv|2.11|2.12\nlibsodium|1.0.15|1.0.18\nlibtiff|4.0.9|4.1.0\nlibunwind|1.2.1|1.4.0\nlibx11|1.6.5|1.6.7\nlibxext|1.3.3|None\nlibxml2|2.9.8|2.9.10\nlibxrender|0.9.10|None\nlibzmq (renamed from zeromq)|4.2.5|4.3.2\nlog4c|1.2.4|None\nlz4|1.8.1.2|1.9.2\nmagma|2.1.0, 2.2.0, 2.3.0, 2.4.0, 2.5.1, 2.5.4|2.5.3\nmercurial|3.9.1, 4.4.1|5.3\nmpip|3.4.1, 3.4.1-1|3.5\nmumps|5.0.1|5.3.3\nnano|2.6.3|4.9\nnasm|2.13.03|2.15.05\nnco|4.6.9, 4.8.1, 4.9.1|4.9.3\nncurses|6.1|6.2\nnetcdf-c (renamed from netcdf)|4.6.1, 4.6.2|4.7.4\nnetcdf-cxx (renamed to netcdf-cxx4)|4.2|4.3.1\nnetcdf-fortran|4.4.4|4.4.5\nnetlib-scalapack|2.0.2|2.1.0\nnpth|1.5|1.6\nnumactl|2.0.11|2.0.14\nopenblas|0.3.5, 0.3.6, 0.3.9|0.3.12\nopen-ce|1.1.3|1.2.0\nopenmpi|4.0.3|4.0.5\npapi|5.5.1, 5.6.0, 5.7.0|6.0.0.1\nparallel-netcdf|1.8.0, 1.8.1|1.12.1\npatchelf|0.9|0.10\npcre|8.42|8.44\nperl|5.26.2|5.30.1\npetsc|3.10.1, 3.10.3, 3.6.3, 3.6.4, 3.7.2|3.14.1\npgi|17.10, 17.9, 18.1, 18.10, 18.3, 18.4, 18.5, 18.7, 19.1, 19.10, 19.4, 19.5, 19.7, 19.9, 19.10|20.1, 20.4\npixman|0.34.0, 0.38.0|0.40.0\npkgconf (renamed from pkg-config)|1.4.2, 1.5.4|1.7.3\npy-certifi|2017.1.23|2020.6.20\npy-cython|0.28.3, 0.29|0.29.21\npy-docutils|0.13.1|0.15.2\npy-h5py|2.8.0|None\npy-mpi4py|3.0.0|3.0.3\npy-nose|1.3.7|None\npy-numpy|1.15.1|1.19.4\npy-pip|10.0.1|None\npy-pkgconfig|1.2.2|None\npy-pygments|2.2.0|2.6.1\npy-setuptools|40.2.0, 40.4.3|50.3.2\npy-six|1.11.0|None\npy-virtualenv|16.0.0|None\npython|2.7.15-anaconda2-5.3.0, 3.6.6-anaconda3-5.3.0, 3.7.0-anaconda3-5.3.0, 2.7.12, 3.5.2, 3.7.0|2.7.15, 3.7.7, 3.8.6\nr|3.5.2|4.0.5\nraja|0.1.0|0.12.1\nrdma-core|20|32.0\nreadline|6.3, 7.0|8.0\nrenderproto|0.11.1|None\nscons|3.0.1|3.1.2\nscreen|4.3.1|4.8.0\nsnappy|1.1.7|1.1.8\nspectral|20181227, 20190401, 20200714, 20200903|20210514\nspectrum-mpi|10.2.0.10-20181214, 10.2.0.11-20190201, 10.2.0.7-20180830, 10.3.0.0-20190419, 10.3.0.1-20190611, 10.3.1.2-20200121|10.4.0.3-20210112\nsqlite|3.23.1, 3.26.0|3.33.0\nsubversion|1.9.3|1.14.0\nsuperlu-dist|4.3, 5.1.3, 5.4.0|6.4.0\nsz|1.4.10.0, 1.4.12.3|2.0.2.0, 2.1.11\ntar|1.30, 1.31|1.32\ntcl|8.6.8|None\ntk|8.6.8|None\ntmux|2.2|3.1b\nucx|1.7.0|None\nudunits (renamed from udunits2)|2.2.24|None\nvalgrind|3.11.0, 3.14.0|3.15.0\nvim|7.4.2367, 8.1.0338|8.2.1201\nxl|16.1.1-4, 16.1.1-5, 16.1.1-6, 16.1.1-7, 16.1.1-9|16.1.1-8, 16.1.1-10\nxz|5.2.4|5.2.5\nzfp|0.5.0, 0.5.2|0.5.5\nzstd|1.3.0|1.4.5\n\n\n\n\n\n<p style=\"font-size:20px\"><b>Summit: Software Installation/Default Software Changes (April 7, 2021)</b></p>\n\nThe following modules were installed as default on April 7, 2021.\n\n\nThe table presents information about software news, specifically regarding the package \"open-ce\". The first row serves as the header, with the columns labeled \"Package\", \"Current Default\", and \"New Default\". The package \"open-ce\" currently has a default version of 0.1-0, but the table shows that a new default version of 1.1.3-py38-0 will be implemented. This update may include bug fixes, new features, or other improvements to the software. Users of the \"open-ce\" package can refer to this table to stay informed about the latest version and its changes. This information is important for users to ensure they are using the most up-to-date and efficient version of the software. \n\n| Package | Current Default | New Default |\n|---------|------------------|--------------|\n| open-ce | 0.1-0            | 1.1.3-py38-0 |\n\n\n\nIn addition, open-ce 1.1.3 is also available for python versions 3.6 and 3.7. These builds can be accessed by\nloading the open-ce/1.1.3-py36-0 and open-ce/1.1.3-py37-0 modules, respectively.\n\nThe following packages are available in this release of open-ce.\n\n\nThe table presents a list of software packages and their corresponding versions that are currently making headlines in the tech industry. The first row serves as the header, with the first column indicating the name of the package and the second column indicating its version. The packages listed include popular machine learning frameworks such as TensorFlow, PyTorch, and XGBoost, as well as libraries for natural language processing such as Transformers, Tokenizers, and SentencePiece. Other notable packages include Spacy, a library for advanced natural language processing, and OpenCV, a library for computer vision. The latest versions of these packages are also listed, with TensorFlow 2.4.1 being the most recent version for multiple packages. This table provides a comprehensive overview of the current versions of popular software packages, making it a valuable resource for developers and tech enthusiasts alike.\n\n| Package | Version |\n|---------|---------|\n| Tensorflow | 2.4.1 |\n| TensorFlow Serving | 2.4.1 |\n| TensorFlow Estimators | 2.4.0 |\n| TensorFlow Probability | 0.12.1 |\n| TensorBoard | 2.4.1 |\n| TensorFlow Text | 2.4.1 |\n| TensorFlow Model Optimizations | 0.5.0 |\n| TensorFlow Addons | 0.11.2 |\n| TensorFlow Datasets | 4.1.0 |\n| TensorFlow Hub | 0.10.0 |\n| TensorFlow MetaData | 0.26.0 |\n| PyTorch | 1.7.1 |\n| TorchText | 0.8.1 |\n| TorchVision | 0.8.2 |\n| PyTorch Lightning | 1.1.0 |\n| PyTorch Lightning Bolts | 0.2.5 |\n| XGBoost | 1.3.3 |\n| Transformers | 3.5.1 |\n| Tokenizers | 0.9.3 |\n| SentencePiece | 0.1.91 |\n| Spacy | 2.3.4 |\n| Thinc | 7.4.1 |\n| DALI | 0.28.0 |\n| OpenCV | 3.4.10 |\n| Horovod | 0.21.0 |\n\n\n\n\n\n<p style=\"font-size:20px\"><b>Summit: Software Installation/Default Software Changes (April 8, 2020)</b></p>\n\nThe following modules were installed as default on April 8, 2020.\n\n\nThe table presents information on the current and new default versions of the IBM Watson Machine Learning Community Edition (ibm-wml-ce) package. The current default version is 1.7.0-1, while the new default version is 1.7.0-2. This package is used for machine learning tasks and is part of the IBM Watson Studio platform. The new default version may include updates, bug fixes, or new features that improve the performance and functionality of the package. Users of this package can refer to this table to ensure they are using the most up-to-date version and take advantage of any improvements that have been made. This information is important for developers and data scientists who rely on the ibm-wml-ce package for their machine learning projects. \n\n| Package      | Current Default | New Default |\n|--------------|-----------------|-------------|\n| ibm-wml-ce   | 1.7.0-1         | 1.7.0-2     |\n\n\n\nThe new IBM Watson Machine Learning (WML) Community Edition (CE) install adds\nimprovements to DDL including support for jsrun.\n\n\n\n<p style=\"font-size:20px\"><b>Summit: Software Installation/Default Software Changes (March 10, 2020)</b></p>\n\nThe following modules will be installed as default on March 10, 2020. The new\nstack requires the latest version of Spectrum MPI and as a result, previous\nversions have been deprecated.\n\n\nThe table presents a comparison of current and new default versions for various software packages. The first row serves as the header, with the columns labeled as \"Package\", \"Current Default\", and \"New Default\". The first package listed is \"cuda\", with the current default version being 10.1.168 and the new default version being 10.1.243. The next package is \"spectrum-mpi\", with the current default version being 10.3.0.1-20190611 and the new default version being 10.3.1.2-20200121. The third package is \"hdf5\", with the current default version being 1.10.3 and the new default version being 1.10.4. The fourth package is \"pgi\", with the current default version being 19.4 and the new default version being 19.9. The fifth package is \"xl\", with the current default version being 16.1.1-3 and the new default version being 16.1.1-5. The final package is \"ibm-wml-ce\", with the current default version being 1.6.2-3 and the new default version being 1.7.0-1. This table provides important information for users of these software packages, allowing them to easily see the changes in default versions and make any necessary updates or adjustments. \n\n| Package       | Current Default | New Default |\n|---------------|-----------------|-------------|\n| cuda          | 10.1.168        | 10.1.243    |\n| spectrum-mpi  | 10.3.0.1-20190611 | 10.3.1.2-20200121 |\n| hdf5          | 1.10.3          | 1.10.4      |\n| pgi           | 19.4            | 19.9        |\n| xl            | 16.1.1-3        | 16.1.1-5    |\n| ibm-wml-ce    | 1.6.2-3         | 1.7.0-1     |\n\n\n\nIn addition, the following new packages have been installed and are available for use:\n\n\nThe table presents the latest updates and versions of various software packages. The first row serves as the header, with the columns labeled as \"Package\" and \"New Version\". The first package listed is \"pgi\" with a new version of \"20.1\". This indicates that the PGI software has been updated to version 20.1. The next package is \"xl\" with a new version of \"16.1.1-6\". This suggests that the XL software has been updated to version 16.1.1-6. The final package listed is \"kokkos\" with a new version of \"3.0.0\". This indicates that the Kokkos software has been updated to version 3.0.0. These updates and versions are important for users to stay informed about the latest developments and improvements in these software packages. It also allows users to make informed decisions about which version to use for their specific needs. Overall, this table provides a concise and organized overview of the latest software news. \n\n| Package | New Version |\n|---------|-------------|\n| pgi     | 20.1        |\n| xl      | 16.1.1-6    |\n| kokkos  | 3.0.0       |\n\n\n\nFinally, the FFTW installations on Summit for the XL compiler have been rebuilt\nusing -O2 to address an issue observed when running the FFTW suite using\nthe default optimization options. All builds of the fftw package that use\nthe XL compiler have been rebuilt.\n\nIf you encounter any issues, please contact help@olcf.ornl.gov.\n\n\n\n<p style=\"font-size:20px\"><b>Rhea: OpenMPI Upgrade (February 18, 2020)</b></p>\n\nOn February 18, 2020, Rhea’s default OpenMPI will be updated to version 3.1.4.\nDue to underlying library changes that will be made on the same day, following\nthe change, all codes should be rebuilt against the updated version.\n\n\nThe table presents information about the current and new default versions of the OpenMPI package. OpenMPI is a popular open-source software library used for parallel computing. The current default version is 3.1.3, which was released in September 2018. However, a new default version, 3.1.4, has been released and is now the recommended version for users. This new version includes bug fixes and performance improvements, making it more efficient and reliable for parallel computing tasks. Users are encouraged to upgrade to the new default version for an enhanced experience. \n\n| Package | Current Default | New Default |\n|---------|------------------|-------------|\n| OpenMPI | 3.1.3            | 3.1.4       |\n\n\n\n\n\n<p style=\"font-size:20px\"><b>All Systems: Python2 End of Life (January 01, 2020)</b></p>\n\nOn January 1, 2020, Python 2 will reach its end of life and will no longer be\nsupported by the project’s core developers. On this date, the OLCF will also\nend its support for Python 2. Users reliant on Python 2 should port code to\nPython 3 for forward compatibility with OLCF systems and many open source\npackages. Python 2 modules will not be removed on January 1, but will no longer\nreceive maintenance or regular updates.\n\nWhile default Python modules on OLCF systems are already set to Python 3, we\nrecommend all users follow PEP394 by explicitly invoking either ‘python2’ or\n‘python3’ instead of simply ‘python’. Python 2 Conda Environments and user\ninstallations of Python 2 will remain as options for using Python 2 on OLCF\nsystems.\n\nOfficial documentation for porting from Python 2 to Python3 can be found at:\nhttps://docs.python.org/3/howto/pyporting.html\n\nGeneral information and a list of open source packages dropping support for\nPython 2 can be found at: https://python3statement.org/\n\n\n\n<p style=\"font-size:20px\"><b>Summit: Software Upgrade (July 16, 2019)</b></p>\n\nThe following modules will be installed and will become the default on July 16,\n2019. The new stack requires Spectrum MPI 10.3 PTF 1 and as a result previous\nversions of Spectrum MPI have been deprecated.\n\n\nThe table presents information related to software news, specifically the default versions of two packages - cuda and spectrum-mpi. The first row serves as the header, with the column titles being \"Package\" and \"Default\". The first package listed is cuda, with the default version being 10.1.168. This indicates that when using this package, the default version that will be used is 10.1.168. The second package listed is spectrum-mpi, with the default version being 10.3.0.1-20190716. This means that when using this package, the default version that will be used is 10.3.0.1-20190716, which was released on July 16, 2019. This table provides a quick and easy reference for users to know the default versions of these packages, allowing them to make informed decisions when using these software. \n\n| Package      | Default       |\n|--------------|---------------|\n| cuda         | 10.1.168      |\n| spectrum-mpi | 10.3.0.1-20190716 |\n\n\n\nDetails about the software stack upgrade can be found in the IBM Service Pack 3.1 site and the Spectrum MPI 10.3.0.1 release notes.\n\n\n\n<p style=\"font-size:20px\"><b>Summit: Software Installation/Default Software Changes (May 21, 2019)</b></p>\n\nThe following modules will be installed as default on May 21, 2019. The new\nstack requires Spectrum MPI 10.3 and as a result previous versions of Spectrum\nMPI have been deprecated.\n\n\nThe table presents a list of software packages and their corresponding default versions. The first row serves as the header, with the columns labeled as \"Package\" and \"Default\". The first package listed is \"xl\", with a default version of 16.1.1.3. This is followed by \"cuda\" with a default version of 10.1.105. The third package is \"essl\" with a default version of 6.2.0-20190419. The last package listed is \"spectrum-mpi\" with a default version of 10.3.0.0-20190419. This table provides important information for users who may need to know the default versions of these software packages for compatibility or troubleshooting purposes. It also serves as a reference for any updates or changes made to the default versions in the future. \n\n| Package       | Default           |\n| ------------- |:-------------:|\n| xl      | 16.1.1.3 |\n| cuda      | 10.1.105      |\n| essl | 6.2.0-20190419      |\n| spectrum-mpi | 10.3.0.0-20190419      |\n\n\n\n\n\n<p style=\"font-size:20px\"><b>Rhea: Default Software Changes (March 12, 2019)</b></p>\n\nThe following modules will become the default on March 12, 2019.\n\n\nThe table presents a list of software packages and their corresponding default versions. The first row serves as the header, with the columns labeled as \"Package\" and \"Default\". The first package listed is \"intel\" with a default version of 19.0.0. This is followed by \"pgi\" with a default version of 18.10, \"gcc\" with a default version of 6.2.0, \"cuda\" with a default version of 10.0.3, \"openmpi\" with a default version of 3.1.3, \"anaconda\" with a default version of 5.3.0, \"adios\" with a default version of 1.11.1, \"atlas\" with a default version of 3.10.2, \"boost\" with a default version of 1.67.0, \"fftw\" with a default version of 3.3.8, \"hdf5\" with a default version of 1.10.3, \"nco\" with a default version of 4.6.9, \"netcdf\" with a default version of 4.6.1, \"netcdf-fortran\" with a default version of 4.4.4, \"netcdf-cxx\" with a default version of 4.3.0, and finally \"parallel-netcdf\" with a default version of 1.8.0. This table provides a comprehensive overview of the default versions of various software packages, which can be useful for users looking to install or update these packages. \n\n| Package | Default |\n|---------|---------|\n| intel | 19.0.0 |\n| pgi | 18.10 |\n| gcc | 6.2.0 |\n| cuda | 10.0.3 |\n| openmpi | 3.1.3 |\n| anaconda | 5.3.0 |\n| adios | 1.11.1 |\n| atlas | 3.10.2 |\n| boost | 1.67.0 |\n| fftw | 3.3.8 |\n| hdf5 | 1.10.3 |\n| nco | 4.6.9 |\n| netcdf | 4.6.1 |\n| netcdf-fortran | 4.4.4 |\n| netcdf-cxx | 4.3.0 |\n| parallel-netcdf | 1.8.0 |\n\n\n\n\n\n<p style=\"font-size:20px\"><b>Summit: Default Software Changes (March 12, 2019)</b></p>\n\nThe following modules will become the default on March 12, 2019.\n\n\nThe table presents information on the current and new default versions of various software packages. The first row serves as the header, with the three columns labeled as \"Package,\" \"Current Default,\" and \"New Default.\" The first package listed is \"spectrum-mpi,\" with the current default version being \"unset\" and the new default version being \"10.2.0.11-20190201.\" The next package is \"xl,\" with the current default version being \"16.1.1-1\" and the new default version being \"16.1.1-2.\" The final package listed is \"pgi,\" with the current default version also being \"unset\" and the new default version being \"18.10.\" This table provides important information for users of these software packages, as it allows them to easily see any changes in default versions and plan accordingly. \n\n| Package      | Current Default | New Default    |\n|--------------|-----------------|----------------|\n| spectrum-mpi | unset           | 10.2.0.11-20190201 |\n| xl           | 16.1.1-1        | 16.1.1-2       |\n| pgi          | unset           | 18.10          |\n\n\n\nIn addition, the following default Spectrum MPI settings will be changed to\naddress issues resolved with the February 19, 2019 software upgrade:\n\n\nThe table presents information related to software news, specifically regarding environment variables. The first row serves as the header, with three columns labeled \"Environment Variable,\" \"Current Default,\" and \"New Default.\" The first row is followed by three rows of data, each representing a different environment variable. The first variable, OMP_MCA_io, has a current default value of romio314 and a new default value of romio321. The second variable, OMPI_MCA_coll_ibm_xml_disable_cache, has a current default value of 1 and a new default value of unset. The third variable, PAMI_PMIX_USE_OLD_MAPCACHE, has a current default value of 1 and a new default value of unset. These changes in default values may have an impact on the performance or functionality of the software, and users should take note of these updates. \n\n| Environment Variable | Current Default | New Default |\n|----------------------|-----------------|-------------|\n| OMP_MCA_io           | romio314        | romio321    |\n| OMPI_MCA_coll_ibm_xml_disable_cache | 1 | unset |\n| PAMI_PMIX_USE_OLD_MAPCACHE | 1 | unset |"}
{"doc":"spack_environments","text":"Spack Environments\n\nPurpose\n\nThis guide meant as an example for a user to setup a Spack environment for application development using the OLCF\nprovided files as a template.\n\nThe OLCF uses an internal mirror of the Spack repo that is customized for use on OLCF systems.  This results in\nthe hash values generated by another version of Spack to not match.  It is recommended to use the existing module\nas external packages instead of chaining at this time.\n\nThe provided spack.yaml files are templates for a user to use as an example.\n\nThis not intended as a guide for a new Spack user.  Please see the Spack 101 tutorial\nif you need assistance starting out with Spack.\n\nThe provided Spack environment files are intended to assist OLCF users in setup their development environment at the\nOLCF.  The base environment file includes the compilers and packages that are installed at the system level.\n\nTraditionally, the user environment is modified by module files.  For example, a user would add use  module load cmake/3.18.2 to\nload CMake version 3.18.2 into their environment.  Using a Spack environment, a user can add an OLCF provided package\nand build against it using Spack without having to load the module file separately.\n\nThe information presented here is a subset of what can be found at the Spack documentation site.\n\nDefinitions\n\nSpack environment - A set of Spack specs for the purpose of building, rebuilding and deploying in a coherent fashion.\n\nExternal Packages - An externally-installed package used by Spack, rather than building its own package.\n\nGetting Started\n\nClone the OLCF User Environment repo and the Spack repo, start a new Spack instance, and create and activate a new Spack environment:\n\n## Using Summit as the example system\n\n$ git clone https://github.com/olcf/spack-environments.git\n$ cd spack-environments\n\n$ git clone https://github.com/spack/spack.git\n$ source spack/share/spack/setup-env.sh\n\n## Make changes to the template environment module before continuing!!\n\n$ spack env create my_env linux-rhel8-ppc64le/summit/spack.yaml\n$ spack env activate my_env\n\nThe template file contains usable, but not advisable, settings for configuration items.  Options marked with FIXME\nare specifically recommended to be changed, like the installation root directory.  Items marked OPTIONAL indicate\npoints that are not required, but are useful as noted.\n\nNow a user can add and install their dependencies with Spack and proceed with developing their application.\n\nAdd Dependencies to the environment\n\nAdding OLCF Modulefiles as External Packages\n\nIf an OLCF installed package is available, these can be added via the template spack.yaml file by adding to the packages\nsection.  For this example, CMake was found on Summit by finding a modulefile for the installed CMake package.\n\nBy marking the CMake package as buildable: false it will force Spack to use the externally installed CMake with the\nlisted modulefile.  If this is not indicated, Spack may build its own version of the package.\n\npackages:\n  # This example is included in the template file\n  cmake:\n    version: [3.23.2]\n    buildable: false\n    externals:\n    - spec: cmake@3.23.2\n      modules:\n      - cmake/3.23.2\n\nAs a reminder, to find modules:\n\n## Using cmake as an example again.\n\n$ module -t av cmake\n/sw/summit/spack-envs/base/modules/spack/linux-rhel8-ppc64le/Core:\ncmake/3.18.4\ncmake/3.20.2\ncmake/3.21.3\ncmake/3.22.2\ncmake/3.23.1\ncmake/3.23.2\n\nAdding User-Defined Dependencies to the environment\n\nA dependency that is not already installed will be built via Spack once the environment is concretized and installed.\nThese can be added to the spack.yaml by adding to the specs section.\n\nspecs:\n- cmake@3.18.2                            ## example from above\n- my_apps_dependency1@version%compiler    ## other explicitly defined specs\n- my_apps_dependency2@version%compiler\n\nInstalling the Environment\n\nWhen in the Spack environment, any packages that are added to the environment file can be installed via:\n\n$ spack concretize -f  ## The -f flag here forces a reconcretization of the entire environment\n$ spack install\n\nAlternatively, a user may install a package and its dependencies manually by:\n\n$ spack install <my_app_dependencies@version%compiler>\n\n## This may or may not add the spec to the spack.yaml depending on the Spack version being used.\n\nMore Details\n\nFor more information regarding Spack and its usage, please see the Spack documentation.\n\n<string>:3: (INFO/1) Duplicate explicit target name: \"the spack 101 tutorial\".\n\nFor an extensive tutorial concerning Spack, go to the Spack 101 tutorial.\n\nFor more information concerning external packages, please see here.\n\nReferences\n\nSpack - package management tool\n\nSpack 101 tutorial - Spack tutorial"}
{"doc":"spock_quick_start_guide","text":"Spock Quick-Start Guide\n\n\n\nThe Spock Early Access System was decommissioned on March 15, 2023. The\nfile systems that were available on Spock are still accessible from the Home\nserver and the Data Transfer Nodes (DTN), so all your data will remain accessible.\nIf you do not have access to other OLCF systems, your project will move to data-only\nfor 30-days. If you have any questions, please contact help@olcf.ornl.gov.\n\n\n\nSystem Overview\n\nSpock is an NCCS moderate-security system that contains similar hardware and\nsoftware as the upcoming Frontier system. It is used as an early-access testbed\nfor Center for Accelerated Application Readiness (CAAR) and Exascale Computing\nProject (ECP) teams as well as NCCS staff and our vendor partners. The system\nhas 3 cabinets, each containing 12 compute nodes, for a total of 36 compute\nnodes.\n\n\n\nSpock Compute Nodes\n\nEach Spock compute node consists of [1x] 64-core AMD EPYC 7662 \"Rome\" CPU (with\n2 hardware threads per physical core) with access to 256 GB of DDR4 memory and\nconnected to [4x] AMD MI100 GPUs. The CPU is connected to all GPUs via PCIe\nGen4, allowing peak host-to-device (H2D) and device-to-host (D2H) data\ntransfers of 32+32 GB/s. The GPUs are connected in an all-to-all arrangement\nvia Infinity Fabric (xGMI), allowing for a peak device-to-device bandwidth of\n46+46 GB/s. Each compute node also has [2x] 3.2 TB NVMe devices (SSDs) with\nsequential read and write speeds of 6900 MB/s and 4200 MB/s, respectively.\n\nThe X+X GB/s values for bandwidths above represent bi-directional bandwidths. So, for example, the Infinity Fabric connecting any two GPUs allows peak data transfers of 46 GB/s in both directions simultaneously.\n\nSpock node architecture diagram\n\nThere are 4 NUMA domains per node, that are defined as follows:\n\nNUMA 0: hardware threads 000-015, 064-079 | GPU 0\n\nNUMA 1: hardware threads 016-031, 080-095 | GPU 1\n\nNUMA 2: hardware threads 032-047, 096-111 | GPU 2\n\nNUMA 3: hardware threads 048-063, 112-127 | GPU 3\n\nSystem Interconnect\n\nThe Spock nodes are connected with Slingshot-10 providing a node injection\nbandwidth of 12.5 GB/s.\n\nFile Systems\n\nSpock is connected to an IBM Spectrum Scale™ filesystem providing 250 PB of\nstorage capacity with a peak write speed of 2.5 TB/s. Spock also has access to\nthe center-wide NFS-based filesystem (which provides user and project home\nareas). While Spock does not have direct access to the center’s High\nPerformance Storage System (HPSS) - for user and project archival storage -\nusers can log in to the dtn-user-guide <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#dtn-user-guide> to move data to/from HPSS.\n\nGPUs\n\nSpock contains a total of 144 AMD MI100 GPUs. The AMD MI100 GPU has a peak\nperformance of up to 11.5 TFLOPS in double-precision for modeling & simulation\nand up to 184.6 TFLOPS in half-precision for machine learning and data\nanalytics. Each GPU contains 120 compute units (7680 stream processors) and 32\nGB of high-bandwidth memory (HBM2) which can be accessed at speeds of up to 1.2\nTB/s.\n\n\n\nConnecting\n\nTo connect to Spock, ssh to spock.olcf.ornl.gov. For example:\n\n$ ssh username@spock.olcf.ornl.gov\n\nFor more information on connecting to OLCF resources, see connecting-to-olcf <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#connecting-to-olcf>.\n\n\n\nData and Storage\n\nFor more detailed information about center-wide file systems and data archiving\navailable on Spock, please refer to the pages on\ndata-storage-and-transfers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers>, but the two subsections below give a quick\noverview of NFS and GPFS storage spaces.\n\nNFS\n\n\nThe table provides a comprehensive overview of the different areas and paths within the Spock system, as well as the type of access, permissions, and quotas associated with each. The first row pertains to the User Home area, which can be accessed through the path /ccs/home/[userid]. This area uses NFS (Network File System) as its type, and the permissions are set by the user. The quota for this area is 50 GB, and backups are enabled. However, purging of old files is not allowed. The retention period for files in this area is 90 days, after which they will be automatically deleted. On compute nodes, this area is only accessible in a read-only mode. The second row pertains to the Project Home area, which can be accessed through the path /ccs/proj/[projid]. Similar to the User Home area, this area also uses NFS as its type, but the permissions are set to 770, meaning that both the owner and group have full access, while others have read and execute access. The quota for this area is also 50 GB, and backups are enabled. However, purging of old files is not allowed. The retention period for files in this area is also 90 days, and on compute nodes, it is only accessible in a read-only mode. This table serves as a quick start guide for users of the Spock system, providing them with important information about the different areas and paths they can access, as well as the permissions and limitations associated with each. \n\n| Area        | Path                 | Type | Permissions | Quota | Backups | Purged | Retention | On Compute Nodes |\n|-------------|----------------------|------|-------------|-------|---------|--------|-----------|------------------|\n| User Home   | /ccs/home/[userid]   | NFS  | User set    | 50 GB | Yes     | No     | 90 days   | Read-only        |\n| Project Home| /ccs/proj/[projid]   | NFS  | 770         | 50 GB | Yes     | No     | 90 days   | Read-only        |\n\n\n\nGPFS\n\n\nThe table above is a quick start guide for using Spock, a high-performance computing system. It provides information on the different areas of work available on the system, their paths, types, permissions, quotas, backups, purged data, retention periods, and whether they are accessible on compute nodes. The first area, Member Work, is located at /gpfs/alpine/[projid]/scratch/[userid] and is of type Spectrum Scale. It has a permission level of 700 and a quota of 50 TB. Backups are not enabled for this area and data is purged after 90 days. The retention period for this area is not applicable (N/A) and it is accessible on compute nodes. The second area, Project Work, is located at /gpfs/alpine/[projid]/proj-shared and is also of type Spectrum Scale. It has a permission level of 770 and a quota of 50 TB. Backups are not enabled for this area and data is purged after 90 days. The retention period for this area is also N/A and it is accessible on compute nodes. The third area, World Work, is located at /gpfs/alpine/[projid]/world-shared and is also of type Spectrum Scale. It has a permission level of 775 and a quota of 50 TB. Backups are not enabled for this area and data is purged after 90 days. The retention period for this area is also N/A and it is accessible on compute nodes. This table serves as a useful reference for users of the Spock system, providing them with important information on the different areas of work available and their corresponding settings. \n\n| Area        | Path                                    | Type          | Permissions | Quota | Backups | Purged   | Retention | On Compute Nodes |\n|-------------|-----------------------------------------|---------------|-------------|-------|---------|----------|-----------|------------------|\n| Member Work | /gpfs/alpine/[projid]/scratch/[userid]  | Spectrum Scale| 700         | 50 TB | No      | 90 days  | N/A       | Yes              |\n| Project Work| /gpfs/alpine/[projid]/proj-shared       | Spectrum Scale| 770         | 50 TB | No      | 90 days  | N/A       | Yes              |\n| World Work  | /gpfs/alpine/[projid]/world-shared      | Spectrum Scale| 775         | 50 TB | No      | 90 days  | N/A       | Yes              |\n\n\n\n\n\nProgramming Environment\n\nOLCF provides Spock users many pre-installed software packages and scientific\nlibraries. To facilitate this, environment management tools are used to handle\nnecessary changes to the shell.\n\nEnvironment Modules (Lmod)\n\nEnvironment modules are provided through Lmod, a Lua-based module system for\ndynamically altering shell environments. By managing changes to the shell’s\nenvironment variables (such as PATH, LD_LIBRARY_PATH, and\nPKG_CONFIG_PATH), Lmod allows you to alter the software available in your\nshell environment without the risk of creating package and version combinations\nthat cannot coexist in a single environment.\n\nGeneral Usage\n\nThe interface to Lmod is provided by the module command:\n\n\nThe table presented is a quick start guide for using the Spock module system. The Spock module system is a tool used for managing software environments on a computer system. The first command listed is \"module -t list\", which shows a terse list of the currently loaded modules. This is useful for quickly checking which modules are currently active in the environment. The next command, \"module avail\", displays a table of all the available modules on the system. This can be helpful for finding specific modules or exploring new options. The \"module help <modulename>\" command provides detailed information about a specific module, including its purpose and usage. Similarly, the \"module show <modulename>\" command shows the specific environment changes made by a module. The \"module spider <string>\" command allows for searching all possible modules based on a given string. The \"module load <modulename> [...]\" command is used to load one or more modules into the current environment. The \"module use <path>\" command adds a specified path to the modulefile search cache and MODULESPATH, making it easier to access modules in that location. Conversely, the \"module unuse <path>\" command removes a path from the modulefile search cache and MODULESPATH. The \"module purge\" command unloads all currently loaded modules, while the \"module reset\" command resets loaded modules to the system defaults. Finally, the \"module update\" command reloads all currently loaded modules, ensuring they are up to date. This table provides a comprehensive overview of the various commands and their functions within the Spock module system.\n\n| Command | Description |\n|---------|-------------|\n| module -t list | Shows a terse list of the currently loaded modules |\n| module avail | Shows a table of the currently available modules |\n| module help <modulename> | Shows help information about <modulename> |\n| module show <modulename> | Shows the environment changes made by the <modulename> modulefile |\n| module spider <string> | Searches all possible modules according to <string> |\n| module load <modulename> [...] | Loads the given <modulename>(s) into the current environment |\n| module use <path> | Adds <path> to the modulefile search cache and MODULESPATH |\n| module unuse <path> | Removes <path> from the modulefile search cache and MODULESPATH |\n| module purge | Unloads all modules |\n| module reset | Resets loaded modules to system defaults |\n| module update | Reloads all currently loaded modules |\n\n\n\nSearching for Modules\n\nModules with dependencies are only available when the underlying dependencies,\nsuch as compiler families, are loaded. Thus, module avail will only display\nmodules that are compatible with the current state of the environment. To\nsearch the entire hierarchy across all possible dependencies, the spider\nsub-command can be used as summarized in the following table.\n\n\nThe table provides a quick start guide for using the \"module spider\" command in the Spock computing environment. The first row explains that the command will display the entire possible graph of modules available. The second row instructs users to use the command \"module spider\" followed by a specific module name to search for that module in the graph. The third row specifies that users can also search for a specific version of a module by adding the version number after the module name. The final row states that users can search for modulefiles containing a specific string of characters. This table is a useful reference for navigating the Spock environment and finding the necessary modules for a particular task.\n\n| Command | Description |\n| --- | --- |\n| module spider | Shows the entire possible graph of modules |\n| module spider <modulename> | Searches for modules named <modulename> in the graph of possible modules |\n| module spider <modulename>/<version> | Searches for a specific version of <modulename> in the graph of possible modules |\n| module spider <string> | Searches for modulefiles containing <string> |\n\n\n\nCompilers\n\nCray, AMD, and GCC compilers are provided through modules on Spock. The Cray\nand AMD compilers are both based on LLVM/Clang. There are also system/OS\nversions of both Clang and GCC available in /usr/bin. The table below lists\ndetails about each of the module-provided compilers.\n\nIt is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) whenever possible. See the next section for more details.\n\n\nThe table provides a comprehensive overview of the programming environments, compilers, and compiler modules available for the Spock system. The first column lists the vendors, with Cray, AMD, and GCC being the three options. The second column specifies the programming environment, which is the set of tools and libraries used for software development. The third column lists the compiler module, which is a software component that translates source code into machine code. The fourth column indicates the language supported by each compiler, with options for C, C++, and Fortran. The fifth column lists the compiler wrapper, which is a tool that simplifies the compilation process by automatically setting up the necessary environment variables. Finally, the last column specifies the actual compiler used, with options for Cray's craycc, AMD's clang, and GCC's gcc. This table is a useful reference for developers looking to utilize the Spock system, as it outlines the available options for programming and compiling their code. \n\n| Vendor | Programming Environment | Compiler Module | Language | Compiler Wrapper | Compiler |\n|--------|-------------------------|-----------------|----------|------------------|----------|\n| Cray   | PrgEnv-cray             | cce             | C        | cc               | craycc   |\n| C++    | CC                      | craycxx or crayCC |          |                  |          |\n| Fortran| ftn                     | crayftn         |          |                  |          |\n| AMD    | PrgEnv-amd              | rocm            | C        | cc               | $ROCM_PATH/llvm/bin/clang |\n| C++    | CC                      | $ROCM_PATH/llvm/bin/clang++ |          |                  |          |\n| Fortran| ftn                     | $ROCM_PATH/llvm/bin/flang |          |                  |          |\n| GCC    | PrgEnv-gnu              | gcc             | C        | cc               | $GCC_PATH/bin/gcc |\n| C++    | CC                      | $GCC_PATH/bin/g++ |          |                  |          |\n| Fortran| ftn                     | $GCC_PATH/bin/gfortran |          |                  |          |\n\n\n\nCray Programming Environment and Compiler Wrappers\n\nCray provides PrgEnv-<compiler> modules (e.g., PrgEnv-cray) that load\ncompatible components of a specific compiler toolchain. The components include\nthe specified compiler as well as MPI, LibSci, and other libraries. Loading the\nPrgEnv-<compiler> modules also defines a set of compiler wrappers for that\ncompiler toolchain that automatically add include paths and link in libraries\nfor Cray software. Compiler wrappers are provided for C (cc), C++ (CC),\nand Fortran (ftn).\n\nUse the -craype-verbose flag to display the full include and link information used by the Cray compiler wrappers. This must be called on a file to see the full output (e.g., CC -craype-verbose test.cpp).\n\nMPI\n\nThe MPI implementation available on Spock is Cray's MPICH, which is \"GPU-aware\"\nso GPU buffers can be passed directly to MPI calls.\n\n\n\nCompiling\n\nThis section covers how to compile for different programming models using the\ndifferent compilers covered in the previous section.\n\nMPI\n\n<string>:266: (INFO/1) Duplicate implicit target name: \"mpi\".\n\n\nThe table provides a quick start guide for implementing the Spock programming language. It includes information on the different modules and compilers that can be used, as well as the necessary header files and linking process. The first row of the table specifies the implementation, which in this case is Cray MPICH. The second row lists the corresponding module, which is cray-mpich. The third row indicates the compiler wrappers that can be used, which are cc, CC, and ftn for the Cray compiler. The final row mentions that the MPI header files and linking are already built into the Cray compiler wrappers, making it easier for users to access and utilize them. The last row is left blank, indicating that there is no specific information for the hipcc compiler in this quick start guide. Overall, this table provides a concise overview of the necessary components for implementing Spock and serves as a helpful reference for users. \n\n| Implementation | Module | Compiler | Header Files & Linking |\n|----------------|--------|----------|-----------------------|\n| Cray MPICH     | cray-mpich | cc, CC, ftn (Cray compiler wrappers) | MPI header files and linking is built into the Cray compiler wrappers |\n| hipcc | | | |\n\n\n\nGPU-Aware MPI\n\nTo use GPU-aware Cray MPICH, there are currently some extra steps needed in addition to the table above, which depend on the compiler that is used.\n\n1. Compiling with the Cray compiler wrappers, cc or CC\n\nTo use GPU-aware Cray MPICH with the Cray compiler wrappers, users must load specific modules, set some environment variables, and include appropriate headers and libraries. The following modules and environment variables must be set:\n\nSetting MPICH_SMP_SINGLE_COPY_MODE=CMA is required as a temporary workaround due to a known issue. Users should make a note of where they set this environment variable (if e.g., set in a script) since it should NOT be set once the known issue has been resolved.\n\nmodule load craype-accel-amd-gfx908\nmodule load PrgEnv-cray\nmodule load rocm\n\n## These must be set before running\nexport MPIR_CVAR_GPU_EAGER_DEVICE_MEM=0\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport MPICH_SMP_SINGLE_COPY_MODE=CMA\n\nIn addition, the following header files and libraries must be included:\n\n-I${ROCM_PATH}/include\n-L${ROCM_PATH}/lib -lamdhip64 -lhsa-runtime64\n\nwhere the include path implies that #include <hip/hip_runtime.h> is included in the source file.\n\n2. Compiling with hipcc\n\nTo use GPU-aware Cray MPICH with hipcc, users must load specific modules, set some environment variables, and include appropriate headers and libraries. The following modules and environment variables must be set:\n\nmodule load craype-accel-amd-gfx908\nmodule load PrgEnv-cray\nmodule load rocm\n\n## These must be set before running\nexport MPIR_CVAR_GPU_EAGER_DEVICE_MEM=0\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport MPICH_SMP_SINGLE_COPY_MODE=CMA\n\nIn addition, the following header files and libraries must be included:\n\n-I${MPICH_DIR}/include\n-L${MPICH_DIR}/lib -lmpi -L${CRAY_MPICH_ROOTDIR}/gtl/lib -lmpi_gtl_hsa\n\nOpenMP\n\nThis section shows how to compile with OpenMP using the different compilers\ncovered above.\n\n\nThe table provides a comprehensive overview of the different vendors, modules, languages, compilers, and OpenMP flags that are relevant for using the Spock supercomputer. The first row lists Cray as the vendor, with their cce module for programming in C and C++. The compiler column is left blank, indicating that the default compiler for this module will be used. However, the OpenMP flag is specified as \"-fopenmp\", which enables the use of multiple CPU threads for parallel processing. The second row lists Fortran as the language, with the ftn module, but the compiler column is again left blank. The third row lists AMD as the vendor, with their rocm module, but the language and compiler columns are left blank. The OpenMP flag is once again specified as \"-fopenmp\". The final row lists GCC as the vendor, with their gcc module, and the language and compiler columns are left blank. The OpenMP flag is again specified as \"-fopenmp\". This table serves as a quick reference guide for users of the Spock supercomputer, providing them with the necessary information to effectively utilize the OpenMP parallel processing capabilities. \n\n| Vendor | Module | Language | Compiler | OpenMP flag (CPU thread) |\n|--------|--------|----------|----------|--------------------------|\n| Cray   | cce    | C, C++   |          | -fopenmp                 |\n| Fortran| ftn    |          |          |                          |\n| AMD    | rocm   |          |          | -fopenmp                 |\n| GCC    | gcc    |          |          | -fopenmp                 |\n\n\n\nOpenMP GPU Offload\n\nThis section shows how to compile with OpenMP Offload using the different compilers covered above.\n\nMake sure the craype-accel-amd-gfx908 module is loaded when using OpenMP offload.\n\n\nThe table above provides a comprehensive overview of the spock quick start guide, specifically focusing on the different vendors, modules, languages, compilers, and OpenMP flags (GPU) that are relevant to this guide. The first row of the table lists the vendor, with Cray being the first entry. The second column specifies the module, with cce being the module used for this particular vendor. Moving on to the third column, we see that the language used for this vendor is C. The fourth column mentions the compiler used, which is not specified for this vendor. However, the fifth column provides the OpenMP flag for GPU, which is -fopenmp. The second row of the table lists AMD as the vendor, with rocm being the module used. The language and compiler are not specified for this vendor. This table serves as a useful reference for those looking to quickly get started with spock, providing all the necessary information in a concise and organized manner.\n\n| Vendor | Module | Language | Compiler | OpenMP flag (GPU) |\n|--------|--------|----------|----------|--------------------|\n| Cray   | cce    | C        |          | -fopenmp           |\n| AMD    | rocm   |          |          |                    |\n\n\n\nHIP\n\nThis section shows how to compile HIP codes using the Cray compiler wrappers and hipcc compiler driver.\n\nMake sure the craype-accel-amd-gfx908 module is loaded when using HIP.\n\n\nThe table provided is a quick start guide for using Spock, a compiler for high-performance computing. It includes information on the compiler, compile/link flags, header files, and libraries. The first column lists the different compilers that can be used with Spock, including CC and hipcc. The second column provides details on the compile/link flags, which are used to specify certain options or settings during the compilation process. The third column lists the necessary header files that need to be included in the code for it to run properly. The last column lists the libraries that need to be linked in order for the code to access certain functions or features. This table serves as a helpful reference for those looking to quickly get started with using Spock for their high-performance computing needs.\n\n| Compiler | Compile/Link Flags | Header Files | Libraries |\n|----------|---------------------|--------------|-----------|\n| CC       |                     |              |           |\n| hipcc    |                     |              |           |\n\n\n\n\n\nRunning Jobs\n\nThis section describes how to run programs on the Spock compute nodes,\nincluding a brief overview of Slurm and also how to map processes and threads\nto CPU cores and GPUs.\n\nSlurm Workload Manager\n\nSlurm is the workload manager used to interact\nwith the compute nodes on Spock. In the following subsections, the most\ncommonly used Slurm commands for submitting, running, and monitoring jobs will\nbe covered, but users are encouraged to visit the official documentation and\nman pages for more information.\n\nBatch Scheduler and Job Launcher\n\nSlurm provides 3 ways of submitting and launching jobs on Spock's compute\nnodes: batch  scripts, interactive, and single-command. The Slurm commands\nassociated with these methods are shown in the table below and examples of\ntheir use can be found in the related subsections.\n\n\nThe table below provides a quick start guide for using the Spock cluster, a high-performance computing system. The first command, \"sbatch,\" is used to submit a batch job to the cluster. This allows users to run multiple tasks in a single job, making it more efficient. The next command, \"salloc,\" is used to allocate resources for a job. This command is useful for interactive jobs, where the user needs to access the cluster in real-time. Finally, the \"srun\" command is used to launch a parallel job on the cluster. This command allows users to specify the number of processors and other parameters for their job. By using these commands, users can efficiently utilize the resources of the Spock cluster for their computing needs.\n\n| Command | Description |\n|---------|-------------|\n| sbatch  | Submit a batch job to the cluster |\n| salloc  | Allocate resources for an interactive job |\n| srun    | Launch a parallel job on the cluster |\n\n\n\nBatch Scripts\n\nA batch script can be used to submit a job to run on the compute nodes at a\nlater time. In this case, stdout and stderr will be written to a file(s) that\ncan be opened after the job completes. Here is an example of a simple batch\nscript:\n\n#!/bin/bash\n#SBATCH -A <project_id>\n#SBATCH -J <job_name>\n#SBATCH -o %x-%j.out\n#SBATCH -t 00:05:00\n#SBATCH -p <partition>\n#SBATCH -N 2\n\nsrun -n4 --ntasks-per-node=2 ./a.out\n\nThe Slurm submission options are preceded by #SBATCH, making them appear as\ncomments to a shell (since comments begin with #). Slurm will look for\nsubmission options from the first line through the first non-comment line.\nOptions encountered after the first non-comment line will not be read by Slurm.\nIn the example script, the lines are:\n\n\nThe table presented is a quick start guide for using Spock, a high-performance computing system. The first line is an optional shell interpreter line, followed by the OLCF project to charge for the job. The third line is for specifying the job name, which will be used in the stdout file name. The fourth line shows the format for the stdout file name, with %x representing the job name and %j representing the job id. The fifth line is for requesting the walltime for the job in HH:MM:SS format. The sixth line is for selecting the batch queue for the job. The seventh line is for specifying the number of compute nodes requested for the job. The eighth line is a blank line for clarity. The final line shows the srun command to launch a parallel job, with a request for 4 processes (2 per node). This table provides a clear and concise guide for users to quickly and efficiently submit jobs on the Spock system.\n\n| Line | Description |\n|------|-------------|\n| 1 | [Optional] shell interpreter line |\n| 2 | OLCF project to charge |\n| 3 | Job name |\n| 4 | stdout file name ( %x represents job name, %j represents job id) |\n| 5 | Walltime requested (HH:MM:SS) |\n| 6 | Batch queue |\n| 7 | Number of compute nodes requested |\n| 8 | Blank line |\n| 9 | srun command to launch parallel job (requesting 4 processes - 2 per node) |\n\n\n\n\n\nInteractive Jobs\n\nTo request an interactive job where multiple job steps (i.e., multiple srun\ncommands) can be launched on the allocated compute node(s), the salloc\ncommand can be used:\n\n$ salloc -A <project_id> -J <job_name> -t 00:05:00 -p <partition> -N 2\nsalloc: Granted job allocation 4258\nsalloc: Waiting for resource configuration\nsalloc: Nodes spock[10-11] are ready for job\n\n$ srun -n 4 --ntasks-per-node=2 ./a.out\n<output printed to terminal>\n\n$ srun -n 2 --ntasks-per-node=1 ./a.out\n<output printed to terminal>\n\nHere, salloc is used to request an allocation of 2 MI100 compute nodes for\n5 minutes. Once the resources become available, the user is granted access to\nthe compute nodes (spock10 and spock11 in this case) and can launch job\nsteps on them using srun.\n\n\n\nSingle Command (non-interactive)\n\n$ srun -A <project_id> -t 00:05:00 -p <partition> -N 2 -n 4 --ntasks-per-node=2 ./a.out\n<output printed to terminal>\n\nThe job name and output options have been removed since stdout/stderr are\ntypically desired in the terminal window in this usage mode.\n\nCommon Slurm Submission Options\n\nThe table below summarizes commonly-used Slurm job submission options:\n\n\nThe table provides a quick start guide for using Spock, a high-performance computing cluster. The first column lists the various options that can be used when submitting a job to the cluster. These options include the project ID to charge for the job, the name of the job, the partition or batch queue to use, the wall clock time for the job in hours, minutes, and seconds, the number of compute nodes to use, and the file names for the standard output and standard error. These options can be specified using the corresponding flags in the command line when submitting a job to the cluster. This table serves as a useful reference for users who are new to using Spock and need a quick overview of the available options for job submission.\n\n| Option | Description |\n| --- | --- |\n| -A <project_id> | Project ID to charge |\n| -J <job_name> | Name of job |\n| -p <partition> | Partition / batch queue |\n| -t <time> | Wall clock time <HH:MM:SS> |\n| -N <number_of_nodes> | Number of compute nodes |\n| -o <file_name> | Standard output file name |\n| -e <file_name> | Standard error file name |\n\n\n\nFor more information about these and/or other options, please see the\nsbatch man page.\n\nOther Common Slurm Commands\n\nThe table below summarizes commonly-used Slurm commands:\n\n\nThe table provides a quick start guide for using the Spock cluster, a high-performance computing system. The first entry, \"sinfo,\" displays information about the current state of the cluster, including the number of nodes, their status, and any partitions or features that are available. The \"squeue\" command allows users to view the current job queue, including the job ID, user, and status. \"Sacct\" provides a detailed summary of completed jobs, including their start and end times, CPU usage, and exit status. The \"scancel\" command allows users to cancel a job that is currently running or in the queue. Finally, \"scontrol\" provides a way to manage and modify job and node properties, such as setting job priorities or changing node features. These commands are essential for efficiently utilizing the Spock cluster and ensuring that jobs are running smoothly. \n\n| Command | Description |\n|---------|-------------|\n| sinfo   | Displays information about the current state of the cluster |\n| squeue  | Displays the current job queue |\n| sacct   | Provides a summary of completed jobs |\n| scancel | Allows users to cancel a job |\n| scontrol| Manages and modifies job and node properties |\n\n\n\n\n\nSlurm Compute Node Partitions\n\nSpock's compute nodes are separated into 2 Slurm partitions (queues): 1 for\nCAAR projects and 1 for ECP projects. Please see the tables below for details.\n\nIf CAAR or ECP teams require a temporary exception to this policy, please\nemail help@olcf.ornl.gov with your request and it will be given to the OLCF\nResource Utilization Council (RUC) for review.\n\nCAAR Partition\n\nThe CAAR partition consists of 24 total compute nodes. On a per-project basis,\neach user can have 1 running and 1 eligible job at a time, with no limit on the\nnumber of jobs submitted.\n\n\nThe table provides a quick start guide for using Spock, a high-performance computing cluster. It outlines the maximum walltime, or the amount of time a job can run, for different numbers of nodes. For 1-4 nodes, the maximum walltime is 3 hours, while for 5-16 nodes, the maximum walltime is 1 hour. This information is important for users to plan their jobs and ensure they do not exceed the allotted time. It also highlights the scalability of Spock, as the maximum walltime decreases as the number of nodes increases. This table serves as a useful reference for users of Spock to optimize their job submissions and utilize the cluster efficiently.\n\n| Number of Nodes | Max Walltime |\n|-----------------|--------------|\n| 1 - 4           | 3 hours      |\n| 5 - 16          | 1 hour       |\n\n\n\nECP Partition\n\nThe ECP partition consists of 12 total compute nodes. On a per-project basis,\neach user can have 1 running and 1 eligible job at a time, with up to 5 jobs\nsubmitted.\n\n\nThe table provides a quick start guide for using Spock, a high-performance computing cluster. It outlines the maximum walltime, or the maximum amount of time a job can run, for different numbers of nodes. For 1 to 4 nodes, the maximum walltime is 3 hours. This information is important for users to know when planning and submitting jobs to the Spock cluster. It ensures that jobs are completed within the allotted time and allows for efficient use of the cluster's resources. This table serves as a helpful reference for users to quickly determine the maximum walltime for their job based on the number of nodes they plan to use. \n\n| Number of Nodes | Max Walltime |\n|-----------------|--------------|\n| 1 - 4           | 3 hours      |\n\n\n\nProcess and Thread Mapping\n\nThis section describes how to map processes (e.g., MPI ranks) and process threads (e.g., OpenMP threads) to the CPUs and GPUs on Spock. The spock-compute-nodes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#spock-compute-nodes> diagram will be helpful when reading this section to understand which hardware threads your processes and threads run on.\n\nCPU Mapping\n\nIn this sub-section, a simple MPI+OpenMP \"Hello, World\" program (hello_mpi_omp) will be used to clarify the mappings. Slurm's interactive <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#interactive> method was used to request an allocation of 1 compute node for these examples: salloc -A <project_id> -t 30 -p <parition> -N 1\n\nThe srun options used in this section are (see man srun for more information):\n\n\nThe table below is a quick start guide for using Spock, a popular testing and specification framework for Java and Groovy applications. The table outlines three different options for configuring the number of CPUs and threads to be used in a task. The first option, \"-c, --cpus-per-task=<ncpus>\", allows the user to specify the exact number of CPUs to be allocated for the task. The second option, \"--threads-per-core=<threads>\", allows the user to specify the number of threads to be used per core. Finally, the \"--cpu-bind=threads\" option allows the user to bind the task to specific threads. These options can be useful for optimizing performance and resource allocation in testing and development environments. The table is presented in markdown format for easy reference.\n\n| Option | Description |\n| --- | --- |\n| -c, --cpus-per-task=<ncpus> | Specify the number of CPUs to be allocated for the task. |\n| --threads-per-core=<threads> | Specify the number of threads to be used per core. |\n| --cpu-bind=threads | Bind the task to specific threads. |\n\n\n\nIn the srun man page (and so the table above), threads refers to hardware threads.\n\n2 MPI ranks - each with 2 OpenMP threads\n\nIn this example, the intent is to launch 2 MPI ranks, each of which spawn 2 OpenMP threads, and have all of the 4 OpenMP threads run on different physical CPU cores.\n\nFirst (INCORRECT) attempt\n\nTo set the number of OpenMP threads spawned per MPI rank, the OMP_NUM_THREADS environment variable can be used. To set the number of MPI ranks launched, the srun flag -n can be used.\n\n$ export OMP_NUM_THREADS=2\n$ srun -n2 ./hello_mpi_omp | sort\n\nWARNING: Requested total thread count and/or thread affinity may result in\noversubscription of available CPU resources!  Performance may be degraded.\nExplicitly set OMP_WAIT_POLICY=PASSIVE or ACTIVE to suppress this message.\nSet CRAY_OMP_CHECK_AFFINITY=TRUE to print detailed thread-affinity messages.\nWARNING: Requested total thread count and/or thread affinity may result in\noversubscription of available CPU resources!  Performance may be degraded.\nExplicitly set OMP_WAIT_POLICY=PASSIVE or ACTIVE to suppress this message.\nSet CRAY_OMP_CHECK_AFFINITY=TRUE to print detailed thread-affinity messages.\n\nMPI 000 - OMP 000 - HWT 000 - Node spock01\nMPI 000 - OMP 001 - HWT 000 - Node spock01\nMPI 001 - OMP 000 - HWT 016 - Node spock01\nMPI 001 - OMP 001 - HWT 016 - Node spock01\n\nThe first thing to notice here is the WARNING about oversubscribing the available CPU cores. Also, the output shows each MPI rank did spawn 2 OpenMP threads, but both OpenMP threads ran on the same hardware thread (for a given MPI rank). This was not the intended behavior; each OpenMP thread was meant to run on its own physical CPU core.\n\nSecond (CORRECT) attempt\n\nBy default, each MPI rank is allocated only 1 hardware thread, so both OpenMP threads only have that 1 hardware thread to run on - hence the WARNING and undesired behavior. In order for each OpenMP thread to run on its own physical CPU core, each MPI rank should be given 2 hardware thread (-c 2) - since, by default, only 1 hardware thread per physical CPU core is enabled (this would need to be -c 4 if --threads-per-core=2 instead of the default of 1. The OpenMP threads will be mapped to unique physical CPU cores unless there are not enough physical CPU cores available, in which case the remaining OpenMP threads will share hardware threads and a WARNING will be issued as shown in the previous example.\n\n$ export OMP_NUM_THREADS=2\n$ srun -n2 -c2 ./hello_mpi_omp | sort\n\nMPI 000 - OMP 000 - HWT 000 - Node spock13\nMPI 000 - OMP 001 - HWT 001 - Node spock13\nMPI 001 - OMP 000 - HWT 016 - Node spock13\nMPI 001 - OMP 001 - HWT 017 - Node spock13\n\nNow the output shows that each OpenMP thread ran on (one of the hardware threads of) its own physical CPU cores. More specifically (see the Spock Compute Node diagram), OpenMP thread 000 of MPI rank 000 ran on hardware thread 000 (i.e., physical CPU core 00), OpenMP thread 001 of MPI rank 000 ran on hardware thread 001 (i.e., physical CPU core 01), OpenMP thread 000 of MPI rank 001 ran on hardware thread 016 (i.e., physical CPU core 16), and OpenMP thread 001 of MPI rank 001 ran on hardware thread 017 (i.e., physical CPU core 17) - as expected.\n\nThere are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.\n\nGPU Mapping\n\nIn this sub-section, an MPI+OpenMP+HIP \"Hello, World\" program (hello_jobstep) will be used to clarify the GPU mappings. Again, Slurm's interactive <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#interactive> method was used to request an allocation of 2 compute node for these examples: salloc -A <project_id> -t 30 -p <parition> -N 2. The CPU mapping part of this example is very similar to the example used above in the CPU Mapping sub-section, so the focus here will be on the GPU mapping part.\n\nThe following srun options will be used in the examples below. See man srun for a complete list of options and more information.\n\n\nThe table provides a quick start guide for using Spock, a high-performance computing system, with GPUs. The first column lists different options that can be used when submitting a job to Spock. The second column provides a brief description of each option. The \"--gpus-per-task\" option allows the user to specify the number of GPUs needed for each task in the job's resource allocation. The \"--gpu-bind=closest\" option binds each task to a GPU that is on the same NUMA domain as the CPU core the MPI rank is running on. The \"--gpu-bind=map_gpu:<list>\" option allows the user to bind tasks to specific GPUs by setting GPU masks on tasks or ranks. The <list> represents a list of GPU IDs that can be used for each task, and if there are more tasks than elements in the list, the list will be reused starting from the beginning. The \"--ntasks-per-gpu=<ntasks>\" option requests a specific number of tasks to be invoked for each GPU. The last option, \"--distribution=<value>[:<value>][:<value>]\", specifies the distribution of MPI ranks across compute nodes, sockets, and cores. The default values for this option are block:cyclic:cyclic. This table serves as a helpful reference for users who are new to using Spock with GPUs and need guidance on how to specify and distribute resources for their jobs.\n\n| Option | Description |\n| --- | --- |\n| --gpus-per-task | Specify the number of GPUs required for the job on each task to be spawned in the job's resource allocation. |\n| --gpu-bind=closest | Binds each task to the GPU which is on the same NUMA domain as the CPU core the MPI rank is running on. |\n| --gpu-bind=map_gpu:<list> | Bind tasks to specific GPUs by setting GPU masks on tasks (or ranks) as specified where <list> is <gpu_id_for_task_0>,<gpu_id_for_task_1>,.... If the number of tasks (or ranks) exceeds the number of elements in this list, elements in the list will be reused as needed starting from the beginning of the list. To simplify support for large task counts, the lists may follow a map with an asterisk and repetition count. (For example map_gpu:0*4,1*4) |\n| --ntasks-per-gpu=<ntasks> | Request that there are ntasks tasks invoked for every GPU. |\n| --distribution=<value>[:<value>][:<value>] | Specifies the distribution of MPI ranks across compute nodes, sockets (NUMA domains on Spock), and cores, respectively. The default values are block:cyclic:cyclic |\n\nIn general, GPU mapping can be accomplished in different ways. For example, an application might map MPI ranks to GPUs programmatically within the code using, say, hipSetDevice. In this case, since all GPUs on a node are available to all MPI ranks on that node by default, there might not be a need to map to GPUs using Slurm (just do it in the code). However, in another application, there might be a reason to make only a subset of GPUs available to the MPI ranks on a node. It is this latter case that the following examples refer to.\n\nMapping 1 task per GPU\n\nIn the following examples, each MPI rank (and its OpenMP threads) will be mapped to a single GPU.\n\nExample 1: 4 MPI ranks - each with 2 OpenMP threads and 1 GPU (single-node)\n\nThis example launches 4 MPI ranks (-n4), each with 2 physical CPU cores (-c2) to launch 2 OpenMP threads (OMP_NUM_THREADS=2) on. In addition, each MPI rank (and its 2 OpenMP threads) should have access to only 1 GPU. To accomplish the GPU mapping, two new srun options will be used:\n\n--gpus-per-task specifies the number of GPUs required for the job on each task\n\n--gpu-bind=closest binds each task to the GPU which is closest.\n\nTo further clarify, --gpus-per-task does not actually bind GPUs to MPI ranks. It allocates GPUs to the job step. The --gpu-bind=closest is what actually maps a specific GPU to each rank; namely, the \"closest\" one, which is the GPU on the same NUMA domain as the CPU core the MPI rank is running on (see the spock-compute-nodes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#spock-compute-nodes> section).\n\nWithout these additional flags, all MPI ranks would have access to all GPUs (which is the default behavior).\n\n$ export OMP_NUM_THREADS=2\n$ srun -N1 -n4 -c2 --gpus-per-task=1 --gpu-bind=closest ./hello_jobstep | sort\n\nMPI 000 - OMP 000 - HWT 000 - Node spock13 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 000 - OMP 001 - HWT 001 - Node spock13 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 001 - OMP 000 - HWT 016 - Node spock13 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 001 - OMP 001 - HWT 017 - Node spock13 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 002 - OMP 000 - HWT 032 - Node spock13 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 002 - OMP 001 - HWT 033 - Node spock13 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 003 - OMP 000 - HWT 048 - Node spock13 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\nMPI 003 - OMP 001 - HWT 049 - Node spock13 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\n\nThe output contains different IDs associated with the GPUs so it is important to first describe these IDs before moving on. GPU_ID is the node-level (or global) GPU ID, which is labeled as one might expect from looking at a node diagram: 0, 1, 2, 3. RT_GPU_ID is the HIP runtime GPU ID, which can be thought of as each MPI rank's local GPU ID numbering (with zero-based indexing). So in the output above, each MPI rank has access to 1 unique GPU - where MPI 000 has access to GPU 0, MPI 001 has access to GPU 1, etc., but all MPI ranks show a HIP runtime GPU ID of 0. The reason is that each MPI rank only \"sees\" one GPU and so the HIP runtime labels it as \"0\", even though it might be global GPU ID 0, 1, 2, or 3. The GPU's bus ID is included to definitively show that different GPUs are being used.\n\nHere is a summary of the different GPU IDs reported by the example program:\n\nGPU_ID is the node-level (or global) GPU ID read from ROCR_VISIBLE_DEVICES. If this environment variable is not set (either by the user or by Slurm), the value of GPU_ID will be set to N/A.\n\nRT_GPU_ID is the HIP runtime GPU ID (as reported from, say hipGetDevice).\n\nBus_ID is the physical bus ID associated with the GPUs. Comparing the bus IDs is meant to definitively show that different GPUs are being used.\n\nSo the job step (i.e., srun command) used above gave the desired output. Each MPI rank spawned 2 OpenMP threads and had access to a unique GPU. The --gpus-per-task=1 allocated 1 GPU for each MPI rank and the --gpu-bind=closest ensured that the closest GPU to each rank was the one used.\n\nExample 2: 8 MPI ranks - each with 2 OpenMP threads and 1 GPU (multi-node)\n\nThis example will extend Example 1 to run on 2 nodes. As the output shows, it is a very straightforward exercise of changing the number of nodes to 2 (-N2) and the number of MPI ranks to 8 (-n8).\n\n$ export OMP_NUM_THREADS=2\n$ srun -N2 -n8 -c2 --gpus-per-task=1 --gpu-bind=closest ./hello_jobstep | sort\n\nMPI 000 - OMP 000 - HWT 000 - Node spock13 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 000 - OMP 001 - HWT 001 - Node spock13 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 001 - OMP 000 - HWT 016 - Node spock13 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 001 - OMP 001 - HWT 017 - Node spock13 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 002 - OMP 000 - HWT 032 - Node spock13 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 002 - OMP 001 - HWT 033 - Node spock13 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 003 - OMP 000 - HWT 048 - Node spock13 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\nMPI 003 - OMP 001 - HWT 049 - Node spock13 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\nMPI 004 - OMP 000 - HWT 000 - Node spock14 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 004 - OMP 001 - HWT 001 - Node spock14 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 005 - OMP 000 - HWT 016 - Node spock14 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 005 - OMP 001 - HWT 017 - Node spock14 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 006 - OMP 000 - HWT 032 - Node spock14 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 006 - OMP 001 - HWT 033 - Node spock14 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 007 - OMP 000 - HWT 048 - Node spock14 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\nMPI 007 - OMP 001 - HWT 049 - Node spock14 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\n\nExample 3: 4 MPI ranks - each with 2 OpenMP threads and 1 *specific* GPU (single-node)\n\nThis example will be very similar to Example 1, but instead of using --gpu-bind=closest to map each MPI rank to the closest GPU, --gpu-bind=map_gpu will be used to map each MPI rank to a specific GPU. The map_gpu option takes a comma-separated list of GPU IDs to specify how the MPI ranks are mapped to GPUs, where the form of the comma-separated list is <gpu_id_for_task_0>, <gpu_id_for_task_1>,....\n\n$ export OMP_NUM_THREADS=2\n$ srun -N1 -n4 -c2 --gpus-per-task=1 --gpu-bind=map_gpu:0,1,2,3 ./hello_jobstep | sort\n\nMPI 000 - OMP 000 - HWT 000 - Node spock13 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 000 - OMP 001 - HWT 001 - Node spock13 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 001 - OMP 000 - HWT 016 - Node spock13 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 001 - OMP 001 - HWT 017 - Node spock13 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 002 - OMP 000 - HWT 032 - Node spock13 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 002 - OMP 001 - HWT 033 - Node spock13 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 003 - OMP 000 - HWT 048 - Node spock13 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\nMPI 003 - OMP 001 - HWT 049 - Node spock13 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\n\nHere, the output is the same as the results from Example 1. This is because the 4 GPU IDs in the comma-separated list happen to specify the GPUs within the same NUMA domains that the MPI ranks are in. So MPI 000 is mapped to GPU 0, MPI 001 is mapped to GPU 1, etc.\n\nWhile this level of control over mapping MPI ranks to GPUs might be useful for some applications, it is always important to consider the implication of the mapping. For example, if the order of the GPU IDs in the map_gpu option is reversed, the MPI ranks and the GPUs they are mapped to would be in different NUMA domains, which could potentially lead to poorer performance.\n\n$ export OMP_NUM_THREADS=2\n$ srun -N1 -n4 -c2 --gpus-per-task=1 --gpu-bind=map_gpu:3,2,1,0 ./hello_jobstep | sort\n\nMPI 000 - OMP 000 - HWT 000 - Node spock13 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\nMPI 000 - OMP 001 - HWT 001 - Node spock13 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\nMPI 001 - OMP 000 - HWT 016 - Node spock13 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 001 - OMP 001 - HWT 017 - Node spock13 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 002 - OMP 000 - HWT 032 - Node spock13 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 002 - OMP 001 - HWT 033 - Node spock13 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 003 - OMP 000 - HWT 048 - Node spock13 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 003 - OMP 001 - HWT 049 - Node spock13 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\n\nHere, notice that MPI 000 now maps to GPU 3, MPI 001 maps to GPU 2, etc., so the MPI ranks are not in the same NUMA domains as the GPUs they are mapped to.\n\nAgain, this particular example would NOT be a very good mapping of GPUs to MPI ranks though. E.g., notice that MPI rank 000 is running on NUMA node 0, whereas GPU 3 is on NUMA node 3. Again, see the spock-compute-nodes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#spock-compute-nodes> section for NUMA descriptions.\n\nExample 4: 8 MPI ranks - each with 2 OpenMP threads and 1 *specific* GPU (multi-node)\n\nExtending Examples 2 and 3 to run on 2 nodes is also a straightforward exercise by changing the number of nodes to 2 (-N2) and the number of MPI ranks to 8 (-n8).\n\n$ export OMP_NUM_THREADS=2\n$ srun -N2 -n8 -c2 --gpus-per-task=1 --gpu-bind=map_gpu:0,1,2,3 ./hello_jobstep | sort\n\nMPI 000 - OMP 000 - HWT 000 - Node spock13 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 000 - OMP 001 - HWT 001 - Node spock13 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 001 - OMP 000 - HWT 016 - Node spock13 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 001 - OMP 001 - HWT 017 - Node spock13 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 002 - OMP 000 - HWT 032 - Node spock13 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 002 - OMP 001 - HWT 033 - Node spock13 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 003 - OMP 000 - HWT 048 - Node spock13 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\nMPI 003 - OMP 001 - HWT 049 - Node spock13 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\nMPI 004 - OMP 000 - HWT 000 - Node spock14 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 004 - OMP 001 - HWT 001 - Node spock14 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 005 - OMP 000 - HWT 016 - Node spock14 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 005 - OMP 001 - HWT 017 - Node spock14 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 006 - OMP 000 - HWT 032 - Node spock14 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 006 - OMP 001 - HWT 033 - Node spock14 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 007 - OMP 000 - HWT 048 - Node spock14 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\nMPI 007 - OMP 001 - HWT 049 - Node spock14 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\n\nMapping multiple MPI ranks to a single GPU\n\nIn the following examples, 2 MPI ranks will be mapped to 1 GPU. For the sake of brevity, OMP_NUM_THREADS will be set to 1, so -c1 will be used unless otherwise specified.\n\nOn AMD's MI100 GPUs, multi-process service (MPS) is not needed since multiple MPI ranks per GPU is supported natively.\n\nExample 5: 8 MPI ranks - where 2 ranks share a GPU (round-robin, single-node)\n\nThis example launches 8 MPI ranks (-n8), each with 1 physical CPU core (-c1) to launch 1 OpenMP thread (OMP_NUM_THREADS=1) on. The MPI ranks will be assigned to GPUs in a round-robin fashion so that each of the 4 GPUs on the node are shared by 2 MPI ranks. To accomplish this GPU mapping, a new srun option will be used:\n\n--ntasks-per-gpu specifies the number of MPI ranks that will share access to a GPU.\n\n$ export OMP_NUM_THREADS=1\n$ srun -N1 -n8 -c1 --ntasks-per-gpu=2 --gpu-bind=closest ./hello_jobstep | sort\n\nMPI 000 - OMP 000 - HWT 000 - Node spock13 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 001 - OMP 000 - HWT 016 - Node spock13 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 002 - OMP 000 - HWT 032 - Node spock13 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 003 - OMP 000 - HWT 048 - Node spock13 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\nMPI 004 - OMP 000 - HWT 001 - Node spock13 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 005 - OMP 000 - HWT 017 - Node spock13 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 006 - OMP 000 - HWT 033 - Node spock13 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 007 - OMP 000 - HWT 049 - Node spock13 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\n\nThe output shows the round-robin (cyclic) distribution of MPI ranks to GPUs. In fact, it is a round-robin distribution of MPI ranks to NUMA domains (the default distribution). The GPU mapping is a consequence of where the MPI ranks are distributed; --gpu-bind=closest simply maps the GPU in a NUMA domain to the MPI ranks in the same NUMA domain.\n\nExample 6: 16 MPI ranks - where 2 ranks share a GPU (round-robin, multi-node)\n\nThis example is an extension of Example 5 to run on 2 nodes.\n\nThis example requires a workaround to run as expected. --ntasks-per-gpu=2 does not force MPI ranks 008-015 to run on the second node, so the number of physical CPU cores per MPI rank is increased to 8 (-c8) to force the desired behavior due to the constraint of the number of physical CPU cores (64) on a node.\n\n$ export OMP_NUM_THREADS=1\n$ srun -N2 -n16 -c8 --ntasks-per-gpu=2 --gpu-bind=closest ./hello_jobstep | sort\n\nMPI 000 - OMP 000 - HWT 005 - Node spock13 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 001 - OMP 000 - HWT 018 - Node spock13 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 002 - OMP 000 - HWT 032 - Node spock13 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 003 - OMP 000 - HWT 050 - Node spock13 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\nMPI 004 - OMP 000 - HWT 010 - Node spock13 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 005 - OMP 000 - HWT 026 - Node spock13 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 006 - OMP 000 - HWT 040 - Node spock13 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 007 - OMP 000 - HWT 059 - Node spock13 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\nMPI 008 - OMP 000 - HWT 003 - Node spock14 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 009 - OMP 000 - HWT 016 - Node spock14 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 010 - OMP 000 - HWT 032 - Node spock14 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 011 - OMP 000 - HWT 048 - Node spock14 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\nMPI 012 - OMP 000 - HWT 008 - Node spock14 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 013 - OMP 000 - HWT 024 - Node spock14 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 014 - OMP 000 - HWT 042 - Node spock14 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 015 - OMP 000 - HWT 056 - Node spock14 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\n\nExample 7: 8 MPI ranks - where 2 ranks share a GPU (packed, single-node)\n\nThis example launches 8 MPI ranks (-n8), each with 8 physical CPU cores (-c8) to launch 1 OpenMP thread (OMP_NUM_THREADS=1) on. The MPI ranks will be assigned to GPUs in a packed fashion so that each of the 4 GPUs on the node are shared by 2 MPI ranks. Similar to Example 5, -ntasks-per-gpu=2 will be used, but a new srun flag will be used to change the default round-robin (cyclic) distribution of MPI ranks across NUMA domains:\n\n--distribution=<value>:[<value>]:[<value>] specifies the distribution of MPI ranks across compute nodes, sockets (NUMA domains on Spock), and cores, respectively. The default values are block:cyclic:cyclic, which is where the cyclic assignment comes from in the previous examples.\n\nIn the job step for this example, --distribution=*:block is used, where * represents the default value of block for the distribution of MPI ranks across compute nodes and the distribution of MPI ranks across NUMA domains has been changed to block from its default value of cyclic.\n\nBecause the distribution across NUMA domains has been changed to a \"packed\" (block) configuration, caution must be taken to ensure MPI ranks end up in the NUMA domains where the GPUs they intend to be mapped to are located. To accomplish this, the number of physical CPU cores assigned to an MPI rank was increased - in this case to 8. Doing so ensures that only 2 MPI ranks can fit into a single NUMA domain. If the value of -c was left at 1, all 8 MPI ranks would be \"packed\" into the first NUMA domain, where the \"closest\" GPU would be GPU 0 - the only GPU in that NUMA domain.\n\nNotice that this is not a workaround like in Example 6, but a requirement due to the block distribution of MPI ranks across NUMA domains.\n\n$ export OMP_NUM_THREADS=1\n$ srun -N1 -n8 -c8 --ntasks-per-gpu=2 --gpu-bind=closest --distribution=*:block ./hello_jobstep | sort\n\nMPI 000 - OMP 000 - HWT 001 - Node spock13 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 001 - OMP 000 - HWT 008 - Node spock13 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 002 - OMP 000 - HWT 016 - Node spock13 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 003 - OMP 000 - HWT 024 - Node spock13 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 004 - OMP 000 - HWT 035 - Node spock13 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 005 - OMP 000 - HWT 043 - Node spock13 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 006 - OMP 000 - HWT 049 - Node spock13 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\nMPI 007 - OMP 000 - HWT 057 - Node spock13 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\n\nThe overall effect of using --distribution=*:block and increasing the number of physical CPU cores available to each MPI rank is to place the first two MPI ranks in NUMA 0 with GPU 0, the next two MPI ranks in NUMA 1 with GPU 1, and so on.\n\nExample 8: 16 MPI ranks - where 2 ranks share a GPU (packed, multi-node)\n\nThis example is an extension of Example 7 to use 2 compute nodes. With the appropriate changes put in place in Example 7, it is a straightforward exercise to change to using 2 nodes (-N2) and 16 MPI ranks (-n16).\n\n$ export OMP_NUM_THREADS=1\n$ srun -N2 -n16 -c8 --ntasks-per-gpu=2 --gpu-bind=closest --distribution=*:block ./hello_jobstep | sort\n\nMPI 000 - OMP 000 - HWT 005 - Node spock13 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 001 - OMP 000 - HWT 008 - Node spock13 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 002 - OMP 000 - HWT 017 - Node spock13 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 003 - OMP 000 - HWT 026 - Node spock13 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 004 - OMP 000 - HWT 033 - Node spock13 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 005 - OMP 000 - HWT 041 - Node spock13 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 006 - OMP 000 - HWT 048 - Node spock13 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\nMPI 007 - OMP 000 - HWT 057 - Node spock13 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\nMPI 008 - OMP 000 - HWT 002 - Node spock14 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 009 - OMP 000 - HWT 011 - Node spock14 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c9\nMPI 010 - OMP 000 - HWT 016 - Node spock14 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 011 - OMP 000 - HWT 026 - Node spock14 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID 87\nMPI 012 - OMP 000 - HWT 033 - Node spock14 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 013 - OMP 000 - HWT 041 - Node spock14 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID 48\nMPI 014 - OMP 000 - HWT 054 - Node spock14 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\nMPI 015 - OMP 000 - HWT 063 - Node spock14 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID 09\n\nMultiple GPUs per MPI rank\n\nAs mentioned previously, all GPUs are accessible by all MPI ranks by default, so it is possible to programatically map any combination of MPI ranks to GPUs. However, there is currently no way to use Slurm to map multiple GPUs to a single MPI rank. If this functionality is needed for an application, please submit a ticket by emailing help@olcf.ornl.gov.\n\nThere are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_jobstep program and test whether or not processes and threads are running where intended.\n\nNVMe Usage\n\nEach Spock compute node has [2x] 3.2 TB NVMe devices (SSDs) with a peak sequential performance of 6900 MB/s (read) and 4200 MB/s (write). To use the NVMe, users must request access during job allocation using the -C nvme option to sbatch, salloc, or srun. Once the devices have been granted to a job, users can access them at /mnt/bb/<userid>. Users are responsible for moving data to/from the NVMe before/after their jobs. Here is a simple example script:\n\n#!/bin/bash\n#SBATCH -A <projid>\n#SBATCH -J nvme_test\n#SBATCH -o %x-%j.out\n#SBATCH -t 00:05:00\n#SBATCH -p batch\n#SBATCH -N 1\n#SBATCH -C nvme\n\ndate\n\n# Change directory to user scratch space (GPFS)\ncd /gpfs/alpine/<projid>/scratch/<userid>\n\necho \" \"\necho \"*****ORIGINAL FILE*****\"\ncat test.txt\necho \"***********************\"\n\n# Move file from GPFS to SSD\nmv test.txt /mnt/bb/<userid>\n\n# Edit file from compute node\nsrun -n1 hostname >> /mnt/bb/<userid>/test.txt\n\n# Move file from SSD back to GPFS\nmv /mnt/bb/<userid>/test.txt .\n\necho \" \"\necho \"*****UPDATED FILE******\"\ncat test.txt\necho \"***********************\"\n\nAnd here is the output from the script:\n\n$ cat nvme_test-<jobid>.out\nMon May 17 12:28:18 EDT 2021\n\n*****ORIGINAL FILE*****\nThis is my file. There are many like it but this one is mine.\n***********************\n\n*****UPDATED FILE******\nThis is my file. There are many like it but this one is mine.\nspock25\n***********************\n\n\n\nGetting Help\n\nIf you have problems or need helping running on Spock, please submit a ticket\nby emailing help@olcf.ornl.gov.\n\n\n\nKnown Issues\n\nJIRA_CONTENT_HERE"}
{"doc":"storage","text":"Persistent Storage\n\n\n\nBy design pods are ephemeral. They are meant to be something that can be killed with relatively\nlittle impact to the application. Since containers are ephemeral, we need a way to consume storage\nthat is persistent and can hold data for our applications. One option that Kubernetes has for\nstoring data is with Persistent Volumes. PersistentVolume objects are created by the cluster\nadministrator and reference storage that is on a resilient storage platform such as NetApp. A user\ncan request storage by creating a PersistentVolumeClaim which requests a desired size for a\nPersistentVolume. The cluster administrator or some automated mechanism will provision the storage\non the backend and make it available to the cluster via the PersistentVolumeClaim.\n\nCreating A Persistent Volume Claim\n\nUsing The CLI\n\nOnce the Persistent Volume is created its allocated memory is available to be claimed by a project. This is done through\na Persistent Volume Claim. PVC's are created using a YAML file in the same manner that PV's are created. An example of a\nPVC that claims the PV created in the previous section follows:\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: storage-1\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n\nThe claim is then placed using:\n\noc create -f storage-1.yaml\n\nOnce this command has run the claim will go into the Pending state. Typically it will only be in this state for\na brief amount of time before transitioning to the Bound state. You can check the state of your PVC with the following command:\n\noc get pvc storage-1\n\nThis will return the state of your claim. That output should look something like this once the claim has been bound:\n\nNAME      STATUS      VOLUME                 CAPACITY   ACCESSMODES   STORAGECLASS   AGE\nstorage-1 Bound       openshift-data-v0019   100Gi      RWO,ROX,RWX                  7s\n\nUsing the web GUI\n\nFrom the main screen of a project go to the Storage option in the hamburger menu on the left hand side of the screen,\nthen click Persistent Volume Claims\n\nPersistent Volume Claim Menu\n\nIn the upper right click the Create Persistent Volume Claim button.\n\nCreate Storage\n\nThis will take you to a screen that allows you to select what you need for your PVC. Give your claim a name and\nrequest an amount of storage and click Create. You can also click Edit YAML to edit the PVC object directly.\n\nPV Settings\n\nThis will redirect you to the status page of your new PVC. You should see Bound in the Status field.\n\nPV Status\n\nAdding PVC To Pod\n\nOnce the claim has been granted to the project a pod within that project can access that storage. To do this, edit the\nYAML of the pod and under the volume portion of the file add a name and the name of the claim. It will look similar\nto the following.\n\nvolumes:\n  name: pvol\n  persistentVolumeClaim:\n    claimName: storage-1\n\nThen all that is left to do is mount the storage into a container:\n\nvolumeMounts:\n  - mountPath: /tmp/\n    name: pvol\n\nBelow is a example of a Deployment that mounts a PVC:\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: test-mount-pvc\n  name: test-mount-pvc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: test-mount-pvc\n  template:\n    metadata:\n      labels:\n        app: test-mount-pvc\n    spec:\n      containers:\n      - image: busybox\n        name: busybox\n        volumeMounts:\n        - mountPath: /data\n          name: storage\n      volumes:\n      - name: storage\n        persistentVolumeClaim:\n          claimName: storage-1\n\nAdding PVC To Pod Using Web GUI\n\nTo add the PVC to a pod using the web GUI first select Workloads and then Deployments in the hamburger menu on the left had side.\n\nApplication Deployments\n\nNext, select the deployment that contains the pod you wish to add the storage to.\n\nSelect Actions in the upper left and then and then Add Storage.\n\nEdit YAML\n\nFill out your Mount point and other options if you need them to be non-default values. Otherwise, hit the Add button at the bottom.\n\nAdd Storage Menu\n\nYou should see a green popup appear in the upper right saying that the storage was added. This should additionally trigger a new deployment. To\nmake sure a new deployment happened look at the Created time of the top most deployment.\n\nBackups\n\nA PersistentVolume is backed up by creating a VolumeSnapshot object. The VolumeSnapshot will create a point in time backup of your PersistentVolume\nand is something that you would likely do before an upgrade.\n\nVolumeSnapshots\n\nA Volume Snapshot will bind to a PersistentVolumeClaim. An example PersistentVolumeClaim to bind a VolumeSnapshot to follows:\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: snapshot-pvc\nspec:\n  accessModes:\n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n\nWe can now create a VolumeSnapshot, which will capture the data in the PersistentVolume that was provisioned by the named PersistentVolumeClaim at the time\nthe VolumeSnapshot is created, to backup the data.\n\napiVersion: snapshot.storage.k8s.io/v1beta1\nkind: VolumeSnapshot\nmetadata:\n  # Name of VolumeSnapshot\n  name: pvc-snap\nspec:\n  source:\n    # Name of persistentVolumeClaim to snapshot\n    persistentVolumeClaimName: snapshot-pvc\n\nNow, if the data in the PersistentVolume becomes corrupted or lost in some way, we can create a new PersistentVolume that is identical to the original\nPersistentVolume at the time the VolumeSnapshot was created.\n\nTo do this, create a PersistentVolumeClaim that contains a dataSource field in the Pod's spec. An example follows:\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  # The name of the new Persistent Volume\n  name: restore-pvc\nspec:\n  dataSource:\n    # The name of the Volume Snapshot to retrieve data from\n    name: pvc-snap\n    kind: VolumeSnapshot\n    apiGroup: snapshot.storage.k8s.io\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n\nOnce the PersistentVolume has been created from the above PersistentVolumeClaim it will contain the data from the original PersistentVolume at the time the\nreferenced VolumeSnapshot was created.\n\nCloning\n\nCloning a persistent volume is just as easy as implementing a snapshot. First, find a\nPersistent Volume Claim in the same namespace that you would like to clone for your new\npersistent volume. Then it's as simple as adding the trident.netapp.io/cloneFromPVC annotation\nwith a value of the name of the Persistent Volume Claim you would like to clone.\n\nIn the below example, we clone a persistent volume named source-clone-pvc into a new volume\ncalled destination-clone-pvc\n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  annotations:\n    trident.netapp.io/cloneFromPVC: \"source-clone-pvc\"\n    volume.beta.kubernetes.io/storage-class: \"basic\"\n    trident.netapp.io/splitOnClone: \"true\"\n  name: destination-clone-pvc\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n\nCloning has applications outside of backups such as testing changes on a new Persistent Volume."}
{"doc":"storage_overview","text":"Storage Overview\n\nThis page has been deprecated, and relevant information is now available on data-storage-and-transfers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers>. Please update any bookmarks to use that page.\n\nOLCF users have many options for data storage. Each user has multiple\nuser-affiliated storage spaces, and each project has multiple project-affiliated\nstorage spaces where data can be shared for collaboration.  Below we give an\noverview and explain where each storage area is mounted.\n\nStorage Areas\n\nThe storage area to use in any given situation depends upon the activity you\nwish to carry out. Each user has a User Home area on a Network File System (NFS)\nand a User Archive area on the archival High Performance Storage System (HPSS).\nThese user storage areas are intended to house user-specific files. Each project\nhas a Project Home area on NFS, multiple Work areas on Spectrum Scale, and\nmultiple Archive areas on HPSS. These project storage areas are intended to\nhouse project-centric files. We have defined several areas as listed below by\nfunction:\n\nUser Home: Long-term data for routine access that is unrelated to a\nproject. It is mounted on compute nodes of Summit as read only\n\nUser Archive: A \"link farm\" with symbolic links to a user's project\ndirectories on HPSS. (Previously this was for non-project data on HPSS; such\nuse is now deprecated)\n\nProject Home: Long-term project data for routine access that's shared\nwith other project members. It is mounted on compute nodes of Summit as read\nonly\n\nMember Work: Short-term user data for fast, batch-job access that is not\nshared with other project members.\n\nProject Work: Short-term project data for fast, batch-job access that's\nshared with other project members.\n\nWorld Work: Short-term project data for fast, batch-job access that's\nshared with users outside your project.\n\nMember Archive: Long-term project data for archival access that is not shared\nwith other project members.\n\nProject Archive: Long-term project data for archival access that's shared\nwith other project members.\n\nWorld Archive: Long-term project data for archival access that's shared\nwith users outside your project.\n\nAlpine IBM Spectrum Scale Filesystem\n\nSummit mounts a POSIX-based IBM Spectrum Scale parallel filesystem called\nAlpine. Alpine's maximum capacity is 250 PB. It is consisted of 77 IBM Elastic\nStorage Server (ESS) GL4 nodes running IBM Spectrum Scale 5.x which are called\nNetwork Shared Disk (NSD) servers. Each IBM ESS GL4 node, is a scalable storage\nunit (SSU), constituted by two dual-socket IBM POWER9 storage servers, and a 4X\nEDR InfiniBand network for up to 100Gbit/sec of networking bandwidth.  The\nmaximum performance of the final production system will be about 2.5 TB/s for\nsequential I/O and 2.2 TB/s for random I/O under FPP mode, which means each\nprocess, writes its own file. Metada operations are improved with around to\nminimum 50,000 file access per sec and aggregated up to 2.6 million accesses of\n32KB small files.\n\n\n\nFigure 1. An example of the NDS servers on Summit\n\nPerformance under not ideal workload\n\nThe I/O performance can be lower than the optimal one when you save one single\nshared file with non-optimal I/O pattern. Moreover, the previous performance\nresults are achieved under an ideal system, the system is dedicated, and a\nspecific number of compute nodes are used. The file system is shared across many\nusers; the I/O performance can vary because other users that perform heavy I/O\nas also executing large scale jobs and stress the interconnection network.\nFinally, if the I/O pattern is not aligned, then the I/O performance can be\nsignificantly lower than the ideal one.  Similar, related to the number of the\nconcurrent users, is applied for the metadata operations, they can be lower than\nthe expected performance.\n\nTips\n\nFor best performance on the IBM Spectrum Scale filesystem, use large page\naligned I/O and asynchronous reads and writes. The filesystem blocksize is\n16MB, the minimum fragment size is 16K so when a file under 16K is stored, it\nwill still use 16K of the disk. Writing files of 16 MB or larger, will achieve\nbetter performance. All files are striped across LUNs which are distributed\nacross all IO servers.\n\nIf your application occupies up to two compute nodes and it requires a\nsignificant number of I/O operations, you could try to add the following flag\nin your job script  file and investigate if the total execution time is\ndecreased. This flag could cause worse results, it depends on the application.\n\n#BSUB -alloc_flags maximizegpfs\n\nMajor difference between Lustre and IBM Spectrum Scale\n\nThe file systems have many technical differences, but we will mention only what\na user needs to be familiar with:\n\nOn Summit, there is no concept of striping from the user point of view, the\nuser uses the Alpine storage without the need to declare the striping for\nfiles/directories. The GPFS will handle the workload, the file system was\ntuned during the installation."}
{"doc":"summit_user_guide","text":"Summit User Guide\n\n\n\n\n\nSummit Documentation Resources\n\nIn addition to this Summit User Guide, there are other sources of\ndocumentation, instruction, and tutorials that could be useful for\nSummit users.\n\nThe OLCF Training Archive<training-archive> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#OLCF Training Archive<training-archive>> provides a list of previous training\nevents, including multi-day Summit Workshops. Some examples of topics addressed during\nthese workshops include using Summit's NVME burst buffers, CUDA-aware MPI, advanced\nnetworking and MPI, and multiple ways of programming multiple GPUs per node. You can also\nfind simple tutorials and code examples for some common programming and running tasks in\nour Github tutorial page .\n\n\n\nSystem Overview\n\nSummit is an IBM system located at the Oak Ridge Leadership Computing\nFacility. With a theoretical peak double-precision performance of\napproximately 200 PF, it is one of the most capable systems in the world\nfor a wide range of traditional computational science applications. It\nis also one of the \"smartest\" computers in the world for deep learning\napplications with a mixed-precision capability in excess of 3 EF.\n\n\n\nSummit Nodes\n\nSummit node architecture diagram\n\nThe basic building block of Summit is the IBM Power System AC922 node.\nEach of the approximately 4,600 compute nodes on Summit contains two IBM\nPOWER9 processors and six NVIDIA Tesla V100 accelerators and provides\na theoretical double-precision capability of\napproximately 40 TF. Each POWER9 processor is connected via dual NVLINK\nbricks, each capable of a 25GB/s transfer rate in each direction.\n\nMost Summit nodes contain 512 GB of DDR4 memory for use by the POWER9\nprocessors, 96 GB of High Bandwidth Memory (HBM2) for use by the accelerators,\nand 1.6TB of non-volatile memory that can be used as a burst buffer. A small\nnumber of nodes (54) are configured as \"high memory\" nodes. These nodes contain 2TB of\nDDR4 memory, 192GB of HBM2, and 6.4TB of non-volatile memory.\n\nThe POWER9 processor is built around IBM’s SIMD\nMulti-Core (SMC). The processor provides 22 SMCs with separate 32kB L1\ndata and instruction caches. Pairs of SMCs share a 512kB L2 cache and a\n10MB L3 cache. SMCs support Simultaneous Multi-Threading (SMT) up to a\nlevel of 4, meaning each physical core supports up to 4 hardware-threads <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#hardware-threads>.\n\nThe POWER9 processors and V100\naccelerators are cooled with cold plate technology. The remaining\ncomponents are cooled through more traditional methods, although exhaust\nis passed through a back-of-cabinet heat exchanger prior to being\nreleased back into the room. Both the cold plate and heat exchanger\noperate using medium temperature water which is more cost-effective for\nthe center to maintain than chilled water used by older systems.\n\nNode Types\n\nOn Summit, there are three major types of nodes you will encounter:\nLogin, Launch, and Compute. While all of these are similar in terms of\nhardware (see: summit-nodes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#summit-nodes>), they differ considerably in their intended\nuse.\n\n\nThe table presented below provides a detailed description of the different types of nodes available on Summit, a high-performance computing system. The first type of node is the Login node, which is the initial point of connection for users. This node allows users to write, edit, compile code, manage data, and submit jobs. However, it is important to note that parallel jobs should not be launched from this node, and threaded jobs should not be run on it. This is because login nodes are shared resources that are used by multiple users simultaneously. \n\nThe second type of node is the Launch node, which is where batch scripts or interactive batch jobs are executed. Similar to the service nodes on the previous system, Titan, all commands within the job script or interactive job will run on the Launch node. It is also not recommended to run multiprocessor or threaded programs on this node as it is also a shared resource. However, it is appropriate to launch the jsrun command from the Launch node. \n\nThe majority of nodes on Summit are Compute nodes, which are where parallel jobs are executed. These nodes are accessed through the jsrun command. It is important to note that these nodes are also shared resources, so it is crucial to use them efficiently and not monopolize them for extended periods. \n\n| Node Type | Description |\n| --- | --- |\n| Login | The initial point of connection for users, where they can write, edit, compile code, manage data, and submit jobs. It is a shared resource and should not be used for parallel or threaded jobs. |\n| Launch | Where batch scripts or interactive batch jobs are executed. All commands within the job script or interactive job will run on this node. It is also a shared resource and should not be used for multiprocessor or threaded programs. |\n| Compute | The majority of nodes on Summit, where parallel jobs are executed. They are accessed through the jsrun command and are also shared resources. |\n\n\n\nAlthough the nodes are logically organized into different types, they\nall contain similar hardware. As a result of this homogeneous\narchitecture there is not a need to cross-compile when building on a\nlogin node. Since login nodes have similar hardware resources as compute\nnodes, any tests that are run by your build process (especially by\nutilities such as autoconf and cmake) will have access to the\nsame type of hardware that is on compute nodes and should not require\nintervention that might be required on non-homogeneous systems.\n\nLogin nodes have (2) 16-core Power9 CPUs and (4) V100 GPUs.\nCompute nodes have (2) 22-core Power9 CPUs and (6) V100 GPUs.\n\nSystem Interconnect\n\nSummit nodes are connected to a dual-rail EDR InfiniBand network\nproviding a node injection bandwidth of 23 GB/s. Nodes are\ninterconnected in a Non-blocking Fat Tree topology. This interconnect is\na three-level tree implemented by a switch to connect nodes within each\ncabinet (first level) along with Director switches (second and third\nlevel) that connect cabinets together.\n\nFile Systems\n\nSummit is connected to an IBM Spectrum Scale™ filesystem providing 250PB\nof storage capacity with a peak write speed of 2.5 TB/s. Summit also has\naccess to the center-wide NFS-based filesystem (which provides user and\nproject home areas) and has access to the center’s High Performance\nStorage System (HPSS) for user and project archival storage.\n\nOperating System\n\nSummit is running Red Hat Enterprise Linux (RHEL) version 8.2.\n\n\n\nHardware Threads\n\nThe IBM POWER9 processor supports Hardware Threads. Each of the POWER9’s\nphysical cores has 4 “slices”. These slices provide Simultaneous Multi\nThreading (SMT) support within the core. Three SMT modes are supported:\nSMT4, SMT2, and SMT1. In SMT4 mode, each of the slices operates\nindependently of the other three. This would permit four separate\nstreams of execution (i.e. OpenMP threads or MPI tasks) on each physical\ncore. In SMT2 mode, pairs of slices work together to run tasks. Finally,\nin SMT1 mode the four slices work together to execute the task/thread\nassigned to the physical core. Regardless of the SMT mode used, the four\nslices share the physical core’s L1 instruction & data caches.\nhttps://vimeo.com/283756938\n\n\n\nGPUs\n\n<string>:161: (INFO/1) Duplicate implicit target name: \"gpus\".\n\nEach Summit Compute node has 6 NVIDIA V100 GPUs.  The NVIDIA Tesla V100\naccelerator has a peak performance of 7.8 TFLOP/s (double-precision) and\ncontributes to a majority of the computational work performed on Summit. Each\nV100 contains 80 streaming multiprocessors (SMs), 16 GB (32 GB on high-memory\nnodes) of high-bandwidth memory (HBM2), and a 6 MB L2 cache that is available to\nthe SMs. The GigaThread Engine is responsible for distributing work among the\nSMs and (8) 512-bit memory controllers control access to the 16 GB (32 GB on\nhigh-memory nodes) of HBM2 memory. The V100 uses NVIDIA's NVLink interconnect\nto pass data between GPUs as well as from CPU-to-GPU. We provide a more in-depth\nlook into the NVIDIA Tesla V100 later in the Summit Guide.\n\n\n\nConnecting\n\n<string>:181: (INFO/1) Duplicate implicit target name: \"connecting\".\n\nTo connect to Summit, ssh to summit.olcf.ornl.gov. For example:\n\nssh username@summit.olcf.ornl.gov\n\nFor more information on connecting to OLCF resources, see connecting-to-olcf <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#connecting-to-olcf>.\n\nData and Storage\n\nFor more information about center-wide file systems and data archiving available\non Summit, please refer to the pages on data-storage-and-transfers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers>.\n\nEach compute node on Summit has a 1.6TB Non-Volatile Memory (NVMe) storage device (high-memory nodes have a 6.4TB NVMe storage device), colloquially known as a \"Burst Buffer\" with\ntheoretical performance peak of 2.1 GB/s for writing and 5.5 GB/s for reading.\nThe NVMes could be used to reduce the time that applications wait for\nI/O.  More information can be found later in the Burst Buffer section.\n\n\n\nSoftware\n\n<string>:208: (INFO/1) Duplicate implicit target name: \"software\".\n\nVisualization and analysis tasks should be done on the Andes cluster. There are a\nfew tools provided for various visualization tasks, as described in the\nandes-viz-tools <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-viz-tools> section of the andes-user-guide <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-user-guide>.\n\nFor a full list of software available at the OLCF, please see the\nSoftware section (coming soon).\n\n\n\nShell & Programming Environments\n\nOLCF systems provide many software packages and scientific\nlibraries pre-installed at the system-level for users to take advantage\nof. To facilitate this, environment management tools are employed to\nhandle necessary changes to the shell. The sections below provide\ninformation about using these management tools on Summit.\n\nDefault Shell\n\nA user’s default shell is selected when completing the User Account\nRequest form. The chosen shell is set across all OLCF resources, and is\nthe shell interface a user will be presented with upon login to any OLCF\nsystem. Currently, supported shells include:\n\nbash\n\ntcsh\n\ncsh\n\nksh\n\nIf you would like to have your default shell changed, please contact the\nOLCF User Assistance Center at\nhelp@nccs.gov.\n\n\n\nEnvironment Management with Lmod\n\nEnvironment modules are provided through Lmod, a Lua-based module system for\ndynamically altering shell environments. By managing changes to the shell’s\nenvironment variables (such as PATH, LD_LIBRARY_PATH, and\nPKG_CONFIG_PATH), Lmod allows you to alter the software available in your\nshell environment without the risk of creating package and version combinations\nthat cannot coexist in a single environment.\n\nLmod is a recursive environment module system, meaning it is aware of module\ncompatibility and actively alters the environment to protect against conflicts.\nMessages to stderr are issued upon Lmod implicitly altering the environment.\nEnvironment modules are structured hierarchically by compiler family such that\npackages built with a given compiler will only be accessible if the compiler\nfamily is first present in the environment.\n\nLmod can interpret both Lua modulefiles and legacy Tcl\nmodulefiles. However, long and logic-heavy Tcl modulefiles may require\nporting to Lua.\n\nGeneral Usage\n\nTypical use of Lmod is very similar to that of interacting with\nmodulefiles on other OLCF systems. The interface to Lmod is provided by\nthe module command:\n\n\nThe table presented in this user guide pertains to the various commands that can be used to manage modules on the Summit supercomputer. The first command, \"module -t list\", displays a concise list of all the modules currently loaded on the system. The next command, \"module avail\", shows a table of all the available modules that can be loaded. The \"module help <modulename>\" command provides detailed information about a specific module, including its purpose and usage. Similarly, the \"module show <modulename>\" command displays the changes made to the environment by a particular module. The \"module spider <string>\" command allows users to search for modules based on a specific string. The \"module load <modulename> [...]\" command is used to load one or more modules into the current environment. The \"module use <path>\" command adds a specified path to the modulefile search cache and MODULESPATH, making it easier to access modules. Conversely, the \"module unuse <path>\" command removes a path from the modulefile search cache and MODULESPATH. The \"module purge\" command unloads all currently loaded modules, while the \"module reset\" command resets all loaded modules to their system defaults. Finally, the \"module update\" command reloads all currently loaded modules, ensuring that the latest versions are being used. These commands provide users with a comprehensive set of tools to effectively manage and utilize modules on the Summit supercomputer.\n\n| Command | Description |\n| --- | --- |\n| module -t list | Shows a terse list of the currently loaded modules. |\n| module avail | Shows a table of the currently available modules. |\n| module help <modulename> | Shows help information about <modulename>. |\n| module show <modulename> | Shows the environment changes made by the <modulename> modulefile. |\n| module spider <string> | Searches all possible modules according to <string>. |\n| module load <modulename> [...] | Loads the given <modulename>(s) into the current environment. |\n| module use <path> | Adds <path> to the modulefile search cache and MODULESPATH. |\n| module unuse <path> | Removes <path> from the modulefile search cache and MODULESPATH. |\n| module purge | Unloads all modules. |\n| module reset | Resets loaded modules to system defaults. |\n| module update | Reloads all currently loaded modules. |\n\n\n\nModules are changed recursively. Some commands, such as\nmodule swap, are available to maintain compatibility with scripts\nusing Tcl Environment Modules, but are not necessary since Lmod\nrecursively processes loaded modules and automatically resolves\nconflicts.\n\nSearching for modules\n\nModules with dependencies are only available when the underlying dependencies,\nsuch as compiler families, are loaded. Thus, module avail will only display\nmodules that are compatible with the current state of the environment. To search\nthe entire hierarchy across all possible dependencies, the spider\nsub-command can be used as summarized in the following table.\n\n\nThe table presents a list of commands and their corresponding descriptions for the Summit user guide. The first command, \"module spider\", allows users to view the entire possible graph of modules available on Summit. This can be useful for users who are not familiar with the available modules and want to explore their options. The second command, \"module spider <modulename>\", allows users to search for a specific module by name in the graph of possible modules. This can be helpful for users who know the name of the module they need and want to quickly find it. The third command, \"module spider <modulename>/<version>\", allows users to search for a specific version of a module in the graph of possible modules. This can be useful for users who need a specific version of a module for their work. The last command, \"module spider <string>\", allows users to search for modulefiles containing a specific string. This can be helpful for users who are looking for modules related to a certain topic or keyword. Overall, these commands provide users with efficient ways to navigate and search for modules on Summit. \n\n| Command         | Description                                                  |\n| --------------- | ------------------------------------------------------------ |\n| module spider   | Shows the entire possible graph of modules                   |\n| module spider <modulename> | Searches for modules named <modulename> in the graph of possible modules |\n| module spider <modulename>/<version> | Searches for a specific version of <modulename> in the graph of possible modules |\n| module spider <string> | Searches for modulefiles containing <string>                 |\n\n\n\nDefining custom module collections\n\nLmod supports caching commonly used collections of environment modules on a\nper-user basis in $HOME/.lmod.d. To create a collection called \"NAME\" from\nthe currently loaded modules, simply call module save NAME. Omitting \"NAME\"\nwill set the user’s default collection. Saved collections can be recalled and\nexamined with the commands summarized in the following table.\n\n\nThe table presents a list of commands that can be used in the Summit user guide. The first command, \"module restore NAME\", allows users to recall a specific saved user collection by providing the name of the collection. This is useful for quickly accessing a frequently used collection. The second command, \"module restore\", recalls the user-defined defaults, which are the settings and configurations that the user has previously saved. The third command, \"module reset\", resets all loaded modules to the system defaults, which are the default settings and configurations set by the system. The fourth command, \"module restore system\", recalls the system defaults, which can be useful for reverting back to the original settings. Lastly, the \"module savelist\" command shows a list of all the user-defined saved collections, providing an easy way for users to keep track of their saved collections. Overall, these commands offer users flexibility and convenience in managing their settings and collections in the Summit user guide.\n\n| Command | Description |\n| --- | --- |\n| module restore NAME | Recalls a specific saved user collection titled \"NAME\" |\n| module restore | Recalls the user-defined defaults |\n| module reset | Resets loaded modules to system defaults |\n| module restore system | Recalls the system defaults |\n| module savelist | Shows the list user-defined saved collections |\n\n\n\nYou should use unique names when creating collections to\nspecify the application (and possibly branch) you are working on. For\nexample, app1-development, app1-production, and\napp2-production.\n\nIn order to avoid conflicts between user-defined collections\non multiple compute systems that share a home file system (e.g.\n/ccs/home/[userid]), lmod appends the hostname of each system to the\nfiles saved in in your ~/.lmod.d directory (using the environment\nvariable LMOD_SYSTEM_NAME). This ensures that only collections\nappended with the name of the current system are visible.\n\nThe following screencast shows an example of setting up user-defined\nmodule collections on Summit. https://vimeo.com/293582400\n\n\n\nCompiling\n\n<string>:376: (INFO/1) Duplicate implicit target name: \"compiling\".\n\nCompilers\n\nAvailable Compilers\n\nThe following compilers are available on Summit:\n\nXL: IBM XL Compilers (loaded by default)\n\nLLVM: LLVM compiler infrastructure\n\nPGI: Portland Group compiler suite\n\nNVHPC: Nvidia HPC SDK compiler suite\n\nGNU: GNU Compiler Collection\n\nNVCC: CUDA C compiler\n\nPGI was bought out by Nvidia and have rebranded their compilers, incorporating\nthem into the NVHPC compiler suite. There will be no more new releases of the\nPGI compilers.\n\nUpon login, the default versions of the XL compiler suite and Spectrum Message\nPassing Interface (MPI) are added to each user's environment through the modules\nsystem. No changes to the environment are needed to make use of the defaults.\n\nMultiple versions of each compiler family are provided, and can be inspected\nusing the modules system:\n\nsummit$ module -t avail gcc\n/sw/summit/spack-envs/base/modules/site/Core:\ngcc/7.5.0\ngcc/9.1.0\ngcc/9.3.0\ngcc/10.2.0\ngcc/11.1.0\n\nC compilation\n\ntype char is unsigned by default\n\n\nThe table above provides a comprehensive overview of the different compilers and their corresponding settings for the Summit user guide. The first column lists the vendors, which include IBM, GNU, LLVM, PGI, and NVHPC. The second column specifies the module used for each vendor, such as xl for IBM, system default for GNU, llvm for LLVM, pgi for PGI, and nvhpc for NVHPC. The third column indicates the compiler used, which includes xlc and xlc_r for IBM, gcc for GNU, clang for LLVM, and pgcc and nvc for PGI and NVHPC, respectively. The next three columns, namely Enable C99, Enable C11, and Default signed char, specify the compiler options for each vendor, with the default being -std=gnu99 for C99 and -std=gnu11 for C11, and -qchar=signed for IBM and -fsigned-char for GNU, LLVM, PGI, and NVHPC. Lastly, the Define macro column shows the macro used for each vendor, with IBM using -WF,-D and the rest using just -D. This table serves as a useful reference for users of Summit, providing them with the necessary information to select the appropriate compiler and settings for their needs.\n\n| Vendor | Module | Compiler | Enable C99 | Enable C11 | Default signed char | Define macro |\n|--------|--------|----------|------------|------------|---------------------|--------------|\n| IBM    | xl     | xlc xlc_r| -std=gnu99 | -std=gnu11 | -qchar=signed       | -WF,-D       |\n| GNU    | system default | gcc | -std=gnu99 | -std=gnu11 | -fsigned-char       | -D           |\n| GNU    | gcc    | gcc      | -std=gnu99 | -std=gnu11 | -fsigned-char       | -D           |\n| LLVM   | llvm   | clang    | default    | -std=gnu11 | -fsigned-char       | -D           |\n| PGI    | pgi    | pgcc     | -c99       | -c11       | -Mschar             | -D           |\n| NVHPC  | nvhpc  | nvc      | -c99       | -c11       | -Mschar             | -D           |\n\n\n\nC++ compilations\n\ntype char is unsigned by default\n\n\nThe table provides a comparison of different vendors and their respective modules and compilers for the Summit user guide. The first column lists the vendors, which include IBM, GNU, LLVM, PGI, and NVHPC. The second column specifies the module used by each vendor, such as xl for IBM, system default for GNU, llvm for LLVM, pgi for PGI, and nvhpc for NVHPC. The third column lists the compiler used by each vendor, which includes xlc++, xlc++_r for IBM, g++ for GNU, clang++ for LLVM, pgc++ for PGI, and nvc++ for NVHPC. The fourth and fifth columns indicate whether C++11 and C++14 are enabled for each vendor, with the option -std=gnu++11 and -std=gnu++1y, respectively. The sixth column specifies the default signed char for each vendor, with the option -qchar=signed for IBM and -fsigned-char for GNU, LLVM, PGI, and NVHPC. The last column lists the define macro for each vendor, with the option -WF,-D for IBM and -D for GNU, LLVM, PGI, and NVHPC. This table serves as a useful reference for users to understand the different options and configurations available for C++11 and C++14 when using the Summit user guide. \n\n| Vendor | Module | Compiler | Enable C++11 | Enable C++14 | Default signed char | Define macro |\n|--------|--------|----------|--------------|--------------|---------------------|--------------|\n| IBM    | xl     | xlc++, xlc++_r | -std=gnu++11 | -std=gnu++1y (PARTIAL)* | -qchar=signed | -WF,-D |\n| GNU    | system default | g++ | -std=gnu++11 | -std=gnu++1y | -fsigned-char | -D |\n| GNU    | gcc    | g++      | -std=gnu++11 | -std=gnu++1y | -fsigned-char | -D |\n| LLVM   | llvm   | clang++  | -std=gnu++11 | -std=gnu++1y | -fsigned-char | -D |\n| PGI    | pgi    | pgc++    | -std=c++11 -gnu_extensions | -std=c++14 -gnu_extensions | -Mschar | -D |\n| NVHPC  | nvhpc  | nvc++    | -std=c++11 -gnu_extensions | -std=c++14 -gnu_extensions | -Mschar | -D |\n\n\n\nFortran compilation\n\n\nThe table provides a comprehensive overview of the different vendors, modules, and compilers that can be used with the Summit user guide. The first column lists the different vendors, including IBM, GNU, LLVM, PGI, and NVHPC. The second column specifies the module or compiler used by each vendor, such as xl, gfortran, xlflang, pgfortran, and nvfortran. The third column indicates the specific compiler options that need to be enabled for each vendor, including -qlanglvl=90std, -std=f90, and -D. The next three columns, labeled \"Enable F90,\" \"Enable F2003,\" and \"Enable F2008,\" specify whether the vendor's compiler supports the corresponding Fortran language versions. The final column, \"Define macro,\" lists any additional macros that need to be defined for each vendor. This table serves as a useful reference for users of the Summit user guide, providing them with the necessary information to select the appropriate vendor, module, and compiler for their needs. \n\n| Vendor | Module | Compiler | Enable F90 | Enable F2003 | Enable F2008 | Define macro |\n|--------|--------|----------|------------|--------------|--------------|--------------|\n| IBM    | xl     | xlf xlf90 xlf95 xlf2003 xlf2008 | -qlanglvl=90std | -qlanglvl=2003std | -qlanglvl=2008std | -WF,-D |\n| GNU    | system default | gfortran | -std=f90 | -std=f2003 | -std=f2008 | -D |\n| LLVM   | llvm   | xlflang  | n/a        | n/a          | n/a          | -D |\n| PGI    | pgi    | pgfortran | use .F90 source file suffix | use .F03 source file suffix | use .F08 source file suffix | -D |\n| NVHPC  | nvhpc  | nvfortran | use .F90 source file suffix | use .F03 source file suffix | use .F08 source file suffix | -D |\n\n\n\nThe xlflang module currently conflicts with the clang\nmodule. This restriction is expected to be lifted in future releases.\n\nMPI\n\nMPI on Summit is provided by IBM Spectrum MPI. Spectrum MPI provides compiler\nwrappers that automatically choose the proper compiler to build your\napplication.\n\nThe following compiler wrappers are available:\n\nC: mpicc\n\nC++: mpic++, mpiCC\n\nFortran: mpifort, mpif77, mpif90\n\nWhile these wrappers conveniently abstract away linking of Spectrum MPI, it's\nsometimes helpful to see exactly what's happening when invoked. The --showme\nflag will display the full link lines, without actually compiling:\n\nsummit$ mpicc --showme\n/sw/summit/xl/16.1.1-10/xlC/16.1.1/bin/xlc_r -I/sw/summit/spack-envs/base/opt/linux-rhel8-ppc64le/xl-16.1.1-10/spectrum-mpi-10.4.0.3-20210112-v7qymniwgi6mtxqsjd7p5jxinxzdkhn3/include -pthread -L/sw/summit/spack-envs/base/opt/linux-rhel8-ppc64le/xl-16.1.1-10/spectrum-mpi-10.4.0.3-20210112-v7qymniwgi6mtxqsjd7p5jxinxzdkhn3/lib -lmpiprofilesupport -lmpi_ibm\n\nOpenMP\n\nWhen using OpenMP with IBM XL compilers, the thread-safe\ncompiler variant is required; These variants have the same name as the\nnon-thread-safe compilers with an additional _r suffix. e.g. to\ncompile OpenMPI C code one would use xlc_r\n\nOpenMP offloading support is still under active development.\nPerformance and debugging capabilities in particular are expected to\nimprove as the implementations mature.\n\n\nThe following table provides a comprehensive overview of the support for OpenMP and OpenMP 4.x in various vendor compilers, as well as the corresponding commands to enable them. OpenMP is a widely used programming model for shared-memory multiprocessing, while OpenMP 4.x introduces support for offloading computations to accelerators such as GPUs. The first column lists the different vendors, including IBM, GNU, clang, xlflang, PGI, and NVHPC. The second and fourth columns indicate the level of support for OpenMP and OpenMP 4.x, respectively, with \"FULL\" indicating full support and \"PARTIAL\" indicating partial support. The third and fifth columns specify the commands needed to enable OpenMP and OpenMP 4.x, respectively, with \"-qsmp=omp\" and \"-qsmp=omp -qoffload\" being the commands for IBM, \"-fopenmp\" and \"-fopenmp -fopenmp-targets=nvptx64-nvidia-cuda --cuda-path=${OLCF_CUDA_ROOT}\" for GNU and clang, and \"-fopenmp\" and \"-fopenmp -fopenmp-targets=nvptx64-nvidia-cuda\" for xlflang. It is worth noting that PGI and NVHPC do not support OpenMP 4.x, as indicated by \"NONE\" in the fourth and fifth columns. Overall, this table serves as a useful reference for users looking to utilize OpenMP and OpenMP 4.x in their programming projects. \n\n| Vendor | 3.1 Support | Enable OpenMP | 4.x Support | Enable OpenMP 4.x Offload |\n|--------|-------------|---------------|-------------|---------------------------|\n| IBM    | FULL        | -qsmp=omp     | FULL        | -qsmp=omp -qoffload       |\n| GNU    | FULL        | -fopenmp      | PARTIAL     | -fopenmp                  |\n| clang  | FULL        | -fopenmp      | PARTIAL     | -fopenmp -fopenmp-targets=nvptx64-nvidia-cuda --cuda-path=${OLCF_CUDA_ROOT} |\n| xlflang| FULL        | -fopenmp      | PARTIAL     | -fopenmp -fopenmp-targets=nvptx64-nvidia-cuda |\n| PGI    | FULL        | -mp           | NONE        | NONE                      |\n| NVHPC  | FULL        | -mp=gpu       | NONE        | NONE                      |\n\n\n\nOpenACC\n\n\nThe table above provides a comprehensive overview of the OpenACC support and enablement options for various vendors and modules in the Summit user guide. The first column lists the different vendors, including IBM, GNU, LLVM, PGI, and NVHPC. The second column specifies the specific module for each vendor, such as xl, system default, gcc, clang, xlflang, and nvhpc. The third column indicates the level of OpenACC support for each module, with options ranging from NONE to version 2.5. Finally, the last column outlines the specific command or flag needed to enable OpenACC for each module, such as -fopenacc, -acc, -ta=nvidia:cc70, and -acc=gpu -gpu=cc70. This table serves as a useful reference for users looking to utilize OpenACC in their programming on the Summit supercomputer. \n\n| Vendor | Module | OpenACC Support | Enable OpenACC |\n|--------|--------|-----------------|----------------|\n| IBM    | xl     | NONE            | NONE           |\n| GNU    | system default | NONE     | NONE           |\n| GNU    | gcc    | 2.5             | -fopenacc      |\n| LLVM   | clang or xlflang | NONE   | NONE           |\n| PGI    | pgi    | 2.5             | -acc, -ta=nvidia:cc70 |\n| NVHPC  | nvhpc  | 2.5             | -acc=gpu -gpu=cc70 |\n\n\n\nCUDA compilation\n\nNVIDIA\n\nCUDA C/C++ support is provided through the cuda module or throught the nvhpc module.\n\nnvcc : Primary CUDA C/C++ compiler\n\nLanguage support\n\n-std=c++11 : provide C++11 support\n\n--expt-extended-lambda : provide experimental host/device lambda support\n\n--expt-relaxed-constexpr : provide experimental host/device constexpr support\n\nCompiler support\n\nNVCC currently supports XL, GCC, and PGI C++ backends.\n\n--ccbin : set to host compiler location\n\nCUDA Fortran compilation\n\nIBM\n\nThe IBM compiler suite is made available through the default loaded xl\nmodule, the cuda module is also required.\n\nxlcuf : primary Cuda fortran compiler, thread safe\n\nLanguage support flags\n\n-qlanglvl=90std : provide Fortran90 support\n\n-qlanglvl=95std : provide Fortran95 support\n\n-qlanglvl=2003std : provide Fortran2003 support\n\n-qlanglvl=2008std : provide Fortran2003 support\n\nPGI\n\nThe PGI compiler suite is available through the pgi module.\n\npgfortran : Primary fortran compiler with CUDA Fortran support\n\nLanguage support:\n\nFiles with .cuf suffix automatically compiled with cuda fortran support\n\nStandard fortran suffixed source files determines the standard involved,\nsee the man page for full details\n\n-Mcuda : Enable CUDA Fortran on provided source file\n\nLinking in Libraries\n\nOLCF systems provide many software packages and scientific\nlibraries pre-installed at the system-level for users to take advantage\nof. In order to link these libraries into an application, users must\ndirect the compiler to their location. The module show command can\nbe used to determine the location of a particular library. For example\n\nsummit$ module show essl\n------------------------------------------------------------------------------------\n   /sw/summit/modulefiles/core/essl/6.1.0-1:\n------------------------------------------------------------------------------------\nwhatis(\"ESSL 6.1.0-1 \")\nprepend_path(\"LD_LIBRARY_PATH\",\"/sw/summit/essl/6.1.0-1/essl/6.1/lib64\")\nappend_path(\"LD_LIBRARY_PATH\",\"/sw/summit/xl/16.1.1-beta4/lib\")\nprepend_path(\"MANPATH\",\"/sw/summit/essl/6.1.0-1/essl/6.1/man\")\nsetenv(\"OLCF_ESSL_ROOT\",\"/sw/summit/essl/6.1.0-1/essl/6.1\")\nhelp([[ESSL 6.1.0-1\n\n]])\n\nWhen this module is loaded, the $OLCF_ESSL_ROOT environment variable\nholds the path to the ESSL installation, which contains the lib64/ and\ninclude/ directories:\n\nsummit$ module load essl\nsummit$ echo $OLCF_ESSL_ROOT\n/sw/summit/essl/6.1.0-1/essl/6.1\nsummit$ ls $OLCF_ESSL_ROOT\nFFTW3  READMES  REDIST.txt  include  iso-swid  ivps  lap  lib64  man  msg\n\nThe following screencast shows an example of linking two libraries into\na simple program on Summit. https://vimeo.com/292015868\n\n\n\nRunning Jobs\n\nAs is the case on other OLCF systems, computational work on Summit is\nperformed within jobs. A typical job consists of several components:\n\nA submission script\n\nAn executable\n\nInput files needed by the executable\n\nOutput files created by the executable\n\nIn general, the process for running a job is to:\n\nPrepare executables and input files\n\nWrite the batch script\n\nSubmit the batch script\n\nMonitor the job's progress before and during execution\n\nThe following sections will provide more information regarding running\njobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as\nthe batch scheduling system.\n\n\n\nLogin, Launch, and Compute Nodes\n\nRecall from the system-overview <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#system-overview>\nsection that Summit has three types of nodes: login, launch, and\ncompute. When you log into the system, you are placed on a login node.\nWhen your batch-scripts <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#batch-scripts> or interactive-jobs <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#interactive-jobs> run,\nthe resulting shell will run on a launch node. Compute nodes are accessed\nvia the jsrun command. The jsrun command should only be issued\nfrom within an LSF job (either batch or interactive) on a launch node.\nOtherwise, you will not have any compute nodes allocated and your parallel\njob will run on the login node. If this happens, your job will interfere with\n(and be interfered with by) other users' login node tasks. jsrun is covered\nin-depth in the Job Launcher (jsrun) section.\n\nPer-User Login Node Resource Limits\n\nBecause the login nodes are resources shared by all Summit users, we utilize\ncgroups to help better ensure resource availability for all users of the\nshared nodes. By default each user is limited to 16 hardware-threads, 16GB\nof memory, and 1 GPU.  Please note that limits are set per user and not\nindividual login sessions. All user processes on a node are contained within a\nsingle cgroup and share the cgroup's limits.\n\nIf a process from any of a user’s login sessions reaches 4 hours of CPU-time,\nall login sessions will be limited to .5 hardware-thread. After 8 hours of\nCPU-time, the process is automatically killed. To reset the cgroup limits on a\nnode to default once the 4 hour CPU-time reduction has been reached, kill the\noffending process and start a new login session to the node.\n\nUsers can run command check_cgroup_user on login nodes to check what processes\nwere recently killed by cgroup limits.\n\nLogin node limits are set per user and not per individual login\nsession.  All user processes on a node are contained within a single cgroup\nand will share the cgroup's limits.\n\n\n\nBatch Scripts\n\nThe most common way to interact with the batch system is via batch jobs.\nA batch job is simply a shell script with added directives to request\nvarious resources from or provide certain information to the batch\nscheduling system. Aside from the lines containing LSF options, the\nbatch script is simply the series commands needed to set up and run your\njob.\n\nTo submit a batch script, use the bsub command: bsub myjob.lsf\n\nIf you’ve previously used LSF, you’re probably used to submitting a job\nwith input redirection (i.e. bsub < myjob.lsf). This is not needed\n(and will not work) on Summit.\n\nAs an example, consider the following batch script:\n\n#!/bin/bash\n# Begin LSF Directives\n#BSUB -P ABC123\n#BSUB -W 3:00\n#BSUB -nnodes 2048\n#BSUB -alloc_flags gpumps\n#BSUB -J RunSim123\n#BSUB -o RunSim123.%J\n#BSUB -e RunSim123.%J\n\ncd $MEMBERWORK/abc123\ncp $PROJWORK/abc123/RunData/Input.123 ./Input.123\ndate\njsrun -n 4092 -r 2 -a 12 -g 3 ./a.out\ncp my_output_file /ccs/proj/abc123/Output.123\n\nFor Moderate Enhanced Projects, job scripts need to add \"-l\" (\"ell\") to the shell specification, similar to interactive usage.\n\n\nThe table provides a detailed description of the options and requirements for running a job on the Summit supercomputer. The first line specifies that the script will run with bash as the shell and suggests adding -l to the shell specification for moderate enhanced projects. The second line is a comment line. The third line states that the job will charge to the ABC123 project, which is a required field. The fourth line specifies the maximum walltime for the job, which is 3 hours. The fifth line states that the job will use 2,048 compute nodes, which is also a required field. The next three lines are optional and provide the option to enable GPU Multi-Process Service, name the job as RunSim123, and write the standard output and error to specific files. The tenth line is a blank line. The next three lines provide instructions for changing into the scratch filesystem, copying input files into place, and running the date command to write a timestamp to the standard output file. The fourteenth line instructs the executable to run on the allocated compute nodes. Finally, the fifteenth line states to copy the output files from the scratch area into a more permanent location. This table serves as a guide for users to properly set up and run their jobs on the Summit supercomputer.\n\n| Line # | Option | Description |\n|--------|--------|-------------|\n| 1      |        | Shell specification. This script will run under with bash as the shell. Moderate enhanced projects should add -l (\"ell\") to the shell specification. |\n| 2      |        | Comment line |\n| 3      | Required | This job will charge to the ABC123 project |\n| 4      | Required | Maximum walltime for the job is 3 hours |\n| 5      | Required | The job will use 2,048 compute nodes |\n| 6      | Optional | Enable GPU Multi-Process Service |\n| 7      | Optional | The name of the job is RunSim123 |\n| 8      | Optional | Write standard output to a file named RunSim123.#, where # is the job ID assigned by LSF |\n| 9      | Optional | Write standard error to a file named RunSim123.#, where # is the job ID assigned by LSF |\n| 10     |        | Blank line |\n| 11     |        | Change into one of the scratch filesystems |\n| 12     |        | Copy input files into place |\n| 13     |        | Run the date command to write a timestamp to the standard output file |\n| 14     |        | Run the executable on the allocated compute nodes |\n| 15     |        | Copy output files from the scratch area into a more permanent location |\n\n\n\n\n\nInteractive Jobs\n\nMost users will find batch jobs to be the easiest way to interact with\nthe system, since they permit you to hand off a job to the scheduler and\nthen work on other tasks; however, it is sometimes preferable to run\ninteractively on the system. This is especially true when developing,\nmodifying, or debugging a code.\n\nSince all compute resources are managed/scheduled by LSF, it is not possible\nto simply log into the system and begin running a parallel code interactively.\nYou must request the appropriate resources from the system and, if necessary,\nwait until they are available. This is done with an “interactive batch” job.\nInteractive batch jobs are submitted via the command line, which\nsupports the same options that are passed via #BSUB parameters in a\nbatch script. The final options on the command line are what makes the\njob “interactive batch”: -Is followed by a shell name. For example,\nto request an interactive batch job (with bash as the shell) equivalent\nto the sample batch script above, you would use the command:\nbsub -W 3:00 -nnodes 2048 -P ABC123 -Is /bin/bash\n\nAs pointed out in login-launch-and-compute-nodes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#login-launch-and-compute-nodes>, you will be placed on\na launch (a.k.a. \"batch\") node upon launching an interactive job and as usual\nneed to use jsrun to access the compute node(s):\n\n$ bsub -Is -W 0:10 -nnodes 1 -P STF007 $SHELL\nJob <779469> is submitted to default queue <batch>.\n<<Waiting for dispatch ...>>\n<<Starting on batch2>>\n\n$ hostname\nbatch2\n\n$ jsrun -n1 hostname\na35n03\n\nCommon bsub Options\n\nThe table below summarizes options for submitted jobs. Unless otherwise\nnoted, these can be used from batch scripts or interactive jobs. For\ninteractive jobs, the options are simply added to the bsub command\nline. For batch scripts, they can either be added on the bsub\ncommand line or they can appear as a #BSUB directive in the batch\nscript. If conflicting options are specified (i.e. different walltime\nspecified on the command line versus in the script), the option on the\ncommand line takes precedence. Note that LSF has numerous options; only\nthe most common ones are described here. For more in-depth information\nabout other LSF options, see the bsub man page.\n\n\nThe table provides a comprehensive guide for users on how to properly use the Summit supercomputer. It includes various options that can be specified when submitting a job to the system, along with their corresponding example usage and a brief description. The first option, -W, allows users to specify the maximum walltime for their job, with the format being [hours:]minutes. The -nnodes option allows users to specify the number of nodes they need for their job. The -P option is used to specify the project to which the job should be charged. The -o and -e options are used to direct the job's standard output and standard error to specific files, with the job ID number being automatically replaced in the file name. The -J option allows users to specify a name for their job, and the -w option allows them to place a dependency on another job. The -N option enables users to receive a job report via email when their job completes. The -XF option is used for X11 forwarding. Finally, the -alloc_flags option is used to request GPU Multi-Process Service (MPS) and set SMT (Simultaneous Multithreading) levels. Users can specify multiple alloc_flags options by enclosing them in quotes and separating them with spaces. The table also provides examples of how to use these options, making it easier for users to understand and utilize them effectively.\n\n| Option | Example Usage | Description |\n| ------ | ------------- | ----------- |\n| -W | #BSUB -W 50 | Requested maximum walltime. NOTE: The format is [hours:]minutes, not [[hours:]minutes:]seconds like PBS/Torque/Moab |\n| -nnodes | #BSUB -nnodes 1024 | Number of nodes. NOTE: This is specified with only one hyphen (i.e. -nnodes, not --nnodes) |\n| -P | #BSUB -P ABC123 | Specifies the project to which the job should be charged |\n| -o | #BSUB -o jobout.%J | File into which job STDOUT should be directed (%J will be replaced with the job ID number). If you do not also specify a STDERR file with -e or -eo, STDERR will also be written to this file. |\n| -e | #BSUB -e jobout.%J | File into which job STDERR should be directed (%J will be replaced with the job ID number) |\n| -J | #BSUB -J MyRun123 | Specifies the name of the job (if not present, LSF will use the name of the job script as the job's name) |\n| -w | #BSUB -w ended() | Place a dependency on the job |\n| -N | #BSUB -N | Send a job report via email when the job completes |\n| -XF | #BSUB -XF | Use X11 forwarding |\n| -alloc_flags | #BSUB -alloc_flags \"gpumps smt1\" | Used to request GPU Multi-Process Service (MPS) and to set SMT (Simultaneous Multithreading) levels. Only one \"#BSUB alloc_flags\" command is recognized, so multiple alloc_flags options need to be enclosed in quotes and space-separated. Setting gpumps enables NVIDIA's Multi-Process Service, which allows multiple MPI ranks to simultaneously access a GPU. Setting smtn (where n is 1, 2, or 4) sets different SMT levels. To run with 2 hardware threads per physical core, you'd use smt2. The default level is smt4. |\n\n\n\nAllocation-wide Options\n\nThe -alloc_flags option to bsub is used to set allocation-wide options.\nThese settings are applied to every compute node in a job. Only one instance of\nthe flag is accepted, and multiple alloc_flags values should be enclosed in\nquotes and space-separated. For example, -alloc_flags \"gpumps smt1.\n\nThe most common values (smt{1,2,4}, gpumps, gpudefault) are detailed in\nthe following sections.\n\nThis option can also be used to provide additional resources to GPFS service\nprocesses, described in the GPFS System Service Isolation section.\n\nHardware Threads\n\n<string>:920: (INFO/1) Duplicate implicit target name: \"hardware threads\".\n\nHardware threads are a feature of the POWER9 processor through which\nindividual physical cores can support multiple execution streams,\nessentially looking like one or more virtual cores (similar to\nhyperthreading on some Intel      microprocessors). This feature is often\ncalled Simultaneous Multithreading or SMT. The POWER9 processor on\nSummit supports SMT levels of 1, 2, or 4, meaning (respectively) each\nphysical core looks like 1, 2, or 4 virtual cores. The SMT level is\ncontrolled by the -alloc_flags option to bsub. For example, to\nset the SMT level to 2, add the line #BSUB –alloc_flags smt2 to your\nbatch script or add the option -alloc_flags smt2 to you bsub\ncommand line.\n\nThe default SMT level is 4.\n\nMPS\n\nThe Multi-Process Service (MPS) enables multiple processes (e.g. MPI\nranks) to concurrently share the resources on a single GPU. This is\naccomplished by starting an MPS server process, which funnels the work\nfrom multiple CUDA contexts (e.g. from multiple MPI ranks) into a single\nCUDA context. In some cases, this can increase performance due to better\nutilization of the resources. As mentioned in the Common bsub Options\nsection above, MPS can be enabled with the -alloc_flags \"gpumps\" option to\nbsub. The following screencast shows an example of how to start an MPS\nserver process for a job: https://vimeo.com/292016149\n\nGPU Compute Modes\n\nSummit's V100 GPUs are configured to have a default compute mode of\nEXCLUSIVE_PROCESS. In this mode, the GPU is assigned to only a single\nprocess at a time, and can accept work from multiple process threads\nconcurrently.\n\nIt may be desirable to change the GPU's compute mode to DEFAULT, which\nenables multiple processes and their threads to share and submit work to it\nsimultaneously. To change the compute mode to DEFAULT, use the\n-alloc_flags gpudefault option.\n\nNVIDIA recommends using the EXCLUSIVE_PROCESS compute mode (the default on\nSummit) when using the Multi-Process Service, but both MPS and the compute mode\ncan be changed by providing both values: -alloc_flags \"gpumps gpudefault\".\n\nBatch Environment Variables\n\nLSF provides a number of environment variables in your job’s shell\nenvironment. Many job parameters are stored in environment variables and\ncan be queried within the batch job. Several of these variables are\nsummarized in the table below. This is not an all-inclusive list of\nvariables available to your batch job; in particular only LSF variables\nare discussed, not the many “standard” environment variables that will\nbe available (such as $PATH).\n\n\nThe summit user guide provides a detailed description of various variables that are used in the job submission process. The first variable, LSB_JOBID, is the unique identification number assigned to each job by the LSF (Load Sharing Facility) system. This number is used to track and manage the job throughout its execution. The next variable, LS_JOBPID, refers to the process ID of the job, which is a unique number assigned to each process running on the system. The third variable, LSB_JOBINDEX, is applicable only to jobs that belong to a job array and indicates the index of the job within the array. The fourth variable, LSB_HOSTS, lists the specific hosts that have been assigned to run the job. This information is useful for troubleshooting and monitoring purposes. The fifth variable, LSB_QUEUE, specifies the queue from which the job was dispatched. This is important as it determines the priority and resources allocated to the job. The sixth variable, LSB_INTERACTIVE, indicates whether the job is interactive or not. If it is set to \"Y\", it means the job is interactive, otherwise it is unset. Finally, the last variable, LS_SUBCWD, specifies the directory from which the job was submitted. This information is useful for keeping track of the job's origin and for any file operations that may be required during the job's execution.\n\n| Variable    | Description                                                                 |\n|-------------|-----------------------------------------------------------------------------|\n| LSB_JOBID   | The ID assigned to the job by LSF                                            |\n| LS_JOBPID   | The job’s process ID                                                        |\n| LSB_JOBINDEX| The job’s index (if it belongs to a job array)                              |\n| LSB_HOSTS   | The hosts assigned to run the job                                            |\n| LSB_QUEUE   | The queue from which the job was dispatched                                  |\n| LSB_INTERACTIVE | Set to “Y” for an interactive job; otherwise unset                       |\n| LS_SUBCWD   | The directory from which the job was submitted                               |\n\n\n\nJob States\n\nA job will progress through a number of states through its lifetime. The\nstates you’re most likely to see are:\n\n\nThe table provides a comprehensive overview of the different states that a job can be in while using the Summit user guide. The first column lists the various states, including PEND, RUN, DONE, EXIT, PSUSP, USUSP, and SSUSP. The second column provides a description of each state, explaining what it means and how it affects the job. For example, PEND indicates that the job is currently pending and has not yet started running. RUN means that the job is currently running, while DONE indicates that the job has completed normally with an exit code of 0. EXIT, on the other hand, signifies that the job has completed abnormally. The last three states, PSUSP, USUSP, and SSUSP, all indicate that the job has been suspended in some way. This could be due to user intervention or system issues. Overall, this table serves as a useful reference for understanding the different states a job can be in while using the Summit user guide.\n\n| State | Description |\n|-------|-------------|\n| PEND  | Job is pending |\n| RUN   | Job is running |\n| DONE  | Job completed normally (with an exit code of 0) |\n| EXIT  | Job completed abnormally |\n| PSUSP | Job was suspended (either by the user or an administrator) while pending |\n| USUSP | Job was suspended (either by the user or an administrator) after starting |\n| SSUSP | Job was suspended by the system after starting |\n\n\n\nJobs may end up in the PSUSP state for a number of reasons. Two common reasons for PSUSP jobs include jobs that have been held by the user or jobs with unresolved dependencies.\n\nAnother common reason that jobs end up in a PSUSP state is a job that the system is unable to start. You may notice a job alternating between PEND and RUN states a few times and ultimately ends up as PSUSP. In this case, the system attempted to start the job but failed for some reason. This can be due to a system issue, but we have also seen this casued by improper settings on user ~/.ssh/config files. (The batch system uses SSH, and the improper settings cause SSH to fail.) If you notice your jobs alternating between PEND and RUN, you might want to check permissions of your ~/.ssh/config file to make sure it does not have write permission for \"group\" or \"other\". (A setting of read/write for the user and no other permissions, which can be set with chmod 600 ~/.ssh/config, is recommended.)\n\nScheduling Policy\n\nIn a simple batch queue system, jobs run in a first-in, first-out (FIFO)\norder. This often does not make effective use of the system. A large job\nmay be next in line to run. If the system is using a strict FIFO queue,\nmany processors sit idle while the large job waits to run. Backfilling\nwould allow smaller, shorter jobs to use those otherwise idle resources,\nand with the proper algorithm, the start time of the large job would not\nbe delayed. While this does make more effective use of the system, it\nindirectly encourages the submission of smaller jobs.\n\nThe DOE Leadership-Class Job Mandate\n\nAs a DOE Leadership Computing Facility, the OLCF has a mandate that a\nlarge portion of Summit's usage come from large, leadership-class (aka\ncapability) jobs. To ensure the OLCF complies with DOE directives, we\nstrongly encourage users to run jobs on Summit that are as large as\ntheir code will warrant. To that end, the OLCF implements queue policies\nthat enable large jobs to run in a timely fashion.\n\nThe OLCF implements queue policies that encourage the\nsubmission and timely execution of large, leadership-class jobs on\nSummit.\n\nThe basic priority-setting mechanism for jobs waiting in the queue is\nthe time a job has been waiting relative to other jobs in the queue.\n\nIf your jobs require resources outside these queue policies such as higher priority or longer walltimes, please contact help@olcf.ornl.gov.\n\nJob Priority by Processor Count\n\nJobs are aged according to the job's requested processor count (older\nage equals higher queue priority). Each job's requested processor count\nplaces it into a specific bin. Each bin has a different aging\nparameter, which all jobs in the bin receive.\n\n\nThe table presents information related to the Summit User Guide. It contains five rows, each representing a different bin. The first column, labeled \"Bin\", indicates the bin number. The second and third columns, labeled \"Min Nodes\" and \"Max Nodes\" respectively, provide the minimum and maximum number of nodes that can be used in a job submission. The fourth column, labeled \"Max Walltime (Hours)\", specifies the maximum walltime (in hours) that can be requested for a job. The last column, labeled \"Aging Boost (Days)\", indicates the number of days a job can be boosted in priority before it ages out. The values in each column vary depending on the bin number, with the values increasing as the bin number increases. For example, in bin 1, the minimum number of nodes is 2,765 and the maximum is 4,608, while in bin 5, the minimum is 1 and the maximum is 45. The maximum walltime also decreases as the bin number increases, with a maximum of 24 hours in bins 1 and 2, and a minimum of 2 hours in bin 5. The aging boost remains constant at 15 days in bin 1, but decreases to 0 days in bins 3, 4, and 5. This table provides important guidelines for users to follow when submitting jobs to the Summit supercomputer, ensuring efficient and effective use of its resources.\n\n| Bin | Min Nodes | Max Nodes | Max Walltime (Hours) | Aging Boost (Days) |\n| --- | --- | --- | --- | --- |\n| 1 | 2,765 | 4,608 | 24.0 | 15 |\n| 2 | 922 | 2,764 | 24.0 | 10 |\n| 3 | 92 | 921 | 12.0 | 0 |\n| 4 | 46 | 91 | 6.0 | 0 |\n| 5 | 1 | 45 | 2.0 | 0 |\n\n\n\nbatch Queue Policy\n\nThe batch queue (and the batch-spi queue for Moderate Enhanced security\nenclave projects) is the default queue for production work on Summit.  Most\nwork on Summit is handled through this queue. It enforces the following\npolicies:\n\nLimit of (4) eligible-to-run jobs per user.\n\nJobs in excess of the per user limit above will be placed into a\nheld state, but will change to eligible-to-run at the appropriate\ntime.\n\nUsers may have only (100) jobs queued in the batch queue at any state at any time.\nAdditional jobs will be rejected at submit time.\n\nThe eligible-to-run state is not the running state.\nEligible-to-run jobs have not started and are waiting for resources.\nRunning jobs are actually executing.\n\nbatch-hm Queue Policy\n\nThe batch-hm queue (and the batch-hm-spi queue for Moderate Enhanced\nsecurity enclave projects) is used to access Summit's high-memory nodes.  Jobs\nmay use all 54 nodes. It enforces the following policies:\n\nLimit of (4) eligible-to-run jobs per user.\n\nJobs in excess of the per user limit above will be placed into a\nheld state, but will change to eligible-to-run at the appropriate\ntime.\n\nUsers may have only (25) jobs queued in the batch-hm queue at any state at any time.\nAdditional jobs will be rejected at submit time.\n\nbatch-hm job limits:\n\n\nThe table presents information regarding the Summit User Guide. It includes three columns: Min Nodes, Max Nodes, and Max Walltime (Hours). The first column, Min Nodes, indicates the minimum number of nodes that can be requested for a job on Summit. The second column, Max Nodes, specifies the maximum number of nodes that can be requested for a job on Summit. The third column, Max Walltime (Hours), denotes the maximum amount of time (in hours) that a job can run on Summit. In this case, the maximum walltime is 24 hours. This table is useful for users who are planning to run jobs on Summit and need to know the minimum and maximum node requirements, as well as the maximum walltime for their jobs. It provides a quick and easy reference for users to ensure that their jobs are within the specified limits and can run successfully on Summit. \n\n| Min Nodes | Max Nodes | Max Walltime (Hours) |\n|-----------|-----------|----------------------|\n| 1         | 54        | 24.0                 |\n\n\n\nTo submit a job to the batch-hm queue, add the -q batch-hm option to your\nbsub command or #BSUB -q batch-hm to your job script.\n\nkillable Queue Policy\n\nThe killable queue is a preemptable queue that allows jobs in bins 4 and 5\nto request walltimes up to 24 hours. Jobs submitted to the killable queue will\nbe preemptable once the job reaches the guaranteed runtime limit as shown in the\ntable below. For example, a job in bin 5 submitted to the killable queue can\nrequest a walltime of 24 hours. The job will be preemptable after two hours of\nrun time. Similarly, a job in bin 4 will be preemptable after six hours of run\ntime. Once a job is preempted, the job will be resubmitted by default with the\noriginal limits as requested in the job script and will have the same JOBID.\n\nPreemptable job limits:\n\n\nThe following table provides a comprehensive overview of the Summit User Guide. The table includes information on the different bins available, the minimum and maximum number of nodes that can be used, the maximum walltime in hours, and the guaranteed walltime in hours. The first bin, labeled as \"4\", allows for a minimum of 46 nodes and a maximum of 91 nodes to be used. The maximum walltime for this bin is 24.0 hours, with a guaranteed walltime of 6.0 hours. The second bin, labeled as \"5\", has a minimum of 1 node and a maximum of 45 nodes, with a maximum walltime of 24.0 hours and a guaranteed walltime of 2.0 hours. This table serves as a useful reference for users of Summit, providing them with important information on the available resources and their corresponding limitations. \n\n| Bin | Min Nodes | Max Nodes | Max Walltime (Hours) | Guaranteed Walltime |\n|-----|-----------|-----------|----------------------|---------------------|\n| 4   | 46        | 91        | 24.0                 | 6.0 (hours)         |\n| 5   | 1         | 45        | 24.0                 | 2.0 (hours)         |\n\n\n\nIf a job in the killable queue does not reach its requested\nwalltime, it will continue to use allocation time with each automatic\nresubmission until it either reaches the requested walltime during a single\ncontinuous run, or is manually killed by the user. Allocations are always\ncharged based on actual compute time used by all jobs.\n\nTo submit a job to the killable queue, add the -q killable option to your\nbsub command or #BSUB -q killable to your job script.\n\nTo prevent a preempted job from being automatically requeued, the BSUB -rn\nflag can be used at submit time.\n\ndebug Queue Policy\n\nThe debug queue (and the debug-spi queue for Moderate Enhanced security\nenclave projects) can be used to access Summit's compute resources for short\nnon-production debug tasks.  The queue provides a higher priority compared to\njobs of the same job size bin in production queues.  Production work and job\nchaining in the debug queue is prohibited.  Each user is limited to one job in\nany state in the debug queue at any one point. Attempts to submit multiple jobs\nto the debug queue will be rejected upon job submission.\n\ndebug job limits:\n\n\nThe summit user guide provides information on the minimum and maximum number of nodes that can be requested for a job, as well as the maximum walltime (in hours) that a job can run for. The table also includes the maximum number of jobs that can be queued in any state per user, and the aging boost in days for queued jobs. The minimum number of nodes that can be requested is 1, while the maximum is unlimited. The maximum walltime for a job is 2 hours, meaning that any job that runs longer than this will be terminated. Additionally, each user is limited to having only 1 job queued in any state at a time. The aging boost of 2 days means that jobs that have been queued for longer than 2 days will receive a boost in priority, increasing their chances of being executed sooner. This information is important for users to understand in order to effectively utilize the summit computing system and ensure efficient job execution. \n\n| Min Nodes | Max Nodes | Max Walltime (Hours) | Max queued any state (per user) | Aging Boost (Days) |\n|-----------|-----------|----------------------|--------------------------------|--------------------|\n| 1         | unlimited | 2.0                  | 1                              | 2                  |\n\n\n\nTo submit a job to the debug queue, add the -q debug option to your\nbsub command or #BSUB -q debug to your job script.\n\nProduction work and job chaining in the debug queue is prohibited.\n\nSPI/KDI Citadel Queue Policy (Moderate Enhanced Projects)\n\nThere are special queue names when submitting jobs to citadel.ccs.ornl.gov\n(the Moderate Enhanced version of Summit). These queues are: batch-spi,\nbatch-hm-spi, and debug-spi.  For example, to submit a job to the\nbatch-spi queue on Citadel, you would need -q batch-spi when using the\nbsub command or #BSUB -q batch-spi when using a job script.\n\nExcept for the enhanced security policies for jobs in these queues, all other\nqueue properties are the same as the respective Summit queues described above,\nsuch as maximum walltime and number of eligible running jobs.\n\nIf you submit a job to a \"normal\" Summit queue while on Citadel, such as\n-q batch, your job will be unable to launch.\n\nAllocation Overuse Policy\n\nProjects that overrun their allocation are still allowed to run on OLCF\nsystems, although at a reduced priority. Like the adjustment for the\nnumber of processors requested above, this is an adjustment to the\napparent submit time of the job. However, this adjustment has the effect\nof making jobs appear much younger than jobs submitted under projects\nthat have not exceeded their allocation. In addition to the priority\nchange, these jobs are also limited in the amount of wall time that can\nbe used. For example, consider that job1 is submitted at the same\ntime as job2. The project associated with job1 is over its\nallocation, while the project for job2 is not. The batch system will\nconsider job2 to have been waiting for a longer time than job1.\nAdditionally, projects that are at 125% of their allocated time will be\nlimited to only 3 running jobs at a time. The adjustment to the\napparent submit time depends upon the percentage that the project is\nover its allocation, as shown in the table below:\n\n\nThe table presented displays the percentage of allocation used by a user and the corresponding priority reduction that will be applied. The first column shows the different ranges of allocation usage, with the first range being less than 100%, the second range being 100% to 125%, and the third range being greater than 125%. The second column indicates the number of days that the user's priority will be reduced based on their allocation usage. For users who have used less than 100% of their allocation, there will be no reduction in their priority. However, for those who have used 100% to 125% of their allocation, their priority will be reduced for 30 days. Lastly, for users who have used more than 125% of their allocation, their priority will be reduced for a longer period of 365 days. This table serves as a guide for users to understand the impact of their allocation usage on their priority and encourages them to manage their usage accordingly. \n\n| % Of Allocation Used | Priority Reduction |\n|-----------------------|--------------------|\n| < 100%                | 0 days             |\n| 100% to 125%          | 30 days            |\n| > 125%                | 365 days           |\n\n\n\nSystem Reservation Policy\n\nProjects may request to reserve a set of nodes for a period of time\nby contacting help@olcf.ornl.gov. If the reservation is granted, the reserved nodes will be\nblocked from general use for a given period of time. Only users that\nhave been authorized to use the reservation can utilize those resources.\nTo access the reservation, please add -U {reservation name} to bsub or job script.\nSince no other users can access the reserved resources, it is crucial\nthat groups given reservations take care to ensure the utilization on\nthose resources remains high. To prevent reserved resources from\nremaining idle for an extended period of time, reservations are\nmonitored for inactivity. If activity falls below 50% of the reserved\nresources for more than (30) minutes, the reservation will be canceled\nand the system will be returned to normal scheduling. A new reservation\nmust be requested if this occurs.\n\nThe requesting project's allocation is charged according to the time window\ngranted, regardless of actual utilization. For example, an 8-hour, 2,000\nnode reservation on Summit would be equivalent to using 16,000 Summit\nnode-hours of a project's allocation.\n\n\n\nJob Dependencies\n\nAs is the case with many other queuing systems, it is possible to place\ndependencies on jobs to prevent them from running until other jobs have\nstarted/completed/etc. Several possible dependency settings are\ndescribed in the table below:\n\n\nThe table presents a list of expressions and their corresponding meanings in the context of the Summit user guide. The first expression, #BSUB -w started(12345), indicates that the job specified by job ID 12345 will not start until it is in one of the following states: USUSP, SSUSP, DONE, EXIT, or RUN with a pre-execution command specified by the bsub -E option. The second expression, #BSUB -w done(12345) #BSUB -w 12345, means that the job will not start until job 12345 has a state of DONE, indicating that it has completed normally. If no condition is specified, the done() condition is assumed. The third expression, #BSUB -w exit(12345), states that the job will not start until job 12345 has a state of EXIT, indicating that it has completed abnormally. Lastly, the expression #BSUB -w ended(12345) means that the job will not start until job 12345 has a state of EXIT or DONE, indicating that it has either completed abnormally or normally. These expressions provide users with the ability to control when their jobs will start based on specific conditions, allowing for more efficient and organized job scheduling on Summit. \n\n| Expression | Meaning |\n|------------|---------|\n| #BSUB -w started(12345) | The job will not start until job 12345 starts. Job 12345 is considered to have started if it is in any of the following states: USUSP, SSUSP, DONE, EXIT, or RUN (with any pre-execution command specified by bsub -E completed). |\n| #BSUB -w done(12345) #BSUB -w 12345 | The job will not start until job 12345 has a state of DONE (i.e. completed normally). If a job ID is given with no condition, done() is assumed. |\n| #BSUB -w exit(12345) | The job will not start until job 12345 has a state of EXIT (i.e. completed abnormally). |\n| #BSUB -w ended(12345) | The job will not start until job 12345 has a state of EXIT or DONE. |\n\n\n\nDependency expressions can be combined with logical operators. For\nexample, if you want a job held until job 12345 is DONE and job 12346\nhas started, you can use #BSUB -w \"done(12345) && started(12346)\"\n\n\n\nJob Launcher (jsrun)\n\nThe default job launcher for Summit is jsrun. jsrun was developed by\nIBM for the Oak Ridge and Livermore Power systems. The tool will execute\na given program on resources allocated through the LSF batch scheduler;\nsimilar to mpirun and aprun functionality.\n\nCompute Node Description\n\nThe following compute node image will be used to discuss jsrun resource\nsets and layout.\n\n\n\n1 node\n\n2 sockets (grey)\n\n42 physical cores* (dark blue)\n\n168 hardware cores (light blue)\n\n6 GPUs (orange)\n\n2 Memory blocks (yellow)\n\n*Core Isolation: 1 core on each socket has been set aside for\noverhead and is not available for allocation through jsrun. The core has\nbeen omitted and is not shown in the above image.\n\nResource Sets\n\nWhile jsrun performs similar job launching functions as aprun and\nmpirun, its syntax is very different. A large reason for syntax\ndifferences is the introduction of the resource set concept. Through\nresource sets, jsrun can control how a node appears to each job. Users\ncan, through jsrun command line flags, control which resources on a node\nare visible to a job. Resource sets also allow the ability to run\nmultiple jsruns simultaneously within a node. Under the covers, a\nresource set is a cgroup.\n\nAt a high level, a resource set allows users to configure what a node\nlook like to their job.\n\njsrun will create one or more resource sets within a node. Each resource\nset will contain 1 or more cores and 0 or more GPUs. A resource set can\nspan sockets, but it may not span a node. While a resource set can span\nsockets within a node, consideration should be given to the cost of\ncross-socket communication. By creating resource sets only within\nsockets, costly communication between sockets can be prevented.\n\nSubdividing a Node with Resource Sets\n\nResource sets provides the ability to subdivide node’s resources into\nsmaller groups. The following examples show how a node can be subdivided\nand how many resource set could fit on a node.\n\n\n\nMultiple Methods to Creating Resource Sets\n\nResource sets should be created to fit code requirements. The following\nexamples show multiple ways to create resource sets that allow two MPI\ntasks access to a single GPU.\n\n6 resource sets per node: 1 GPU, 2 cores per (Titan)\n\n\n\nIn this case, CPUs can only see single assigned GPU.\n\n2 resource sets per node: 3 GPUs and 6 cores per socket\n\n\n\nIn this case, all 6 CPUs can see 3 GPUs. Code must manage CPU -> GPU\ncommunication. CPUs on socket0 can not access GPUs or Memory on socket1.\n\nSingle resource set per node: 6 GPUs, 12 cores\n\n\n\nIn this case, all 12 CPUs can see all node’s 6 GPUs. Code must manage CPU to\nGPU communication. CPUs on socket0 can access GPUs and Memory on socket1.\nCode must manage cross socket communication.\n\nDesigning a Resource Set\n\nResource sets allow each jsrun to control how the node appears to a\ncode. This method is unique to jsrun, and requires thinking of each job\nlaunch differently than aprun or mpirun. While the method is unique, the\nmethod is not complicated and can be reasoned in a few basic steps.\n\nThe first step to creating resource sets is understanding how a code would\nlike the node to appear. For example, the number of tasks/threads per\nGPU. Once this is understood, the next step is to simply calculate the\nnumber of resource sets that can fit on a node. From here, the number of\nneeded nodes can be calculated and passed to the batch job request.\n\nThe basic steps to creating resource sets:\n\nUnderstand how your code expects to interact with the system.\n\nHow many tasks/threads per GPU?\n\nDoes each task expect to see a single GPU? Do multiple tasks expect\nto share a GPU? Is the code written to internally manage task to GPU\nworkload based on the number of available cores and GPUs?\n\nCreate resource sets containing the needed GPU to task binding\n\nBased on how your code expects to interact with the system, you can\ncreate resource sets containing the needed GPU and core resources.\nIf a code expects to utilize one GPU per task, a resource set would\ncontain one core and one GPU. If a code expects to pass work to a\nsingle GPU from two tasks, a resource set would contain two cores\nand one GPU.\n\nDecide on the number of resource sets needed\n\nOnce you understand tasks, threads, and GPUs in a resource set, you\nsimply need to decide the number of resource sets needed.\n\nAs on any system, it is useful to keep in mind the hardware underneath every\nexecution. This is particularly true when laying out resource sets.\n\nLaunching a Job with jsrun\n\njsrun Format\n\njsrun    [ -n #resource sets ]   [tasks, threads, and GPUs within each resource set]   program [ program args ]\n\nCommon jsrun Options\n\nBelow are common jsrun options. More flags and details can be found in the jsrun\nman page. The defaults listed in the table below are the OLCF defaults and take\nprecedence over those mentioned in the man page.\n\n\nThe table provides a detailed description of the various flags and their default values that can be used in the Summit user guide. The first column lists the flags, while the second column provides a brief description of each flag. The third column lists the default value for each flag. The first flag, \"--nrs\" or \"-n\", refers to the number of resource sets and has a default value of \"All available physical cores\". The next flag, \"--tasks_per_rs\" or \"-a\", refers to the number of MPI tasks (ranks) per resource set and does not have a default value. Instead, the total tasks (-p) must be set. The \"--cpu_per_rs\" or \"-c\" flag refers to the number of CPUs (cores) per resource set and has a default value of 1. The \"--gpu_per_rs\" or \"-g\" flag refers to the number of GPUs per resource set and has a default value of 0. The \"--bind\" or \"-b\" flag allows for the binding of tasks within a resource set and can be set to \"none\", \"rs\", or \"packed:#\" where \"#\" represents the number of tasks to be packed. The \"--rs_per_host\" or \"-r\" flag refers to the number of resource sets per host and does not have a default value. The \"--latency_priority\" or \"-l\" flag controls layout priorities and can be set to \"gpu-cpu\", \"cpu-mem\", or \"cpu-cpu\". The default value for this flag is \"gpu-cpu,cpu-mem,cpu-cpu\". Finally, the \"--launch_distribution\" or \"-d\" flag determines how tasks are started on resource sets and has a default value of \"packed\". \n\n| Flags | Description | Default Value |\n|-------|-------------|---------------|\n| --nrs or -n | Number of resource sets | All available physical cores |\n| --tasks_per_rs or -a | Number of MPI tasks (ranks) per resource set | Not set by default, instead total tasks (-p) set |\n| --cpu_per_rs or -c | Number of CPUs (cores) per resource set | 1 |\n| --gpu_per_rs or -g | Number of GPUs per resource set | 0 |\n| --bind or -b | Binding of tasks within a resource set | Can be none, rs, or packed:# |\n| --rs_per_host or -r | Number of resource sets per host | No default |\n| --latency_priority or -l | Latency Priority | Controls layout priorities. Can currently be cpu-cpu or gpu-cpu,cpu-mem,cpu-cpu |\n| --launch_distribution or -d | How tasks are started on resource sets | Packed |\n\n\n\nIt's recommended to explicitly specify jsrun options and not rely on the\ndefault values. This most often includes --nrs,--cpu_per_rs,\n--gpu_per_rs, --tasks_per_rs, --bind, and --launch_distribution.\n\nJsrun Examples\n\nThe below examples were launched in the following 2 node interactive\nbatch job:\n\nsummit> bsub -nnodes 2 -Pprj123 -W02:00 -Is $SHELL\n\nSingle MPI Task, single GPU per RS\n\nThe following example will create 12 resource sets each with 1 MPI task\nand 1 GPU. Each MPI task will have access to a single GPU.\n\nRank 0 will have access to GPU 0 on the first node ( red resource set).\nRank 1 will have access to GPU 1 on the first node ( green resource set).\nThis pattern will continue until 12 resources sets have been created.\n\nThe following jsrun command will request 12 resource sets (-n12) 6\nper node (-r6). Each resource set will contain 1 MPI task (-a1),\n1 GPU (-g1), and 1 core (-c1).\n\n\n\nsummit> jsrun -n12 -r6 -a1 -g1 -c1 ./a.out\nRank:    0; NumRanks: 12; RankCore:   0; Hostname: h41n04; GPU: 0\nRank:    1; NumRanks: 12; RankCore:   4; Hostname: h41n04; GPU: 1\nRank:    2; NumRanks: 12; RankCore:   8; Hostname: h41n04; GPU: 2\nRank:    3; NumRanks: 12; RankCore:  88; Hostname: h41n04; GPU: 3\nRank:    4; NumRanks: 12; RankCore:  92; Hostname: h41n04; GPU: 4\nRank:    5; NumRanks: 12; RankCore:  96; Hostname: h41n04; GPU: 5\n\nRank:    6; NumRanks: 12; RankCore:   0; Hostname: h41n03; GPU: 0\nRank:    7; NumRanks: 12; RankCore:   4; Hostname: h41n03; GPU: 1\nRank:    8; NumRanks: 12; RankCore:   8; Hostname: h41n03; GPU: 2\nRank:    9; NumRanks: 12; RankCore:  88; Hostname: h41n03; GPU: 3\nRank:   10; NumRanks: 12; RankCore:  92; Hostname: h41n03; GPU: 4\nRank:   11; NumRanks: 12; RankCore:  96; Hostname: h41n03; GPU: 5\n\nMultiple tasks, single GPU per RS\n\nThe following jsrun command will request 12 resource sets (-n12).\nEach resource set will contain 2 MPI tasks (-a2), 1 GPU\n(-g1), and 2 cores (-c2). 2 MPI tasks will have access to a\nsingle GPU. Ranks 0 - 1 will have access to GPU 0 on the first node (\nred resource set). Ranks 2 - 3 will have access to GPU 1 on the first\nnode ( green resource set). This pattern will continue until 12 resource\nsets have been created.\n\n\n\nAdding cores to the RS: The -c flag should be used to request\nthe needed cores for tasks and treads. The default -c core count is 1.\nIn the above example, if -c is not specified both tasks will run on a\nsingle core.\n\nsummit> jsrun -n12 -a2 -g1 -c2 -dpacked ./a.out | sort\nRank:    0; NumRanks: 24; RankCore:   0; Hostname: a01n05; GPU: 0\nRank:    1; NumRanks: 24; RankCore:   4; Hostname: a01n05; GPU: 0\n\nRank:    2; NumRanks: 24; RankCore:   8; Hostname: a01n05; GPU: 1\nRank:    3; NumRanks: 24; RankCore:  12; Hostname: a01n05; GPU: 1\n\nRank:    4; NumRanks: 24; RankCore:  16; Hostname: a01n05; GPU: 2\nRank:    5; NumRanks: 24; RankCore:  20; Hostname: a01n05; GPU: 2\n\nRank:    6; NumRanks: 24; RankCore:  88; Hostname: a01n05; GPU: 3\nRank:    7; NumRanks: 24; RankCore:  92; Hostname: a01n05; GPU: 3\n\nRank:    8; NumRanks: 24; RankCore:  96; Hostname: a01n05; GPU: 4\nRank:    9; NumRanks: 24; RankCore: 100; Hostname: a01n05; GPU: 4\n\nRank:   10; NumRanks: 24; RankCore: 104; Hostname: a01n05; GPU: 5\nRank:   11; NumRanks: 24; RankCore: 108; Hostname: a01n05; GPU: 5\n\nRank:   12; NumRanks: 24; RankCore:   0; Hostname: a01n01; GPU: 0\nRank:   13; NumRanks: 24; RankCore:   4; Hostname: a01n01; GPU: 0\n\nRank:   14; NumRanks: 24; RankCore:   8; Hostname: a01n01; GPU: 1\nRank:   15; NumRanks: 24; RankCore:  12; Hostname: a01n01; GPU: 1\n\nRank:   16; NumRanks: 24; RankCore:  16; Hostname: a01n01; GPU: 2\nRank:   17; NumRanks: 24; RankCore:  20; Hostname: a01n01; GPU: 2\n\nRank:   18; NumRanks: 24; RankCore:  88; Hostname: a01n01; GPU: 3\nRank:   19; NumRanks: 24; RankCore:  92; Hostname: a01n01; GPU: 3\n\nRank:   20; NumRanks: 24; RankCore:  96; Hostname: a01n01; GPU: 4\nRank:   21; NumRanks: 24; RankCore: 100; Hostname: a01n01; GPU: 4\n\nRank:   22; NumRanks: 24; RankCore: 104; Hostname: a01n01; GPU: 5\nRank:   23; NumRanks: 24; RankCore: 108; Hostname: a01n01; GPU: 5\n\nsummit>\n\nMultiple Task, Multiple GPU per RS\n\nThe following example will create 4 resource sets each with 6 tasks and\n3 GPUs. Each set of 6 MPI tasks will have access to 3 GPUs. Ranks 0 - 5\nwill have access to GPUs 0 - 2 on the first socket of the first node (\nred resource set). Ranks 6 - 11 will have access to GPUs 3 - 5 on the\nsecond socket of the first node ( green resource set). This pattern will\ncontinue until 4 resource sets have been created. The following jsrun\ncommand will request 4 resource sets (-n4). Each resource set will\ncontain 6 MPI tasks (-a6), 3 GPUs (-g3), and 6 cores\n(-c6).\n\n\n\nsummit> jsrun -n 4 -a 6 -c 6 -g 3 -d packed -l GPU-CPU ./a.out\nRank:    0; NumRanks: 24; RankCore:   0; Hostname: a33n06; GPU: 0, 1, 2\nRank:    1; NumRanks: 24; RankCore:   4; Hostname: a33n06; GPU: 0, 1, 2\nRank:    2; NumRanks: 24; RankCore:   8; Hostname: a33n06; GPU: 0, 1, 2\nRank:    3; NumRanks: 24; RankCore:  12; Hostname: a33n06; GPU: 0, 1, 2\nRank:    4; NumRanks: 24; RankCore:  16; Hostname: a33n06; GPU: 0, 1, 2\nRank:    5; NumRanks: 24; RankCore:  20; Hostname: a33n06; GPU: 0, 1, 2\n\nRank:    6; NumRanks: 24; RankCore:  88; Hostname: a33n06; GPU: 3, 4, 5\nRank:    7; NumRanks: 24; RankCore:  92; Hostname: a33n06; GPU: 3, 4, 5\nRank:    8; NumRanks: 24; RankCore:  96; Hostname: a33n06; GPU: 3, 4, 5\nRank:    9; NumRanks: 24; RankCore: 100; Hostname: a33n06; GPU: 3, 4, 5\nRank:   10; NumRanks: 24; RankCore: 104; Hostname: a33n06; GPU: 3, 4, 5\nRank:   11; NumRanks: 24; RankCore: 108; Hostname: a33n06; GPU: 3, 4, 5\n\nRank:   12; NumRanks: 24; RankCore:   0; Hostname: a33n05; GPU: 0, 1, 2\nRank:   13; NumRanks: 24; RankCore:   4; Hostname: a33n05; GPU: 0, 1, 2\nRank:   14; NumRanks: 24; RankCore:   8; Hostname: a33n05; GPU: 0, 1, 2\nRank:   15; NumRanks: 24; RankCore:  12; Hostname: a33n05; GPU: 0, 1, 2\nRank:   16; NumRanks: 24; RankCore:  16; Hostname: a33n05; GPU: 0, 1, 2\nRank:   17; NumRanks: 24; RankCore:  20; Hostname: a33n05; GPU: 0, 1, 2\n\nRank:   18; NumRanks: 24; RankCore:  88; Hostname: a33n05; GPU: 3, 4, 5\nRank:   19; NumRanks: 24; RankCore:  92; Hostname: a33n05; GPU: 3, 4, 5\nRank:   20; NumRanks: 24; RankCore:  96; Hostname: a33n05; GPU: 3, 4, 5\nRank:   21; NumRanks: 24; RankCore: 100; Hostname: a33n05; GPU: 3, 4, 5\nRank:   22; NumRanks: 24; RankCore: 104; Hostname: a33n05; GPU: 3, 4, 5\nRank:   23; NumRanks: 24; RankCore: 108; Hostname: a33n05; GPU: 3, 4, 5\nsummit>\n\nCommon Use Cases\n\nThe following table provides a quick reference for creating resource\nsets of various common use cases. The -n flag can be altered to\nspecify the number of resource sets needed.\n\n\nThe table presents information on the resource sets, MPI tasks, threads, physical cores, GPUs, and jsrun command for the Summit user guide. The first row shows that there is one resource set with 42 MPI tasks and 0 threads, utilizing all 42 physical cores and no GPUs. The corresponding jsrun command for this configuration is \"jsrun -n1 -a42 -c42 -g0\". The second row has the same number of resource sets and MPI tasks, but only 1 thread and 1 GPU. The jsrun command for this configuration is \"jsrun -n1 -a1 -c1 -g1\". The third row also has 1 resource set and 1 GPU, but now with 2 threads and 2 physical cores. The jsrun command for this configuration is \"jsrun -n1 -a2 -c2 -g1\". The fourth row has 1 resource set and 2 GPUs, with 1 MPI task and 1 thread utilizing 1 physical core. The corresponding jsrun command is \"jsrun -n1 -a1 -c1 -g2\". Finally, the last row has 1 resource set, 3 GPUs, and 21 MPI tasks with 21 threads, utilizing all 21 physical cores. The jsrun command for this configuration is \"jsrun -n1 -a1 -c21 -g3 -bpacked:21\". This table provides a comprehensive overview of the different configurations and corresponding jsrun commands that can be used for Summit, allowing users to optimize their resource utilization for their specific needs.\n\n| Resource Sets | MPI Tasks | Threads | Physical Cores | GPUs | jsrun Command |\n|----------------|-----------|---------|----------------|------|----------------|\n| 1              | 42        | 0       | 42             | 0    | jsrun -n1 -a42 -c42 -g0 |\n| 1              | 1         | 0       | 1              | 1    | jsrun -n1 -a1 -c1 -g1 |\n| 1              | 2         | 0       | 2              | 1    | jsrun -n1 -a2 -c2 -g1 |\n| 1              | 1         | 0       | 1              | 2    | jsrun -n1 -a1 -c1 -g2 |\n| 1              | 1         | 21      | 21             | 3    | jsrun -n1 -a1 -c21 -g3 -bpacked:21 |\n\n\n\njsrun Tools\n\nThis section describes tools that users might find helpful to better\nunderstand the jsrun job launcher.\n\nhello_jsrun\n\nhello_jsrun is a \"Hello World\"-type program that users can run on\nSummit nodes to better understand how MPI ranks and OpenMP threads are\nmapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A\nscreencast showing how to use Hello_jsrun is also available:\nhttps://vimeo.com/261038849\n\nJob Step Viewer\n\nJob Step Viewer provides a graphical view of an application's runtime layout on Summit.\nIt allows users to preview and quickly iterate with multiple jsrun options to\nunderstand and optimize job launch.\n\nFor bug reports or suggestions, please email help@olcf.ornl.gov.\n\nUsage\n\nRequest a Summit allocation\n\nbsub -W 10 -nnodes 2 -P $OLCF_PROJECT_ID -Is $SHELL\n\nLoad the job-step-viewer module\n\nmodule load job-step-viewer\n\nTest out a jsrun line by itself, or provide an executable as normal\n\njsrun -n12 -r6 -c7 -g1 -a1 -EOMP_NUM_THREADS=7 -brs\n\nVisit the provided URL\n\nhttps://jobstepviewer.olcf.ornl.gov/summit/871957-1\n\nMost Terminal applications have built-in shortcuts to directly open\nweb addresses in the default browser.\n\nMacOS Terminal.app: hold Command (⌘) and double-click on the URL\n\niTerm2: hold Command (⌘) and single-click on the URL\n\nLimitations\n\n(currently) Compiled with GCC toolchain only\n\nDoes not support MPMD-mode via ERF\n\nOpenMP only supported with use of the OMP_NUM_THREADS environment variable.\n\nMore Information\n\nThis section provides some of the most commonly used LSF commands as\nwell as some of the most useful options to those commands and\ninformation on jsrun, Summit's job launch command. Many commands\nhave much more information than can be easily presented here. More\ninformation about these commands is available via the online manual\n(i.e. man jsrun). Additional LSF information can be found on IBM’s\nwebsite.\n\nUsing Multithreading in a Job\n\nHardware Threads: Multiple Threads per Core\n\nEach physical core on Summit contains 4 hardware threads. The SMT level\ncan be set using LSF flags (the default is smt4):\n\nSMT1\n\n#BSUB -alloc_flags smt1\njsrun -n1 -c1 -a1 -bpacked:1 csh -c 'echo $OMP_PLACES’\n0\n\nSMT2\n\n#BSUB -alloc_flags smt2\njsrun -n1 -c1 -a1 -bpacked:1 csh -c 'echo $OMP_PLACES’\n{0:2}\n\nSMT4\n\n#BSUB -alloc_flags smt4\njsrun -n1 -c1 -a1 -bpacked:1 csh -c 'echo $OMP_PLACES’\n{0:4}\n\nControlling Number of Threads for Tasks\n\nIn addition to specifying the SMT level, you can also control the\nnumber of threads per MPI task by exporting the OMP_NUM_THREADS\nenvironment variable. If you don't export it yourself, Jsrun will\nautomatically set the number of threads based on the number of cores\nrequested (-c) and the binding (-b) option. It is better to be\nexplicit and set the OMP_NUM_THREADS value yourself rather than\nrelying on Jsrun constructing it for you. Especially when you are\nusing Job Step Viewer which relies on the presence of that\nenvironment variable to give you visual thread assignment information.\n\nIn the below example, you could also do export OMP_NUM_THREADS=16 in your\njob script instead of passing it as a -E flag to jsrun. The below example\nstarts 1 resource set with 2 tasks and 8 cores, 4 cores bound to each task,\n16 threads for each task. We can set 16 threads since there are 4 cores\nper task and the default is smt4 for each core (4 * 4 = 16 threads).\n\njsrun -n1 -a2 -c8 -g1 -bpacked:4 -dpacked -EOMP_NUM_THREADS=16 csh -c 'echo $OMP_NUM_THREADS $OMP_PLACES'\n\n16 0:4,4:4,8:4,12:4\n16 16:4,20:4,24:4,28:4\n\nBe careful with assigning threads to tasks, as you might end up\noversubscribing your cores. For example\n\njsrun -n1 -a2 -c8 -g1 -bpacked:4 -dpacked -EOMP_NUM_THREADS=32 csh -c 'echo $OMP_NUM_THREADS $OMP_PLACES'\n\nWarning: OMP_NUM_THREADS=32 is greater than available PU's\nWarning: OMP_NUM_THREADS=32 is greater than available PU's\nWarning: OMP_NUM_THREADS=32 is greater than available PU's\nWarning: OMP_NUM_THREADS=32 is greater than available PU's\n32 16:4,20:4,24:4,28:4\n32 0:4,4:4,8:4,12:4\n\nYou can use hello_jsrun or Job Step Viewer to see how the cores\nare being oversubscribed.\n\nBecause of how jsrun sets up OMP_NUM_THREADS based on -c and\n-b options if you don't specify the environment variable yourself,\nyou can accidentally end up oversubscribing your cores. For example\n\njsrun -n1 -a2 -c8 -g1 -brs -dpacked  csh -c 'echo $OMP_NUM_THREADS $OMP_PLACES'\n\nWarning: more than 1 task/rank assigned to a core\nWarning: more than 1 task/rank assigned to a core\n32 0:4,4:4,8:4,12:4,16:4,20:4,24:4,28:4\n32 0:4,4:4,8:4,12:4,16:4,20:4,24:4,28:4\n\nBecause jsrun sees 8 cores and the -brs flag, it assigns all 8 cores to\neach of the 2 tasks in the resource set. Jsrun will set up OMP_NUM_THREADS\nas 32 (8 cores with 4 threads per core) which will apply to all the\ntasks in the resource set. This means that each task sees that it can\nhave 32 threads (which means 64 threads for the 2 tasks combined) which\nwill oversubscribe the cores and may decrease efficiency as a result.\n\nExample: Single Task, Single GPU, Multiple Threads per RS\n\nThe following example will create 12 resource sets each with 1 task, 4\nthreads, and 1 GPU. Each MPI task will start 4 threads and have access\nto 1 GPU. Rank 0 will have access to GPU 0 and start 4 threads on the\nfirst socket of the first node ( red resource set). Rank 2 will have\naccess to GPU 1 and start 4 threads on the second socket of the first\nnode ( green resource set). This pattern will continue until 12 resource\nsets have been created. The following jsrun command will create 12\nresource sets (-n12). Each resource set will contain 1 MPI task\n(-a1), 1 GPU (-g1), and 4 cores (-c4). Notice that\nmore cores are requested than MPI tasks; the extra cores will be needed\nto place threads. Without requesting additional cores, threads will be\nplaced on a single core.\n\nRequesting Cores for Threads: The -c flag should be used to\nrequest additional cores for thread placement. Without requesting\nadditional cores, threads will be placed on a single core.\n\nBinding Cores to Tasks: The -b binding flag should be used to\nbind cores to tasks. Without specifying binding, all threads will be\nbound to the first core.\n\nsummit> setenv OMP_NUM_THREADS 4\nsummit> jsrun -n12 -a1 -c4 -g1 -b packed:4 -d packed ./a.out\nRank: 0; RankCore: 0; Thread: 0; ThreadCore: 0; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}\nRank: 0; RankCore: 0; Thread: 1; ThreadCore: 4; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}\nRank: 0; RankCore: 0; Thread: 2; ThreadCore: 8; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}\nRank: 0; RankCore: 0; Thread: 3; ThreadCore: 12; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}\n\nRank: 1; RankCore: 16; Thread: 0; ThreadCore: 16; Hostname: a33n06; OMP_NUM_PLACES: {16},{20},{24},{28}\nRank: 1; RankCore: 16; Thread: 1; ThreadCore: 20; Hostname: a33n06; OMP_NUM_PLACES: {16},{20},{24},{28}\nRank: 1; RankCore: 16; Thread: 2; ThreadCore: 24; Hostname: a33n06; OMP_NUM_PLACES: {16},{20},{24},{28}\nRank: 1; RankCore: 16; Thread: 3; ThreadCore: 28; Hostname: a33n06; OMP_NUM_PLACES: {16},{20},{24},{28}\n\n...\n\nRank: 10; RankCore: 104; Thread: 0; ThreadCore: 104; Hostname: a33n05; OMP_NUM_PLACES: {104},{108},{112},{116}\nRank: 10; RankCore: 104; Thread: 1; ThreadCore: 108; Hostname: a33n05; OMP_NUM_PLACES: {104},{108},{112},{116}\nRank: 10; RankCore: 104; Thread: 2; ThreadCore: 112; Hostname: a33n05; OMP_NUM_PLACES: {104},{108},{112},{116}\nRank: 10; RankCore: 104; Thread: 3; ThreadCore: 116; Hostname: a33n05; OMP_NUM_PLACES: {104},{108},{112},{116}\n\nRank: 11; RankCore: 120; Thread: 0; ThreadCore: 120; Hostname: a33n05; OMP_NUM_PLACES: {120},{124},{128},{132}\nRank: 11; RankCore: 120; Thread: 1; ThreadCore: 124; Hostname: a33n05; OMP_NUM_PLACES: {120},{124},{128},{132}\nRank: 11; RankCore: 120; Thread: 2; ThreadCore: 128; Hostname: a33n05; OMP_NUM_PLACES: {120},{124},{128},{132}\nRank: 11; RankCore: 120; Thread: 3; ThreadCore: 132; Hostname: a33n05; OMP_NUM_PLACES: {120},{124},{128},{132}\n\nsummit>\n\n\n\nLaunching Multiple Jsruns\n\nJsrun provides the ability to launch multiple jsrun job launches within a\nsingle batch job allocation. This can be done within a single node, or across\nmultiple nodes.\n\nSequential Job Steps\n\nBy default, multiple invocations of jsrun in a job script will execute\nserially in order. In this configuration, jobs will launch one at a time and\nthe next one will not start until the previous is complete. The batch node\nallocation is equal to the largest jsrun submitted, and the total walltime\nmust be equal to or greater then the sum of all jsruns issued.\n\n\n\nSimultaneous Job Steps\n\nTo execute multiple job steps concurrently, standard UNIX process\nbackgrounding can be used by adding a & at the end of the command. This\nwill return control to the job script and execute the next command immediately,\nallowing multiple job launches to start at the same time. The jsruns will not\nshare core/gpu resources in this configuration. The batch node allocation is\nequal to the sum of those of each jsrun, and the total walltime must be equal\nto or greater than that of the longest running jsrun task.\n\nA wait command must follow all backgrounded processes to prevent the job\nfrom appearing completed and exiting prematurely.\n\n\n\nThe following example executes three backgrounded job steps and waits for them\nto finish before the job ends.\n\n#!/bin/bash\n#BSUB -P ABC123\n#BSUB -W 3:00\n#BSUB -nnodes 1\n#BSUB -J RunSim123\n#BSUB -o RunSim123.%J\n#BSUB -e RunSim123.%J\n\ncd $MEMBERWORK/abc123\njsrun <options> ./a.out &\njsrun <options> ./a.out &\njsrun <options> ./a.out &\nwait\n\nAs submission scripts (and interactive sessions) are executed on batch nodes,\nthe number of concurrent job steps is limited by the per-user process limit on\na batch node, where a single user is only permitted 4096 simultaneous\nprocesses. This limit is per user on each batch node, not per batch job.\n\nEach job step will create 3 processes, and JSM management may create up to ~23\nprocesses. This creates an upper-limit of ~1350 simultaneous job steps.\n\nIf JSM or PMIX errors occur as the result of backgrounding many job steps, using the\n--immediate option to jsrun may help, as shown in the following example.\n\n#!/bin/bash\n#BSUB -P ABC123\n#BSUB -W 3:00\n#BSUB -nnodes 1\n#BSUB -J RunSim123\n#BSUB -o RunSim123.%J\n#BSUB -e RunSim123.%J\n\ncd $MEMBERWORK/abc123\njsrun <options> --immediate ./a.out\njsrun <options> --immediate ./a.out\njsrun <options> --immediate ./a.out\n\nBy default, jsrun --immediate does not produce stdout or\nstderr. To capture stdout and/or stderr when using this option,\nadditionally include --stdio_stdout/-o and/or\n--stdio_stderr/-k.\n\nUsing jslist\n\nTo view the status of multiple jobs launched sequentially or concurrently within a\nbatch script, you can use jslist to see which are completed, running, or still\nqueued. If you are using it outside of an interactive batch job, use the -c option\nto specify the CSM allocation ID number. The following example shows how to obtain the\nCSM allocation number for a non interactive job and then check its status.\n\n$ bsub test.lsf\nJob <26238> is submitted to default queue <batch>.\n\n$ bjobs -l 26238 | grep CSM_ALLOCATION_ID\nSun Feb 16 19:01:18: CSM_ALLOCATION_ID=34435\n\n$ jslist -c 34435\n  parent         cpus     gpus     exit\n  ID  ID    nrs  per RS  per RS   status    status\n ===========================================================\n   1   0    12     4       1        0       Running\n\nExplicit Resource Files (ERF)\n\nExplicit Resource Files\nprovide even more fine-granied control over how processes are mapped onto\ncompute nodes. ERFs can define job step options such as rank placement/binding,\nSMT/CPU/GPU resources, compute hosts, among many others. If you find that the\nmost common jsrun options do not readily provide the resource layout you need,\nwe recommend considering ERF files.\n\nA common source of confusion when using ERFs is how physical cores are\nenumerated. See the tutorial on ERF CPU\nIndexing for a\ndiscussion of the cpu_index_using control and its interaction with various\nSMT modes.\n\nPlease note, a known bug is currently preventing execution of most ERF use cases. We are working to resolve the issue. If you experience issues using the ERF feature, please see the work around in Known Issues.\n\n\n\nCUDA-Aware MPI\n\n<string>:2001: (INFO/1) Duplicate implicit target name: \"cuda-aware mpi\".\n\nCUDA-aware MPI and GPUDirect are often used interchangeably, but they\nare distinct topics.\n\nCUDA-aware MPI allows GPU buffers (e.g., GPU memory allocated with\ncudaMalloc) to be used directly in MPI calls rather than requiring\ndata to be manually transferred to/from a CPU buffer (e.g., using\ncudaMemcpy) before/after passing data in MPI calls. By itself,\nCUDA-aware MPI does not specify whether data is staged through\nCPU memory or, for example, transferred directly between GPUs when\npassing GPU buffers to MPI calls. That is where GPUDirect comes in.\n\nGPUDirect is a technology that can be implemented on a system to enhance\nCUDA-aware MPI by allowing data transfers directly between GPUs on the\nsame node (peer-to-peer) and/or directly between GPUs on different nodes\n(with RDMA support) without the need to stage data through CPU memory.\nOn Summit, both peer-to-peer and RDMA support are implemented. To enable\nCUDA-aware MPI in a job, use the following argument to jsrun:\n\njsrun --smpiargs=\"-gpu\" ...\n\nNot using the --smpiargs=\"-gpu\" flag might result in confusing segmentation\nfaults. If you see a segmentation fault when trying to do GPU aware MPI, check to\nsee if you have the flag set correctly.\n\nMonitoring Jobs\n\nLSF provides several utilities with which you can monitor jobs. These\ninclude monitoring the queue, getting details about a particular job,\nviewing STDOUT/STDERR of running jobs, and more.\n\nThe most straightforward monitoring is with the bjobs command. This\ncommand will show the current queue, including both pending and running\njobs. Running bjobs -l will provide much more detail about a job (or\ngroup of jobs). For detailed output of a single job, specify the job id\nafter the -l. For example, for detailed output of job 12345, you can\nrun bjobs -l 12345 . Other options to bjobs are shown below. In\ngeneral, if the command is specified with -u all it will show\ninformation for all users/all jobs. Without that option, it only shows\nyour jobs. Note that this is not an exhaustive list. See man bjobs\nfor more information.\n\n\nThe table provides a comprehensive list of commands that can be used in the Summit user guide. These commands are used to view and manage jobs in the queue. The first command, \"bjobs\", shows the current jobs in the queue for the user. The second command, \"bjobs -u all\", displays the currently queued jobs for all users. The third command, \"bjobs -P ABC123\", shows the currently queued jobs for a specific project, in this case, project ABC123. The fourth command, \"bjobs -UF\", allows the user to view the output without any formatting, which can be useful for scripting purposes. The fifth command, \"bjobs -a\", displays jobs in all states, including recently finished jobs. The sixth command, \"bjobs -l\", provides a more detailed and long output. The seventh command, \"bjobs -l 12345\", shows the long and detailed output for a specific job, in this case, job 12345. The eighth command, \"bjobs -d\", displays details for recently completed jobs. The ninth command, \"bjobs -s\", shows suspended jobs and the reasons for their suspension. The tenth command, \"bjobs -r\", displays running jobs. The eleventh command, \"bjobs -p\", shows pending jobs. Lastly, the twelfth command, \"bjobs -w\", uses a \"wide\" formatting for the output. This table serves as a useful reference for users of Summit, providing them with a variety of commands to efficiently manage their jobs in the queue.\n\n| Command | Description |\n|---------|-------------|\n| bjobs | Show your current jobs in the queue |\n| bjobs -u all | Show currently queued jobs for all users |\n| bjobs -P ABC123 | Shows currently-queued jobs for project ABC123 |\n| bjobs -UF | Don't format output (might be useful if you're using the output in a script) |\n| bjobs -a | Show jobs in all states, including recently finished jobs |\n| bjobs -l | Show long/detailed output |\n| bjobs -l 12345 | Show long/detailed output for jobs 12345 |\n| bjobs -d | Show details for recently completed jobs |\n| bjobs -s | Show suspended jobs, including the reason(s) they're suspended |\n| bjobs -r | Show running jobs |\n| bjobs -p | Show pending jobs |\n| bjobs -w | Use \"wide\" formatting for output |\n\n\n\nIf you want to check the STDOUT/STDERR of a currently running job, you\ncan do so with the bpeek command. The command supports several\noptions:\n\n\nThe table presented is a part of the summit user guide and provides a detailed description of the various commands that can be used with the bpeek function. The first command, \"bpeek -J jobname\", allows users to view the standard output and error messages for the most recently submitted job with the specified name. The second command, \"bpeek 12345\", allows users to view the standard output and error messages for a specific job with the job ID 12345. The third command, \"bpeek -f ...\", is used in conjunction with other options and enables the use of the \"tail -f\" command, which continuously displays the output of a file as it is updated, and exits once the job is completed. This table serves as a useful reference for users of the summit system, providing them with a clear understanding of the different options available for viewing job output. \n\n| Command         | Description                                                                                         |\n|-----------------|-----------------------------------------------------------------------------------------------------|\n| bpeek -J jobname| Show STDOUT/STDERR for the job you've most recently submitted with the name jobname                   |\n| bpeek 12345     | Show STDOUT/STDERR for job 12345                                                                    |\n| bpeek -f ...    | Used with other options. Makes bpeek use tail -f and exit once the job completes.                    |\n\n\n\nThe OLCF also provides jobstat, which adds dividers in the queue to\nidentify jobs as running, eligible, or blocked. Run without arguments,\njobstat provides a snapshot of the entire batch queue. Additional\ninformation, including the number of jobs in each state, total nodes\navailable, and relative job priority are also included.\n\njobstat -u <username> restricts output to only the jobs of a\nspecific user. See the jobstat man page for a full list of\nformatting arguments.\n\n$ jobstat -u <user>\n--------------------------- Running Jobs: 2 (4544 of 4604 nodes, 98.70%) ---------------------------\nJobId    Username   Project          Nodes Remain     StartTime       JobName\n331590   user     project           2     57:06      04/09 10:06:23  Not_Specified\n331707   user     project           40    39:47      04/09 11:04:04  runA\n----------------------------------------- Eligible Jobs: 3 -----------------------------------------\nJobId    Username   Project          Nodes Walltime   QueueTime       Priority JobName\n331712   user     project           80    45:00      04/09 11:06:23  501.00   runB\n331713   user     project           90    45:00      04/09 11:07:19  501.00   runC\n331714   user     project           100   45:00      04/09 11:07:49  501.00   runD\n----------------------------------------- Blocked Jobs: 1 ------------------------------------------\nJobId    Username   Project          Nodes Walltime   BlockReason\n331715   user        project           12    2:00:00    Job dependency condition not satisfied\n\nInspecting Backfill\n\nbjobs and jobstat help to identify what’s currently running and\nscheduled to run, but sometimes it’s beneficial to know how much of the\nsystem is not currently in use or scheduled for use.\n\nThe bslots command can be used to inspect backfill windows and answer\nthe question “How many nodes are currently available, and for how long\nwill they remain available?” This can be thought of as identifying gaps in\nthe system’s current job schedule. By intentionally requesting resources\nwithin the parameters of a backfill window, one can potentially shorten\ntheir queued time and improve overall system utilization.\n\nLSF uses “slots” to describe allocatable resources. Summit compute nodes have 1\nslot per CPU core, for a total of 42 per node ([2x] Power9 CPUs, each\nwith 21 cores). Since Summit nodes are scheduled in whole-node\nallocations, the output from bslots can be divided by 42 to see how\nmany nodes are currently available.\n\nBy default, bslots output includes launch node slots, which can\ncause unwanted and inflated fractional node values. The output can\nbe adjusted to reflect only available compute node slots with the\nflag  -R”select[CN]”. For example,\n\n$ bslots -R\"select[CN]\"\nSLOTS          RUNTIME\n42             25 hours 42 minutes 51 seconds\n27384          1 hours 11 minutes 50 seconds\n\n27384 compute node slots / 42 slots per node = 652 compute nodes are\navailable for 1 hour, 11 minutes, 50 seconds.\n\nA more specific bslots query could check for a backfill window with\nspace to fit a 1000 node job for 10 minutes:\n\n$ bslots -R\"select[CN]\" -n $((1000*42)) -W10\nSLOTS          RUNTIME\n127764         22 minutes 55 seconds\n\nThere is no guarantee that the slots reported by bslots will still\nbe available at time of new job submission.\n\nInteracting With Jobs\n\nSometimes it’s necessary to interact with a batch job after it has been\nsubmitted. LSF provides several commands for interacting with\nalready-submitted jobs.\n\nMany of these commands can operate on either one job or a group of jobs.\nIn general, they only operate on the most recently submitted job that\nmatches other criteria provided unless “0” is specified as the job id.\n\nSuspending and Resuming Jobs\n\nLSF supports user-level suspension and resumption of jobs. Jobs are\nsuspended with the bstop command and resumed with the bresume\ncommand. The simplest way to invoke these commands is to list the job id\nto be suspended/resumed:\n\nbstop 12345\nbresume 12345\n\nInstead of specifying a job id, you can specify other criteria that will\nallow you to suspend some/all jobs that meet other criteria such as a\njob name, a queue name, etc. These are described in the manpages for\nbstop and bresume.\n\nSignaling Jobs\n\nYou can send signals to jobs with the bkill command. While the\ncommand name suggests its only purpose is to terminate jobs, this is not\nthe case. Similar to the kill command found in Unix-like operating\nsystems, this command can be used to send various signals (not just\nSIGTERM and SIGKILL) to jobs. The command can accept both\nnumbers and names for signals. For a list of accepted signal names, run\nbkill -l. Common ways to invoke the command include:\n\n\nThe table presents a list of commands and their corresponding descriptions for the Summit user guide. The first command, \"bkill 12345\", allows users to forcefully stop a job by sending three signals in a specific order: SIGINT, SIGTERM, and SIGKILL. This allows for a controlled exit of applications that may have trapped these signals. The second command, \"bkill -s USR1 12345\", allows users to send the SIGUSR1 signal to job 12345. It is important to note that when specifying a signal by name, the \"SIG\" prefix should be omitted. Therefore, users should specify \"USR1\" instead of \"SIGUSR1\" on the bkill command line. The third command, \"bkill -s 9 12345\", allows users to send signal 9 to job 12345. This table provides a clear and concise reference for users to understand and utilize these commands effectively. \n\n| Command | Description |\n| --- | --- |\n| bkill 12345 | Force a job to stop by sending SIGINT, SIGTERM, and SIGKILL. These signals are sent in that order, so users can write applications such that they will trap SIGINT and/or SIGTERM and exit in a controlled manner. |\n| bkill -s USR1 12345 | Send SIGUSR1 to job 12345 NOTE: When specifying a signal by name, omit SIG from the name. Thus, you specify USR1 and not SIGUSR1 on the bkill command line. |\n| bkill -s 9 12345 | Send signal 9 to job 12345 |\n\n\n\nLike bstop and bresume, bkill command also supports\nidentifying the job(s) to be signaled by criteria other than the job id.\nThese include some/all jobs with a given name, in a particular queue,\netc. See man bkill for more information.\n\nCheckpointing Jobs\n\nLSF documentation mentions the bchkpnt and brestart commands for\ncheckpointing and restarting jobs, as well as the -k option to\nbsub for configuring checkpointing. Since checkpointing is very\napplication specific and a wide range of applications run on OLCF\nresources, this type of checkpointing is not configured on Summit. If\nyou wish to use checkpointing (which is highly encouraged), you’ll need\nto configure it within your application.\n\nIf you wish to implement some form of on-demand checkpointing, keep in mind\nthe bkill command is really a signaling command and you can have your\njob script/application checkpoint as a response to certain signals (such\nas SIGUSR1).\n\nOther LSF Commands\n\nThe table below summarizes some additional LSF commands that might be\nuseful.\n\n\nThe table presented below is a part of the summit user guide and provides a detailed description of two commands - bparams and bjdepinfo. The first command, bparams -a, is used to display the current parameters for LSF (Load Sharing Facility). This command is particularly useful as the behavior and available options for some LSF commands are dependent on the settings in various configuration files. With the bparams -a command, users can easily view these settings without having to manually search for the actual files. The second command, bjdepinfo, is used to show job dependency information. This command can be helpful in determining which job is causing another job to remain in a pending state. By using bjdepinfo, users can quickly identify and resolve any job dependency issues. Both of these commands are essential tools for efficiently managing and troubleshooting jobs on the summit system.\n\n| Command   | Description                                                                                                              |\n|-----------|--------------------------------------------------------------------------------------------------------------------------|\n| bparams -a| Show current parameters for LSF. The behavior/available options for some LSF commands depend on settings in various configuration files. This command shows those settings without having to search for the actual files. |\n| bjdepinfo | Show job dependency information (could be useful in determining what job is keeping another job in a pending state)        |\n\n\n\nPBS/Torque/MOAB-to-LSF Translation\n\nMore details about these commands are given elsewhere in this section;\nthe table below is simply for your convenience in looking up various LSF\ncommands.\n\nUsers of other OLCF resources are likely familiar with\nPBS-like commands which are used by the Torque/Moab instances on other\nsystems. The table below summarizes the equivalent LSF command for\nvarious PBS/Torque/Moab commands.\n\n\nThe table presents a comparison between LSF commands and their equivalent PBS/Torque/Moab commands, along with a brief description of their functions. The first row shows the command to submit a job script to the batch system, while the second row shows how to submit an interactive batch job. The third row provides a command to show all jobs currently in the queue, with a note that without the \"-u all\" argument, only the user's jobs will be shown. The fourth row shows how to get information about a specific job, and the fifth and sixth rows show how to get information about completed and pending jobs, respectively. The seventh row provides a command to get information about running jobs, and the eighth and ninth rows show how to send a signal or terminate/kill a job, respectively. The tenth and eleventh rows show how to hold or release a job, and the twelfth row shows how to get information about queues. Finally, the last row shows how to get information about job dependencies. Overall, this table serves as a useful reference for users of the Summit supercomputer, providing a quick and easy way to navigate and utilize the various commands available for managing jobs on the system.\n\n| LSF Command | PBS/Torque/Moab Command | Description |\n| --- | --- | --- |\n| bsub job.sh | qsub job.sh | Submit the job script job.sh to the batch system |\n| bsub -Is /bin/bash | qsub -I | Submit an interactive batch job |\n| bjobs -u all | qstat showq | Show jobs currently in the queue NOTE: without the -u all argument, bjobs will only show your jobs |\n| bjobs -l | checkjob | Get information about a specific job |\n| bjobs -d | showq -c | Get information about completed jobs |\n| bjobs -p | showq -i <br> showq -b <br> checkjob | Get information about pending jobs |\n| bjobs -r | showq -r | Get information about running jobs |\n| bkill | qsig | Send a signal to a job |\n| bkill | qdel | Terminate/Kill a job |\n| bstop | qhold | Hold a job/stop a job from running |\n| bresume | qrls | Release a held job |\n| bqueues | qstat -q | Get information about queues |\n| bjdepinfo | checkjob | Get information about job dependencies |\n\n\n\nThe table below shows shows LSF (bsub) command-line/batch script options\nand the PBS/Torque/Moab (qsub) options that provide similar\nfunctionality.\n\n\nThe following table provides a detailed comparison between LSF and PBS/Torque/Moab options for the Summit user guide. The first column lists the LSF options, while the second column lists the corresponding PBS/Torque/Moab options. The third column provides a brief description of each option. \n\nThe first row of the table compares the #BSUB -W 60 option in LSF with the #PBS -l walltime=1:00:00 option in PBS/Torque/Moab. This option is used to request a walltime of 1 hour for the job. \n\nThe second row compares the #BSUB -nnodes 1024 option in LSF with the #PBS -l nodes=1024 option in PBS/Torque/Moab. This option is used to request 1024 nodes for the job. \n\nThe third row compares the #BSUB -P ABC123 option in LSF with the #PBS -A ABC123 option in PBS/Torque/Moab. This option is used to charge the job to the project ABC123. \n\nThe fourth row compares the #BSUB -alloc_flags gpumps option in LSF with the No equivalent (set via environment variable) option in PBS/Torque/Moab. This option enables multiple MPI tasks to simultaneously access a GPU. \n\n| LSF Option | PBS/Torque/Moab Option | Description |\n|------------|------------------------|-------------|\n| #BSUB -W 60 | #PBS -l walltime=1:00:00 | Request a walltime of 1 hour |\n| #BSUB -nnodes 1024 | #PBS -l nodes=1024 | Request 1024 nodes |\n| #BSUB -P ABC123 | #PBS -A ABC123 | Charge the job to project ABC123 |\n| #BSUB -alloc_flags gpumps | No equivalent (set via environment variable) | Enable multiple MPI tasks to simultaneously access a GPU |\n\n\n\n\n\nEasy Mode vs. Expert Mode\n\nThe Cluster System Management (CSM) component of the job launch\nenvironment supports two methods of job submission, termed “easy” mode\nand “expert” mode. The difference in the modes is where the\nresponsibility for creating the LSF resource string is placed.\n\nIn easy mode, the system software converts options such as -nnodes in\na batch script into the resource string needed by the scheduling system.\nIn expert mode, the user is responsible for creating this string and\noptions such as -nnodes cannot be used. In easy mode, you will not be\nable to use bsub -R to create resource strings. The system will\nautomatically create the resource string based on your other bsub\noptions. In expert mode, you will be able to use -R, but you will\nnot be able to use the following options to bsub: -ln_slots,\n-ln_mem, -cn_cu, or -nnodes.\n\nMost users will want to use easy mode. However, if you need precise\ncontrol over your job’s resources, such as placement on (or avoidance\nof) specific nodes, you will need to use expert mode. To use expert\nmode, add #BSUB -csm y to your batch script (or -csm y to\nyour bsub command line).\n\nSystem Service Core Isolation\n\nOne core per socket is set aside for system service tasks. The cores are\nnot available to jsrun. When listing available resources through jsrun,\nyou will not see cores with hyperthreads 84-87 and 172-175. Isolating a\nsocket's system services to a single core helps to reduce jitter and\nimprove performance of tasks performed on the socket's remaining cores.\n\nThe isolated core always operates at SMT4 regardless of the batch job's\nSMT level.\n\nGPFS System Service Isolation\n\nBy default, GPFS system service tasks are forced onto only the isolated\ncores. This can be overridden at the batch job level using the\nmaximizegpfs argument to LSF's alloc_flags. For example:\n\n#BSUB -alloc_flags maximizegpfs\n\nThe maximizegpfs flag will allow GPFS tasks to utilize any core on the\ncompute node. This may be beneficial because it provides more resources\nfor GPFS service tasks, but it may also cause resource contention for\nthe jsrun compute job.\n\nJob Accounting on Summit\n\nJobs on Summit are scheduled in full node increments; a node's cores cannot be\nallocated to multiple jobs. Because the OLCF charges based on what a job makes\nunavailable to other users, a job is charged for an entire node even if it\nuses only one core on a node. To simplify the process, users request and are allocated\nmultiples of entire nodes through LSF.\n\nAllocations on Summit are separate from those on Andes and other OLCF resources.\n\nNode-Hour Calculation\n\nThe node-hour charge for each batch job will be calculated as follows:\n\nnode-hours = nodes requested * ( batch job endtime - batch job starttime )\n\nWhere batch job starttime is the time the job moves into a running state, and\nbatch job endtime is the time the job exits a running state.\n\nA batch job's usage is calculated solely on requested nodes and the batch job's\nstart and end time. The number of cores actually used within any particular node\nwithin the batch job is not used in the calculation. For example, if a job\nrequests (6) nodes through the batch script, runs for (1) hour, uses only (2)\nCPU cores per node, the job will still be charged for 6 nodes * 1 hour = 6\nnode-hours.\n\nViewing Usage\n\nUtilization is calculated daily using batch jobs which complete between 00:00\nand 23:59 of the previous day. For example, if a job moves into a run state on\nTuesday and completes Wednesday, the job's utilization will be recorded\nThursday. Only batch jobs which write an end record are used to calculate\nutilization. Batch jobs which do not write end records due to system failure or\nother reasons are not used when calculating utilization. Jobs which fail because\nof run-time errors (e.g. the user's application causes a segmentation fault) are\ncounted against the allocation.\n\nEach user may view usage for projects on which they are members from the command\nline tool showusage and the myOLCF site.\n\nOn the Command Line via showusage\n\nThe showusage utility can be used to view your usage from January 01\nthrough midnight of the previous day. For example:\n\n$ showusage\n  Usage:\n                           Project Totals\n  Project             Allocation      Usage      Remaining     Usage\n  _________________|______________|___________|____________|______________\n  abc123           |  20000       |   126.3   |  19873.7   |   1560.80\n\nThe -h option will list more usage details.\n\nOn the Web via myOLCF\n\nMore detailed metrics may be found on each project's usage section of the myOLCF\nsite. The following information is available\nfor each project:\n\nYTD usage by system, subproject, and project member\n\nMonthly usage by system, subproject, and project member\n\nYTD usage by job size groupings for each system, subproject, and\nproject member\n\nWeekly usage by job size groupings for each system, and subproject\n\nBatch system priorities by project and subproject\n\nProject members\n\nThe myOLCF site is provided to aid in the utilization and management of OLCF\nallocations. See the myOLCF Documentation <https://docs.olcf.ornl.gov/services_and_applications/myolcf/index.html> for more information.\n\nIf you have any questions or have a request for additional data,\nplease contact the OLCF User Assistance Center.\n\nOther Notes\n\nCompute nodes are only allocated to one job at a time; they are not\nshared. This is why users request nodes (instead of some other resource\nsuch as cores or GPUs) in batch jobs and is why projects are charged\nbased on the number of nodes allocated multiplied by the amount of time\nfor which they were allocated. Thus, a job using only 1 core on each of\nits nodes is charged the same as a job using every core and every GPU on\neach of its nodes.\n\n\n\nDebugging\n\n<string>:2475: (INFO/1) Duplicate implicit target name: \"debugging\".\n\nLinaro DDT\n\nLinaro DDT is an advanced debugging tool used for scalar, multi-threaded,\nand large-scale parallel applications. In addition to traditional\ndebugging features (setting breakpoints, stepping through code,\nexamining variables), DDT also supports attaching to already-running\nprocesses and memory debugging. In-depth details of DDT can be found in\nthe Official DDT User Guide, and\ninstructions for how to use it on OLCF systems can be found on the Debugging Software <https://docs.olcf.ornl.gov/software/debugging/index.html> page. DDT is the\nOLCF's recommended debugging software for large parallel applications.\n\nOne of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Frontier from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Frontier. For guidance in setting up the remote client see the Debugging Software <https://docs.olcf.ornl.gov/software/debugging/index.html> page.\n\nGDB\n\nGDB, the GNU Project Debugger,\nis a command-line debugger useful for traditional debugging and\ninvestigating code crashes. GDB lets you debug programs written in Ada,\nC, C++, Objective-C, Pascal (and many other languages).\n\nGDB is available on Summit under all compiler families:\n\nmodule load gdb\n\nTo use GDB to debug your application run:\n\ngdb ./path_to_executable\n\nAdditional information about GDB usage can befound on the GDB Documentation Page.\n\nValgrind\n\nValgrind is an instrumentation framework for\nbuilding dynamic analysis tools. There are Valgrind tools that can\nautomatically detect many memory management and threading bugs, and\nprofile your programs in detail. You can also use Valgrind to build new\ntools.\n\nThe Valgrind distribution currently includes five production-quality\ntools: a memory error detector, a thread error detector, a cache and\nbranch-prediction profiler, a call-graph generating cache profiler,\nand a heap profiler. It also includes two experimental tools: a data\nrace detector, and an instant memory leak detector.\n\nThe Valgrind tool suite provides a number of debugging and\nprofiling tools. The most popular is Memcheck, a memory checking tool\nwhich can detect many common memory errors such as:\n\nTouching memory you shouldn’t (eg. overrunning heap block boundaries,\nor reading/writing freed memory).\n\nUsing values before they have been initialized.\n\nIncorrect freeing of memory, such as double-freeing heap blocks.\n\nMemory leaks.\n\nValgrind is available on Summit under all compiler families:\n\nmodule load valgrind\n\nAdditional information about Valgrind usage and OLCF-provided builds can\nbe found on the Valgrind Software\nPage.\n\n\n\nOptimizing and Profiling\n\nProfiling GPU Code with NVIDIA Developer Tools\n\nNVIDIA provides developer tools for profiling any code that runs on NVIDIA\nGPUs. These are the Nsight suite of developer tools: NVIDIA Nsight Systems for\ncollecting a timeline of your application, and NVIDIA Nsight Compute for\ncollecting detailed performance information about specific GPU kernels.\n\nNVIDIA Nsight Systems\n\nThe first step to GPU profiling is collecting a timeline of your application.\n(This operation is also sometimes called \"tracing,\" that is, finding\nthe start and stop timestamps of all activities that occurred on the GPU\nor involved the GPU, such as copying data back and forth.) To do this, we\ncan collect a timeline using the command-line interface, nsys. To use\nthis tool, load the nsight-systems module.\n\nsummit> module load nsight-systems\n\nFor example, we can profile the vectorAdd CUDA sample (the CUDA samples\ncan be found in $OLCF_CUDA_ROOT/samples if the cuda module is loaded.)\n\nsummit> jsrun -n1 -a1 -g1 nsys profile -o vectorAdd --stats=true ./vectorAdd\n\n(Note that even if you do not ask for Nsight Systems to create an output file,\nbut just ask it to print summary statistics with --stats=true, it will create\na temporary file for storing the profiling data, so you will need to work on a\nfile system that can be written to from a compute node such as GPFS.)\n\nThe profiler will print several sections including information about the\nCUDA API calls made by the application, as well as any GPU kernels that were\nlaunched. Nsight Systems can be used for CUDA C++, CUDA Fortran, OpenACC,\nOpenMP offload, and other programming models that target NVIDIA GPUs, because\nunder the hood they all ultimately take the same path for generating the binary\ncode that runs on the GPU.\n\nIf you add the -o option, as above, the report will be saved to file\nwith the extension .qdrep. That report file can later be analyzed in\nthe Nsight Systems UI by selecting File > Open and locating the vectorAdd.qdrep\nfile on your filesystem. Nsight Systems does not currently have a Power9\nversion of the UI, so you will need to download the UI for your local system, which is supported on\nWindows, Mac, and Linux (x86). Then use scp or some other file transfer\nutility for copying the report file from Summit to your local machine.\n\nNsight Systems can be used for MPI runs with multiple ranks, but it is\nnot a parallel profiler and cannot combine output from multiple ranks.\nInstead, each rank must be profiled and analyzed independently. The file\nname should be unique for every rank. Nsight Systems knows how to parse\nenvironment variables with the syntax %q{ENV_VAR}, and since Spectrum\nMPI provides an environment variable for every process with its MPI rank,\nyou can do\n\nsummit> jsrun -n6 -a1 -g1 nsys profile -o vectorAdd_%q{OMPI_COMM_WORLD_RANK} ./vectorAdd\n\nThen you will have vectorAdd_0.qdrep through vectorAdd_5.qdrep.\n(Of course, in this case each rank does the same thing as this is not\nan MPI application, but it works the same way for an MPI code.)\n\nFor more details about Nsight Systems, consult the product page and the documentation. If you previously\nused nvprof and would like to start using the Nsight Developer Tools,\ncheck out this transition guide.\nAlso, in March 2020 NVIDIA presented a webinar on Nsight Systems which you\ncan watch on demand.\n\nNVIDIA Nsight Compute\n\nIndividual GPU kernels (the discrete chunks of work that are launched by\nprogramming languages such as CUDA and OpenACC) can be profiled in detail\nwith NVIDIA Nsight Compute. The typical workflow is to profile your code\nwith Nsight Systems and identify the major performance bottleneck in your\napplication. If that performance bottleneck is on the CPU, it means more\ncode should be ported to the GPU; or, if that bottleneck is in memory\nmanagement, such as copying data back and forth between the CPU and GPU,\nyou should look for opportunities to reduce that data motion. But if that\nbottleneck is a GPU kernel, then Nsight Compute can be used to collect\nperformance counters to understand whether the kernel is running efficiently\nand if there's anything you can do to improve.\n\nThe Nsight Compute command-line interface, nv-nsight-cu-cli, can be\nprefixed to your application to collect a report.\n\nsummit> module load nsight-compute\n\nsummit> jsrun -n1 -a1 -g1 nv-nsight-cu-cli ./vectorAdd\n\nSimilar to Nsight Systems, Nsight Compute will create a temporary report file,\neven when -o is not specified.\n\nThe most important output to look at is the \"GPU Speed of Light\" section,\nwhich tells you what fraction of peak memory throughput and what fraction\nof peak compute throughput you achieved. Typically if you have achieved\nhigher than 60% of the peak of either subsystem, your kernel would be\nconsidered memory-bound or compute-bound (respectively), and if you have\nnot achieved 60% of either this is often a latency-bound kernel. (A common\ncause of latency issues is not exposing enough parallelism to saturate\nthe GPU's compute capacity -- peak GPU performance can only be achieved when\nthere is enough work to hide the latency of memory accesses and to keep all\ncompute pipelines busy.)\n\nBy default, Nsight Compute will collect this performance data for every kernel\nin your application. This will take a long time in a real-world application.\nIt is recommended that you identify a specific kernel to profile and then use\nthe -k argument to just profile that kernel. (If you don't know the name of\nyour kernel, use nsys to obtain that. The flag will pattern match on any\nsubstring of the kernel name.) You can also use the -s option to skip some\nnumber of kernel calls and the -c option to specify how many invocations of\nthat kernel you want to profile.\n\nIf you want to collect information on just a specific performance measurement,\nfor example the number of bytes written to DRAM, you can do so with the\n--metrics option:\n\nsummit> jsrun -n1 -a1 -g1 nv-nsight-cu-cli -k vectorAdd --metrics dram__bytes_write.sum ./vectorAdd\n\nThe list of available metrics can be obtained with nv-nsight-cu-cli\n--query-metrics. Most metrics have both a base name and suffix. Together\nthese  make up the full metric name to pass to nv-nsight-cu-cli. To list\nthe full names for a collection of metrics, use --query-metrics-mode suffix\n--metrics <metrics list>.\n\nAs with Nsight Systems, there is a graphical user interface you can load a\nreport file into (The GUI is only available for Windows, x86_64 Linux and Mac).\nUse the -o flag to create a file (the added report extension will be\n.nsight-cuprof-report), copy it to your local system, and use the File >\nOpen File menu item. If you are using multiple MPI ranks, make sure you name\neach one independently. Nsight Compute does not yet support the %q syntax\n(this will come in a future release), so your job script will have to do the\nnaming manually; for example, you can create a simple shell script:\n\n$ cat run.sh\n#!/bin/bash\n\nnv-nsight-cu-cli -o vectorAdd_$OMPI_COMM_WORLD_RANK ./vectorAdd\n\nFor more details on Nsight Compute, check out the product page and the documentation. If you previously used\nnvprof and would like to start using Nsight Compute, check out this transition\nguide.\nAlso, in March 2020 NVIDIA presented a webinar on Nsight Compute which you can watch on\ndemand.\n\nnvprof and nvvp\n\nPrior to Nsight Systems and Nsight Compute, the NVIDIA command line profiling\ntool was nvprof, which provides both tracing and kernel profiling\ncapabilities. Like with Nsight Systems and Nsight Compute, the profiler data\noutput can be saved and imported into the NVIDIA Visual Profiler for additional\ngraphical analysis. nvprof is in maintenance mode now: it still works on\nSummit and significant bugs will be fixed, but no new feature development is\noccurring on this tool.\n\nTo use nvprof, the cuda module must be loaded.\n\nsummit> module load cuda\n\nA simple \"Hello, World!\" run using nvprof can be done by adding\n\"nvprof\" to the jsrun (see: job-launcher-jsrun <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#job-launcher-jsrun>)\nline in your batch script (see batch-scripts <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#batch-scripts>).\n\n...\njsrun -n1 -a1 -g1 nvprof ./hello_world_gpu\n...\n\nAlthough nvprof doesn't provide aggregated MPI data, the %h and\n%p output file modifiers can be used to create separate output files\nfor each host and process.\n\n...\njsrun -n1 -a1 -g1 nvprof -o output.%h.%p ./hello_world_gpu\n...\n\nThere are many various metrics and events that the profiler can capture.\nFor example, to output the number of double-precision FLOPS, you may use\nthe following:\n\n...\njsrun -n1 -a1 -g1 nvprof --metrics flops_dp -o output.%h.%p ./hello_world_gpu\n...\n\nTo see a list of all available metrics and events, use the following:\n\nsummit> nvprof --query-metrics\nsummit> nvprof --query-events\n\nWhile using nvprof on the command-line is a quick way to gain\ninsight into your CUDA application, a full visual profile is often even\nmore useful. For information on how to view the output of nvprof in\nthe NVIDIA Visual Profiler, see the NVIDIA\nDocumentation.\n\nScore-P\n\nThe Score-P measurement infrastructure is a\nhighly scalable and easy-to-use tool suite for profiling, event\ntracing, and online analysis of HPC applications. Score-P supports\nanalyzing C, C++ and Fortran applications that make use of\nmulti-processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and\naccelerators (CUDA, OpenCL, OpenACC) and combinations.\n\nFor detailed information about using Score-P on Summit and the\nbuilds available, please see the\nScore-P Software Page.\n\nVampir\n\nVampir is a software performance visualizer focused on highly\nparallel applications. It presents a unified view on an application\nrun including the use of programming paradigms like MPI, OpenMP,\nPThreads, CUDA, OpenCL and OpenACC. It also incorporates file I/O,\nhardware performance counters and other performance data sources.\nVarious interactive displays offer detailed insight into the performance\nbehavior of the analyzed application. Vampir’s scalable analysis\nserver and visualization engine enable interactive navigation of\nlarge amounts of performance data. Score-P\nand TAU generate OTF2\ntrace files for Vampir to visualize.\n\nFor detailed information about using Vampir on Summit and the builds available,\nplease see the Vampir Software Page.\n\nHPCToolkit\n\nHPCToolkit is an integrated suite of tools for measurement and analysis of program performance on computers ranging from multicore desktop systems to the nation’s largest supercomputers. HPCToolkit provides accurate measurements of a program’s work, resource consumption, and inefficiency, correlates these metrics with the program’s source code, works with multilingual, fully optimized binaries, has very low measurement overhead, and scales to large parallel systems. HPCToolkit’s measurements provide support for analyzing a program execution cost, inefficiency, and scaling characteristics both within and across nodes of a parallel system.\n\nProgramming models supported by HPCToolkit include MPI, OpenMP, OpenACC, CUDA, OpenCL, DPC++, HIP, RAJA, Kokkos, and others.\n\nBelow is an example that generates a profile and loads the results in their GUI-based viewer.\n\nmodule use /gpfs/alpine/csc322/world-shared/modulefiles/ppc64le\nmodule load hpctoolkit\n\n# 1. Profile and trace an application using CPU time and GPU performance counters\njsrun <jsrun_options> hpcrun -o <measurement_dir> -t -e CPUTIME -e gpu=nvidia <application>\n\n# 2. Analyze the binary of executables and its dependent libraries\nhpcstruct <measurement_dir>\n\n# 3. Combine measurements with program structure information and generate a database\nhpcprof -o <database_dir> <measurement_dir>\n\n# 4. Understand performance issues by analyzing profiles and traces with the GUI\nhpcviewer <database_dir>\n\nA quick summary of HPCToolkit options can be found in the HPCTookit wiki page. More detailed information on HPCToolkit can be found in the HPCToolkit User's Manual.\n\nHPCToolkit does not require a recompile to profile the code. It is recommended to use the -g optimization flag for attribution to source lines.\n\n\n\n\n\nNVIDIA V100 GPUs\n\nThe NVIDIA Tesla V100 accelerator has a peak performance of 7.8 TFLOP/s\n(double-precision) and contributes to a majority of the computational\nwork performed on Summit. Each V100 contains 80 streaming\nmultiprocessors (SMs), 16 GB (32 GB on high-memory nodes) of high-bandwidth\nmemory (HBM2), and a 6 MB L2 cache that is available to the SMs. The\nGigaThread Engine is responsible for distributing work among the SMs and\n(8) 512-bit memory controllers control access to the 16 GB (32 GB on\nhigh-memory nodes) of HBM2 memory. The V100 uses NVIDIA's NVLink interconnect\nto pass data between GPUs as well as from CPU-to-GPU.\n\n\n\nNVIDIA V100 SM\n\nEach SM on the V100 contains 32 FP64 (double-precision) cores, 64 FP32\n(single-precision) cores, 64 INT32 cores, and 8 tensor cores. A 128-KB\ncombined memory block for shared memory and L1 cache can be configured\nto allow up to 96 KB of shared memory. In addition, each SM has 4\ntexture units which use the (configured size of the) L1 cache.\n\n\n\nHBM2\n\nEach V100 has access to 16 GB (32GB for high-memory nodes) of\nhigh-bandwidth memory (HBM2), which can be accessed at speeds of\nup to 900 GB/s. Access to this memory is controlled by (8) 512-bit\nmemory controllers, and all accesses to the high-bandwidth memory\ngo through the 6 MB L2 cache.\n\nNVIDIA NVLink\n\nThe processors within a node are connected by NVIDIA's NVLink\ninterconnect. Each link has a peak bandwidth of 25 GB/s (in each\ndirection), and since there are 2 links between processors, data can be\ntransferred from GPU-to-GPU and CPU-to-GPU at a peak rate of 50 GB/s.\n\nThe 50-GB/s peak bandwidth stated above is for data transfers\nin a single direction. However, this bandwidth can be achieved in both\ndirections simultaneously, giving a peak \"bi-directional\" bandwidth of\n100 GB/s between processors.\n\nThe figure below shows a schematic of the NVLink connections between the\nCPU and GPUs on a single socket of a Summit node.\n\n\n\nVolta Multi-Process Service\n\nWhen a CUDA program begins, each MPI rank creates a separate CUDA\ncontext on the GPU, but the scheduler on the GPU only allows one CUDA\ncontext (and so one MPI rank) at a time to launch on the GPU. This means\nthat multiple MPI ranks can share access to the same GPU, but each rank\ngets exclusive access while the other ranks wait (time-slicing). This\ncan cause the GPU to become underutilized if a rank (that has exclusive\naccess) does not perform enough work to saturate the resources of the\nGPU. The following figure depicts such time-sliced access to a pre-Volta\nGPU.\n\n\n\nThe Multi-Process Service (MPS) enables multiple processes (e.g. MPI ranks) to\nconcurrently share the resources on a single GPU. This is accomplished by\nstarting an MPS server process, which funnels the work from multiple CUDA\ncontexts (e.g. from multiple MPI ranks) into a single CUDA context. In some\ncases, this can increase performance due to better utilization of the resources.\nThe figure below illustrates MPS on a pre-Volta GPU.\n\n\n\nVolta GPUs improve MPS with new capabilities. For instance, each Volta\nMPS client (MPI rank) is assigned a \"subcontext\" that has its own GPU\naddress space, instead of sharing the address space with other clients.\nThis isolation helps protect MPI ranks from out-of-range reads/writes\nperformed by other ranks within CUDA kernels. Because each subcontext\nmanages its own GPU resources, it can submit work directly to the GPU\nwithout the need to first pass through the MPS server. In addition,\nVolta GPUs support up to 48 MPS clients (up from 16 MPS clients on\nPascal).\n\n\n\nFor more information, please see the following document from NVIDIA:\nhttps://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf\n\nUnified Memory\n\nUnified memory is a single virtual address space that is accessible to\nany processor in a system (within a node). This means that programmers\nonly need to allocate a single unified-memory pointer (e.g. using\ncudaMallocManaged) that can be accessed by both the CPU and GPU, instead\nof requiring separate allocations for each processor. This \"managed\nmemory\" is automatically migrated to the accessing processor, which\neliminates the need for explicit data transfers.\n\n\n\nOn Pascal-generation GPUs and later, this automatic migration is\nenhanced with hardware support. A page migration engine enables GPU page\nfaulting, which allows the desired pages to be migrated to the GPU \"on\ndemand\" instead of the entire \"managed\" allocation. In addition, 49-bit\nvirtual addressing allows programs using unified memory to access the\nfull system memory size. The combination of GPU page faulting and larger\nvirtual addressing allows programs to oversubscribe the system memory,\nso very large data sets can be processed. In addition, new CUDA API\nfunctions introduced in CUDA8 allow users to fine tune the use of\nunified memory.\n\nUnified memory is further improved on Volta GPUs through\nthe use of access counters that can be used to automatically tune\nunified memory by determining where a page is most often accessed.\n\nFor more information, please see the following section of NVIDIA's\nCUDA Programming Guide:\nhttp://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-unified-memory-programming-hd\n\nIndependent Thread Scheduling\n\nThe V100 supports independent thread scheduling, which allows threads to\nsynchronize and cooperate at sub-warp scales. Pre-Volta GPUs implemented\nwarps (groups of 32 threads which execute instructions in\nsingle-instruction, multiple thread - SIMT - mode) with a single call\nstack and program counter for a warp as a whole.\n\n\n\nWithin a warp, a mask is used to specify which threads are currently\nactive when divergent branches of code are encountered. The (active)\nthreads within each branch execute their statements serially before\nthreads in the next branch execute theirs. This means that programs on\npre-Volta GPUs should avoid sub-warp synchronization; a sync point in\nthe branches could cause a deadlock if all threads in a warp do not\nreach the synchronization point.\n\n\n\nThe Tesla V100 introduces warp-level synchronization by implementing warps with\na program counter and call stack for each individual thread (i.e.  independent\nthread scheduling).\n\n\n\nThis implementation allows threads to diverge and synchronize at the sub-warp\nlevel using the __syncwarp() function. The independent thread scheduling\nenables the thread scheduler to stall execution of any thread, allowing other\nthreads in the warp to execute different statements. This means that threads in\none branch can stall at a sync point and wait for the threads in the other\nbranch to reach their sync point.\n\n\n\nFor more information, please see the following section of NVIDIA's CUDA\nProgramming Guide:\nhttp://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#independent-thread-scheduling-7-x\n\nTensor Cores\n\nThe Tesla V100 contains 640 tensor cores (8 per SM) intended to enable\nfaster training of large neural networks. Each tensor core performs a\nD = AB + C operation on 4x4 matrices. A and B are FP16 matrices,\nwhile C and D can be either FP16 or FP32:\n\n\n\nEach of the 16 elements that result from the AB matrix multiplication\ncome from 4 floating-point fused-multiply-add (FMA) operations\n(basically a dot product between a row of A and a column of B). Each\nFP16 multiply yields a full-precision product which is accumulated in a\nFP32 result:\n\n\n\nEach tensor core performs 64 of these FMA operations per clock. The 4x4\nmatrix operations outlined here can be combined to perform matrix\noperations on larger (and higher dimensional) matrices.\n\nUsing the Tensor Cores on Summit\n\nThe NVIDIA Tesla V100 GPUs in Summit are capable of over 7TF/s of\ndouble-precision and 15 TF/s of single-precision floating point performance.\nAdditionally, the V100 is capable of over 120 TF/s of half-precision floating\npoint performance when using its Tensor Core feature. The Tensor Cores are\npurpose-built accelerators for half-precision matrix multiplication operations.\nWhile they were designed especially to accelerate machine learning workflows,\nthey are exposed through several other APIs that are useful to other HPC\napplications. This section provides information for using the V100 Tensor\nCores.\n\nThe V100 Tensor Cores perform a warp-synchronous multiply and accumulate of\n16-bit matrices in the form of D = A * B + C. The operands of this matrix\nmultiplication are 16-bit A and B matrices, while the C and D accumulation\nmatrices may be 16 or 32-bit matrices with comparable performance for either\nprecision.\n\n\n\nHalf precision floating point representation has a dramatically lower range of\nnumbers than Double or Single precision. Half precision representation consists\nof 1 sign bit, a 5-bit exponent, and a 10-bit mantissa. This results in a\ndynamic range of 5.96e-8 to 65,504\n\nTensor Core Programming Models\n\nThis section details a variety of high and low-level Tensor Core programming\nmodels. Which programming model is appropriate to a given application is highly\nsituational, so this document will present multiple programming models to allow\nthe reader to evaluate each for their merits within the needs of the\napplication.\n\ncuBLAS Library\n\ncuBLAS is NVIDIA’s implementation of the Basic Linear Algebra Subroutines\nlibrary for GPUs. It contains not only the Level 1, 2, and 3 BLAS routines, but\nseveral extensions to these routines that add important capabilities to the\nlibrary, such as the ability to batch operations and work with varying\nprecisions.\n\nThe cuBLAS libraries provides access to the TensorCores using 3 different\nroutines, depending on the application needs. The cublasHgemm routine\nperforms a general matrix multiplication of half-precision matrices. The\nnumerical operands to this routine must be of type half and math mode must be\nset to CUBLAS_TENSOR_OP_MATH to enable Tensor Core use. Additionally, if the\ncublasSgemm routine\nwill down-convert from single precision to half precision when the math mode is\nset to CUBLAS_TENSOR_OP_MATH, enabling simple conversion from SGEMM to HGEMM\nusing Tensor Cores. For either of these two methods the cublasSetMathMode function\nmust be used to change from CUBLAS_DEFAULT_MATH to CUBLAS_TENSOR_OP_MATH mode.\n\ncuBLAS provides a non-standard extension of GEMM with the cublasGemmEx routine, which\nprovides additional flexibility about the data types of the operands. In\nparticular, the A, B, and C matrices can be of arbitrary and different types,\nwith the types of each declared using the Atype, Btype, and Ctype parameters.\nThe algo parameter works similar to the math mode above. If the math mode is\nset to CUBLAS_TESNOR_OP_MATH and the algo parameter is set to\nCUBLAS_GEMM_DEFAULT, then the Tensor Cores will be used. If algo is\nCUBLAS_GEMM_DEFAULT_TENSOR_OP or CUBLAS_GEMM_ALGO{0-15}_TENSOR_OP, then the\nTensor Cores will be used regardless of the math setting. The table below\noutlines the rules stated in the past two paragraphs.\n\n\nThe table above presents information related to the summit user guide, specifically regarding the math mode options for CUBLAS. The first column lists the different math modes available, with the first option being \"CUBLAS_DEFAULT_MATH\" and the second option being \"CUBLAS_TENSOR_OP_MATH\". The second column provides a list of functions that are allowed to be used with each math mode. For the \"CUBLAS_DEFAULT_MATH\" option, the functions cublasHgemm, cublasSgemm, and cublasGemmEx with the algorithm set to \"DEFAULT\" are allowed. However, for the \"CUBLAS_TENSOR_OP_MATH\" option, these functions are disallowed. The third column indicates whether the function cublasGemmEx with the algorithm set to \"*_TENSOR_OP\" is allowed or not for each math mode. In this case, it is allowed for both \"CUBLAS_TENSOR_OP_MATH\" and \"CUBLAS_DEFAULT_MATH\" options. This table serves as a useful reference for users of the summit user guide to understand the restrictions and allowances for different math modes and functions. \n\n| Math Mode | Allowed Functions | cublasGemmEx(algo=*_TENSOR_OP) |\n|-----------|--------------------|---------------------------------|\n| CUBLAS_DEFAULT_MATH | cublasHgemm, cublasSgemm, cublasGemmEx(algo=DEFAULT) | Disallowed |\n| CUBLAS_TENSOR_OP_MATH | Disallowed | Allowed |\n| CUBLAS_GemmEx(algo=*_TENSOR_OP) | Allowed | Allowed |\n\n\n\nWhen using any of these methods to access the Tensor Cores, the M, N, K, LDA,\nLDB, LDC, and A, B, and C pointers must all be aligned to 8 bytes due to the\nhigh bandwidth necessary to utilize the Tensor Cores effective.\n\nMany of the routines listed above are also available in batched form, see the\ncuBLAS documentation for\nmore information. Advanced users wishing to have increased control over the\nspecifics of data layout, type, and underlying algorithms may wish to use the\nmore advanced cuBLAS-Lt interface. This\ninterface uses the same underlying GPU kernels, but provides developers with a\nhigher degree of control.\n\nIterative Refinement of Linear Solvers\n\nIterative Refinement is a technique for performing linear algebra solvers in a\nreduced precision, then iterating to improve the results and return them to\nfull precision. This technique has been used for several years to use 32-bit\nmath operations and achieve 64-bit results, which often results in a speed-up\ndue to single precision math often have a 2X performance advantage on modern\nCPUs and many GPUs. NVIDIA and the University of Tennessee have been working to\nextend this technique to perform operations in half-precision and obtain higher\nprecision results. One such place where this technique has been applied is in\ncalculating an LU factorization of the linear system Ax = B. This operation is\ndominated by a matrix multiplication operation, which is illustrated in green\nin the image below. It is possible to perform the GEMM operations at a reduced\nprecision, while leaving the panel and trailing matrices in a higher precision.\nThis technique allows for the majority of the math operations to be done at the\nhigher FP16 throughput. The matrix used in the GEMM is generally not square,\nwhich is often the best performing GEMM operation, but is referred to as rank-k\nand generally still very fast when using matrix multiplication libraries.\n\n\n\nA summary of the algorithm used for calculating in mixed precision is in the\nfollowing image.\n\n\n\nWe see in the graph below that it is possible to achieved a 3-4X performance\nimprovement over the double-precision solver, while achieving the same level of\naccuracy. It has also been observed that the use of Tensor Cores makes the\nproblem more likely to converge than strict half-precision GEMMs due to the\nability to accumulate into 32-bit results.\n\n\n\nNVIDIA will be shipping official support for IR solvers in their cuSOLVER\nlibrary in the latter half of 2019. The image below provides estimated release\ndates, which are subject to change.\n\n\n\nAutomatic Mixed Precision (AMP) in Machine Learning Frameworks\n\nNVIDIA has a Training With Mixed Precision guide available for developers\nwishing to explicitly use mixed precision and Tensor Cores in their training of\nneural networks. This is a good place to start when investigating Tensor Cores\nfor machine learning applications. Developers should specifically read the\nOptimizing For Tensor Cores section.\n\nNVIDIA has also integrated a technology called Automatic Mixed Precision (AMP)\ninto several common frameworks, TensorFlow, PyTorch, and MXNet at time of\nwriting. In most cases AMP can be enabled via a small code change or via\nsetting and environment variable. AMP does not strictly replace all matrix\nmultiplication operations with half precision, but uses graph optimization\ntechniques to determine whether a given layer is best run in full or half\nprecision.\n\nExamples are provided for using AMP, but the following sections summarize the\nusage in the three supported frameworks.\n\nTensorFlow\n\nWith TensorFlow AMP can be enabled using one of the following techniques.\n\nos.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'\n\nOR\n\nexport TF_ENABLE_AUTO_MIXED_PRECISION=1\n\nExplicit optimizer wrapper available in NVIDIA Container 19.07+, TF 1.14+, TF\n2.0:\n\nopt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)\n\nPyTorch\n\nAdding the following to a PyTorch model will enable AMP:\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\nwith amp.scale_loss(loss, optimizer) as scaled_loss:\n  scaled_loss.backward()\n\nMXNet\n\nThe code below will enable AMP for MXNet:\n\namp.init()\namp.init_trainer(trainer)\nwith amp.scale_loss(loss, trainer) as scaled_loss:\n  autograd.backward(scaled_loss)\n\nWMMA\n\nThe Warp Matrix Multiply and Accumulate (WMMA) API was introduced in CUDA 9\nexplicitly for programming the Tesla V100 Tensor Cores. This is a low-level API\nthat supports loading matrix data into fragments within the threads of a warp,\napplying a Tensor Core multiplication on that data, and then restoring it to\nthe main GPU memory. This API is called within CUDA kernels and all WMMA\noperations are warp-synchronous, meaning the threads in a warp will leave the\noperation synchronously. Examples are available for using the WMMA instructions\nin C++ and CUDA Fortran. The image below demonstrates the general pattern for\nWMMA usage.\n\n\n\nThe example above performs a 16-bit accumulate operation, but 32-bit is also\nsupported. Please see the provided samples and the WMMA documentation for\nmore details.\n\nCUDA 10 introduced a lower-level alternative to WMMA with the mma.sync()\ninstruction. This is a very low-level instruction that requires the programmer\nhandle the data movement provided by WMMA explicitly, but is capable of higher\nperformance. Details of mma.sync\ncan be found in the PTX documentation and examples for using this feature via\nCUTLASS cane be found in the second half of this GTC presentation.\n\nCUTLASS\n\n<string>:5: (INFO/1) Duplicate implicit target name: \"cutlass\".\n\nCUTLASS is an open-source library\nprovided by NVIDIA for building matrix multiplication operations using C++\ntemplates. The goal is to provide performance that is nearly as good as the\nhand-tuned cuBLAS library, but in a more expressive, composible manner.\n\nThe CUTLASS library provides a variety of primitives that are optimized for\nproper data layout and movement to achieve the maximum possible performance of\na matrix multiplation on an NVIDIA GPU. These include iterators for blocking,\nloading, and storing matrix tiles, plus optimized classes for transforming the\ndata and performing the actual multiplication. CUTLASS provides extensive\ndocumentation of\nthese features and examples have been provided. Interested developers are also\nencouraged to watch the CUTLASS introduction video\nfrom GTC2018.\n\nMeasuring Tensor Core Utilization\n\nWhen attempting to use Tensor Cores it is useful to measure and confirm that\nthe Tensor Cores are being used within your code. For implicit use via a\nlibrary like cuBLAS, the Tensor Cores will only be used above a certain\nthreshold, so Tensor Core use should not be assumed. The NVIDIA Tools provide a\nperformance metric to measure Tensor Core utilization on a scale from 0 (Idle)\nto 10 (Max) utilization.\n\nWhen using NVIDIA’s nvprof profiler, one should add the -m\ntensor_precision_fu_utilization option to measure Tensor Core utilization.\nBelow is the output from measuring this metric on one of the example programs.\n\n$ nvprof -m tensor_precision_fu_utilization ./simpleCUBLAS\n==43727== NVPROF is profiling process 43727, command: ./simpleCUBLAS\nGPU Device 0: \"Tesla V100-SXM2-16GB\" with compute capability 7.0\n\nsimpleCUBLAS test running..\nsimpleCUBLAS test passed.\n==43727== Profiling application: ./simpleCUBLAS\n==43727== Profiling result:\n==43727== Metric result:\nInvocations                               Metric Name                           Metric Description         Min         Max         Avg\nDevice \"Tesla V100-SXM2-16GB (0)\"\n    Kernel: volta_h884gemm_128x64_ldg8_nn\n          1           tensor_precision_fu_utilization   Tensor-Precision Function Unit Utilization     Low (3)     Low (3)     Low (3)\n\nNVIDIA’s Nsight Compute may also be used to measure tensor core utilization via\nthe sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_active metric, as\nfollows:\n\n$ nv-nsight-cu-cli --metrics sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_active ./cudaTensorCoreGemm\n\n[  compute_gemm, 2019-Aug-08 12:48:39, Context 1, Stream 7\n      Section: Command line profiler metrics\n      ----------------------------------------------------------------------\n      sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_active                    %                       43.44\n      ----------------------------------------------------------------------\n\nWhen to Try Tensor Cores\n\nTensor Cores provide the potential for an enormous performance boost over\nfull-precision operations, but when their use is appropriate is highly\napplication and even problem independent. Iterative Refinement techniques can\nsuffer from slow or possible a complete lack of convergence if the condition\nnumber of the matrix is very large. By using Tensor Cores, which support 32-bit\naccumulation, rather than strict 16-bit math operations, iterative refinement\nbecomes a viable option in a much larger number of cases, so it should be\nattempted when an application is already using a supported solver.\n\nEven if iterative techniques are not available for an application, direct use\nof Tensor Cores may be beneficial if at least the A and B matrices can be\nconstructed from the input data without significant loss of precision. Since\nthe C and D matrices may be 32-bit, the output may have a higher degree of\nprecision than the input. It may be possible to try these operations\nautomatically by setting the math mode in cuBLAS, as detailed above, to\ndetermine whether the loss of precision is an acceptable trade-off for\nincreased performance in a given application. If it is, the cublasGemmEx API\nallows the programmer to control when the conversion to 16-bit occurs, which\nmay result in higher throughput than allowing the cuBLAS library to do the\nconversion at call time.\n\nSome non-traditional uses of Tensor Cores can come from places where integers\nthat fall within the FP16 range are used in an application. For instance, in\n“Attacking the Opioid Epidemic: Determining the Epistatic and Pleiotropic\nGenetic Architectures for Chronic Pain and Opioid Addiction,” a 2018 Gordon\nBell Prize-winning paper, the authors used Tensor Cores in place of small\nintegers, allowing them very high performance over performing the same\ncalculation in integer space. This technique is certainly not applicable to all\napplications, but does show that Tensor Cores may be used in algorithms that\nmight not have been represented by a floating point matrix multiplication\notherwise.\n\nLastly, when performing the training step of a deep learning application it is\noften beneficial to do at least some of the layer calculations in reduced\nprecision. The AMP technique described above can be tried with little to know\ncode changes, making it highly advisable to attempt in any machine learning\napplication.\n\nTensor Core Examples and Other Materials\n\nNVIDIA has provided several example codes for using Tensor Cores from a variety\nof the APIs listed above. These examples can be found on GitHub.\n\nNVIDIA Tensor Core Workshop (August 2018): slides,\nrecording (coming soon)\n\nTesla V100 Specifications\n\n\nThe table provides detailed information about the compute capabilities and specifications of the Summit user guide. The first column lists the compute capability, which is 7.0 for this particular guide. The next two rows show the peak double and single precision floating point performance, which are 7.8 TFLOP/s and 15.7 TFLOP/s respectively. The following three rows display the number of CUDA cores for single and double precision, as well as the number of Tensor cores. The clock frequency is listed as 1530 MHz, and the memory bandwidth is 900 GB/s. The memory size is either 16 or 32 GB, and there is a 6 MB L2 cache. The shared memory size per streaming multiprocessor (SM) is configurable up to 96 KB, and there is a constant memory of 64 KB. The register file size per SM is 256 KB, with 65536 32-bit registers per SM. The maximum number of registers per thread is 255. The number of multiprocessors (SMs) is 80, and the warp size is 32 threads. The maximum resident warps, blocks, and threads per SM are 64, 32, and 2048 respectively. The maximum threads per block is 1024, and the maximum block dimensions are 1024, 1024, 64. The maximum grid dimensions are 2147483647, 65535, 65535. Lastly, the table mentions that the maximum number of MPS clients is 48. \n\n| Compute Capability | 7.0 |\n| Peak double precision floating point performance | 7.8 TFLOP/s |\n| Peak single precision floating point performance | 15.7 TFLOP/s |\n| Single precision CUDA cores | 5120 |\n| Double precision CUDA cores | 2560 |\n| Tensor cores | 640 |\n| Clock frequency | 1530 MHz |\n| Memory Bandwidth | 900 GB/s |\n| Memory size (HBM2) | 16 or 32 GB |\n| L2 cache | 6 MB |\n| Shared memory size / SM | Configurable up to 96 KB |\n| Constant memory | 64 KB |\n| Register File Size | 256 KB (per SM) |\n| 32-bit Registers | 65536 (per SM) |\n| Max registers per thread | 255 |\n| Number of multiprocessors (SMs) | 80 |\n| Warp size | 32 threads |\n| Maximum resident warps per SM | 64 |\n| Maximum resident blocks per SM | 32 |\n| Maximum resident threads per SM | 2048 |\n| Maximum threads per block | 1024 |\n| Maximum block dimensions | 1024, 1024, 64 |\n| Maximum grid dimensions | 2147483647, 65535, 65535 |\n| Maximum number of MPS clients | 48 |\n\n\n\nFurther Reading\n\nFor more information on the NVIDIA Volta architecture, please visit the\nfollowing (outside) links.\n\nNVIDIA Volta Architecture White Paper\n\nNVIDIA PARALLEL FORALL blog article\n\n\n\nBurst Buffer\n\nNVMe (XFS)\n\nEach compute node on Summit has a 1.6TB Non-Volatile Memory (NVMe) storage device (high-memory nodes have a 6.4TB NVMe storage device), colloquially known as a \"Burst Buffer\" with\ntheoretical performance peak of 2.1 GB/s for writing and 5.5 GB/s for reading.\n100GB of each NVMe is reserved for NFS cache to help speed access to common\nlibraries. When calculating maximum usable storage size, this cache and\nformatting overhead should be considered; We recommend a maximum storage of\n1.4TB (6TB for high-memory nodes). The NVMes could be used to reduce the time that applications wait for\nI/O. Using an SSD drive per compute node, the burst buffer will be used to\ntransfer data to or from the drive before the application reads a file or\nafter it writes a file.  The result will be that the application benefits from\nnative SSD performance for a portion of its I/O requests. Users are not\nrequired to use the NVMes.  Data can also be written directly to the parallel\nfilesystem.\n\n\n\nThe NVMes on Summit are local to each node.\n\nCurrent NVMe Usage\n\nTools for using the burst buffers are still under development.  Currently, the\nuser will have access to a writeable directory on each node's NVMe and then\nexplicitly move data to and from the NVMes with posix commands during a job.\nThis mode of usage only supports writing file-per-process or file-per-node.\nIt does not support automatic \"n to 1\" file writing, writing from multiple nodes\nto a single file.  After a job completes the NVMes are trimmed, a process\nthat irreversibly deletes data from the devices, so all desired data from the\nNVMes will need to be copied back to the parallel filesystem before the job\nends. This largely manual mode of usage will not be the recommended way to use\nthe burst buffer for most applications because tools are actively being\ndeveloped to automate and improve the NVMe transfer and data management process.\nHere are the basic steps for using the BurstBuffers in their current limited\nmode of usage:\n\nModify your application to write to /mnt/bb/$USER, a directory that will be\ncreated on each NVMe.\n\nModify either your application or your job submission script to copy the\ndesired data from /mnt/bb/$USER back to the parallel filesystem before the\njob ends.\n\nModify your job submission script to include the -alloc_flags NVME  bsub\noption. Then on each reserved Burst Buffer node will be available a directory\ncalled /mnt/bb/$USER.\n\nSubmit your bash script or run the application.\n\nAssemble the resulting data as needed.\n\nInteractive Jobs Using the NVMe\n\nThe NVMe can be setup for test usage within an interactive job as follows:\n\nbsub -W 30 -nnodes 1 -alloc_flags \"NVME\" -P project123 -Is bash\n\nThe -alloc_flags NVME option will create a directory called /mnt/bb/$USER on\neach requested node's NVMe. The /mnt/bb/$USER directories will be writeable\nand readable until the interactive job ends. Outside of a job /mnt/bb/ will\nbe empty and you will not be able to write to it.\n\nNVMe Usage Example\n\nThe following example illustrates how to use the burst buffers (NVMes) by\ndefault on Summit. This example uses a submission script, check_nvme.lsf. It is\nassumed that the files are saved in the user's GPFS scratch area,\n/gpfs/alpine/scratch/$USER/projid, and that the user is operating from there as\nwell. Do not forget that for all the commands on NVMe, it is required to use\njsrun. This will submit a job to run on one node.\n\nJob submssion script: check_nvme.lsf.\n\n#!/bin/bash\n#BSUB -P project123\n#BSUB -J name_test\n#BSUB -o nvme_test.o%J\n#BSUB -W 2\n#BSUB -nnodes 1\n#BSUB -alloc_flags NVME\n\n#Declare your project in the variable\nprojid=xxxxx\ncd /gpfs/alpine/scratch/$USER/$projid\n\n#Save the hostname of the compute node in a file\njsrun -n 1 echo $HOSTNAME > test_file\n\n#Check what files are saved on the NVMe, always use jsrun to access the NVMe devices\njsrun -n 1 ls -l /mnt/bb/$USER/\n\n#Copy the test_file in your NVMe\njsrun -n 1 cp test_file /mnt/bb/$USER/\n\n#Delete the test_file from your local space\nrm test_file\n\n#Check again what the NVMe folder contains\njsrun -n 1 ls -l /mnt/bb/$USER/\n\n#Output of the test_file contents\njsrun -n 1 cat /mnt/bb/$USER/test_file\n\n#Copy the file from the NVMe to your local space\njsrun -n 1 cp /mnt/bb/$USER/test_file .\n\n#Check the file locally\nls -l test_file\n\nTo run this example: bsub ./check_nvme.lsf.   We could include all the\ncommands in a script and call this file as a jsrun argument in an interactive\njob, in order to avoid changing numbers of processes for all the jsrun\ncalls. You can see in the table below an example of the differences in a\nsubmission script for executing an application on GPFS and NVMe. In the example,\na binary ./btio reads input from an input file and generates output files.\nIn this particular case we copy the binary and the input file onto the NVMe, but\nthis depends on the application as it is not always necessary, we can execute\nthe binary on the GPFS and write/read the data from NVMe if it is supported by\nthe application.\n\n\nThe table above provides a comparison between using GPFS and NVMe for the NAS-BTIO job on Summit. The first column lists the commands and options used for the job when using GPFS, while the second column lists the commands and options used for the job when using NVMe. The first row includes the bash script and project ID used for the job, while the second row includes the job name and output file name. The third row specifies the error file name and the maximum wall time for the job. The fourth row indicates the number of nodes used for the job, with both GPFS and NVMe using only one node. The fifth row includes the command to list the files in the current directory, while the sixth row specifies the allocation flags for NVMe. The remaining rows include commands for copying files, setting the BBPATH variable, and running the job with specific parameters. The last row includes the command to copy all files from the BBPATH directory to the current directory. This table serves as a useful reference for users who want to compare the commands and options used for running the NAS-BTIO job with GPFS and NVMe on Summit. \n\n| Using GPFS | Using NVMe |\n|------------|------------|\n| #!/bin/bash | #!/bin/bash |\n| #BSUB -P xxx | #BSUB -P xxx |\n| #BSUB -J NAS-BTIO | #BSUB -J NAS-BTIO |\n| #BSUB -o nasbtio.o%J | #BSUB -o nasbtio.o%J |\n| #BSUB -e nasbtio.e%J | #BSUB -e nasbtio.e%J |\n| #BSUB -W 10 | #BSUB -W 10 |\n| #BSUB -nnodes 1 | #BSUB -nnodes 1 |\n| ls -l ` | #BSUB -alloc_flags nvme |\n| export BBPATH=/mnt/bb/$USER/ | |\n| jsrun -n 1 cp btio ${BBPATH} | |\n| jsrun -n 1 cp input* ${BBPATH} | |\n| jsrun -n 1 -a 16 -c 16 -r 1 ${BBPATH}/btio | |\n| jsrun -n 1 ls -l ${BBPATH}/ | |\n| jsrun -n 1 cp ${BBPATH}/* . | |\n\n\n\nWhen a user occupies more than one compute node, then they are using more NVMes\nand the I/O can scale linearly. For example in the following plot you can observe\nthe scalability of the IOR benchmark on 2048 compute nodes on Summit where the\nwrite performance achieves 4TB/s and the read 11.3 TB/s\n\n\n\nRemember that by default NVMe support one file per MPI process up to one file\nper compute node. If users desire a single file as output from data staged on\nthe NVMe they will need to construct it.  Tools to save automatically checkpoint\nfiles from NVMe to GPFS as also methods that allow automatic n to 1 file writing\nwith NVMe staging are under development.   Tutorials about NVME:   Burst Buffer\non Summit (slides,\nvideo) Summit Burst Buffer Libraries (slides,\nvideo).\n\n\n\nSpectral Library\n\nSpectral is a portable and transparent middleware library to enable use of the\nnode-local burst buffers for accelerated application output on Summit. It is\nused to transfer files from node-local NVMe back to the parallel GPFS file\nsystem without the need of the user to interact during the job execution.\nSpectral runs on the isolated core of each reserved node, so it does not occupy\nresources and based on some parameters the user could define which folder to be\ncopied to the GPFS. In order to use Spectral, the user has to do the following\nsteps in the submission script:\n\nRequest Spectral resources instead of NVMe\n\nDeclare the path where the files will be saved in the node-local NVMe\n(PERSIST_DIR)\n\nDeclare the path on GPFS where the files will be copied (PFS_DIR)\n\nExecute the script spectral_wait.py when the application is finished in order\nto copy the files from NVMe to GPFS\n\nThe following table shows the differences of executing an application on GPFS,\nNVMe, and NVMe with Spectral. This example is using one compute node. We copy\nthe executable and input file for the NVMe cases but this is not always\nnecessary. Depending on the application, you could execute the binary from the\nGPFS and save the output files on NVMe. Adjust your parameters to copy, if\nnecessary, the executable and input files onto all the NVMe devices.\n\n\nThe table presented here is a user guide for the Summit supercomputer, providing instructions for using GPFS, NVMe, and NVME with Spectral library. The first column lists the commands to be used in the bash shell, including setting project ID, job name, output and error files, time limit, and number of nodes. The second column includes additional commands for using NVMe, such as loading the Spectral module and setting the BBPATH and PERSIST_DIR variables. The third column provides instructions for using NVME with the Spectral library, including setting the PFS_DIR variable and using jsrun to copy files and run the btio program. The final row includes a command for checking the status of the job using the spectral_wait.py script. This table serves as a comprehensive guide for users to effectively utilize the Summit supercomputer for their computing needs.\n\n| Command/Instructions | Using GPFS | Using NVMe | Using NVME with Spectral library |\n| --- | --- | --- | --- |\n| #!/bin/bash | #!/bin/bash | #!/bin/bash |\n| #BSUB -P xxx | #BSUB -P xxx | #BSUB -P xxx |\n| #BSUB -J NAS-BTIO | #BSUB -J NAS-BTIO | #BSUB -J NAS-BTIO |\n| #BSUB -o nasbtio.o%J | #BSUB -o nasbtio.o%J | #BSUB -o nasbtio.o%J |\n| #BSUB -e nasbtio.e%J | #BSUB -e nasbtio.e%J | #BSUB -e nasbtio.e%J |\n| #BSUB -W 10 | #BSUB -W 10 | #BSUB -W 10 |\n| #BSUB -nnodes 1 | #BSUB -nnodes 1 | #BSUB -nnodes 1 |\n| | #BSUB -alloc_flags nvme | #BSUB -alloc_flags spectral |\n| | | module load spectral |\n| | export BBPATH=/mnt/bb/$USER | export BBPATH=/mnt/bb/$USER |\n| | | export PERSIST_DIR=${BBPATH} |\n| | | export PFS_DIR=$PWD/spect/ |\n| jsrun -n 1 cp btio ${BBPATH} | jsrun -n 1 cp btio ${BBPATH} | jsrun -n 1 cp btio ${BBPATH} |\n| jsrun -n 1 cp input* ${BBPATH} | jsrun -n 1 cp input* ${BBPATH} | jsrun -n 1 cp input* ${BBPATH} |\n| | jsrun -n 1 -a 16 -c 16 -r 1 ./btio | jsrun -n 1 -a 16 -c 16 -r 1 ${BBPATH}/btio | jsrun -n 1 -a 16 -c 16 -r 1 ${BBPATH}/btio |\n| ls -l | jsrun -n 1 ls -l ${BBPATH}/ | jsrun -n 1 ls -l ${BBPATH}/ |\n| | jsrun -n 1 cp ${BBPATH}/* . | spectral_wait.py |\n\nWhen the Spectral library is not used, any output data produced has to be copied\nback from NVMe.  You can observe that with the Spectral library there is no reason\nto explicitly ask for the data to be copied to GPFS as it is done automatically\nthrough the spectral_wait.py script. Also a log file called spectral.log will be\ncreated with information on the files that were copied.\n\n\n\nKnown Issues\n\nLast Updated: 07 February 2023\n\nOpen Issues\n\nHIP code cannot be built using CMake using hip::host/device or HIP language support\n\nUsing the hip-cuda/5.1.0 module on Summit, applications cannot build using a CMakeLists.txt that requires HIP language support or references the hip::host and hip::device identifiers. There is no known workaround for this issue. Applications wishing to compile HIP code with CMake need to avoid using HIP language support or hip::host and hip::device identifiers.\n\nUnsupported CUDA versions do not work with GPU-aware MPI\n\nAlthough there are newer CUDA modules on Summit, cuda/11.0.3 is the latest version that is officially supported by the version of IBM's software stack installed on Summit. When loading the newer CUDA modules, a message is printed to the screen stating that the module is for “testing purposes only”. These newer unsupported CUDA versions might work with some users’ applications, but importantly, they are known not to work with GPU-aware MPI.\n\nHIP does not currently work with GPU-aware MPI\n\nUsing the hip-cuda/5.1.0 module on Summit requires CUDA v11.4.0 or later. However, only CUDA v11.0.3 and earlier currently support GPU-aware MPI, so GPU-aware MPI is currently not available when using HIP on Summit.\n\nMPS does not currently work with codes compiled with post 11.0.3 CUDA\n\nAny codes compiled with post 11.0.3 CUDA (cuda/11.0.3) will not work with MPS enabled (-alloc_flags \"gpumps\") on Summit. The code will hang indefinitely. CUDA v11.0.3 is the latest version that is officially supported by IBM's software stack installed on Summit. We are continuing to look into this issue.\n\nSystem not sourcing .bashrc, .profile, etc. files as expected\n\nSome users have noticed that their login shells, batch jobs, etc. are not sourcing shell run control files as expected. This is related to the way bash is initialized. The initialization process is discussed in the INVOCATION section of the bash manpage, but is summarized here.\n\nBash sources different files based on two attributes of the shell: whether or not it's a login shell, and whether or not it's an interactive shell. These attributes are not mutually exclusive (so a shell can be \"interactive login\", \"interactive non-login\", etc.):\n\nIf a shell is an interactive login shell (i.e. an ssh to the system) or a non-interactive shell started with the --login option (say, a batch script with #!/bin/bash --login as the first line), it will source /etc/profile and will then search your home directory for ~/.bash_profile, ~/.bash_login, and ~/.profile. It will source the first of those that it finds (once it sources one, it stops looking for the others).\n\nIf a shell is an interactive, non-login shell (say, if you run 'bash' in your login session to start a subshell), it will source ~/.bashrc\n\nIf a shell is a non-interactive, non-login shell, it will source whatever file is defined by the $BASH_ENV variable in the shell from which it was invoked.\n\nIn any case, if the files listed above that should be sourced in a particular situation do not exist, it is not an error.\n\nOn Summit and Andes, batch-interactive jobs using bash (i.e. those submitted with bsub -Is or salloc) run as interactive, non-login shells (and therefore source ~/.bashrc, if it exists). Regular batch jobs using bash on those systems are non-interactive, non-login shells and source the file defined by the variable $BASH_ENV in the shell from which you submitted the job. This variable is not set by default, so this means that none of these files will be sourced for a regular batch job unless you explicitly set that variable.\n\nSome systems are configured to have additional files in /etc sourced, and sometimes the files in /etc look for and source files in your home directory such as ~/.bashrc, so the behavior on any given system may seem to deviate a bit from the information above (which is from the bash manpage). This can explain why jobs (or other shells) on other systems you've used have sourced your .bashrc file on login.\n\nImproper permissions on ~/.ssh/config cause job state flip-flop/jobs ending in suspended state\n\nImproper permissions on your SSH configuration file (~/.ssh/config) will cause jobs to alternate between pending & running states until the job ultimately ends up in a PSUSP state.\n\nLSF uses SSH to communicate with nodes allocated to your job, and in this case the improper permissions (i.e. write permission for anyone other than the user) cause SSH to fail, which in turn causes the job launch to fail. Note that SSH only checks the permissions of the configuration file itself. Thus, even if the ~/.ssh/ directory itself grants no group or other permissions, SSH will fail due to permissions on the configuration file.\n\nTo fix this, use a more secure permission setting on the configuration file. An appropriate setting would be read and write permission for the user and no other permissions. You can set this with the command chmod 600 ~/.ssh/config.\n\nSetting TMPDIR causes JSM (jsrun) errors / job state flip-flop\n\nSetting the TMPDIR environment variable causes jobs to fail with JSM\n(jsrun) errors and can also cause jobs to bounce back and forth between\neligible and running states until a retry limit has been reached and the job is\nplaced in a blocked state (NOTE: This \"bouncing\" of job state can be caused for\nmultiple reasons. Please see the known issue Jobs suspended due to retry limit\n/ Queued job flip-flops between queued/running states if you are not setting\nTMPDIR). A bug has been filed with IBM to address this issue.\n\nWhen TMPDIR is set within a running job (i.e., in an interactive session or\nwithin a batch script), any attempt to call jsrun will lead to a job\nfailure with the following error message:\n\nError: Remote JSM server is not responding on host batch503-25-2020 15:29:45:920 90012 main: Error initializing RM connection. Exiting.\n\nWhen TMPDIR is set before submitting a job (i.e., in the shell/environment\nwhere a job is submitted from), the job will bounce back and forth between a\nrunning and eligible state until its retry limit has been reached and the job\nwill end up in a blocked state. This is true for both interactive jobs and jobs\nsubmitted with a batch script, but interactive jobs will hang without dropping\nyou into your interactive shell. In both cases, JSM log files (e.g.,\njsm-lsf-wait.username.1004985.log) will be created in the location set for\nTMPDIR containing the same error message as shown above.\n\nSegfault when running executables on login nodes\n\nExecuting a parallel binary on the login node or a batch node without using the\njob step launcher jsrun will result in a segfault.\n\nThis also can be encountered when importing parallel Python libraries like\nmpi4py and h5py directly on these nodes.\n\nThe issue has been reported to IBM. The current workaround is to run the binary\ninside an interactive or batch job via jsrun.\n\nNsight Compute cannot be used with MPI programs\n\nWhen profiling an MPI application using NVIDIA Nsight Compute, like the following,\nyou may see an error message in Spectrum MPI that aborts the program:\n\njsrun -n 1 -a 1 -g 1 nv-nsight-cu-cli ./a.out\n\nError: common_pami.c:1049 - ompi_common_pami_init() Unable to create PAMI client (rc=1)\n--------------------------------------------------------------------------\nNo components were able to be opened in the pml framework.\n\nThis typically means that either no components of this type were\ninstalled, or none of the installed components can be loaded.\nSometimes this means that shared libraries required by these\ncomponents are unable to be found/loaded.\n\nHost:      <host>\nFramework: pml\n--------------------------------------------------------------------------\nPML pami cannot be selected\n\nThis is due to an incompatibility in the 2019.x versions of Nsight Compute with\nSpectrum MPI. As a workaround, you can disable CUDA hooks in Spectrum MPI using\n\njsrun -n 1 -a 1 -g 1 --smpiargs=\"-disable_gpu_hooks\" nv-nsight-cu-cli ./a.out\n\nUnfortunately, this is incompatible with using CUDA-aware MPI in your application.\n\nThis will be resolved in a future release of CUDA.\n\nCUDA hook error when program uses CUDA without first calling MPI_Init()\n\nSerial applications, that are not MPI enabled, often face the following\nissue when compiled with Spectrum MPI's wrappers and run with jsrun:\n\nCUDA Hook Library: Failed to find symbol mem_find_dreg_entries, ./a.out: undefined symbol: __PAMI_Invalidate_region\n\nThe same issue can occur if CUDA API calls that interact with the GPU\n(e.g. allocating memory) are called before MPI_Init() in an MPI enabled\napplication. Depending on context, this error can either be harmless or\nit can be fatal.\n\nThe reason this occurs is that the PAMI messaging backend, used by Spectrum\nMPI by default, has a \"CUDA hook\" that records GPU memory allocations.\nThis record is used later during CUDA-aware MPI calls to efficiently detect\nwhether a given message is sent from the CPU or the GPU. This is done by\ndesign in the IBM implementation and is unlikely to be changed.\n\nThere are two main ways to work around this problem. If CUDA-aware MPI is\nnot a relevant factor for your work (which is naturally true for serial\napplications) then you can simply disable the CUDA hook with:\n\n--smpiargs=\"-disable_gpu_hooks\"\n\nas an argument to jsrun. Note that this is not compatible with the -gpu\nargument to --smpiargs, since that is what enables CUDA-aware MPI and\nthe CUDA-aware MPI functionality depends on the CUDA hook.\n\nIf you do need CUDA-aware MPI functionality, then the only known working\nsolution to this problem is to refactor your code so that no CUDA calls\noccur before MPI_Init(). (This includes any libraries or programming models\nsuch as OpenACC or OpenMP that would use CUDA behind the scenes.) While it\nis not explicitly codified in the standard, it is worth noting that the major\nMPI implementations all recommend doing as little as possible before MPI_Init(),\nand this recommendation is consistent with that.\n\nSpindle is not currently supported\n\nUsers should not use USE_SPINDLE=1 or LOAD_SPINDLE=1 in their\n~/.jsm.conf file at this time. A bug has been filed with IBM to\naddress this issue.\n\nSpectrum MPI tunings needed for maximum bandwidth\n\nBy default, Spectrum MPI is configured for minimum latency. If your\napplication needs maximum bandwidth, the following settings are\nrecommended:\n\n$ export PAMI_ENABLE_STRIPING=1\n$ export PAMI_IBV_ADAPTER_AFFINITY=1\n$ export PAMI_IBV_DEVICE_NAME=\"mlx5_0:1,mlx5_3:1\"\n$ export PAMI_IBV_DEVICE_NAME_1=\"mlx5_3:1,mlx5_0:1\"\n\nDebugging slow application startup or slow performance\n\nIn order for debugging and profiling tools to work, you need to unload\nDarshan\n\n$ module unload darshan-runtime\n\nSpectrum MPI provides a tracing library that can be helpful to gather\nmore detail information about the MPI communication of your job. To\ngather MPI tracing data, you can set\nexport OMPI_LD_PRELOAD_POSTPEND=$OLCF_SPECTRUM_MPI_ROOT/lib/libmpitrace.so\nin your environment. This will generate profile files with timings for\nthe individual processes of your job.\n\nIn addition, to debug slow startup JSM provides the option to create a\nprogress file. The file will show information that can be helpful to\npinpoint if a specific node is hanging or slowing down the job step\nlaunch. To enable it, you can use: jsrun --progress\n./my_progress_file_output.txt.\n\n-a flag ignored when using a jsrun resource set file with -U\n\nWhen using file-based specification of resource sets with jsrun -U,\nthe -a flag (number of tasks per resource set) is ignored. This has\nbeen reported to IBM and they are investigating. It is generally\nrecommended to use jsrun explicit resource files (ERF) with\n--erf_input and --erf_output instead of -U.\n\nJobs suspended due to retry limit / Queued job flip-flops between queued/running states\n\nSome users have reported seeing their jobs transition from the normal\nqueued state, into a running state, and then back again to queued.\nSometimes this can happen multiple times. Eventually, internal limits in\nthe LSF scheduler will be reached, at which point the job will no longer\nbe eligible for running. The bhist command can be used to see if a job\nis cycling between running and eligible states. The pending reason given\nby bhist can also be useful to debug. This can happen due to\nmodifications that the user has made to their environment on the system,\nincorrect SSH key setup, attempting to load unavailable/broken modules.\nor system problems with individual nodes. When jobs are observed to\nflip-flop between running and queued, and/or become ineligible without\nexplanation, then deeper investigation is required and the user should\nwrite to help@olcf.ornl.gov.\n\njsrun explicit resource file (ERF) output format error\n\njsrun's option to create an explicit resource file (--erf_output) will\nincorrectly create a file with one line per rank. When reading the file\nin with (--erf_input) you will see warnings for overlapping resource\nsets. This issue has been reported. The workaround is to manually\nupdate the created ERF file to contain a single line per resource set\nwith multiple ranks per line.\n\njsrun latency priority capitalization allocates incorrect resources\n\njsrun's latency priority (-l) flag can be given lowercase values\n(i.e. gpu-cpu) or capitalized values (i.e. GPU-CPU).\n\nExpected behavior:\n\nWhen capitalized, jsrun should not compromise on the resource layout,\nand will wait to begin the job step until the ideal resources are\navailable. When given a lowercase value, jsrun will not wait, but\ninitiate the job step with the most ideal layout as is available at the\ntime. This also means that when there's no resource contention, such as\nrunning a single job step at a time, capitalization should not matter,\nas they should both yield the same resources.\n\nActual behavior:\n\nCapitalizing the latency priority value may allocate incorrect\nresources, or even cause the job step to fail entirely.\n\nRecommendation:\n\nIt is currently recommended to only use the lowercase values to (-l /\n--latency_priority). The system default is: gpu-cpu,cpu-mem,cpu-cpu.\nSince this ordering is used implicitly when the -l flag is omitted, this\nissue only impacts submissions which explicitly include a latency\npriority in the jsrun command.\n\nError when using complex datatypes with MPI Collectives and GPUDirect\n\nUsers have reported errors when using complex datatypes with MPI\nCollectives and GPUDirect:\n\njsrun --smpiargs=\"-gpu\" -n 6 -a 1 -g 1   ./a.out\n[h35n05:113506] coll:ibm:allreduce: GPU awareness in PAMI requested. It is not safe to defer to another component.\n[h35n05:113506] *** An error occurred in MPI_Allreduce\n[h35n05:113506] *** reported by process [3199551009,2]\n[h35n05:113506] *** on communicator MPI_COMM_WORLD\n[h35n05:113506] *** MPI_ERR_UNSUPPORTED_OPERATION: operation not supported\n[h35n05:113506] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n[h35n05:113506] ***    and potentially your MPI job)\n[h35n05:113509] coll:ibm:allreduce: GPU awareness in PAMI requested. It is not safe to defer to another component.\n\nThis is a known issue with libcoll and the SMPI team is working to\nresolve it. In the meantime, a workaround is to treat the complex array\nas a real array with double the length if the operation is not\nMPI_Prod. Note: This requires code modification. An alternative\nworkaround is to disable IBM optimized collectives. This will impact\nperformance however but requires no code changes and should be correct\nfor all MPI_Allreduce operations. You can do this by adding the\nfollowing option to your jsrun command line:\n--smpiargs=\"-HCOLL -FCA -mca coll_hcoll_enable 1 -mca coll_hcoll_np 0\n-mca coll ^basic -mca coll ^ibm -async\"\n\nError when using ERF\n\nExplicit Resource Files\nprovide even more fine-granied control over how processes are mapped onto\ncompute nodes.\nUsers have reported errors when using ERF on Summit:\n\nFailed to bind process to ERF smt array, err: Invalid argument\n\nThis is a known issue with the current version of jsrun. A workaround is to add the following lines in your job script.\n\nexport JSM_ROOT=/gpfs/alpine/stf007/world-shared/vgv/inbox/jsm_erf/jsm-10.4.0.4/opt/ibm/jsm\n$JSM_ROOT/bin/jsm &\n$JSM_ROOT/bin/jsrun --erf_input=Your_erf ./Your_app\n\nResolved Issues\n\n'Received msg header indicates a size that is too large' error message from Spectrum MPI\n\nIf you get an error message that looks like:\n\nA received msg header indicates a size that is too large:\n Requested size: 25836785\n Size limit: 16777216\nIf you believe this msg is legitimate, please increase the\nmax msg size via the ptl_base_max_msg_size parameter.\n\nThis can be resolved by setting export PMIX_MCA_ptl_base_max_msg_size=18 where the value is size in MB. Setting it to 18 or higher usually works. The default if its not explicitly set is around 16 MB.\n\nJSM Fault Tolerance causes jobs to fail to start\n\nAdding FAULT_TOLERANCE=1 in your individual ~/.jsm.conf file,\nwill result in LSF jobs failing to successfully start.\n\nThe following issues were resolved with the July 16, 2019 software upgrade:\n\nDefault nvprof setting clobbers LD_PRELOAD, interfering with SpectrumMPI (Resolved: July 16, 2019)\n\nCUDA 10 adds a new feature to profile CPU side OpenMP constructs (see\nhttps://docs.nvidia.com/cuda/profiler-users-guide/index.html#openmp).\nThis feature is enabled by default and has a bug which will cause it to\noverwrite the contents of LD_PRELOAD. SpectrumMPI requires a library\n(libpami_cuda_hook.so) to be preloaded in order to function. All MPI\napplications on Summit will break when run in nvprof with default\nsettings. The workaround is to disable the new OpenMP profiling feature:\n\n$ jsrun  nvprof --openmp-profiling off\n\nCSM-based launch is not currently supported (Resolved: July 16, 2019)\n\nUsers should not use JSMD_LAUNCH_MODE=csm in their ~/.jsm.conf\nfile at this time. A bug has been filed with IBM to address this issue.\n\n\n\nParallel I/O crash on GPFS with latest MPI ROMIO\n\nIn some cases with large number of MPI processes when there is not\nenough memory available on the compute node, the Abstract-Device\nInterface for I/O (ADIO) driver can break with this error:\n\nOut of memory\nin file\n../../../../../../../opensrc/ompi/ompi/mca/io/romio321/romio/adio/ad_gpfs/ad_gpfs_rdcoll.c,\nline 1178\n\nThe solution is to declare in your submission script:\n\nexport GPFSMPIO_COMM=1\n\nThis command will use non-blocking MPI calls and not MPI_Alltoallv for\nexchange of data between the MPI I/O aggregators which requires\nsignificant more amount of memory.\n\n\n\nThe following issues were resolved\nwith the May 21, 2019 upgrade:\n\n-g flag causes internal compiler error with XL compiler (Resolved: May 21, 2019)\n\nSome users have reported an internal compiler error when compiling their\ncode with XL with the `-g` flag. This has been reported to IBM and\nthey are investigating.\n\nThis bug was fixed in xl/16.1.1-3\n\nIssue with CUDA Aware MPI with >1 resource set per node (Resolved: May 21, 2019)\n\nAttempting to run an application with CUDA-aware MPI using more than one\nresource set per node with produce the following error on each MPI rank:\n\n/__SMPI_build_dir__________________________________________/ibmsrc/pami/ibm-pami/buildtools/pami_build_port/../pami/components/devices/ibvdevice/CudaIPCPool.h:300:\n[0]Error opening IPC Memhandle from peer:1, invalid argument\nCUDA level IPC failure: this has been observed in environments where cgroups separate the visible GPUs between ranks. The option -x PAMI_DISABLE_IPC=1 can be used to disable CUDA level IPC.[:] *** Process received signal ***\n\nSpectrum MPI relies on CUDA Inter-process Communication (CUDA IPC) to\nprovide fast on-node between GPUs. At present this capability cannot\nfunction with more than one resource set per node.\n\nSet the environment variable PAMI_DISABLE_IPC=1 to force Spectrum\nMPI to not use fast GPU Peer-to-peer communication. This option will\nallow your code to run with more than one resource set per host, but\nyou may see slower GPU to GPU communication.\n\nRun in a single resource set per host, i.e. with\njsrun --gpu_per_rs 6\n\nIf on-node MPI communication between GPUs is critical to your\napplication performance, option B is recommended but you’ll need to set\nthe GPU affinity manually. This could be done with an API call in your\ncode (e.g. cudaSetDevice), or by using a wrapper script.\n\nSimultaneous backgrounded jsruns (Resolved: May 21, 2019)\n\nWe have seen occasional errors from batch jobs with multiple\nsimultaneous backgrounded jsrun commands. Jobs may see pmix errors\nduring the noted failures.\n\n\n\nThe following issue was resolved with the software default changes from\nMarch 12, 2019 that set Spectrum MPI 10.2.0.11 (20190201) as default and\nmoved ROMIO to version 3.2.1:\n\nSlow performance using parallel HDF5 (Resolved: March 12, 2019)\n\nA performance issue has been identified using parallel HDF5 with the\ndefault version of ROMIO provided in\nspectrum-mpi/10.2.0.10-20181214. To fully take advantage of parallel\nHDF5, users need to switch to the newer version of ROMIO and use ROMIO\nhints. The following shows recommended variables and hints for a 2 node\njob. Please note that hints must be tuned for a specific job.\n\n$ module unload darshan-runtime\n$ export OMPI_MCA_io=romio321\n$ export ROMIO_HINTS=./my_romio_hints\n$ cat $ROMIO_HINTS\nromio_cb_write enable\nromio_ds_write enable\ncb_buffer_size 16777216\ncb_nodes 2\n\nJob hangs in MPI_Finalize (Resolved: March 12, 2019)\n\nThere is a known issue in Spectrum MPI 10.2.0.10 provided by the\nspectrum-mpi/10.2.0.10-20181214 modulefile that causes a hang in\nMPI_Finalize when ROMIO 3.2.1 is being used and the\ndarshan-runtime modulefile is loaded. The recommended and default\nSpectrum MPI version as of March 3, 2019 is Spectrum MPI 10.2.0.11\nprovided by the spectrum-mpi/10.2.0.11-20190201 modulefile. If you\nare seeing this issue, please make sure that you are using the latest\nversion of Spectrum MPI. If you need to use a previous version of\nSpectrum MPI, your options are:\n\nUnload the darshan-runtime modulefile.\n\nAlternatively, set export OMPI_MCA_io=romio314 in your\nenvironment to use the previous version of ROMIO. Please note that\nthis version has known performance issues with parallel HDF5 (see\n\"Slow performance using parallel HDF5\" issue below).\n\n\n\nThe following issues were resolved with the February 19, 2019 upgrade:\n\nJob step cgroups are not currently supported (Resolved: February 19, 2019)\n\nA regression was introduced in JSM 10.02.00.10rtm2 that prevents job\nstep cgroups from being created as a result, JSM, is defaulting to\nsetting CUDA_VISIBLE_DEVICES in order to allocate GPUs to specific\nresource sets. Because of this issue, even if using --gpu_per_rs 0\nor -g 0, every resource set in the step will be able to see all 6\nGPUs in a node.\n\nJSM stdio options do not create files (Resolved: February 19, 2019)\n\nWhen using --stdio_stdout or --stdio_stderr users must use\nabsolute paths. Using relative paths (e.g. ./my_stdout) will not\nsuccessfully create the file in the user's current working directory. An\nbug has been filed with IBM to fix this issue and allow relative paths.\n\nJSM crash when using different number of resource sets per host (Resolved: February 19, 2019)\n\nIn some cases users will encounter a segmentation fault when running job\nsteps that have uneven number of resource sets per node. For example:\n\n$ jsrun --nrs 41 -c 21 -a 1 --bind rs ./a.out\n[a03n07:74208] *** Process received signal ***\n[a03n07:74208] Signal: Segmentation fault (11)\n[a03n07:74208] Signal code: Address not mapped (1)\n[a03n07:74208] Failing at address: (nil)\n...\n\nAs a workaround, two environment variables are set as default in the\nuser environment PAMI_PMIX_USE_OLD_MAPCACHE=1 and\nOMPI_MCA_coll_ibm_xml_disable_cache=1.\n\nCUDA 10.1 Known Issues\n\nIntermittent failures with `nvprof` (Identified: July 11, 2019)\n\nWe are seeing an intermittent issue that causes an error when\nprofiling a code using nvprof from CUDA 10.1.168. We have filed\na bug with NVIDIA (NV bug 2645669) and they have reproduced the\nproblem. An update will be posted when a fix becomes available.\n\nWhen this issue is encountered, the profiler will exit with the\nfollowing error message:\n\n==99756== NVPROF is profiling process 99756, command: ./a.out\n==99756== Error: Internal profiling error 4306:999.\n======== Profiling result:\n======== Metric result:\n\nMPI annotation may cause segfaults with applications using MPI_Init_thread\n\nUsers on Summit can have MPI calls automatically annotated in nvprof\ntimelines using the nvprof --annotate-mpi openmpi option. If the\nuser calls MPI_Init_thread instead of MPI_Init, nvprof may\nsegfault, as MPI_Init_thread is currently not being wrapped by\nnvprof. The current alternative is to build and follow the\ninstructions from\nhttps://github.com/NVIDIA/cuda-profiler/tree/mpi_init_thread.\n\ncudaMemAdvise before context creation leads to a kernel panic\n\nThere is a (very rare) driver bug involving cudaManagedMemory that can\ncause a kernel panic. If you encounter this bug, please contact the\nOLCF User Support team. The easiest\nmitigation is for the user code to initialize a context on every GPU\nwith which it intends to interact (for example by calling\ncudaFree(0) while each device is active).\n\nSome uses of Thrust complex vectors fail at compile time with warnings of identifiers being undefined in device code\n\nThis issue comes from the fact that std::complex is not\n__host__/__device__ annotated, so all its functions are\nimplicitly __host__. There is a mostly simple workaround, assuming\nthis is compiled as C++11: in complex.h and complex.inl,\nannotate the functions that deal with std::complex as\n__host__ __device__ (they are the ones that are annotated only as\n__host__ right now), and then compile with\n--expt-relaxed-constexpr.\n\nUsers that encounter this issue, can use\nthe following workaround. copy the entirety of\n${OLCF_CUDA_ROOT}/include/thrust to a private location, make the\nabove edits to thrust/complex.h and\nthrust/detail/complex/complex.inl, and then add that to your include\npath:\n\n$ nvcc -ccbin=g++ --expt-relaxed-constexpr assignment.cu -I./\n\nA permanent fix of this issue is expected in the version of Thrust\npacked with CUDA 10.1 update 1\n\nBreakpoints in CUDA kernels recommendation\n\ncuda-gdb allows for breakpoints to be set inside CUDA kernels to\ninspect the program state on the GPU. This can be a valuable debugging\ntool but breaking inside kernels does incur significant overhead that\nshould be included in your expected runtime.\n\nThe time required to hit a breakpoint inside a CUDA kernel depends on\nhow many CUDA threads are used to execute the kernel. It may take\nseveral seconds to stop at kernel breakpoints for very large numbers\nof threads. For this reason, it is recommended to choose breakpoints\njudiciously, especially when running the debugger in \"batch\" or\n\"offline\" mode where this overhead may be misperceived as the code\nhanging. If possible, debugging a smaller problem size with fewer\nactive threads can be more pleasant.\n\n\n\n\n\nScalable Protected Infrastructure (SPI)\n\nThe OLCF’s Scalable Protected Infrastructure (SPI) provides resources and protocols that enable researchers to process protected data at scale. The SPI is built around a framework of security protocols that allows researchers to process large datasets containing private information. Using this framework researchers can use the center’s large HPC resources to compute data containing protected health information (PHI), personally identifiable information (PII), data protected under International Traffic in Arms Regulations, and other types of data that require privacy.\n\nThe SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads. Because of this, many processes used within the SPI are very similar to those used for standard non-SPI. This page lists the differences you may see when using OLCF resources to execute SPI resources.\n\nThe SPI provides access to the OLCF's Summit resource for compute.  To safely separate SPI and non-SPI workflows, SPI workflows must use a separate login node named Citadel<spi-compute-citadel> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Citadel<spi-compute-citadel>>. Because Citadel is largely a front end for Summit, you can use the Summit documentation when using Citadel. The SPI<spi-compute-citadel> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#SPI<spi-compute-citadel>> page can be used to see notable differences when using the Citadel resource.\n\n\n\n\n\nTraining System (Ascent)\n\nAscent is a training system that is not intended to be used as\nan OLCF user resource. Access to the system is only obtained through\nOLCF training events.\n\nAscent is an 18-node stand-alone system with the same architecture as\nSummit (see summit-nodes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#summit-nodes> section above), so most of this Summit User Guide can be referenced for\nAscent as well. However, aside from the number of compute nodes, there\nare other differences between the two systems. Most notably, Ascent sits\nin the NCCS Open Security Enclave, which is subject to fewer\nrestrictions than the Moderate Security Enclave that systems such as\nSummit belong to. This means that participants in OLCF training events\ncan go through a streamlined version of the approval process before\ngaining access to the system. The remainder of this section of the user\nguide describes \"Ascent-specific\" information intended for participants\nof OLCF training events.\n\nFile Systems\n\n<string>:4399: (INFO/1) Duplicate implicit target name: \"file systems\".\n\nIt is important to note that because Ascent sits in the NCCS Open\nSecurity Enclave, it also mounts different file systems than Summit.\nThese file systems provide both user-affiliated and project-affiliated\nstorage areas for each user.\n\nNFS Directories\n\nUpon logging into Ascent, users will be placed in their own personal\nhome (NFS) directory, /ccsopen/home/[userid], which can only be\naccessed by that user. Users also have access to an NFS project\ndirectory, /ccsopen/proj/[projid], which is visible to all members\nof a project. Both of these NFS directories are commonly used to store\nsource code and build applications.\n\nGPFS Directories\n\nUsers also have access to a (GPFS) parallel file system, called wolf,\nwhich is where data should be written when running on Ascent's compute\nnodes. Under /gpfs/wolf/[projid], there are 3 directories:\n\n$ ls /gpfs/wolf/[projid]\nproj-shared  scratch  world-shared\n\nproj-shared can be accessed by all members of a project.\n\nscratch contains directories for each user of a project and only\nthat user can access their own directory.\n\nworld-shared can be accessed by any users on the system in any\nproject.\n\nObtaining Access to Ascent\n\nAscent is a training system that is not intended to be used as\nan OLCF user resource. Access to the system is only obtained through\nOLCF training events.\n\nThis sub-section describes the process of obtaining access to Ascent for\nan OLCF training event. Please follow the steps below to request access.\n\nStep 1: Go to the myOLCF Account Application Form\n\nOnce on the form, linked above, fill in the project ID in the \"Enter the Project ID of the project you wish to join\" field and click \"Next\".\n\n\n\nAfter you enter the Project ID, use the sliders to select \"Yes\" for OLCF as the Project Organization and select \"Yes\" for Open as the Security Enclave.\n\n<string>:5: (INFO/1) Enumerated list start value not ordinal-1: \"2\" (ordinal 2)\n\n\n\nThe next screen will show you some information about the project, you don't need to change anything, just click \"Next\".\n\nFill in your personal information and then click \"Next\".\n\nFill in your shipping information and then click \"Next\".\n\nFill in your Employment/Institution Information. If you are student please use your school affiliation for both \"Employer\" and \"Funding Source\". If you are a student and you do not see your school listed, choose \"other\" for both \"Employer\" and \"Funding Source\" and then manually enter your school affiliation in the adjacent fields.  Click “Next” when you are done.\n\nOn the Project information screen fill the \"Proposed Contribution to Project\" with \"Participating in OLCF training.\" Leave all the questions about the project set to \"no\" and click \"Next\".\n\n<string>:5: (INFO/1) Enumerated list start value not ordinal-1: \"3\" (ordinal 3)\n\n\n\nOn the user account page, selected \"yes\" or \"no\" for the questions asking about any pre-existing account names. If this is your first account with us, leave those questions set to \"no\". Also enter your preferred shell. If you do not know which shell to use, select \"/bin/bash\". We can change this later if needed. Click \"Next\".\n\nOn the \"Policies & Agreements\" page click the links to read the polices and then answer \"Yes\" to affirm you have read each. Certify your application by selecting \"Yes\" as well. Then Click \"Submit.\"\n\n<string>:5: (INFO/1) Enumerated list start value not ordinal-1: \"8\" (ordinal 8)\n\nAfter submitting your application, it will need to pass\nthrough the approval process. Depending on when you submit, approval\nmight not occur until the next business day.\n\nStep 2: Set Your XCAMS/UCAMS Password\n\nOnce approved, if you are a new user, your account will be created and\nan email will be sent asking you to set up a password. If you already\nhad an XCAMS/UCAMS account, you will not be sent the email asking you to\nsetup a new password (simply use your existing credentials). Once\npasswords are known, users can log in to Ascent using their XCAMS/UCAMS\nusername and password (see the next section)\n\nLogging In to Ascent\n\nTo log in to Ascent, please use your XCAMS/UCAMS username and password:\n\n$ ssh USERNAME@login1.ascent.olcf.ornl.gov\n\nYou do not need to use an RSA token to log in to Ascent.\nPlease use your XCAMS/UCAMS username and password (which is different\nfrom the username and PIN + RSA token code used to log in to other OLCF\nsystems such as Summit).\n\nIt will take ~5 minutes for your directories to be created, so\nif your account was just created and you log in and you do not have a\nhome directory, this is likely the reason.\n\nPreparing For Frontier\n\nThis section of the Summit User Guide is intended to show current OLCF\nusers how to start preparing their applications to run on the upcoming\nFrontier system. We will continue to add more topics to this section in\nthe coming months. Please see the topics below to get started.\n\nHIP\n\nHIP (Heterogeneous-Compute Interface for Portability) is a C++ runtime\nAPI that allows developers to write portable code to run on AMD and NVIDIA\nGPUs. It is an interface that uses the underlying Radeon Open Compute (ROCm)\nor CUDA platform that is installed on a system. The API is similar to CUDA\nso porting existing codes from CUDA to HIP should be fairly straightforward\nin most cases. In addition, HIP provides porting tools which can be used to\nhelp port CUDA codes to the HIP layer, with no overhead compared to the\noriginal CUDA application. HIP is not intended to be a drop-in replacement\nfor CUDA, so some manual coding and performance tuning work should be\nexpected to complete the port.\n\nKey features include:\n\nHIP is a thin layer and has little or no performance impact over\ncoding directly in CUDA.\n\nHIP allows coding in a single-source C++ programming language including\nfeatures such as templates, C++11 lambdas, classes, namespaces, and more.\n\nThe “hipify” tools automatically convert source from CUDA to HIP.\n\nDevelopers can specialize for the platform (CUDA or HIP) to tune for\nperformance or handle tricky cases.\n\nUsing HIP on Summit\n\nAs mentioned above, HIP can be used on systems running on either the ROCm\nor CUDA platform, so OLCF users can start preparing their applications for\nFrontier today on Summit. To use HIP on Summit, you must load the HIP module:\n\n$ module load hip-cuda\n\nCUDA 11.4.0 or later must be loaded in order to load the hip-cuda module.\nhipBLAS, hipFFT, hipSOLVER, hipSPARSE, and hipRAND have also been added\nto hip-cuda.\n\nLearning to Program with HIP\n\nThe HIP API is very similar to CUDA, so if you are already familiar with\nusing CUDA, the transition to using HIP should be fairly straightforward.\nWhether you are already familiar with CUDA or not, the best place to start\nlearning about HIP is this Introduction to HIP webinar that was recently\ngiven by AMD:\n\nIntroduction to AMD GPU Programming with HIP:\n(slides | recording)\n\nMore useful resources, provided by AMD, can be found here:\n\nHIP Programming Guide\n\nHIP API Documentation\n\nHIP Porting Guide\n\nThe OLCF is currently adding some simple HIP tutorials here as well:\n\nOLCF Tutorials – Simple HIP Examples\n\nPrevious Frontier Training Events\n\nThe links below point to event pages from previous Frontier training events. Under the \"Presentations\" tab on each event page, you will find the presentations given during the event.\n\nFrontier Application Readiness Kick-Off Workshop (October 2019)\n\nPlease check back to this section regularly as we will continue\nto add new content for our users."}
{"doc":"swift_t","text":"Swift/T\n\n\n\nOverview\n\nSwift/T is a completely new implementation of the Swift language for\nhigh-performance computing which translates Swift scripts into MPI programs\nthat use the Turbine (hence, /T) and ADLB runtime libraries. This tutorial\nshows how to get up and running with Swift/T on Summit specifically. For more\ninformation about Swift/T, please refer to its\ndocumentation.\n\nPrerequisites\n\nSwift/T is available as a module on Summit, and it can be loaded as follows:\n\n$ module load workflows\n$ module load swift/1.5.0\n\nYou will also need to set the PROJECT environment variable:\n\n$ export PROJECT=\"ABC123\"\n\nHello world!\n\nTo run an example \"Hello world\" program with Swift/T on Summit, create a\nfile called hello.swift with the following contents:\n\ntrace(\"Hello world!\");\n\nNow, run the program from a shell or script:\n\n$ swift-t -m lsf hello.swift\n\nThe output should look something like the following:\n\nTURBINE-LSF SCRIPT\nNODES=2\nPROCS=2\nPPN=1\nTURBINE_OUTPUT=/ccs/home/seanwilk/turbine-output/2021/06/18/17/11/29\nwrote: /ccs/home/seanwilk/turbine-output/2021/06/18/17/11/29/turbine-lsf.sh\nPWD: /autofs/nccs-svm1_home2/seanwilk/turbine-output/2021/06/18/17/11/29\nJob <1095064> is submitted to default queue <batch>.\nJOB_ID=1095064\n\nCongratulations! You have now submitted a Swift/T job to Summit. Inspect the\nTURBINE_OUTPUT directory to find the workflow outputs and other artifacts.\n\nCross Facility Workflow\n\nThis example demonstrates a continuously running cross-facility workflow. The\nidea is that there is a science facility (eg. SNS at ORNL) that produces\nscientific data to be processed by the remote compute facility (eg. OLCF at\nORNL). The data is continuously arriving in a designated directory at the compute facility from science facility. The\nworkflow picks data from that directory and does the processing to the\ndata to produce some output. The Swift source file workflow.swift looks as follows:\n\nimport files;\nimport io;\n\napp (void v) processdata(file f)\n{\n // change path per your location\n \"/gpfs/alpine/scratch/ketan2/stf019/swift-work/cross-facility/processdata.sh\" f ;\n}\n\nfor (boolean b = true; b; b=c)\n{\n  boolean c;\n  // You can change the number of data files while the workflow is running\n  file data[] = glob(\"*.jpg\");\n  void V[];\n  foreach f, i in data\n  {\n    V[i] = processdata(f);\n  }\n  printf(\"processed %i files.\", size(V)) => c = true;\n}\n\nIn order to demonstrate the data generation, we have a script that downloads image data from the NOAA website periodically. The image is a geographical image showing current cloud cover over south-east US. The code gendata.sh looks like so:\n\n#!/bin/bash\nset -eu\n\nfunction cleanup() {\n  \\rm -f ./data/earth*.jpg\n}\n\nwhile true\ndo\n  uid=$(uuidgen | awk -F- '{print $1}')\n  wget -q https://cdn.star.nesdis.noaa.gov/GOES16/ABI/SECTOR/se/GEOCOLOR/1200x1200.jpg -O ./data/earth${uid}.jpg\n  sleep 5\n  trap cleanup EXIT\ndone\n\nNext, we have the data processing script called processdata.sh that looks as follows:\n\n#!/bin/bash\nset -eu\n\nTASK=convert\nDATA=$1\necho \"\\nProcessing ${DATA}\\n\"\n${TASK} ${DATA} -fuzz 10% -fill white -opaque white -fill black +opaque white -format \"%[fx:100*mean]\" info:\nsleep 5\n\nThe above script computes the cloud cover percentage by looking at the amount of white pixels in the image. Note that it uses ImageMagick's convert utility.\n\nThe suggested directory structure is to have a outer directory say swift-work that has the swift source and shell scripts. Inside of swift-work create a new directory called data.\n\nAdditionally, we will need two terminals open. In the first terminal window, navigate to the swift-work directory and invoke the data generation script like so:\n\n$ ./gendata.sh\n\nIn the second terminal, we will run the swift workflow as follows (make sure to change the project name per your allocation):\n\n$ module load imagemagick # for convert utility\n$ export WALLTIME=00:10:00\n$ export PROJECT=STF019\n$ export TURBINE_OUTPUT=/gpfs/alpine/scratch/ketan2/stf019/swift-work/cross-facility/data\n$ swift-t -O0 -m lsf workflow.swift\n\nIf all goes well, and when the job starts running, the output will be produced in the data directory output.txt file."}
{"doc":"TAU","text":"Tuning and Analysis Utilities (TAU)\n\n\n\nTAU is a portable profiling and tracing toolkit that supports many programming\nlanguages. The instrumentation can be done by inserting in the source code\nusing an automatic tool based on the Program Database Toolkit (PDT), on the\ncompiler instrumentation, or manually using the instrumentation API.\n\nWebpage: https://www.cs.uoregon.edu/research/tau/home.php\n\nTAU is installed with Program Database Toolkit (PDT) on Summit. PDT is a\nframework for analyzing source code written in several programming languages.\nMoreover, Performance Application Programming Interface (PAPI) is supported. PAPI counters are used to assess\nthe CPU performance. In this section, some approaches for profiling and tracing\nwill be presented.\n\nIn most cases, we need to use wrappers to recompile the application:\n\nFor C: replace the compiler with the TAU wrapper tau_cc.sh\n\nFor C++: replace the compiler with the TAU wrapper tau_cxx.sh\n\nFor Fortran: replace the compiler with the TAU wrapper tau_f90.sh / tau_f77.sh\n\nEven if you don't compile your application with a TAU wrapper, you can\nprofile some basic functionalities with tau_exec, for example:\n\njsrun -n 4 –r 4 –a 1 –c 1 tau_exec -T mpi ./test\n\nThe above command profiles MPI for the binary test, which was not compiled\nwith the TAU wrapper.\n\nRun-Time Environment Variables\n\nThe following TAU environment variables may be useful in job submission scripts.\n\n\nThe table provided contains a list of variables related to TAU, a performance analysis and profiling tool. The first column lists the variable names, followed by their default values and a brief description of their purpose. The TAU_TRACE variable, with a default value of 0, can be set to 1 to enable tracing, which tracks the execution of a program and records information about its behavior. The TAU_CALLPATH variable, also with a default value of 0, can be set to 1 to enable callpath profiling, which tracks the execution path of a program and records information about the functions and routines called. The TAU_TRACK_MEMORY_LEAKS variable, with a default value of 0, can be set to 1 to enable leak detection, which identifies and reports any memory leaks in a program. The TAU_TRACK_HEAP variable, with a default value of 0, can be set to 1 to enable tracking of heap memory routines, which records information about the allocation and deallocation of memory on the heap. The TAU_CALLPATH_DEPTH variable, with a default value of 2, specifies the depth of the callpath to be recorded. The TAU_TRACK_IO_PARAMS variable, with a default value of 0, can be set to 1 when using the -optTrackIO option, which tracks input/output parameters of a program. The TAU_SAMPLING variable, with a default value of 1, generates sample-based profiles, which collects data at regular intervals to represent the overall behavior of a program. The TAU_COMM_MATRIX variable, with a default value of 0, can be set to 1 to generate a communication matrix, which shows the communication patterns between different processes in a parallel program. The TAU_THROTTLE variable, with a default value of 1, can be set to 0 to turn off throttling, which removes any overhead caused by TAU. The TAU_THROTTLE_NUMCALLS variable, with a default value of 100000, specifies the number of calls before testing throttling. The TAU_THROTTLE_PERCALL variable, with a default value of 10, specifies the threshold for throttling a routine if it is called more than 100000 times and takes less than 10 microseconds of inclusive time. The TAU_COMPENSATE variable, with a default value of 10, can be set to 1 to enable runtime compensation of instrumentation overhead, which adjusts the profiling data to account for the overhead caused by TAU. The TAU_PROFILE_FORMAT variable, with a default value of \"Profile\", can be set to \"merged\" to generate a single file containing all the profiling data, or \"snapshot\" to generate a separate snapshot file for each thread. Finally, the TAU_METRICS variable, with a default value of \"TIME\", can be set to a comma-separated list of metrics, such as \"TIME:PAPI_TOT_INS\", to specify which metrics to collect during profiling. \n\n| Variable | Default | Description |\n| --- | --- | --- |\n| TAU_TRACE | 0 | Setting to 1 turns on tracing |\n| TAU_CALLPATH | 0 | Setting to 1 turns on callpath profiling |\n| TAU_TRACK_MEMORY_LEAKS | 0 | Setting to 1 turns on leak detection |\n| TAU_TRACK_HEAP | 0 | Setting to 1 turns on heap memory routine entry/exit |\n| TAU_CALLPATH_DEPTH | 2 | Specifies depth of callpath |\n| TAU_TRACK_IO_PARAMS | 0 | Setting 1 with -optTrackIO |\n| TAU_SAMPLING | 1 | Generates sample based profiles |\n| TAU_COMM_MATRIX | 0 | Setting to 1 generates communication matrix |\n| TAU_THROTTLE | 1 | Setting to 0 turns off throttling, by default removes overhead |\n| TAU_THROTTLE_NUMCALLS | 100000 | Number of calls before testing throttling |\n| TAU_THROTTLE_PERCALL | 10 | If a routine is called more than 100000 times and it takes less than 10 usec of inclusive time, throttle it |\n| TAU_COMPENSATE | 10 | Setting to 1 enables runtime compensation of instrumentation overhead |\n| TAU_PROFILE_FORMAT | Profile | Setting to \"merged\" generates a single file, \"snapshot\" generates a snapshot per thread |\n| TAU_METRICS | TIME | Setting to a comma separated list (TIME:PAPI_TOT_INS) |\n\n\n\nCompile-Time Environment Variables\n\nEnvironment variables to be used during compilation through the environment variable TAU_OPTIONS\n\n\nThe table presents a list of variables and their corresponding descriptions related to TAU, a performance analysis and profiling tool for parallel and distributed applications. The first variable, optVerbose, allows users to turn on verbose debugging messages for more detailed information during the instrumentation process. The next two variables, optCompInst and optNoCompInst, pertain to the use of compiler-based instrumentation. The former enables the use of compiler instrumentation while the latter prevents the tool from reverting to compiler instrumentation if source instrumentation fails. The optTrackIO variable allows for the wrapping of POSIX I/O calls and the calculation of volume and bandwidth of I/O operations. The optKeepFiles variable instructs the tool to not remove .pdb and .inst.* files after instrumentation. The optPreProcess variable enables the preprocessing of Fortran sources before instrumentation. The next two variables, optTauSelectFile and optTwauWrapFile, allow users to specify selective instrumentation and the path to the link_options.tau file generated by tau_gen_wrapper, respectively. The optHeaderInst variable enables the instrumentation of headers. The last four variables, optLinking, optCompile, optPdtF95Opts, and optPdtF95Reset, provide options for the linker, compiler, and Fortran parser in PDT (Program Database Toolkit). The optPdtCOpts and optPdtCXXOpts variables also provide options for the C and C++ parsers in PDT. Overall, these variables provide users with a range of options and customization for the instrumentation process in TAU.\n\n| Variable | Description |\n| --- | --- |\n| optVerbose | Turn on verbose debugging messages |\n| optCompInst | Use compiler based instrumentation |\n| optNoCompInst | Do not revert to compiler instrumentation if source instrumentation fails |\n| optTrackIO | Wrap POSIX I/O call and calculate vol/bw of I/O operations |\n| optKeepFiles | Do not remove .pdb and .inst.* files |\n| optPreProcess | Preprocess Fortran sources before instrumentation |\n| optTauSelectFile=\"<file>\" | Specify selective instrumentation file for tau_instrumentor |\n| optTwauWrapFile=\"<file>\" | Specify path to link_options.tau generated by tau_gen_wrapper |\n| optHeaderInst | Enable instrumentation of headers |\n| optLinking=\"\" | Options passed to the linker |\n| optCompile=\"\" | Options passed to the compiler |\n| optPdtF95Opts=\"\" | Add options to the Fortran parser in PDT |\n| optPdtF95Reset=\"\" | Reset options for Fortran parser in PDT |\n| optPdtCOpts=\"\" | Options for C parser in PDT |\n| optPdtCXXOpts=\"\" | Options for C++ parser in PDT |\n\nMiniWeather Example Application\n\nGetting the source code\n\nConnect to Summit and navigate to your project space\n\nFor the following examples, we'll use the MiniWeather application:\nhttps://github.com/mrnorman/miniWeather\n\n$ git clone https://github.com/mrnorman/miniWeather.git\n\nCompile the application\n\nWe'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP,\nand MPI+OpenACC\n\n$ module load pgi\n$ module load parallel-netcdf\n\nDifferent compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:\n\n$ module load cmake\n$ cd miniWeather/c/build\n$ ./cmake_summit_pgi.sh\n$ make serial\n$ make mpi\n$ make openmp\n$ make openacc\n\nBelow, we'll look at using TAU to profile each case.\n\nModifications\n\nEdit the cmake_summit_pgi.sh and replace mpic++ with tau_cxx.sh. This applies\nonly for the non-GPU versions.\n\nTAU works with special TAU makefiles to declare what programming models are\nexpected from the application:\n\nThe available makefiles are located inside TAU installation:\n\n$ module show tau\n---------------------------------------------------------------\n   /sw/summit/modulefiles/core/tau/2.28.1:\n---------------------------------------------------------------\nwhatis(\"TAU 2.28.1 github \")\nsetenv(\"TAU_DIR\",\"/sw/summit/tau/tau2/ibm64linux\")\nprepend_path(\"PATH\",\"/sw/summit/tau/tau2/ibm64linux/bin\")\nhelp([[https://www.olcf.ornl.gov/software_package/tau\n]])\n\nThe available Makefiles are named per-compiler and are located in:\n\n$ ls ${TAU_DIR}/lib/Makefile.tau-pgi*\n/sw/summit/tau/tau2/ibm64linux/lib/Makefile.tau-pgi-papi-mpi-cupti-pdt-openmp-pgi\n/sw/summit/tau/tau2/ibm64linux/lib/Makefile.tau-pgi-papi-mpi-cupti-pdt-pgi\n/sw/summit/tau/tau2/ibm64linux/lib/Makefile.tau-pgi-papi-pdt-pgi\n/sw/summit/tau/tau2/ibm64linux/lib/Makefile.tau-pgi_memory_manager-papi-mpi-cupti-pdt-pgi\n\nTo list all TAU makefiles:\n\n$ ls ${TAU_DIR}/lib/Makefile.tau*\n\nInstrumenting the serial version of MiniWeather\n\nFor a serial application, we should not use a Makefile with a programming\nmodel such as MPI or OpenMP. However, as the source code for this specific\ncase includes MPI headers that are not excluded during the compilation of the\nserial version, we should declare a Makefile with MPI. We can declare a TAU\nmakefile with the environment variable TAU_MAKEFILE. Moreover, with\nTAU_OPTIONS below, we add options to the linker as the application depends on\nPNetCDF.\n\n$ module load tau\n$ export TAU_MAKEFILE=/sw/summit/tau/tau2/ibm64linux/lib/Makefile.tau-pgi-papi-mpi-cupti-pdt-pgi\n$ export TAU_OPTIONS='-optLinking=-lpnetcdf -optVerbose'\n$ ./cmake_summit_pgi.sh\n$ make serial\n\nIf there were no MPI headers, you should select the makefile\n/sw/summit/tau/tau2//ibm64linux/lib/Makefile.tau-pgi-papi-pdt-pgi or if\nyou don't want PDT support,\n/sw/summit/tau/tau2//ibm64linux/lib/Makefile.tau-pgi-papi-pgi\nAdd to your submission script the TAU variables that you want to use (or\nuncomment them below). By default the TAU will apply profiling, and not apply tracing.\n\n#PAPI metrics\n#export TAU_METRICS=TIME:PAPI_TOT_INS:PAPI_TOT_CYC\n\n# Instrument the callpath\nexport TAU_CALLPATH=1\nexport TAU_CALLPATH_DEPTH=10\n\n#Activate tracing\n#export TAU_TRACE=1\n\njsrun -n 1 -r 1 -a 1 -c 1 -g 1  ./miniWeather_serial\n\nWhen the execution finishes, one directory is created for each TAU_METRICS\ndeclaration with the format MULTI__\n\nIf you do not declare the TAU_METRICS variable, then TIME is used by\ndefault, and the profiling files are not in a directory. When the execution\nends there will be one file per process called profile.X.Y.Z. In this\ncase there is just one file, called profile.0.0.0\n\nWe can export a text file with some information through the pprof tool or\nvisualize it by using paraprof.\n\nIf an application has no MPI at all, use the argument --smpiargs=\"off\" for\njsrun. Otherwise, TAU will fail as MPI is active by default.\n\n$ pprof profile.0.0.0\nReading Profile files in profile.*\n\nNODE 0;CONTEXT 0;THREAD 0:\n---------------------------------------------------------------------------------------\n%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name\n              msec   total msec                          usec/call\n---------------------------------------------------------------------------------------\n100.0        0.038     1:10.733           1           1   70733442 .TAU application\n100.0            9     1:10.733           1        4654   70733404 int main(int, char **)\n 97.1           15     1:08.668        4501       27006      15256 void perform_timestep(double *, double *, double *, double *, double)\n 97.1        1,167     1:08.653       27006       54012       2542 void semi_discrete_step(double *, double *, double *, double, int, double *, double *)\n 48.4       34,240       34,240       13503           0       2536 void compute_tendencies_z(double *, double *, double *)\n 46.9       33,199       33,199       13503           0       2459 void compute_tendencies_x(double *, double *, double *)\n  2.5          224        1,752         151       33361      11608 void output(double *, double)\n  1.7        1,211        1,211         604         604       2006 MPI_File_write_at_all()\n  0.4           36          250           1      100003     250708 void init(int *, char ***)\n...\n\n\nUSER EVENTS Profile :NODE 0, CONTEXT 0, THREAD 0\n---------------------------------------------------------------------------------------\nNumSamples   MaxValue   MinValue  MeanValue  Std. Dev.  Event Name\n---------------------------------------------------------------------------------------\n      1058    1.6E+05          4  9.134E+04  7.919E+04  MPI-IO Bytes Written\n       454        284          4      5.947       13.2  MPI-IO Bytes Written : int main(int, char **) => void output(double *, double) => MPI_File_write_at()\n       604    1.6E+05    1.6E+05    1.6E+05          0  MPI-IO Bytes Written : int main(int, char **) => void output(double *, double) => MPI_File_write_at_all()\n      1058       9412     0.1818       3311       3816  MPI-IO Write Bandwidth (MB/s)\n       454      1.856     0.1818     0.5083     0.1904  MPI-IO Write Bandwidth (MB/s) : int main(int, char **) => void output(double *, double) => MPI_File_write_at()\n       604       9412      2.034       5799       3329  MPI-IO Write Bandwidth (MB/s) : int main(int, char **) => void output(double *, double) => MPI_File_write_at_all()\n       755          8          8          8          0  Message size for all-reduce\n       302  2.621E+05          4  1.302E+05  1.311E+05  Message size for broadcast\n---------------------------------------------------------------------------------------\n\nExplanation:\n\nOne process was running as it is a serial application, even MPI calls\nare executed from a single thread.\n\nThe total execution time is 70.733 seconds and only 9 msec are\nexclusive for the main routine. The rest are caused by subroutines.\n\nThe exclusive time is the time caused by the mentioned routine, and\nthe inclusive is with the execution time from the subroutines.\n\nThe #Subrs is the number of the called subroutines.\n\nThere is also information about the parallel I/O if any exists, the\nbytes, and the bandwidth.\n\nNext, we will look at using the paraprof tool for the MPI version of MiniWeather.\n\nInstrumenting the MPI version of MiniWeather\n\nFor the MPI version, we should use a makefile with MPI. The compilation could\nfail if the makefile supports MPI+OpenMP, but the code doesn't include any\nOpenMP calls. Moreover, with TAU_OPTIONS declared below, we will add options to\nthe linker.\n\n$ module load tau\n$ export TAU_MAKEFILE=/sw/summit/tau/tau2/ibm64linux/lib/Makefile.tau-pgi-papi-mpi-cupti-pdt-pgi\n$ export TAU_OPTIONS='-optLinking=-lpnetcdf -optVerbose'\n$ make mpi\n\nAdd to your submission script the TAU variables that you want to use (or\nuncomment them below). By default, the TAU will apply profiling, and not\ntracing.\n\n#PAPI metrics\nexport TAU_METRICS=TIME:PAPI_TOT_INS:PAPI_TOT_CYC\n\n# Instrument the callpath\nexport TAU_CALLPATH=1\nexport TAU_CALLPATH_DEPTH=10\n\n#Track MPI messages\nexport TAU_TRACK_MESSAGE=1\nexport TAU_COMM_MATRIX=1\n\n#Activate tracing\n#export TAU_TRACE=1\n\njsrun -n 64 -r 8 -a 1 -c 1 ./miniWeather_mpi\n\nInstrumenting the MPI+OpenMP version of MiniWeather\n\nThe difference with the MPI instrumentation is the TAU Makefile, the jsrun\nexecution command, and the declaration of the OpenMP threads.\n\n$ module load tau\n$ export TAU_MAKEFILE=/sw/summit/tau/tau2/ibm64linux/lib/Makefile.tau-pgi-papi-mpi-cupti-pdt-openmp-pgi\n$ export TAU_OPTIONS='-optLinking=-lpnetcdf -optVerbose'\n$ make openmp\n\nAdd to your submission script the TAU variables that you want to use (or\nuncomment them below). By default, the TAU will apply profiling, and not\ntracing.\n\n#PAPI metrics\nexport TAU_METRICS=TIME:PAPI_TOT_INS:PAPI_TOT_CYC\n\n# Instrument the callpath\nexport TAU_CALLPATH=1\nexport TAU_CALLPATH_DEPTH=10\n\n#Track MPI messages\nexport TAU_TRACK_MESSAGE=1\nexport TAU_COMM_MATRIX=1\n\n#Activate tracing\n#export TAU_TRACE=1\n\nexport OMP_NUM_THREADS=4\njsrun -n 16 -r 8 -a 1 -c 4 -b packed:4 ./miniWeather_mpi_openmp\n\nInstrumenting the MPI+OpenACC version of MiniWeather\n\nFor the current TAU version, you should use the tau_exec and not the TAU\nwrappers only for the compilation.\n\nUse the mpic++ compiler in the Makefile, do not use TAU wrapper.\n\nBuild the MPI+OpenACC version by running make openacc.\n\nAdd the following in your submission file:\n\nexport TAU_METRICS=TIME\nexport TAU_PROFILE=1\nexport TAU_TRACK_MESSAGE=1\nexport TAU_COMM_MATRIX=1\njsrun -n 6 -r 6 --smpiargs=\"-gpu\" -g 1  tau_exec -T mpi,pgi,pdt -openacc ./miniWeather_mpi_openacc\n\nWe declare to TAU to profile the MPI with PDT support through the -T\nparameters, as well as using the pgi tag for the TAU makefile and OpenACC.\n\nCUPTI metrics for OpenACC are not yet supported for TAU.\n\nPreparing profiling data\n\nWhen the execution of the instrumented application finishes, there is one\ndirectory for each TAU_METRICS declaration with the format MULTI__\n\nIf you do not declare the TAU_METRICS variable, then by default TIME\nis used and the profiling files are not in a directory. When the execution\nends, there will be one file per process, called profile.X.Y.Z.\n\nIn order to use paraprof to visualize the data, your ssh connection should\nsupport X11 forwarding.\n\nPack the profiling data with a name that you prefer and start the paraprof GUI\n\n$ paraprof --pack name.ppk\n$ paraprof name.ppk &\n\nParaprof\n\nThe first window that opens when the paraprof name.ppk command is\nexecuted shows the experiment and the used metrics, for this case, TIME,\nPAPI_FP_OPS, PAPI_TOT_INS, PAPI_TOT_CYC.\n\n\n\nThe user is responsible for understanding which PAPI metrics should be used\n\nThe second window that is automatically loaded shows the TIME metric for\neach process (they are called \"nodes\") where each color is a different call.\nEach horizontal line is a process or Std.Dev./mean/max/min. The length of each\ncolor is related to the metric, if it is TIME, it is duration.\n\n\n\nSelect Options -> Uncheck Stack Bars Together\n\nIt is easier to check the load imbalance across the processes\n\n\n\nIf you click on any color, then a new window opens with information about the\nspecific routine.\n\n\n\nIf you click on the label (node 0, node 1, max, etc.), you can see the value\nacross each routine in your application.\n\n\n\nRight click on the label (node 0, node 1, max, etc.), and then select \"Show\nContext Event Window\" (with callpath activated). We can then see various calls\nfrom where they were executed, how many times, and other various information.\n\n\n\nSelect Options -> Show Derived Metric Panel, choose the metrics and then the\noperator that you want, then click Apply. Uncheck the Show Derived\nMetric.\n\n\n\nClick on the new metric, \"PAPI_TOT_INS / PAPI_TOT_CYC\" to see the instructions per\ncycle (IPC) across the various routines.\n\n\n\nClick on the label mean:\n\n\n\nFor the non-MPI routines/calls, an IPC that is lower than 1.5 means that\nthere is a potential for performance improvement.\n\nMenu Windows -> 3D Visualization (3D demands OpenGL) will not work on\nSummit, and you will need to download the data on your laptop and install\nTAU locally to use this feature.\n\nYou can see per MPI rank, per routine, the exclusive time and the floating\noperations.\n\n\n\nChange the PAPI_FP_OPS to (PAPI_TOT_INS/PAPI_TOT_CYC)\n\nYou can see per MPI rank, per routine, the exclusive time and the\ncorresponding IPC.\n\n\n\nWhich loops consume most of the time?\n\nCreate a file called, for example, select.tau with the content:\n\nBEGIN_INSTRUMENT_SECTION\nloops routine=\"#\"\nEND_INSTRUMENT_SECTION\n\nThen declare the options in your submission script:\n\nexport TAU_OPTIONS=\"-optTauSelectFile=select.tau -optLinking=lpnetcdf -optVerbose\"\n\nThe linking option is required for this application, but may not be for all applications.\n\nDo not forget to unset TAU_OPTIONS when it's not necessary.\n\nExecute the application as previously shown.\n\nNow you can see the duration of all the loops\n\n\n\nSelect Options -> Select Metric… -> Exclusive… -> PAPI_TOT_INS/PAPI_TOT_CYC\n\n\n\nThe loops with less than 1.5 IPC have poor performance and could likely be improved.\n\nMPI+OpenMP\n\nExecute the MPI+OpenMP version\n\nNow you can see the duration of parallelfor loops and decide when they should\nbe improved or even removed.\n\n\n\nGPU\n\nWhen we instrument the MPI with OpenACC, we can see the following through paraprof\n\nWe can observe the duration of the OpenACC calls\n\n\n\nFrom the main window right click one label and select “Show User Event\nStatistics Window”. Then, we can see the data transfered to the devices\n\n\n\nCUDA Profiling Tools Interface\n\nThe CUDA Profiling Tools Interface (CUPTI) is used by profiling and tracing\ntools that target CUDA applications.\n\n\n\nMatrix multiplication with MPI+OpenMP:\n\n$ export TAU_METRICS=TIME,achieved_occupancy\n$ jsrun -n 2 -r 2 -g 1  tau_exec -T mpi,pdt,papi,cupti,openmp -ompt -cupti  ./add\n\nWe choose to use tau_exec with MPI, PDT, PAPI, CUPTI, and OpenMP.\n\nOutput directories:\n\nMULTI__TAUGPU_TIME\nMULTI__CUDA.Tesla_V100-SXM2-16GB.domain_d.active_warps\nMULTI__CUDA.Tesla_V100-SXM2-16GB.domain_d.active_cycles\nMULTI__achieved_occupancy\n\nThere are many directories because the achieved occupancy is calculated with this\nformula\n\nAchieved_occupancy = CUDA.Tesla_V100-SXM2-16GB.domain_d.active_warps / CUDA.Tesla_V100-SXM2-16GB.domain_d.active_cycles\n\nYou can see in the window with the profiling data after you pack them and\nexecute paraprof, the profiling data are not across all the processes, it\ndepends if a routine (color) is executed across all of them or not based on\nthe type of the rourine CPU/GPU.\n\n\n\n\n\nSelect the metric achieved occupancy\n\n\n\nClick on the colored bar\n\nThe achieved occupancy for this simple benchmark is 6.2%\n\n\n\nA similar approach for other metrics, not all of them can be used.\nTAU provides a tool called tau_cupti_avail, where we can see the list of\navailable metrics, then we have to figure out which CUPTI metrics use these ones.\n\nTracing\n\nActivate tracing and declare the data format to OTF2. OTF2 format is supported\nonly by MPI and OpenSHMEM applications.\n\n$ export TAU_TRACE=1\n$ export TAU_TRACE_FORMAT=otf2\n\nUse Vampir for visualization.\n\nSelective Instrumentation\n\nFor example, do not instrument routine sort*(int *)\n\nCreate a file select.tau\n\nBEGIN_EXCLUDE_LIST\nvoid sort_#(int *)\nEND_EXCLUDE_LIST\n\nDeclare the TAU_OPTIONS variable\n\nexport TAU_OPTIONS=“-optTauSelectFile=select.tau”\n\nNow, the routine sort*(int *) is excluded from the instrumentation.\n\nDynamic Phase\n\nCreate a file called phase.tau.\n\nBEGIN_INSTRUMENT_SECTION\ndynamic phase name=“phase1” file=“miniWeather_mpi.cpp” line=300 to line=327\nEND_INSTRUMENT_SECTION\n\nDeclare the TAU_OPTIONS variable.\n\nexport TAU_OPTIONS=“-optTauSelectFile=phase.tau”\n\nNow when you instrument your application, the phase called phase 1 are the\nlines 300-327 of the file miniWeather_mpi.cpp. Every call will be\ninstrumented. This could create signiificant overhead, thus you should be\ncareful when you use it.\n\nStatic Phase\n\nCreate a file called phases.tau.\n\nBEGIN_INSTRUMENT_SECTION\nstatic phase name=\"phase1\" file=\"miniWeather_mpi.cpp\" line=300 to line=327\nstatic phase name=\"phase2\" file=\"miniWeather_mpi.cpp\" line=333 to line=346\nEND_INSTRUMENT_SECTION\n\nDeclare the TAU_OPTIONS variable.\n\nexport TAU_OPTIONS=“-optTauSelectFile=phases.tau”\n\nNow, when you use paraprof, you can see different colors for the phase1 and phase2\n\n\n\nOpenMP Offload\n\nInitially compile your application without TAU and create a dynamic binary\n\nUse all the compiler options requiried for OpenMP offload\n\nThen execute, for example with XL compiler, 1 resource set, 1 MPI process with 2 OpenMP threads, and 2 GPUs:\n\njsrun --smpiargs=\"-gpu\" -n 1 -a 2 -c 2 -r 1 -g 2 -b packed:2 tau_exec -T cupti,papi,openmp,xl -cupti ./a.out"}
{"doc":"training_archive","text":"OLCF Training Archive\n\n\n\nThe table below lists presentations given at previous OLCF training events. For a list of upcoming training events, please visit the OLCF Training Calendar\n\n\nDate|Title|Speaker|Event|Presentation\n2023-07-27|Coarray Fortran Tutorial|Damian Rouson, Computer Languages and Systems Software Group Lead (LBNL)|Introduction to High-Performance Parallel Distributed Computing using Chapel, UPC++ and Coarray Fortran https://www.olcf.ornl.gov/calendar/introduction-to-high-performance-parallel-distributed-computing-using-chapel-upc-and-coarray-fortran/|(slides | tutorial site) https://bitbucket.org/berkeleylab/cuf23/downloads/cuf23-fortran.pdf https://go.lbl.gov/cuf23\n2023-07-27|UPC++: An Asynchronous RMA/RPC Library for Distributed C++ Applications|Amir Kamil, Visiting Faculty (LBNL)|Introduction to High-Performance Parallel Distributed Computing using Chapel, UPC++ and Coarray Fortran https://www.olcf.ornl.gov/calendar/introduction-to-high-performance-parallel-distributed-computing-using-chapel-upc-and-coarray-fortran/|(slides | tutorial site) https://bitbucket.org/berkeleylab/cuf23/downloads/cuf23-upcxx.pdf https://go.lbl.gov/cuf23\n2023-07-26|Introduction to the Chapel Programming Language|Michelle Mills Strout, Sr. Engineering Manager (HPE)|Introduction to High-Performance Parallel Distributed Computing using Chapel, UPC++ and Coarray Fortran https://www.olcf.ornl.gov/calendar/introduction-to-high-performance-parallel-distributed-computing-using-chapel-upc-and-coarray-fortran/|(slides | tutorial site) https://bitbucket.org/berkeleylab/cuf23/downloads/cuf23-chapel.pdf https://go.lbl.gov/cuf23\n2023-07-26|Welcome and Introduction|Michelle Mills Strout (HPE), Damian Rouson (LBNL), Amir Kamil (LBNL)|Introduction to High-Performance Parallel Distributed Computing using Chapel, UPC++ and Coarray Fortran https://www.olcf.ornl.gov/calendar/introduction-to-high-performance-parallel-distributed-computing-using-chapel-upc-and-coarray-fortran/|(slides | tutorial site) https://bitbucket.org/berkeleylab/cuf23/downloads/cuf23-intro.pdf https://go.lbl.gov/cuf23\n2023-07-26|Data Transfer Overview|Suzanne Parete-Koon (OLCF)|July 2023 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-july2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/Data-Transfer.pdf https://vimeo.com/849465200\n2023-07-13|AI Training Series: SmartSim at OLCF|Andrew Shao, Matt Ellis, Matt Drozt (HPE)|SmartSim at OLCF https://www.olcf.ornl.gov/calendar/smartsim-at-olcf/|(slides | recording | Q&A | tutorial site) https://www.olcf.ornl.gov/wp-content/uploads/20230713_OLCF_SmartSim.pdf https://vimeo.com/845346288 https://www.olcf.ornl.gov/wp-content/uploads/ZoomQA_smartsim.txt https://github.com/CrayLabs/OLCF_SmartSim2023\n2023-06-28|Blender on Frontier|Michael Sandoval (OLCF)|Blender on Frontier https://www.olcf.ornl.gov/calendar/userconcall-june2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/Blender_on_Frontier_published.pdf https://vimeo.com/840891737\n2023-06-15|AI Training Series: AI for Science at Scale - Introduction|Sajal Dash, Junqi Yin, Wes Brewer (OLCF)|AI for Science at Scale - Introduction https://www.olcf.ornl.gov/calendar/ai-for-science-at-scale-intro/|(slides | recording | tutorial site) https://www.olcf.ornl.gov/wp-content/uploads/AI-For-Science-at-Scale-Introduction.pdf https://vimeo.com/836918490 https://github.com/olcf/ai-training-series/tree/main/ai_at_scale\n2023-05-31|OLCF Storage and Orion Best Practices|Suzanne Parete-Koon and Jesse Hanley (OLCF)|May 2023 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-may2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/May2023_Usercall_OLCFStorage.pdf https://vimeo.com/833369509\n2023-05-24|Julia for High Performance Computing Tutorial|William Godoy (ORNL)|Julia for High Performance Computing Tutorial https://www.olcf.ornl.gov/calendar/julia-for-high-performance-computing-tutorial/|(recording) https://vimeo.com/830368460\n2023-05-18|Using Slurm on Frontier|Tom Papatheodore (OLCF)|Using Slurm on Frontier https://www.olcf.ornl.gov/calendar/using-slurm-on-frontier/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2023_05_18_slurm_on_frontier.pdf https://vimeo.com/828638016\n2023-04-26|AI for HPC|Arjun Shankar, Junqi Yin, Wes Brewer (OLCF)|April 2023 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-apr2023/|(slides | recording | Q&A) https://www.olcf.ornl.gov/wp-content/uploads/HPC-AI_Combination_UserMeeting_26Apr2023.pdf https://vimeo.com/823104570 https://www.olcf.ornl.gov/wp-content/uploads/QA_Apr23_Usercall.txt\n2023-03-29|Checkpointing Best Practices for Frontier|Scott Atchley (OLCF)|March 2023 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-mar2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/Checkpointing-Tips-OLCF-User-Call-20230329.pdf https://vimeo.com/814713985\n2023-02-22|Cybersecurity Best Practices|Ryan Adamson (OLCF)|February 2023 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-feb2023/|(slides | recording | Q&A) https://www.olcf.ornl.gov/wp-content/uploads/OLCF_Security_Awareness.pdf https://vimeo.com/802845205 https://www.olcf.ornl.gov/wp-content/uploads/QA_Feb23_Usercall.txt\n2023-02-17|Checkpointing Tips|Scott Atchley, HPC Systems Engineer, Distinguished R&D Staff, ORNL|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/Checkpointing-Tips-Frontier-Training-Workshop-20230217.pdf https://vimeo.com/803634715\n2023-02-17|Frontier Tips & Tricks|Balint Joo, Group Leader, Advanced Computing for Nuclear, Particles, & Astrophysics, ORNL|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/Joo-FrontierTipsAndTricks.pdf https://vimeo.com/803633277\n2023-02-17|GPU Debugging|Mark Stock, HPC Applications Engineer, HPE|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2-17-23_GPU_Debugging_distribute-2.pdf https://vimeo.com/840552596\n2023-02-17|GPU Profiling|Alessandro Fanfarillo, Senior Member of Technical Staff, Exascale Application Performance, AMD|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2-17-23_Omniperf_2295_20230217.pdf https://vimeo.com/803631137\n2023-02-17|Application Profiling|Trey White, Master Engineer, HPE|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2-17-22_application_profiling.pdf https://vimeo.com/840552061\n2023-02-16|Orion Lustre and Best Practices|Jesse Hanley, Senior HPC Linux Systems Engineer, ORNL|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2-16-22_orion_lustre_and_best_practices.pdf https://vimeo.com/802887822\n2023-02-16|Node Performance|Tom Papatheodore, HPC Engineer, ORNL|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2-16-23-node_performance.pdf https://vimeo.com/802887222\n2023-02-16|NVMe Usage|Chris Zimmer,  Group Leader, Technology Integration, ORNL|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2-17-23_nvme.pdf https://vimeo.com/803630815\n2023-02-16|AI on Frontier|Junqi Yin, Computational Scientist, ORNL|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2-16-23_AIonFrontier.pdf https://vimeo.com/802883846\n2023-02-16|Python on Frontier|Michael Sandoval, HPC Engineer, ORNL|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2-16-23_python_on_frontier.pdf' https://vimeo.com/802883471\n2023-02-16|HPE Cray MPI|Tim Mattox, HPC Performance Engineer, HPE|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/20230216_HPE_Cray_MPI.pdf' https://vimeo.com/840565839\n2023-02-16|GPU Programming Models|GPU Programming Models|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2-16-23-gpu_programmin_models.pdf' https://vimeo.com/802882946\n2023-02-15|Slurm on Frontier|Tom Papatheodore, HPC Engineer, ORNL|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2-15-23-slurm-on-frontier.pdf https://vimeo.com/803622922\n2023-02-15|Storage Areas and Data Transfers|Suzanne Parete-Koon, HPC Engineer, ORNL|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/Data-and-Storage-areas-3.pdf https://vimeo.com/803622140\n2023-02-15|Using the Frontier Programming Environment|Matt Belhorn, HPC Engineer, ORNL|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/frontier_programming_environment_20230215.pdf https://vimeo.com/803621185\n2023-02-15|Frontier Programming Environment|Wael Elwasif, Computer Scientist, ORNL|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2-15-23_Frontier_Programming_Environment.pdf https://vimeo.com/803620593\n2023-02-15|Epyc CPU and Instinct GPU|Nick Malaya, Principal Member of Technical Staff, Exascale Application Performance, AMD|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2-15-23-AMD-CPU-GPU-Frontier-Public.pdf https://vimeo.com/803618546\n2023-02-15|Frontier Architecture Overview|Joe Glenski, Sr. Distinguished Technologist, HPE|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2-15-23-Frontier-System-Architecture-public-v7.pdf https://vimeo.com/840551316\n2023-02-15|Welcome to the Frontier Workshop|Ashley Barker, Section Head, Operations, National Center for Computational Sciences, ORNL|Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2-15-23_welcome_address.pdf https://vimeo.com/803618138\n2023-01-25|Containers on Summit|Subil Abraham (OLCF)|January 2023 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-jan2023/|(slides | recording | Q&A) https://www.olcf.ornl.gov/wp-content/uploads/Containers_Usercall_Jan23.pdf https://vimeo.com/792760790 https://www.olcf.ornl.gov/wp-content/uploads/QA_Jan23_Usercall.txt\n2022-12-14|Using HIP and GPU Libraries with OpenMP|Reuben Budiardja|Using HIP and GPU Libraries with OpenMP https://www.olcf.ornl.gov/calendar/preparing-for-frontier-openmp-part3/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2022-12-OLCF_OpenMP_GPU_Library.pdf https://vimeo.com/manage/videos/781271547\n2022-12-14|HPCToolkit Overview|John Mellor-Crummey (Rice)|December 2022 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-dec2022/|(slides | recording | Q&A) https://www.olcf.ornl.gov/wp-content/uploads/HPCToolkit-ORNL-2022-12-14.pdf https://vimeo.com/781264043 https://www.olcf.ornl.gov/wp-content/uploads/HPCToolkit_QA.txt\n2022-12-09|Crusher User Experience Talks - Cholla|Evan Schneider & Robert Caddy (University of Pittsburgh|Crusher User Experience Talks https://www.olcf.ornl.gov/calendar/crusher-user-experience-talks/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/CrusherUserExperience_Cholla.pdf https://vimeo.com/780853547\n2022-12-09|Crusher User Experience Talks - NuCCOR|Justin Lietz (OLCF)|Crusher User Experience Talks https://www.olcf.ornl.gov/calendar/crusher-user-experience-talks/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/CrusherUserExperience_NuCCOR.pdf https://vimeo.com/780853881\n2022-12-09|Crusher User Experience Talks - LatticeQCD|Balint Joo (OLCF)|Crusher User Experience Talks https://www.olcf.ornl.gov/calendar/crusher-user-experience-talks/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/CrusherUserExperience_LatticeQCD.pdf https://vimeo.com/780853714\n2022-12-01|Lessons & Tips from OLCF's Crusher Hackathons|Tom Papatheodore (ORNL)|Lessons and Tips from OLCF's Crusher Hackathons https://www.olcf.ornl.gov/calendar/lessons-and-tips-from-olcfs-crusher-hackathons/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/crusher_tips_lessons.pdf https://vimeo.com/777356705\n2022-10-13|Data Visualization and Analytics Training Series: VisIt at OLCF|Michael Sandoval (ORNL)|VisIt at OLCF 2022 https://www.olcf.ornl.gov/calendar/visit-at-olcf/|(recording | tutorial site) https://vimeo.com/760322024 https://github.com/olcf/dva-training-series/tree/main/visit\n2022-10-11|Hierarchical Roofline Profiling on AMD GPUs|Noah Wolfe (AMD) and Xiaomin Lu (AMD)|Special Session|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/AMD_Hierarchical_Roofline_ORNL_10-12-22.pdf https://vimeo.com/759293583\n2022-09-28|Julia for HPC on OLCF Systems|William F Godoy, Pedro Valero-Lara, Philip Fackler (ORNL)|September 2022 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-sep2022/|(slides pt.1 | slides pt. 2 | recording | Q&A) https://www.olcf.ornl.gov/wp-content/uploads/PedroValeroLara-Julia-HPC.pdf https://www.olcf.ornl.gov/wp-content/uploads/2022_OLCF_UsersCall_WFGodoy.pdf https://vimeo.com/755203498 https://www.olcf.ornl.gov/wp-content/uploads/Julia_at_OLCF_QA.txt\n2022-09-15|Data Visualization and Analytics Training Series: ParaView at OLCF|Ken Moreland (ORNL)|ParaView at OLCF 2022 https://www.olcf.ornl.gov/calendar/paraview-at-olcf/|(recording | tutorial site) https://vimeo.com/750382858 https://kmorel.gitlab.io/pv-tutorial-olcf-2022/\n2022-08-31|Andes Overview|Leah Huk (OLCF)|August 2022 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-aug2022/|(Announcement slides | Andes slides | recording ) https://www.olcf.ornl.gov/wp-content/uploads/Aug22_UserCall_Announcements.pdf https://www.olcf.ornl.gov/wp-content/uploads/Andes_User_Call_08_31_22.pdf https://vimeo.com/745108997\n2022-08-23|Understanding GPU Register Pressure (part 1)|Alessandro Fanfarillo (AMD)|Special Session|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/Intro_Register_pressure_ORNL_20220812_2083.pdf https://vimeo.com/742349001\n2022-08-19|Using R on HPC Clusters Part 2|George Ostrouchov, (ORNL)|Using R on HPC Clusters https://www.olcf.ornl.gov/calendar/using-r-on-hpc-clusters-webinar/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/Using-_R_on_HPC_Clusters_Part-2.pdf https://vimeo.com/manage/videos/742349613\n2022-08-17|Using R on HPC Clusters Part 1|George Ostrouchov, (ORNL)|Using R on HPC Clusters https://www.olcf.ornl.gov/calendar/using-r-on-hpc-clusters-webinar/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/Using_R_on_HPC_Clusters_Part_1.pdf https://vimeo.com/741133171\n2022-08-11|Introduction to OpenMP Offload Part 1 Basics of Offload|Swaroop Pophale Computer Scientist (ORNL)|Basics of Offload https://www.olcf.ornl.gov/calendar/introduction-to-openmp-offload-part-1/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/OLCF_Intro_to_OpenMP_Aug11.pdf https://vimeo.com/manage/videos/742336327\n2022-07-27|Remote Visualization with VNC|Benjamin Hernandez (OLCF)|July 2022 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-jul2022|(slides | recording ) https://www.olcf.ornl.gov/wp-content/uploads/Jul_2022_VNC.pdf https://vimeo.com/735781882\n2022-07-21|HIP for CUDA Programmers|Subil Abraham (OLCF)|HIP for CUDA Programmers https://www.olcf.ornl.gov/calendar/hip-for-cuda-programmers/|(slides | HIP with Fortran slides | recording | repo ) https://www.olcf.ornl.gov/wp-content/uploads/hip_for_cuda_programmers_slides.pdf https://www.olcf.ornl.gov/wp-content/uploads/09212021_HIPFort_ORNL.pdf https://vimeo.com/736989695 https://github.com/olcf/HIP_for_CUDA_programmers/\n2022-07-14|Data Visualization and Analytics Training Series: Jupyter Workflow at OLCF|Ryan Prout, Benjamin Hernandez, Junqi Yin (OLCF)|Jupyter Workflow at OLCF https://www.olcf.ornl.gov/calendar/data-visualization-and-analytics-training-series-jupyter-workflow-at-olcf/|(Overview slides | Workflow slides | DL slides | recording ) https://www.olcf.ornl.gov/wp-content/uploads/Jupyter_Overview.pdf https://www.olcf.ornl.gov/wp-content/uploads/Jupyter_Analysis_Workflow.pdf https://www.olcf.ornl.gov/wp-content/uploads/Jupyter_DL_Workflow.pdf https://vimeo.com/730396217\n2022-07-14|Introduction to HIP Programming|Tom Papatheodore (OLCF)|Introduction to HIP Programming https://www.olcf.ornl.gov/calendar/introduction-to-hip-programming/|(slides | recording | repo ) https://www.olcf.ornl.gov/wp-content/uploads/intro_to_hip.pdf https://vimeo.com/736962754 https://github.com/olcf/intro_to_hip\n2022-07-12|Introduction to the Frontier Supercomputer|Scott Atchley & David Bernholdt (OLCF)|Introduction to the Frontier Supercomputer https://www.olcf.ornl.gov/calendar/introduction-to-the-frontier-supercomputer/|(architecture slides | programming environment slides | recording ) https://www.olcf.ornl.gov/wp-content/uploads/Frontiers-Architecture-Frontier-Training-Series-final.pdf https://www.olcf.ornl.gov/wp-content/uploads/frontier-pet-v02.pdf https://vimeo.com/731064231\n2022-06-29|Frontier Announcement and Overview|Bronson Messer (OLCF)|June 2022 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-jun2022|(slides | recording | Q&A) https://www.olcf.ornl.gov/wp-content/uploads/UsersMeetingJune2022.pdf https://vimeo.com/727066482 https://www.olcf.ornl.gov/wp-content/uploads/June-2022-Concall-QA.pdf\n2022-05-25|Automating Science with Workflows at OLCF|Ketan Maheshwari, Sean Wilkinson, Rafael Ferreira da Silva (OLCF)|May 2022 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-may2022|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/Automating-Science-With-Workflows-at-OLCF.pdf https://vimeo.com/730109850\n2022-04-27|myOLCF Self-Service Portal|Leah Huk (OLCF)|April 2022 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-apr2022|(slides) https://www.olcf.ornl.gov/wp-content/uploads/2022_myOLCF_User_Concall.pdf\n2022-04-07|CODING FOR GPUS USING STANDARD Fortran|Jeff Larkin (NVIDIA)|CODING FOR GPUS USING STANDARD Fortran https://www.olcf.ornl.gov/calendar/coding-for-gpus-using-standard-fortran/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/20220513_OLCF_Fortran.pdf https://vimeo.com/manage/videos/711784748\n2022-04-07|CODING FOR GPUS USING STANDARD C++|Robert Searles (NVIDIA)|CODING FOR GPUS USING STANDARD C++ https://www.olcf.ornl.gov/calendar/coding-for-gpus-using-standard-c/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/4-7-22-ORNL-Stdpar.pdf https://vimeo.com/697495123\n2022-03-30|Machine Learning for HPC simulations: Using PyTorch, TensorFlow in Fortran, C, and C++ with SmartSim|Sam Partee (HPE)|March 2022 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-mar2022|(recording) https://vimeo.com/694124650\n2022-02-23|OLCF Best Practices and Overview for New Users (Hands-on/Exercises)|Suzanne Parete-Koon (OLCF)|February 2022 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-feb2022/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2022/02/Introduction-to-Job-Submission-on-Summit.pdf https://vimeo.com/681464497\n2022-02-23|OLCF Best Practices and Overview for New Users (Presentation)|Bill Renaud (OLCF)|February 2022 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-feb2022/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2022/02/Best-Practices-2022.pdf https://vimeo.com/681464868\n2022-01-26|HPSS Overview|Gregg Gawinski (OLCF)|January 2022 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-jan2022/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2022/01/HPSS-Archive-Overview.pdf https://vimeo.com/671261399\n2021-12-08|Analysis Tools at OLCF|Benjamin Hernandez (OLCF)|December 2021 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-dec2021/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/08/Dec_2021_Analysis_Tools_at_OLCF.pdf https://vimeo.com/654969964\n2021-11-12|Introduction to Leadership Computing|Bronson Messer, Tom Papatheodore|Introduction to Leadership Computing https://www.olcf.ornl.gov/introduction-to-leadership-computing/|(slides | recording) https://www.olcf.ornl.gov/introduction-to-leadership-computing/ https://vimeo.com/manage/videos/645378867\n2021-10-27|Node Local Storage: Common Use Cases and Some Tools to Help|Chris Zimmer (OLCF)|October 2021 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-oct2021/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/08/Users-NVMe.pdf https://vimeo.com/640037283\n2021-09-29|OLCF's User Managed Software (UMS) Program|Jamie Finney (OLCF)|September 2021 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-sep2021/|(slides) https://www.olcf.ornl.gov/wp-content/uploads/2021/08/UMS_con_call.pdf\n2021-09-23|Introduction to OpenMP GPU Offloading Day2|Swaroop Pophale, OLCF|Introduction to OpenMP GPU Offloading https://www.olcf.ornl.gov/calendar/introduction-to-openmp-gpu-offloading/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/08/ITOpenMPO_Day2.pdf https://vimeo.com/manage/videos/613827694\n2021-09-22|Introduction to OpenMP GPU Offloading Day1|Swaroop Pophale, OLCF|Introduction to OpenMP GPU Offloading https://www.olcf.ornl.gov/calendar/introduction-to-openmp-gpu-offloading/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/08/ITOpenMP_Day1.pdf https://vimeo.com/manage/videos/613828158\n2021-09-14|CUDA Debugging|Robert Crovella (NVIDIA)|CUDA Training Series https://www.olcf.ornl.gov/calendar/cuda-debugging/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/06/cuda_training_series_cuda_debugging.pdf https://vimeo.com/605842702\n2021-08-25|NVIDIA RAPIDS Updates at OLCF|Benjamin Hernandez (OLCF)|August 2021 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-aug2021/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/06/August_2021_NVIDIA_RAPIDS_update.pdf https://vimeo.com/593301463\n2021-08-25|Slate Hackathon|Jason Kincl (OLCF)|Slate Hackathon https://www.olcf.ornl.gov/calendar/olcf-slate-hackathon/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/07/2021-08-25-slate-hackathon-slides.pptx https://vimeo.com/592862993\n2021-08-17|CUDA Multi Process Service|Max Katz (NVIDIA)|CUDA Training Series https://www.olcf.ornl.gov/calendar/cuda-multi-process-service/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/06/MPS_ORNL_20210817.pdf https://vimeo.com/589019347\n2021-07-28|NVIDIA HPC SDK|Robert Searles (NVIDIA)|July 2021 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-jul2021/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/06/OLCF_User_Call_July_2021_HPC-SDK.pdf https://vimeo.com/582093007\n2021-07-16|CUDA Multithreading with Streams|Robert Searles (NVIDIA)|CUDA Training Series https://www.olcf.ornl.gov/calendar/cuda-multithreading/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/05/10-Multithreading-and-CUDA-Concurrency.pdf https://vimeo.com/575930839\n2021-05-26|ROCgdb and HIP Math Libraries|Justin Chang (AMD)|HIP Training Workshop https://www.olcf.ornl.gov/calendar/2021hip/|(slides | exercises | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/04/rocgdb_hipmath_ornl_2021_v2.pdf https://www.olcf.ornl.gov/wp-content/uploads/2021/04/HIP-Training-Day-3-Exercises.pdf https://vimeo.com/575434256\n2021-05-25|Converting CUDA Codes to HIP|Julio Maia (AMD)|HIP Training Workshop https://www.olcf.ornl.gov/calendar/2021hip/|(slides | exercises | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/04/ORNL_Hackathon_HIPification_profiling_jmaia_05192021.pdf https://www.olcf.ornl.gov/wp-content/uploads/2021/04/HIP-Training-Day-2-Exercises.pdf https://vimeo.com/574590364\n2021-05-24|Introduction to GPU Programming|Gina Sitaraman (AMD)|HIP Training Workshop https://www.olcf.ornl.gov/calendar/2021hip/|(slides | exercises | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/04/IntroGPUProgramming-ORNL-Hackathon-May24-26-2021.pdf https://www.olcf.ornl.gov/wp-content/uploads/2021/04/HIP-Training-Day-1-Exercises-1.pdf https://vimeo.com/575103496\n2021-05-21|GPU Concurrency|Robert Searles (NVIDIA)|May 2021 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-may2021/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/05/GPU-Concurrency-Overview.pdf https://vimeo.com/558811623\n2021-05-20|Spock System Architecture|Joe Glenski (HPE)|Spock Training https://www.olcf.ornl.gov/spock-training/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/04/Glenski-Spock-Architecture-public-v4.pdf https://vimeo.com/554875266\n2021-05-20|MI100 GPU|Nick Malaya (AMD)|Spock Training https://www.olcf.ornl.gov/spock-training/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/04/Spock-MI100-Update-5.20.21.pdf https://vimeo.com/554871957\n2021-05-20|Available Storage Areas & NVMe|Tom Papatheodore (OLCF)|Spock Training https://www.olcf.ornl.gov/spock-training/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/04/Storage_Areas_NVMe.pdf https://vimeo.com/554876284\n2021-05-20|State of HIP|Nick Malaya (AMD)|Spock Training https://www.olcf.ornl.gov/spock-training/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/04/Spock-ROCm-Update-5.20.21.pdf https://vimeo.com/554876026\n2021-05-20|Programming Environment|John Levesque (HPE)|Spock Training https://www.olcf.ornl.gov/spock-training/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/04/SPOCK-PE-UPDATE.pdf https://vimeo.com/554874286\n2021-05-20|Compilers|Jeff Sandoval (HPE)|Spock Training https://www.olcf.ornl.gov/spock-training/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/04/2021-05-20-Frontier-Tutorial-CCE.pdf https://vimeo.com/554872321\n2021-05-20|HPE Cray MPICH & GPU-Aware  MPI|Noah Reddell (HPE)|Spock Training https://www.olcf.ornl.gov/spock-training/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/04/HPE-Cray-MPI-Update-nfr-presented.pdf https://vimeo.com/554872977\n2021-05-20|Running Jobs - Slurm|Hong Liu (OLCF) & Matt Davis (OLCF)|Spock Training https://www.olcf.ornl.gov/spock-training/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/04/Spock-Slurm.pdf https://vimeo.com/554874637\n2021-05-20|Node-Level Profiling|Julio Maia (AMD)|Spock Training https://www.olcf.ornl.gov/spock-training/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/04/SPOCK_Libraries_profiling_JMaia.pdf https://vimeo.com/554874027\n2021-05-20|Cray Performance & Correctness Tools|Kostas Makrides (HPE)|Spock Training https://www.olcf.ornl.gov/spock-training/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/04/CrayToolsAndDebuggers_v1.0_pdfVersion.pdf https://vimeo.com/554873364\n2021-05-20|Spock Tips & Information|Tom Papatheodore (OLCF)|Spock Training https://www.olcf.ornl.gov/spock-training/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/04/Spock_Tips.pdf https://vimeo.com/554875783\n2021-03-31|NVIDIA RAPIDS|Joe Eaton (NVIDIA) and Benjamin Hernandez (OLCF)|March 2021 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-mar2021/|(recording) https://vimeo.com/558811249\n2021-02-24|New User Training/Best Practices @ OLCF|Bill Renaud, Suzanne Parete-Koon, and Subil Abraham (OLCF)|February 2021 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-feb2021/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/01/Best-Practices-2021.pdf https://vimeo.com/519216250\n2020-12-09|Open CE|Junqi Yin (OLCF)|December 2020 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-dec2020/|(slides) https://www.olcf.ornl.gov/wp-content/uploads/2020/09/open-ce.pdf\n2020-10-21|CUDA 11 Features|Jeff Larkin (NVIDIA)|October 2020 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-oct2020/|(slides) https://www.olcf.ornl.gov/wp-content/uploads/2020/09/OLCF_Users_Call_Oct2020.pdf\n2020-09-17|CUDA Cooperative Groups|Bob Crovella (NVIDIA)|CUDA Cooperative Groups https://www.olcf.ornl.gov/calendar/cuda-cooperative-groups/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/06/09_Cooperative_Groups.pdf https://vimeo.com/461821629\n2020-08-18|GPU Performance Analysis|Bob Crovella (NVIDIA)|GPU Performance Analysis https://www.olcf.ornl.gov/calendar/gpu-performance-analysis/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/04/08_GPU_Performance_Analysis.pdf https://vimeo.com/454873041\n2020-07-28|TAU Performance Analysis|Sameer Shende|TAU Performance Analysis https://www.olcf.ornl.gov/calendar/tau-performance-analysis-training/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/tau_ornl20.pdf https://vimeo.com/442482720\n2020-07-21|CUDA Concurrency|Bob Crovella (NVIDIA)|CUDA Concurrency https://www.olcf.ornl.gov/calendar/cuda-concurrency/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/07/07_Concurrency.pdf https://vimeo.com/442361242\n2020-06-23|Loop Optimizations with OpenACC|Robbie Searles (NVIDIA)|Loop Optimizations with OpenACC https://www.olcf.ornl.gov/calendar/loop-optimizations-with-openacc/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/06/OpenACC_Course_2020_Module_3_updated.pdf https://vimeo.com/431954101\n2020-06-18|CUDA Managed Memory|Bob Crovella (NVIDIA)|CUDA Managed Memory https://www.olcf.ornl.gov/calendar/cuda-managed-memory/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/06/06_Managed_Memory.pdf https://vimeo.com/431616420\n2020-06-03|Summit Tips & Tricks|Tom Papatheodore (OLCF)|2020 OLCF User Meeting (Summit New User Training) https://www.olcf.ornl.gov/calendar/2020-olcf-user-meeting/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/Summit_Tips_and_Tricks_2020-06-03.pdf https://vimeo.com/427798547\n2020-06-03|MLDL on Summit|Junqi Yin (OLCF)|2020 OLCF User Meeting (Summit New User Training) https://www.olcf.ornl.gov/calendar/2020-olcf-user-meeting/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/MLDL-on-Summit-June2020.pdf https://vimeo.com/427791205\n2020-06-03|Python Best Practices|Matt Belhorn (OLCF)|2020 OLCF User Meeting (Summit New User Training) https://www.olcf.ornl.gov/calendar/2020-olcf-user-meeting/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/06/20200603_summit_workshop_python.pdf https://vimeo.com/427794043\n2020-06-03|NVMe - Burst Buffers (Part2)|George Markomanolis (OLCF)|2020 OLCF User Meeting (Summit New User Training) https://www.olcf.ornl.gov/calendar/2020-olcf-user-meeting/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/Burst_Buffer_summit_june_2020.pdf https://vimeo.com/427792243\n2020-06-03|NVMe - Burst Buffers (Part1)|Chris Zimmer (OLCF)|2020 OLCF User Meeting (Summit New User Training) https://www.olcf.ornl.gov/calendar/2020-olcf-user-meeting/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/Burst_Buffer_Training_June2020.pdf https://vimeo.com/427790836\n2020-06-03|LSF Batch Scheduler & jsrun Job Launcher|Chris Fuson (OLCF)|2020 OLCF User Meeting (Summit New User Training) https://www.olcf.ornl.gov/calendar/2020-olcf-user-meeting/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/Summit-Job-Launch-Intro-June03-2020.pdf https://vimeo.com/427788434\n2020-06-03|Summit Programming Environment|Matt Belhorn (OLCF)|2020 OLCF User Meeting (Summit New User Training) https://www.olcf.ornl.gov/calendar/2020-olcf-user-meeting/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/06/20200603_summit_workshop_programming_environment.pdf https://vimeo.com/427796661\n2020-06-03|File Systems & Data Transfers|George Markomanolis (OLCF)|2020 OLCF User Meeting (Summit New User Training) https://www.olcf.ornl.gov/calendar/2020-olcf-user-meeting/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/file_systems_summit_june_2020.pdf https://vimeo.com/427795205\n2020-06-03|Summit System Overview|Tom Papatheodore (OLCF)|2020 OLCF User Meeting (Summit New User Training) https://www.olcf.ornl.gov/calendar/2020-olcf-user-meeting/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/Summit_System_Overview_2020-06-03.pdf https://vimeo.com/427796035\n2020-06-03|OLCF Best Practices|Bill Renaud (OLCF)|2020 OLCF User Meeting (Summit New User Training) https://www.olcf.ornl.gov/calendar/2020-olcf-user-meeting/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/OLCF_Overview_for_New_Users_2020_User_Meeting.pdf https://vimeo.com/427792537\n2020-05-28|OpenACC Data Management|Robbie Searles (NVIDIA)|OpenACC Data Management https://www.olcf.ornl.gov/calendar/openacc-data-management/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/OpenACC_Course_2020_Module_2.pdf https://vimeo.com/428638662\n2020-05-13|CUDA Atomics, Reductions, and Warp Shuffle|Bob Crovella (NVIDIA)|CUDA Atomics Reductions and Warp Shuffle https://www.olcf.ornl.gov/calendar/cuda-atomics-reductions-and-warp-shuffle/|(slides | recording 1 recording 2) https://www.olcf.ornl.gov/wp-content/uploads/2019/12/05_Atomics_Reductions_Warp_Shuffle.pdf https://vimeo.com/419029739 https://vimeo.com/428453188\n2020-04-17|Introduction to OpenACC|Robbie Searles (NVIDIA)|Introduction to OpenACC https://www.olcf.ornl.gov/calendar/introduction-to-openacc/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/04/OpenACC-Course-2020-Module-1.pdf https://vimeo.com/414875219\n2020-04-16|CUDA Optimization (Part 2)|Bob Crovella (NVIDIA)|Fundamental CUDA Optimization (Part 2) https://www.olcf.ornl.gov/calendar/fundamental-cuda-optimization-part2/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/04/04-CUDA-Fundamental-Optimization-Part-2.pdf https://vimeo.com/414827487\n2020-03-25|Job Step Viewer|Jack Morrison (OLCF)|March 2020 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-mar2020/|(slides) https://www.olcf.ornl.gov/wp-content/uploads/2020/01/OLCF_March_Con_Call_Job_Step_Viewerpdf.pdf\n2020-03-18|CUDA Optimizations (Part 1)|Bob Crovella (NVIDIA)|Fundamental CUDA Optimization (Part 1) https://www.olcf.ornl.gov/calendar/fundamental-cuda-optimization-part1/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/12/03-CUDA-Fundamental-Optimization-Part-1.pdf https://vimeo.com/398824746\n2020-03-10|Nsight Compute|Felix Schmitt (NVIDIA)|NVIDIA Profiling Tools - Nsight Compute https://www.olcf.ornl.gov/calendar/nvidia-profiling-tools-nsight-compute/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/OLCF-Webinar-Nsight-Compute.pdf https://vimeo.com/398929189\n2020-03-09|Nsight Systems|Holly Wilper (NVIDIA)|NVIDIA Profiling Tools - Nsight Systems https://www.olcf.ornl.gov/calendar/nvidia-profiling-tools-nsight-systems/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/Summit-Nsight-Systems-Introduction.pdf https://vimeo.com/398838139\n2020-02-26|OLCF Overview for New Users|Bill Renaud (OLCF)|February 2020 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-feb2020/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/Best-Practices-202002.pdf https://vimeo.com/405885960\n2020-02-19|CUDA Shared Memory|Bob Crovella (NVIDIA)|CUDA Shared Memory https://www.olcf.ornl.gov/calendar/cuda-shared-memory/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/12/02-CUDA-Shared-Memory.pdf https://vimeo.com/393552516\n2020-02-18|Explicit Resource Files (ERFs)|Tom Papatheodore (OLCF)|jsrun Tutorial https://www.olcf.ornl.gov/calendar/jsrun-tutorial/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/12/ERF.pdf https://vimeo.com/393782415\n2020-02-18|Multiple jsrun Commands|Chris Fuson (OLCF)|jsrun Tutorial https://www.olcf.ornl.gov/calendar/jsrun-tutorial/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/12/Jsrun-Multi.pdf https://vimeo.com/393782415\n2020-02-18|jsrun Basics|Jack Morrison (OLCF)|jsrun Tutorial https://www.olcf.ornl.gov/calendar/jsrun-tutorial/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/12/jsrun_basics.pdf https://vimeo.com/393782415\n2020-02-10|Scaling Up Deep Learning Applications on Summit|Junqi Yin (OLCF)|Scaling Up Deep Learning Applications on Summit https://www.olcf.ornl.gov/calendar/scaling-up-deep-learning-applications-on-summit/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/12/Scaling-DL-on-Summit.pdf https://vimeo.com/391520479\n2020-02-10|NCCL on Summit|Sylvain Jeaugey (NVIDIA)|Scaling Up Deep Learning Applications on Summit https://www.olcf.ornl.gov/calendar/scaling-up-deep-learning-applications-on-summit/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/12/Summit-NCCL.pdf https://vimeo.com/391520479\n2020-02-10|Introduction to Watson Machine Learning CE|Brad Nemanich & Bryant Nelson (IBM)|Scaling Up Deep Learning Applications on Summit https://www.olcf.ornl.gov/calendar/scaling-up-deep-learning-applications-on-summit/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/12/ORNL-Scaling-20200210.pdf https://vimeo.com/391520479\n2020-01-29|MyOLCF - A New Self-Service Portal for OLCF Users|Adam Carlyle (OLCF)|January 2020 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-jan2020/|(slides) https://www.olcf.ornl.gov/wp-content/uploads/2020/01/2020.01.29_OLCF_ConCall_myOLCF.pdf\n2020-01-15|Introduction to CUDA C++|Bob Crovella (NVIDIA)|Introduction to CUDA C++ https://www.olcf.ornl.gov/calendar/introduction-to-cuda-c/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/12/01-CUDA-C-Basics.pdf https://vimeo.com/386244979\n2019-10-30|Distributed Deep Learning on Summit|Brad Nemanich & Bryant Nelson (IBM)|October 2019 OLCF User Conference Call - Distributed Deep Learning on Summit https://www.olcf.ornl.gov/calendar/userconcall-oct2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/10/DDLonSummit.pdf https://vimeo.com/377551223\n2019-09-06|Intro to AMD GPU Programming with HIP|Damon McDougall, Chip Freitag, Joe Greathouse, Nicholas Malaya, Noah Wolfe, Noel Chalmers, Scott Moe, Rene van Oostrum, Nick Curtis (AMD)|Intro to AMD GPU Programming with HIP https://www.olcf.ornl.gov/calendar/intro-to-amd-gpu-programming-with-hip/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/09/AMD_GPU_HIP_training_20190906.pdf https://vimeo.com/359154970\n2019-08-28|Intro to Slurm|Chris Fuson (OLCF)|August 2019 OLCF User Conference Call - Intro to Slurm https://www.olcf.ornl.gov/calendar/userconcall-aug2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/08/OLCF-Slurm-Transition-08282019.pdf https://vimeo.com/360822772\n2019-08-09|Profiling Tools Training Workshop: Issues and Lessons Learned|George Markomanolis & Mike Brim (OLCF)|Profiling Tools Workshop https://www.olcf.ornl.gov/calendar/profiling-tools-workshop/|(slides) https://www.olcf.ornl.gov/wp-content/uploads/2019/08/profiling_tools_lessons.pdf\n2019-08-08|Optimizing Dynamical Cluster Approximation on the Summit Supercomputer|Ronnie Chatterjee (OLCF)|Profiling Tools Workshop https://www.olcf.ornl.gov/calendar/profiling-tools-workshop/|(slides) https://www.olcf.ornl.gov/wp-content/uploads/2019/08/optimizingDCA_profilingWorkshop.pdf\n2019-08-08|Advanced Score-P|Mike Brim (OLCF)|Profiling Tools Workshop https://www.olcf.ornl.gov/calendar/profiling-tools-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/08/ScorepAdvanced.pdf https://vimeo.com/428153152\n2019-08-08|Performance Analysis with Scalasca|George Makomanolis (OLCF)|Profiling Tools Workshop https://www.olcf.ornl.gov/calendar/profiling-tools-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/08/profiling_tools_scalasca_2.pdf https://vimeo.com/428148261\n2019-08-08|Performance Analysis with Tau|George Makomanolis (OLCF)|Profiling Tools Workshop https://www.olcf.ornl.gov/calendar/profiling-tools-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/08/profiling_tools_tau_day_2.pdf https://vimeo.com/428143973\n2019-08-07|Introduction to Extrae/Paraver|George Makomanolis (OLCF)|Profiling Tools Workshop https://www.olcf.ornl.gov/calendar/profiling-tools-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/08/extrae_day_1.pdf https://vimeo.com/428142542\n2019-08-07|NVIDIA Profilers|Jeff Larkin (NVIDIA)|Profiling Tools Workshop https://www.olcf.ornl.gov/calendar/profiling-tools-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/08/NVIDIA-Profilers.pdf https://vimeo.com/428132931\n2019-08-07|Intro to Scalasca|George Makomanolis (OLCF)|Profiling Tools Workshop https://www.olcf.ornl.gov/calendar/profiling-tools-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/08/5_scalasca_day_1.pdf https://vimeo.com/427553064\n2019-08-07|Intro to Score-P|George Makomanolis (OLCF)|Profiling Tools Workshop https://www.olcf.ornl.gov/calendar/profiling-tools-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/08/ScorepIntro.pdf https://vimeo.com/427534253\n2019-08-07|Intro to Tau|George Makomanolis (OLCF)|Profiling Tools Workshop https://www.olcf.ornl.gov/calendar/profiling-tools-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/08/3_tau_day_1.pdf https://vimeo.com/427531006\n2019-08-07|Introduction to Performance Analysis Concepts|George Makomanolis (OLCF)|Profiling Tools Workshop https://www.olcf.ornl.gov/calendar/profiling-tools-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/08/2_profiling_introduction.pdf https://vimeo.com/424901100\n2019-06-19|OLCF Best Practices|Bill Renaud (OLCF)|June 2019 OLCF User Conference Call - OLCF Best Practices https://www.olcf.ornl.gov/calendar/userconcall-jun2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/06/Best-Practices-20190619.pdf https://vimeo.com/343636411\n2019-06-11|Linux Command Line Productivity Tools|Ketan Maheshwari (OLCF)|Linux Command Line Productivity Tools https://www.olcf.ornl.gov/calendar/linux-command-line-productivity-tools/|(slides) https://www.olcf.ornl.gov/wp-content/uploads/2019/12/LPT_OLCF.pdf\n2019-06-07|Introduction to AMD GPU Programming with HIP|Damon McDougall, Chip Freitag, Joe Greathouse, Nicholas Malaya, Noah Wolfe, Noel Chalmers, Scott Moe, Rene van Oostrum, Nick Curtis (AMD)|Introduction to AMD GPU Programming with HIP https://www.olcf.ornl.gov/calendar/introduction-to-amd-gpu-programming-with-hip/|(slides | recording) https://exascaleproject.org/wp-content/uploads/2017/05/ORNL_HIP_webinar_20190606_final.pdf https://www.youtube.com/watch?v=3ZXbRJVvgJs&feature=youtu.be\n2019-05-20|Job Scheduler/Launcher|Chris Fuson (OLCF)|Introduction to Summit https://www.olcf.ornl.gov/calendar/introduction-to-summit-workshop/|(slides) https://www.olcf.ornl.gov/wp-content/uploads/2019/05/Summit-Job-Launch-Intro-May20-2019.pdf\n2019-05-20|Programming Environment|Matt Belhorn (OLCF)|Introduction to Summit https://www.olcf.ornl.gov/calendar/introduction-to-summit-workshop/|(slides) https://www.olcf.ornl.gov/wp-content/uploads/2019/05/20190520_summit_workshop_programming_environment.pdf\n2019-05-20|File Systems & Data Transfers|George Markomanolis (OLCF)|Introduction to Summit https://www.olcf.ornl.gov/calendar/introduction-to-summit-workshop/|(slides) https://www.olcf.ornl.gov/wp-content/uploads/2019/05/file_systems_summit_may_2019.pdf\n2019-05-20|Summit System Overview|Tom Papatheodore (OLCF)|Introduction to Summit https://www.olcf.ornl.gov/calendar/introduction-to-summit-workshop/|(slides) https://www.olcf.ornl.gov/wp-content/uploads/2019/05/Summit_System_Overview_20190520.pdf\n2019-04-11|Introduction to NVIDIA Profilers on Summit|Tom Papatheodore (OLCF) & Jeff Larkin (NVIDIA)|Introduction to NVIDIA Profilers on Summit https://www.olcf.ornl.gov/calendar/introduction-to-nvidia-profilers-on-summit/|(slides | recording 1 recording 2) https://www.olcf.ornl.gov/wp-content/uploads/2019/04/Intro_to_NVIDIA_profilers.pdf https://vimeo.com/393747416 https://vimeo.com/393776567\n2019-02-13|CAAR Porting Experience: RAPTOR|Ramanan Sankaran (OLCF)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_RAPTOR.pdf https://vimeo.com/346452450\n2019-02-13|CAAR Porting Experience: LS-DALTON|Ashleigh Barnes (OLCF)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_LSDALTON.pdf\n2019-02-13|CAAR Porting Experience: FLASH|Austin Harris (OLCF)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_FLASH_Harris.pdf https://vimeo.com/346452020\n2019-02-13|Network Features & MPI Tuning|Christopher Zimmer (OLCF)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_Network_Training.pdf https://vimeo.com/346452117\n2019-02-13|Burst Buffers / NVMe / SSDs|Christopher Zimmer (OLCF)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_Burst_Buffer.pdf https://vimeo.com/346452105\n2019-02-13|Burst Buffers / NVMe / SSDs|George Markomanolis (OLCF)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_Burst_Buffer_summit.pdf https://vimeo.com/346452253\n2019-02-13|GPFS / Spectrum Scale|George Markomanolis (OLCF)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_spectrum_scale.pdf\n2019-02-13|Arm Tools|Nick Forrington (ARM)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_Arm_Tools_reduced.pdf\n2019-02-12|Summit Node Performance|Wayne Joubert (OLCF)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_2019-02-SummitNodePerformance-WJ.pdf https://vimeo.com/346452621\n2019-02-12|Using V100 Tensor Cores|Jeff Larkin (NVIDIA)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Tensor-Cores.pdf https://vimeo.com/346452359\n2019-02-12|NVIDIA Profilers|Jeff Larkin (NVIDIA)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Libraries.pdf https://vimeo.com/346452291\n2019-02-12|GPU-Accelerated Libraries|Jeff Larkin (NVIDIA)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Libraries.pdf https://vimeo.com/346452291\n2019-02-12|CUDA-Aware MPI & GPUDirect|Steve Abbott (NVIDIA)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_CUDA-Aware-MP_febI.pdf https://vimeo.com/346452547\n2019-02-12|Programming Methods for Summit's Multi-GPU Nodes|Steve Abbott (NVIDIA)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_MultiGPU-nodes_feb.pdf\n2019-02-12|CUDA Unified Memory|Steve Abbott (NVIDIA)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STF_Feb_UVM_feb.pdf https://vimeo.com/346452488\n2019-02-11|Summit System Overview|Scott Atchley (OLCF)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_Summit-Overview_20190211.pdf https://vimeo.com/346452584\n2019-02-11|Storage Areas & Data Transfers|George Markomanolis (OLCF)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_storage_areas_summit_videos_feb_19_links.pdf https://vimeo.com/346452224\n2019-02-11|Programming Environment|Matt Belhorn (OLCF)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/SMT_Feb_programming_environment.pdf https://vimeo.com/346452383\n2019-02-11|Resource Scheduler & Job Launcher|Chris Fuson (OLCF)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_Summit-Job-Launch-Intro-Feb11-2019.pdf https://vimeo.com/346452041\n2019-02-11|Python on Summit|Matt Belhorn (OLCF)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_20190211_summit_workshop_python.pdf https://vimeo.com/346452419\n2019-02-11|Practical Tips for Running on Summit|David Appelhans (IBM)|Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_GettingStartedExamples_169ratio.pdf https://vimeo.com/346452176\n2018-12-06|ML/DL Frameworks on Summit|Junqi Yin (OLCF)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_training_mldl.pdf https://vimeo.com/307071617\n2018-12-06|Experiences Porting XGC to Summit|Ed Dazevedo (OLCF)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_XGC_Ed.pdf https://vimeo.com/307071032\n2018-12-06|E3SM Application Readiness Experiences on Summit|Matt Norman (OLCF)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(recording) http://vimeo.com/307071495\n2018-12-06|CAAR Porting Experience: QMCPACK|Andreas Tillack (OLCF)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Tillack.pdf https://vimeo.com/307071565\n2018-12-06|Python Environments|Matt Belhorn (OLCF)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_20181206_python.pdf https://vimeo.com/307070906\n2018-12-06|Mixing OpenMP & OpenACC|Lixiang Eric Luo (IBM)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_mixingOpenMPOpenACC.pdf https://vimeo.com/307071416\n2018-12-06|ARM MAP/Performance Reports|Nick Forrington (ARM)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(recording) https://vimeo.com/307071262\n2018-12-06|Debugging: ARM DDT|Nick Forrington (ARM)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(recording) https://vimeo.com/307071124\n2018-12-05|Summit Node Performance|Wayne Joubert (OLCF)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(recording) http://vimeo.com/306890606\n2018-12-05|Targeting GPUs Using GPU Directives on Summit with GenASiS: A Simple and Effective Fortran Experience|Reuben Budiardja (OLCF)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_budiardja.pdf https://vimeo.com/306890448\n2018-12-05|Experiences Using the Volta Tensor Cores|Wayne Joubert (OLCF)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(recording) http://vimeo.com/306890517\n2018-12-05|IBM Power9 SMT Deep Dive|Brian Thompto (IBM)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_thompto_smt.pdf https://vimeo.com/306890804\n2018-12-05|Network Features & MPI Tuning|Christopher Zimmer (OLCF)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_zimmer_network.pdf https://vimeo.com/306891057\n2018-12-05|NVMe / Burst Buffers|Christopher Zimmer (OLCF)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_BB_zimmer.pdf https://vimeo.com/306891012\n2018-12-05|NVMe / Burst Buffers|George Markomanolis (OLCF)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_BB_markomanolis.pdf https://vimeo.com/306890779\n2018-12-05|Spectrum Scale - GPFS|George Markomanolis (OLCF)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/spectrum_scale_summit_workshop.pdf https://vimeo.com/306890694\n2018-12-04|Directive-Based GPU Programming|Oscar Hernandez (OLCF)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(recording) https://vimeo.com/306440151\n2018-12-04|Using V100 Tensor Cores|Jeff Larkin (NVIDIA)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Tensor-Cores.pdf https://vimeo.com/306437682\n2018-12-04|NVIDIA Profilers|Jeff Larkin (NVIDIA)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Profilers.pdf https://vimeo.com/306437439\n2018-12-04|GPU-Accelerated Libraries|Jeff Larkin (NVIDIA)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Libraries.pdf https://vimeo.com/306437127\n2018-12-04|Targeting Summit's Multi-GPU Nodes|Steve Abbott (NVIDIA)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_MultiGPU-nodes.pdf https://vimeo.com/306436688\n2018-12-04|GPU Direct, RDMA, CUDA-Aware MPI|Steve Abbott (NVIDIA)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_CUDA-Aware-MPI.pdf https://vimeo.com/306436248\n2018-12-04|CUDA Unified Memory|Jeff Larkin (NVIDIA)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_UVM.pdf https://vimeo.com/306435487\n2018-12-03|Experiences Porting/Optimizing Codes for Acceptance Testing|Bob Walkup (IBM)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording 1 recording 2) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_walkup.pdf https://vimeo.com/306890861 https://vimeo.com/306890949\n2018-12-03|Practical Tips for Running on Summit|David Appelhans (IBM)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_appelhans.pdf https://vimeo.com/306434784\n2018-12-03|Summit Scheduler & Job Launcher|Chris Fuson (OLCF)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_fuson.pdf https://vimeo.com/306434362\n2018-12-03|Storage Areas & Data Transfers|George Markomanolis (OLCF)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/storage_areas_summit_links.pdf https://vimeo.com/306433952\n2018-12-03|Summit Programming Environment|Matt Belhorn (OLCF)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_20181203_programming_environment.pdf https://vimeo.com/306433318\n2018-12-03|IBM Power9|Brian Thompto (IBM)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_thompto.pdf https://vimeo.com/306003413\n2018-12-03|NVIDIA V100|Jeff Larkin (NVIDIA)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Volta-Architecture.pdf https://vimeo.com/306004462\n2018-12-03|Summit System Overview|Scott Atchley (OLCF)|Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_atchley.pdf https://vimeo.com/306002085\n2018-11-05|Programming Methods for Summit's Multi-GPU Nodes|Jeff Larkin & Steve Abbott (NVIDIA)|Programming Methods for Summit's Multi-GPU Nodes https://www.olcf.ornl.gov/calendar/programming-methods-for-summits-multi-gpu-nodes/|(slides | recording 1 recording 2) https://www.olcf.ornl.gov/wp-content/uploads/2018/11/multi-gpu-workshop.pdf https://vimeo.com/308290719 https://vimeo.com/308290811\n2018-06-28|Intro to OpenACC|Steve Abbott (NVIDIA)|Introduction to HPC https://www.olcf.ornl.gov/calendar/introduction-to-hpc/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/06/IntroToOpenACC_Titan.pdf https://vimeo.com/279329112\n2018-06-28|Intro to CUDA|Jeff Larkin (NVIDIA)|Introduction to HPC https://www.olcf.ornl.gov/calendar/introduction-to-hpc/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/06/intro_to_HPC_cuda.pdf https://vimeo.com/279313842\n2018-06-28|Intro to GPU Computing|Jeff Larkin (NVIDIA)|Introduction to HPC https://www.olcf.ornl.gov/calendar/introduction-to-hpc/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/06/intro_to_HPC_gpu_computing.pdf https://vimeo.com/279319729\n2018-06-27|Advanced UNIX & Shell Scripting|Bill Renaud (OLCF)|Introduction to HPC https://www.olcf.ornl.gov/calendar/introduction-to-hpc/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/07/Intro_to_Unix_2018.pdf https://vimeo.com/279313457\n2018-06-27|Intro to MPI|Brian Smith (OLCF)|Introduction to HPC https://www.olcf.ornl.gov/calendar/introduction-to-hpc/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/06/intro_to_HPC_intro_to_mpi.pdf https://vimeo.com/279313080\n2018-06-27|Intro to OpenMP|Dmitry Liakh & Markus Eisenbach (OLCF)|Introduction to HPC https://www.olcf.ornl.gov/calendar/introduction-to-hpc/|(slides | recording 1 recording 2) https://www.olcf.ornl.gov/wp-content/uploads/2018/06/intro_to_HPC_OpenMP.pdf https://vimeo.com/279300607 https://vimeo.com/279301009\n2018-06-27|Intro to Parallel Computing|John Levesque (Cray)|Introduction to HPC https://www.olcf.ornl.gov/calendar/introduction-to-hpc/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/06/Intro_to_HPC_parallel_computing.pdf https://vimeo.com/279288481\n2018-06-27|Intro to git|Jack Morrison & James Wynne (OLCF)|Introduction to HPC https://www.olcf.ornl.gov/calendar/introduction-to-hpc/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/06/Intro_to_HPC_Git.pdf https://vimeo.com/279287047\n2018-06-26|Intro to UNIX|Bill Renaud (OLCF)|Introduction to HPC https://www.olcf.ornl.gov/calendar/introduction-to-hpc/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/07/Intro_to_Unix_2018.pdf https://vimeo.com/279273125\n2018-06-26|Intro to vim|Jack Morrison (OLCF)|Introduction to HPC https://www.olcf.ornl.gov/calendar/introduction-to-hpc/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/06/Intro_to_HPC_Vim.pdf https://vimeo.com/279277260\n2018-06-26|Intro to C|Tom Papatheodore (OLCF)|Introduction to HPC https://www.olcf.ornl.gov/calendar/introduction-to-hpc/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/06/intro_to_c.pdf https://vimeo.com/279284053\n2018-06-26|Intro to Fortran|Bronson Messer (OLCF)|Introduction to HPC https://www.olcf.ornl.gov/calendar/introduction-to-hpc/|(slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/06/Intro_to_HPC_fotranbasicsBM.pdf https://vimeo.com/279286109\n2017-06-19|Intro to CUDA C/C++|Tom Papatheodore (OLCF)|Introduction to CUDA C/C++ https://www.olcf.ornl.gov/calendar/introduction-to-cuda-cc/|(slides) https://www.olcf.ornl.gov/wp-content/uploads/2018/03/Intro_to_CUDA.pdf"}
{"doc":"transferring","text":"Transferring Data\n\nThis page has been deprecated, and relevant information is now available on data-storage-and-transfers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers>. Please update any bookmarks to use that page.\n\nUsing common terminal tools\n\nIn general, when transferring data into or out of the OLCF from the command\nline, it's best to initiate the transfer from outside the OLCF. If moving many\nsmall files, it can be beneficial to compress them into a single archive file,\nthen transfer just the one archive file.\n\nscp and rsync are available for remote transfers.\n\nscp - secure copy (remote file copy program)\n\nSending a file to OLCF\n\nscp yourfile $USER@dtn.ccs.ornl.gov:/path/\n\nRetrieving a file from OLCF\n\nscp $USER@dtn.ccs.ornl.gov:/path/yourfile .\n\nSending a directory to OLCF\n\nscp -r yourdirectory $USER@dtn.ccs.ornl.gov:/path/\n\nrsync - a fast, versatile, remote (and local) file-copying tool\n\nSync a directory named mydir from your local system to the OLCF\n\nrsync -avz mydir/ $USER@dtn.ccs.ornl.gov:/path/\n\nwhere:\n\na is for archive mode\n\nv is for verbose mode\n\nz is for compressed mode\n\nSync a directory from the OLCF to a local directory\n\nrsync -avz  $USER@dtn.ccs.ornl.gov:/path/dir/ mydir/\n\nTransfer data and show progress while transferring\n\nrsync -avz --progress mydir/ $USER@dtn.ccs.ornl.gov:/path/\n\nInclude files or directories starting with T and exclude all others\n\nrsync -avz --progress --include 'T*' --exclude '*' mydir/ $USER@dtn.ccs.ornl.gov:/path/\n\nIf the file or directory exists at the target but not on the source, then delete it\n\nrsync -avz --delete $USER@dtn.ccs.ornl.gov:/path/ .\n\nTransfer only the files that are smaller than 1MB\n\nrsync -avz --max-size='1m' mydir/ $USER@dtn.ccs.ornl.gov:/path/\n\nIf you want to verify the behavior is as intended, execute a dry-run\n\nrsync -avz --dry-run mydir/ $USER@dtn.ccs.ornl.gov:/path/\n\nSee the manual pages for more information:\n\n$ man scp\n$ man rsync\n\nDifferences:\n\nscp cannot continue if it is interrupted. rsync can.\n\nrsync is optimized for performance.\n\nBy default, rsync checks if the transfer of the data was successful.\n\nStandard file transfer protocol (FTP) and remote copy (RCP) should not be used to transfer files to the NCCS high-performance computing (HPC) systems due to security concerns.\n\nUsing Globus from your local machine\n\nGlobus is most frequently used to facilitate data transfer between two\ninstitutional filesystems. However, it can also be used to facilitate data\ntransfer involving an individual workstation or laptop. The following\ninstructions demonstrate creating a local Globus endpoint, and initiating a\ntransfer from it to the OLCF's Alpine GPFS filesystem.\n\nVisit https://www.globus.org/globus-connect-personal and Install Globus\nConnect Personal, it is available for Windows, Mac, and Linux.\n\nMake note of the endpoint name given during setup. In this example, the\nendpoint is laptop_gmarkom.\n\nWhen the installation has finished, click on the Globus icon and select Web:\nTransfer Files as below\n\n\n\nGlobus will ask you to login. If your institution does not have an\norganizational login, you may choose to either Sign in with Google or Sign\nin with ORCiD iD.\n\n\n\nIn the main Globus web page, select the two-panel view, then set the source\nand destination endpoints. (Left/Right order does not matter)\n\n\n\nNext, navigate to the appropriate source and destination paths to select the\nfiles you want to transfer. Click the \"Start\" button to begin the transfer.\n\n\n\nAn activity report will appear, and you can click on it to see the status of\nthe transfer.\n\n\n\nVarious information about the transfer is shown in the activity report. You\nwill receive an email once the transfer is finished, including if it fails\nfor any reason.\n\n"}
{"doc":"user_centric","text":"User-Centric Data Storage\n\nThis page has been deprecated, and relevant information is now available on data-storage-and-transfers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers>. Please update any bookmarks to use that page.\n\nThe following table summarizes user-centric storage areas available on OLCF\nresources and lists relevant polices.\n\nUser-Centric Storage Areas\n\n\nThe table presented below provides a detailed breakdown of the user-centric content available on the system. The first column, \"Area\", lists the different areas of the system that are accessible to users. These include the User Home, Moderate Enhanced Projects, User Archive 1, and User Archive 2. The \"Path\" column specifies the specific path or directory where the content can be found. For example, the User Home is located at /ccs/home/[userid], while the Moderate Enhanced Projects can be found at /gpfs/arx/[projid]/home/[userid]. The \"Type\" column indicates the type of storage used for each area, such as NFS, Spectrum Scale, or HPSS. The \"Permissions\" column specifies the level of access that users have to the content, with some areas allowing users to set their own permissions. The \"Quota\" column lists the maximum amount of storage space available for each area, with the User Home having a quota of 50 GB and the Moderate Enhanced Projects having a quota of 50 TB. The \"Backups\" column indicates whether or not the content in each area is backed up, with the User Home being backed up and the Moderate Enhanced Projects not being backed up. The \"Purged\" column specifies whether or not the content in each area is subject to being purged or deleted after a certain period of time. The \"Retention\" column lists the amount of time that content is retained before being purged, with the User Home and User Archive 1 having a retention period of 90 days. Finally, the \"On Compute Nodes\" column indicates whether or not the content in each area is accessible on the compute nodes of the system. The User Home and Moderate Enhanced Projects are both accessible, while the User Archive 1 and User Archive 2 are not. Overall, this table provides a comprehensive overview of the user-centric content available on the system, including the type of storage, permissions, quotas, backups, and retention policies for each area.\n\n| Area | Path | Type | Permissions | Quota | Backups | Purged | Retention | On Compute Nodes |\n|------|------|------|-------------|--------|---------|---------|-----------|-----------------|\n| User Home | /ccs/home/[userid] | NFS | User set | 50 GB | Yes | No | 90 days | Read-only |\n| Moderate Enhanced Projects | /gpfs/arx/[projid]/home/[userid] | Spectrum Scale | 700 | 50 TB | No | Yes | Subject to Purge | Read/Write |\n| User Archive 1 | /home/[userid] | HPSS | User set | 2TB | No | No | 90 days | No |\n| User Archive 2 | /home/[userid] | HPSS | 700 | N/A | N/A | N/A | N/A | No |\n\nfootnotes\n\n1\n\nThis entry is for legacy User Archive directories which contained user data on January 14, 2020. There is also a quota/limit of 2,000 files on this directory.\n\n2\n\nUser Archive directories that were created (or had no user data) after January 14, 2020. Settings other than permissions are not applicable because directories are root-owned and contain no user files.\n\nModerate Enhanced projects do not have HPSS storage\n\nUser Home Directories (NFS)\n\nThe environment variable $HOME will always point to your current home\ndirectory. It is recommended, where possible, that you use this variable to\nreference your home directory. In cases in which using $HOME is not\nfeasible, it is recommended that you use /ccs/home/$USER.\n\nUsers should note that since this is an NFS-mounted filesystem, its performance\nwill not be as high as other filesystems.\n\nUser Home Quotas\n\nQuotas are enforced on user home directories. To request an increased quota,\ncontact the OLCF User Assistance Center. To view your current quota and usage,\nuse the quota command:\n\n$ quota -Qs\nDisk quotas for user usrid (uid 12345):\n     Filesystem  blocks   quota   limit   grace   files   quota   limit   grace\nnccsfiler1a.ccs.ornl.gov:/vol/home\n                  4858M   5000M   5000M           29379   4295m   4295m\n\nModerate enhanced projects have home directores located in GPFS. There is no enforced quota, but it is recommended that users not exceed 50 TB. These home directories are subject to the 90 day purge\n\nUser Home Permissions\n\nThe default permissions for user home directories are 0750 (full access to\nthe user, read and execute for the group). Users have the ability to change\npermissions on their home directories, although it is recommended that\npermissions be set to as restrictive as possible (without interfering with your\nwork).\n\nModerate enhanced projects have home directory permissions set to 0700 and are automatically reset to that if changed by the user.\n\nUser Home Backups\n\nThere are no backups for moderate enhanced project home directories.\n\nIf you accidentally delete files from your home directory\n(/ccs/home/$USER), you may be able to retrieve them. Online backups are\nperformed at regular intervals. Hourly backups for the past 24 hours, daily\nbackups for the last 7 days, and once-weekly backups are available. It is\npossible that the deleted files are available in one of those backups. The\nbackup directories are named hourly.*, daily.*, and weekly.* where\n* is the date/time stamp of backup creation. For example,\nhourly.2020-01-01-0905 is an hourly backup made on January 1st, 2020 at\n9:05 AM.\n\nThe backups are accessed via the .snapshot subdirectory. Note that ls\nalone (or even ls -a) will not show the .snapshot subdirectory exists,\nthough ls .snapshot will show its contents. The .snapshot feature is\navailable in any subdirectory of your home directory and will show the online\nbackups available for that subdirectory.\n\nTo retrieve a backup, simply copy it into your desired destination with the\ncp command.\n\nUser Website Directory\n\nUsers interested in sharing files publicly via the World Wide Web can request a\nuser website directory be created for their account. User website directories\n(~/www) have a 5GB storage quota and allow access to files at\nhttp://users.nccs.gov/~user (where user is your userid). If you are\ninterested in having a user website directory created, please contact the User\nAssistance Center at help@olcf.ornl.gov.\n\nUser Archive Directories (HPSS)\n\nUse of User Archive areas for data storage is deprecated as of January 14, 2020.\nThe user archive area for any user account created after that date (or for any\nuser archive directory that is empty of user files after that date) will contain\nonly symlinks to the top-level directories for each of the user's projects on\nHPSS. Users with existing data in a User Archive directory are encouraged to\nmove that data to an appropriate project-based directory as soon as possible.\n\nThe information below is simply for reference for those users with existing\ndata in User Archive directories.\n\nThe High Performance Storage System (HPSS) at the OLCF provides longer-term\nstorage for the large amounts of data created on the OLCF compute systems. The\nmass storage facility consists of tape and disk storage components, servers, and\nthe HPSS software. After data is uploaded, it persists on disk for some period\nof time. The length of its life on disk is determined by how full the disk\ncaches become. When data is migrated to tape, it is done so in a first-in,\nfirst-out fashion.\n\nUser archive areas on HPSS are intended for storage of data not immediately\nneeded in either User Home directories (NFS) or User Work directories (GPFS).\nUser Archive directories should not be used to store project-related data.\nRather, Project Archive directories should be used for project data.\n\nUser archive directories are located at /home/$USER.\n\nUser Archive Access\n\nEach OLCF user receives an HPSS account automatically. Users can transfer data\nto HPSS from any OLCF system using the HSI or HTAR utilities. For more\ninformation on using HSI or HTAR, see the data-hpss <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-hpss> .\n\nUser Archive Accounting\n\nEach file and directory on HPSS is associated with an HPSS storage allocation.\nFor information on HPSS storage allocations, please visit the data-policy <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-policy>\nsection.\n\nFor information on usage and best practices for HPSS, please see the data-hpss <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-hpss>\ndocumentation."}
{"doc":"Vampir","text":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVampir\n\n<string>:17: (INFO/1) Duplicate implicit target name: \"vampir\".\n\nOverview\n\nVampir is a software performance visualizer focused on highly parallel applications. It presents a unified view on an application run\nincluding the use of programming paradigms like MPI, OpenMP, PThreads, CUDA, OpenCL and OpenACC.\nIt also incorporates file I/O, hardware performance counters and other performance data sources.\nVarious interactive displays offer detailed insight into the performance behavior of the analyzed application.\nVampir's scalable analysis server and visualization engine enable interactive navigation of large amounts of performance data.\nScore-P and TAU generate OTF2 trace files for Vampir to visualize.\n\nUsage\n\nVampir can be run a few different ways depending on a couple of factors. If you have a large trace file it\nwould benefit from utilizing VampirServer to process the trace file with the system's compute power\nwhile reverse connected to the Vampir GUI. If the trace file is small enough (< ~1 GB), it would do just fine viewing on a\nlogin node.\n\n\n\nvampirserver is the backend software component that can run across multiple compute nodes taking advantage\nof the machine's memory, this in turn provides an increase in performance for viewing large trace files i.e. >1GB.\n\nVampirServer does not take advantage of GPU components\n\nThe following sections will cover the 3 different methods for using Vampir on Summit.\nFor each method, you will need to enable X11 forwarding\nwhen logging in to Summit to allow for launching a GUI from Summit.\nTo do so, you can use the ssh option -X as shown below\n\n$ ssh -X <USERID>@summit.olcf.ornl.gov\n\nPlease visit this link if you need more information for logging onto Summit\n\nVampir on a Login Node\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo not run Vampir on a login node for trace files > 1 GB! Please see the next 2 sections for running larger trace files.\n\n<string>:17: (INFO/1) Duplicate explicit target name: \"summit\".\n\n<string>:17: (INFO/1) Duplicate explicit target name: \"x11 forwarding\".\n\nAfter logging onto Summit (with X11 forwarding), execute the series of commands below:\n\n$ module load vampir\n\n$ vampir &\n\nOnce the GUI pops up (might take a few seconds), you can load a file resident on the file system by\nselecting Local File for file selection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVampir Using VampirServer\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease use this connection method for trace files larger than (> 1 GB), and see the next section Vampir Tunneling to VampirServer <vamptunnel> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Vampir Tunneling to VampirServer <vamptunnel>> for an even more optimal solution.\nAttempting to visualize large trace files (> 1 GB) will be very slow over X11 forwarding and can cause decreased performance on the shared\nlogin nodes for other users\n\n<string>:17: (INFO/1) Duplicate explicit target name: \"summit\".\n\n<string>:17: (INFO/1) Duplicate explicit target name: \"x11 forwarding\".\n\nAfter connecting to Summit using X11 forwarding\nyou will need to load the Vampir module and start the VampirServer. <vamps> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#VampirServer. <vamps>>\n\n$ module load vampir\n\n$ vampirserver start -- -P <projectID> -w <walltime> -q <queue>\n\n#Example: vampirserver start -- -P 123456 -w 60 -q debug\n\n\n\nSuccessful VampirServer startup message should appear in terminal window. You will need this information!\n\n[Test@login5.summit ~]$ vampirserver start  - - -P 123456 -w 60 -q debug\nLaunching VampirServer...\nSubmitting LSF batch job (this might take a while)...\nWarning: more than 1 task/rank assigned to a core\nVampirServer 9.11.1 OLCF (4626dba5)\nLicensed to ORNL\nRunning 4 analysis processes... (abort with vampirserver stop 10102)\nUser: Test\nPassword: XXXXXXXXXXXX\nVampirServer <10102> listens on: h50n05:30040\n\n\n\nLaunch the Vampir GUI\n\n$ vampir &\n\n\n\nOnce the GUI has opened, you will need to connect to the VampirServer <vamps> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#VampirServer <vamps>> using the\nRemote File option as shown below. If there is a 'recent files' window open, select 'open other'.\nEnter the node ID and the port number and press 'Connect'. Also, you will need to select Encrypted password from the Authentication dropdown option.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen the server authentication window pops up, you will need to enter your USERID\n& the VampirServer password <vampserpw> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#VampirServer password <vampserpw>> that was printed on the terminal screen.\nOnce authenticated, you will be able to navigate through the filesystem to your .otf2 files\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVampir Tunneling to VampirServer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis connection method is more complex than the other 2 methods, however it also can provide a more optimal experience for very large trace files.\n\nThis method will require you to have a local copy of the Vampir GUI already installed on your machine.\n\nIf you do not have a local copy, please reach out to the help desk at help@olcf.ornl.gov for instructions on getting a local copy.\n\n<string>:17: (INFO/1) Duplicate explicit target name: \"summit\".\n\nSimilar to the previous methods outlined above, you will start by connecting to Summit.\nOnce connected you will then need to start the VampirServer. <vamps> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#VampirServer. <vamps>>\n\n$ module load vampir\n\n#Start the VampirServer\n\n$ vampirserver start -- -P <projectID> -w <walltime> -q <queue>\n\nOnce you have successfully authenticated, you will need the information printed on the terminal window. <vampserpw> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#terminal window. <vampserpw>>\nThat includes:\n\nNode ID\n\nPort Number\n\npassword\n\nOnce the VampirServer is started, in a fresh terminal window on your Local machine you can then initiate the\nport forward command. This will open a secure tunnel from your local machine to the backend server running VampirServer.\n\nPort Forwarding\n\nssh -L <localport>:<Node ID>:<Remote port>  <USERID>@summit.olcf.ornl.gov\n\nThe local port number can be any unused port number on your local machine...try a number between 30000-30030.\n\nTo check if the port you picked is open run:\n\n  $ netstat -ab | grep \"<selected port number>\"\n\n#This can take a minute to return anything. If nothing is returned, your selected port is open\n\nAfter submitting the port forward command as seen above, it will ask for your login password to access Summit. Leave this terminal window open!\n\nLaunch the Vampir GUI on your local machine\n\nSimilar to how we have connected Vampir to the VampirServer in the previous section, <vampauth> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#previous section, <vampauth>> you will follow the same steps\nexcept you will use localhost for the server name and your local machine port number you selected.\nPress 'Connect' and this should open the authentication window where you will enter your UserID and the VampirServer password <vampserpw> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#VampirServer password <vampserpw>>\nprinted after a successful connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnce the authentication step is complete, it should open up the remote filesystem for you to navigate to and load\nyour .otf2 trace file.\n\nVampir GUI Demo\n\nPlease see the provided video below to get a brief demo of the Vampir GUI provided by TU-Dresden and presented by Ronny Brendel.\n\nYou can skip ahead to around the 22 minute mark!\n\n<div style=\"padding:56.25% 0 0 0;position:relative;\"><iframe src=\"https://player.vimeo.com/video/285908215?h=26f33f1775\" style=\"position:absolute;top:0;left:0;width:100%;height:100%;\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen></iframe></div><script src=\"https://player.vimeo.com/api/player.js\"></script>\n<p><a href=\"https://vimeo.com/285908215\">2018 Score-P / Vampir Workshop</a> from <a href=\"https://vimeo.com/olcf\">OLCF</a> on <a href=\"https://vimeo.com\">Vimeo</a>.</p>\n<p>This recording is from the 2018 Score-P / Vampir workshop that took place at ORNL on August 17, 2018. In the video, Ronny Brendel gives an introduction to the Score-P and Vampir tools, which are often used together to collect performance profiles/traces from an application and visualize the results.</p>"}
{"doc":"visit","text":"VisIt\n\nVisIt is now available on Frontier through the UMS022 module.\n\nOverview\n\nVisIt is an interactive, parallel analysis and visualization tool for\nscientific data. VisIt contains a rich set of visualization features so you can\nview your data in a variety of ways. It can be used to visualize scalar and\nvector fields defined on two- and three-dimensional (2D and 3D) structured and\nunstructured meshes. Further information regarding VisIt can be found at the\nlinks provided in the visit-resources <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#visit-resources> section.\n\n\n\nInstalling and Setting Up Visit\n\nVisIt uses a client-server architecture. You will obtain the best performance\nby running the VisIt client on your local computer and running the server on\nOLCF resources. VisIt for your local computer can be obtained here:\nVisIt Installation.\n\nRecommended VisIt versions on our systems:\n\nSummit: VisIt 3.1.4\n\nAndes: VisIt 3.2.2\n\nFrontier: VisIt 3.3.3\n\nUsing a different version than what is listed above is not guaranteed to work properly.\n\nFor sample data and additional examples, explore the\nVisIt Data Archive\nand various VisIt Tutorials.\nSupplementary test data can be found in your local installation in the data\ndirectory:\n\nLinux: /path/to/visit/data\n\nmacOS: /path/to/VisIt.app/Contents/Resources/data\n\nWindows: C:\\path\\to\\LLNL\\VisIt x.y.z\\data\n\nAdditionally, check out our beginner friendly\nOLCF VisIt Tutorial\nwhich uses Andes to visualize example datasets.\n\n\n\nCreating ORNL/OLCF Host Profiles\n\nThe first time you launch VisIt (after installing), you will be prompted for a\nremote host preference. Unfortunately, ORNL does not maintain that list\noften, so the ORNL entry may be outdated. Click the “None” option instead.\nRestart VisIt, and go to Options→Host Profiles. Select “New Host”\n\nAndes\n\n**For Andes:**\n\n- **Host nickname**: ``Andes`` (this is arbitrary)\n- **Remote hostname**: ``andes.olcf.ornl.gov`` (required)\n- **Host name aliases**: ``andes-login#g`` (required)\n- **Maximum Nodes**: Unchecked\n- **Maximum processors**: Unchecked (arbitrary)\n- **Path to VisIt Installation**: ``/sw/andes/visit`` (required)\n- **Username**: Your OLCF Username (required)\n- **Tunnel data connections through SSH**: Checked (required)\n\nUnder the “Launch Profiles” tab create a launch profile. Most of these values\nare arbitrary\n\n- **Profile Name**: ``batch`` (arbitrary)\n- **Timeout**: 480 (arbitrary)\n- **Number of threads per task**: 0 (arbitrary, but not tested\n  with OMP/pthread support)\n- **Additional arguments**: blank (arbitrary)\n\nUnder the “Parallel” Tab:\n\n- **Launch parallel engine**: Checked (required)\n- Launch Tab:\n\n    - **Parallel launch method**:\n      ``sbatch/srun`` (required)\n    - **Partition/Pool/Queue**: ``batch`` (required)\n    - **Number of processors**: 1 (arbitrary, but\n      high number may lead to OOM errors) (max for ``batch`` queue is 32)\n    - **Number of nodes**: 1 (arbitrary)\n    - **Bank/Account**: Your OLCF project to use (required)\n    - **Time Limit**: 1:00:00 (arbitrary, ``HH:MM:SS``)\n    - **Machine file**: Unchecked (required – Lets VisIt get\n      the nodelist from the scheduler)\n    - **Constraints**: Unchecked\n- Advanced tab – All boxes unchecked\n- GPU Acceleration\n\n    - **Use cluster’s graphics cards**: Unchecked (even if using the ``gpu`` partition)\n\nClick “Apply” and make sure to save the settings (Options/Save Settings).\nExit and re-launch VisIt.\n\n.. note::\n    Users with large datasets may see a slight performance boost by\n    using the high-memory ``gpu`` partition or by increasing\n    the number of processors if memory is not an issue. See the\n    `visit-modify-host <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#visit-modify-host>` section below for how to add a ``gpu`` partition\n    launch profile on Andes.\n\nFrontier\n\n**For Frontier:**\n\n- **Host nickname**: ``Frontier`` (this is arbitrary)\n- **Remote hostname**: ``frontier.olcf.ornl.gov`` (required)\n- **Host name aliases**: ``login#`` (required)\n- **Maximum Nodes**: Unchecked\n- **Maximum processors**: Unchecked (arbitrary)\n- **Path to VisIt Installation**: ``/sw/frontier/ums/ums022/linux-sles15-zen3/gcc-11.2.0/visit-3.3.3-zfoh2caq5tbshlvtujditymjizstvewe/`` (required)\n- **Username**: Your OLCF Username (required)\n- **Tunnel data connections through SSH**: Checked (required)\n\nUnder the “Launch Profiles” tab create a launch profile. Most of these values\nare arbitrary\n\n- **Profile Name**: ``batch`` (arbitrary)\n- **Timeout**: 480 (arbitrary)\n- **Number of threads per task**: 0 (arbitrary, but not tested\n  with OMP/pthread support)\n- **Additional arguments**: blank (arbitrary)\n\nUnder the “Parallel” Tab:\n\n- **Launch parallel engine**: Checked (required)\n- Launch Tab:\n\n    - **Parallel launch method**:\n      ``sbatch/srun`` (required)\n    - **Partition/Pool/Queue**: ``batch`` (required)\n    - **Number of processors**: 1 (arbitrary, but\n      high number may lead to OOM errors) (max is 56)\n    - **Number of nodes**: 1 (arbitrary)\n    - **Bank/Account**: Your OLCF project to use (required)\n    - **Time Limit**: 01:00:00 (arbitrary, ``HH:MM:SS``)\n    - **Machine file**: Unchecked (required – Lets VisIt get\n      the nodelist from the scheduler)\n    - **Constraints**: Unchecked\n- Advanced tab – All boxes unchecked\n- GPU Acceleration\n\n    - **Use cluster’s graphics cards**: Unchecked\n\nClick “Apply” and make sure to save the settings (Options/Save Settings).\nExit and re-launch VisIt.\n\n.. note::\n    If you want to use the ``debug`` QOS on Frontier, you can add ``-q debug``\n    to the \"Launcher arguments\" section under the \"Advanced\" tab (make sure\n    to also check the \"Launcher arguments\" box).\n\nSummit\n\n**For Summit:**\n\n- **Host nickname**: ``Summit`` (this is arbitrary)\n- **Remote hostname**: ``summit.olcf.ornl.gov`` (required)\n- **Host name aliases**: ``login#`` (required)\n- **Maximum Nodes**: Unchecked\n- **Maximum processors**: Unchecked (arbitrary)\n- **Path to VisIt Installation**: ``/sw/summit/visit`` (required)\n- **Username**: Your OLCF Username (required)\n- **Tunnel data connections through SSH**: Checked (required)\n\nUnder the “Launch Profiles” tab create a launch profile. Most of these values\nare arbitrary\n\n- **Profile Name**: ``batch`` (arbitrary)\n- **Timeout**: 480 (arbitrary)\n- **Number of threads per task**: 0 (arbitrary, but not tested\n  with OMP/pthread support)\n- **Additional arguments**: blank (arbitrary)\n\nUnder the “Parallel” Tab:\n\n- **Launch parallel engine**: Checked (required)\n- Launch Tab:\n\n    - **Parallel launch method**:\n      ``bsub`` (required)\n    - **Partition/Pool/Queue**: ``batch`` (required)\n    - **Number of processors**: 1 (arbitrary, but\n      high number may lead to OOM errors) (max is 42)\n    - **Number of nodes**: 1 (arbitrary)\n    - **Bank/Account**: Your OLCF project to use (required)\n    - **Time Limit**: 01:00 (arbitrary, ``HH:MM``)\n    - **Machine file**: Unchecked (required – Lets VisIt get\n      the nodelist from the scheduler)\n    - **Constraints**: Unchecked\n- Advanced tab – All boxes unchecked\n- GPU Acceleration\n\n    - **Use cluster’s graphics cards**: Unchecked\n\nClick “Apply” and make sure to save the settings (Options/Save Settings).\nExit and re-launch VisIt.\n\n\n\nModifying Host Profiles\n\nSee visit-host-profiles <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#visit-host-profiles> section above for creating your initial host profile.\n\nTo make changes to an existing host profile, do the following:\n\nGo to \"Options→Host Profiles\".\n\nThe window will display the known hosts on the left, with the\nsettings for that host shown on the right in the \"Host Settings\" tab.\n\nYou can modify settings relevant to this host machine. For example,\nyou can change the \"Username\" field if your OLCF username differs\nfrom your local computer username.\n\nOnce you have made your changes, press the \"Apply\" button, and then\nsave the settings (Options/Save Settings).\n\nEach host can have several launch profiles. A launch profile specifies how VisIt\nruns on a given host computer. To make changes to a host's launch profile, do\nthe following:\n\nGo to \"Options→Host Profiles\".\n\nSelect the host in the left side of the window.\n\nSelect the \"Launch Profiles\" tab in the right side of the window.\nThis will display the known launch profiles for this host.\n\nSelect a \"Launch Profile\" and the settings are displayed in the tabs\nbelow.\n\nYou can set your Project ID in the \"Default Bank/Account\" field in\nthe \"Parallel\" tab.\n\nYou can change the queue used by modifying the \"Partition/Pool/Queue\"\nfield in the \"Parallel\" tab.\n\nOnce you have made your changes, press the \"Apply\" button, and then\nsave the settings (Options/Save Settings).\n\nFor example, this is how you would modify the Andes profile to use the gpu partition:\n\nUnder Andes' \"Launch Profiles\":\n\nClick on \"New Profile\"\n\nName the profile something like \"gpu\" (arbitrary)\n\nClick on \"Parallel\"\n\nCheck \"Launch Parallel Engine\"\n\nSet \"Launch Method\" to sbatch/srun (required)\n\nSet \"Partition/Pool/Queue\" to gpu (required)\n\nSet default number of processors to 28 (max without hyperthreading) (arbitrary)\n\nSet default number of nodes to 1 (arbitrary)\n\nSet default \"Bank/Account\" to your OLCF project with Andes allocation\n\nSet a default \"Time Limit\" in format of (HH:MM:SS)\n\nClick \"Apply\"\n\nAt the top menu click on \"Options\"→\"Save Settings\"\n\n\n\nRemote GUI Usage\n\nOnce you have VisIt installed and set up on your local computer:\n\nOpen VisIt on your local computer.\n\nGo to: \"File→Open file\" or click the \"Open\" button on the GUI.\n\nClick the \"Host\" dropdown menu on the \"File open\" window that popped\nup and choose \"ORNL_Andes\".\n\nThis will prompt you for your OLCF password, and connect you to Andes.\n\nNavigate to the appropriate file.\n\nOnce you choose a file, you will be prompted for the number of nodes\nand processors you would like to use (remember that each node of Andes\ncontains 32 processors, or 28 if using the high-memory GPU partition)\nand the Project ID, which VisIt calls a \"Bank\" as shown below.\n\n\n\nOnce specified, the server side of VisIt will be launched, and you\ncan interact with your data.\n\nThe above procedure can also be followed to connect to Summit or Frontier, with\nthe main difference being the number of available processors. The time limit\nsyntax for Andes, Summit, and Frontier also differ. Summit uses the format\nHH:MM while Andes and Frontier follow HH:MM:SS.\n\nPlease do not run VisIt's GUI client from an OLCF machine. You will get much\nbetter performance if you install a client on your workstation and launch\nlocally. You can directly connect to OLCF machines from inside VisIt and\naccess your data remotely.\n\n\n\nCommand Line Example\n\nUsing VisIt via the command line should always result in a batch job, and\nshould always be executed on a compute node -- never the login or launch nodes.\n\nAlthough most users find better performance following the approach outlined in\nvisit-remote-gui <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#visit-remote-gui>, some users that don't require a GUI may find better\nperformance using VisIt's CLI in a batch job. An example for doing this on\nOLCF systems is provided below.\n\nFor Summit (module):\n\n$ module load visit\n$ visit -nowin -cli -v 3.1.4 -l bsub/jsrun -p batch -b XXXYYY -t 00:05 -np 42 -nn 1 -s visit_example.py\n\nDue to the nature of the custom VisIt launcher for Summit, users are unable to\nsolely specify -l jsrun for VisIt to work properly. Instead of manually\ncreating a batch script, as seen in the Andes method outlined below, VisIt\nsubmits its own through -l bsub/jsrun. The -t flag sets the time limit,\n-b specifies the project to be charged, and -p designates the queue the\njob will submit to.\n\nThis method on Summit requires the user to be present until the job completes.\nFor users who have long scripts or are unable to monitor the job, you can\nsubmit the above lines in a batch script. However, you will wait in the queue\ntwice, so this is not recommended. Alternatively, one can use Andes.\n\nFor Andes/Frontier (Slurm Script):\n\nAndes\n\n.. code-block:: bash\n\n\n   #!/bin/bash\n   #SBATCH -A XXXYYY\n   #SBATCH -J visit_test\n   #SBATCH -N 1\n   #SBATCH -p gpu\n   #SBATCH -t 0:05:00\n\n   cd $SLURM_SUBMIT_DIR\n   date\n\n   module load visit\n\n   visit -nowin -cli -v 3.2.2 -l srun -np 28 -nn 1 -s visit_example.py\n\nFrontier\n\n.. code-block:: bash\n\n\n   #!/bin/bash\n   #SBATCH -A XXXYYY\n   #SBATCH -J visit_test\n   #SBATCH -N 1\n   #SBATCH -p batch\n   #SBATCH -t 0:05:00\n\n   cd $SLURM_SUBMIT_DIR\n   date\n\n   module load ums\n   module load ums022\n\n   visit -nowin -cli -v 3.3.3 -l srun -np 28 -nn 1 -s visit_example.py\n\nFollowing one of the methods above will submit a batch job for five minutes to\neither Summit, Andes, or Frontier.  Once the batch job makes its way through\nthe queue, the script will launch VisIt version X.Y.Z (specified with the\n-v flag, required on Andes) and execute a python script called\nvisit_example.py (specified with the -s flag, required if using a\nPython script). Note that the -nowin -cli options are also required, which\nlaunches the CLI and tells VisIt to not launch the GUI. Although a Python\nscript is used for this example, not calling the -s flag will launch the\nCLI in the form of a Python shell, which can be useful for interactive batch\njobs.  The -np and -nn flags represent the number of processors and\nnodes VisIt will use to execute the Python script, while the -l flag\nspecifies the specific parallel method to do so (required). Execute visit\n-fullhelp to get a list of all command line options.\n\nThe example script visit_example.py is detailed below and uses data\npackaged with a standard VisIt installation (tire.silo). Although the\ntire.silo dataset does not need a large number of MPI tasks to render\nquickly, users visualizing large datasets may find the syntax helpful outside\nof this example, however a performance boost is not guaranteed. All users are\nencouraged to test the effect of additional processors on their own data, as\nrendering speeds can widely vary depending on the amount of MPI tasks utilized.\nUsers are highly encouraged to use this script (especially after system\nupgrades) for testing purposes.\n\nThe following script renders a 3D pseudocolor plot of the temperature variable\nfrom the tire.silo dataset:\n\n# visit_example.py:\nimport sys\n\n# Open the file to visualize\nOpenDatabase(\"/sw/andes/visit/data/tire.silo\")\n\n# Set options for output\nswa = SaveWindowAttributes()\nswa.outputToCurrentDirectory = 1      # Save images in current directory\nswa.fileName = \"tire_pseudocolor\"     # Image filename\nswa.family = 0                        # Do not append numbers to filename\nswa.format = swa.PNG                  # Save as PNG\n#swa.width = 1100                     # Image width (does not apply to screen capture)\n#swa.height = 1000                    # Image height (does not apply to screen capture)\nswa.resConstraint = swa.NoConstraint  # Do not force aspect ratio, use width and height\nswa.screenCapture = 1                 # Enable screen capture\nResizeWindow(1, 1100, 1000)           # Setting Window 1's size (for screen capture)\nSetSaveWindowAttributes(swa)\n\n# Create a pseudocolor plot\nAddPlot(\"Pseudocolor\", \"temperature\") # Plot type, variable name\n\n# Pseudocolor attributes settings\nPseudocolorAtts = PseudocolorAttributes()\nPseudocolorAtts.centering = PseudocolorAtts.Nodal  # Natural, Nodal, Zonal -- Nodal for smoothing\nPseudocolorAtts.colorTableName = \"viridis_light\"   # Set colormap\nPseudocolorAtts.invertColorTable = 1               # Invert colors\nSetPlotOptions(PseudocolorAtts)\n\n# Annotation attributes settings\nAnnotationAtts = AnnotationAttributes()\nAnnotationAtts.userInfoFlag = 0 # Turn off display of user information\nSetAnnotationAttributes(AnnotationAtts)\n\n# Set viewpoint\nvatts = View3DAttributes()\nvatts.viewNormal = (0.7, 0.1, 0.7)\nvatts.focus = (0, 0, 0)\nvatts.viewUp = (0, 1, 0)\nvatts.viewAngle = 30\nvatts.parallelScale = 82.9451\nvatts.nearPlane = -165.89\nvatts.farPlane = 165.89\nvatts.imagePan = (0, 0)\nvatts.imageZoom = 1\nvatts.perspective = 1\nvatts.eyeAngle = 2\nSetView3D(vatts)\n\n# Draw plots and save resulting image\nDrawPlots()\nSaveWindow()\n\n# Quit\nsys.exit(0)\n\n\n\nIf everything is working properly, the above image should be generated after\nthe batch job is complete.\n\nFor users not interested in using screen capture, one would need to comment out\nline 16 (or change the value to 0), and syntax for resizing the window is\ndisplayed on lines 13 and 14 -- however saving the window in this manner on\nOLCF systems has resulted in errors in the past.\n\nAll of the above can also be achieved in an interactive batch job through the\nuse of the salloc command on Andes or the bsub -Is command on Summit.\nRecall that login nodes should not be used for memory- or compute-intensive\ntasks, including VisIt.\n\n\n\nTroubleshooting\n\nScalable Render Request Failed when using VisIt (fixed Feb. 2022)\n\nSome users have encountered their compute engine exiting abnormally on Andes\nafter VisIt reaches 100% when drawing a plot, resulting in a \"Scalable Render\nRequest Failed (VisItException)\" error message. This message has also been\nreported when users try to save plots, if VisIt was successfully able to draw.\nThe error seems to more commonly occur for users that are trying to visualize\nlarge datasets.\n\nVisIt developers have been notified, and at this time the current workaround is\nto disable Scalable Rendering from being used. To do this, go to\nOptions→Rendering→Advanced and set the \"Use Scalable Rendering\" option to\n\"Never\".\n\nHowever, this workaround has been reported to affect VisIt's ability to save\nimages, as scalable rendering is utilized to save plots as image files (which\ncan result in another compute engine crash). To avoid this, screen capture must\nbe enabled. Go to File→\"Set save options\" and check the box labeled \"Screen\ncapture\".\n\nUsing VisIt on Summit is also an option, as the scalable rendering problem is\ncurrently not an issue on Summit (as of Sept. 2021).\n\nAs of February 2022, this issue on Andes has been fixed (must use VisIt 3.2.2).\n\nSSH error after accepting passcode (duplicate host profile bug)\n\nIf you see an error similar to \"The metadata server on host andes.olcf.ornl.gov\ncould not be launched or it could not connect back to your local computer\" with\nthe specific error listed as \"The reason for the exception was not described\",\ndouble check your host profiles. This bug may occur when you have two or more\nhost profiles that represent the same system (e.g., if you have two host\nprofiles that connect to andes.olcf.ornl.gov, but may have different settings /\nusernames for both). This bug can affect both Summit and Andes.\n\nOne solution is to change the host nickname of the duplicate host profile to\nstart with \"Copy of\".  For example, if my original host profile was named \"ORNL\nAndes\", a proper duplicate should be named \"Copy of ORNL Andes\" (this is the\nsame nickname that would be generated when clicking the \"Copy Host\" button in\nVisIt). After renaming, make sure to save your settings via \"Options/Save\nSettings\" then close and restart VisIt.\n\nAnother solution is to delete all copies of a host profile (including the\noriginal) and remake them. This can be achieved with the \"Delete Host\" button\nin the Host Profiles window. Make sure to save your settings after deleting the\nprofiles, exit and restart VisIt, and then proceed with remaking your profiles.\n\nIf none of the above solutions work for you, the final option would be to\ndelete the duplicate host profile entirely and just modify the settings of the\noriginal when needed.\n\nVisIt launch continues indefinitely after entering passcode\n\nIf the pop-up box called \"metadata server launch progress\" never goes away\nafter entering your passcode, you may need to check if you have enough storage\nspace available in your home directory (/ccs/home/[user id]). When\nconnecting to OLCF systems, VisIt creates some small temporary files in your\nhome directory that are unable to be created if you are over your quota (50 GB\nis the default quota limit).\n\nIf the above does not apply to you, double check that you set up your host\nprofile exactly as how it is outlined in the visit-host-profiles <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#visit-host-profiles> section.\nIt may be helpful to delete and remake your host profile, but just remember\nto always save your settings via \"Options/Save Settings\".\n\nVisIt keeps asking for your password.\n\nIf VisIt keeps asking for your \"Password\" in the dialog box below, and you are\nentering your correct PIN + RSA token code, you might need to select \"Change\nusername\" and then enter your OLCF username when prompted.\n\n\n\nThis will give you a new opportunity to enter your PIN + token code and your\nusername will appear in login request box as shown below. If you want you OLCF\nusername to be filled in by default, go to \"Options→Host profiles\" and enter it\nunder \"Username\".\n\n\n\nVisIt will not connect when you try to draw an image.\n\nIf VisIt will not connect to Andes or Summit when you try to draw an image, you\nshould login to the system and check if a job is in the queue. To do this on\nAndes, enter squeue from the command line. To do this on Summit, enter\nbjobs from the command line. Your VisIt job should appear in the queue. If\nyou see it in a state marked \"PD\" or \"PEND\" you should wait a bit longer to see\nif it will start. If you do not see your job listed in the queue, check to make\nsure your project ID is entered in your VisIt host profile. See the\nvisit-modify-host <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#visit-modify-host> section for instructions.\n\nFatal Python error when launching the CLI\n\nIf VisIt immediately crashes after launching it via the command line (like in a\nbatch script or interactive batch job) and displays a Fatal Python error:\ninitfsencoding: Unable to get the locale encoding error message, you should\nspecify a specific VisIt version with the -v flag when launching VisIt.\nThis is necessary even if you plan to use the default version of VisIt on the\nsystem. See visit-command-line <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#visit-command-line> for proper syntax.\n\nVisIt never asks for passcode then hangs\n\nIf VisIt never asks for your passcode and hangs after trying to connect to one\nof our systems, then this means VisIt is unable to establish a proper\nSSH connection. Here are a few different approaches to fix this issue:\n\nDouble check your host profile, especially the \"remote host name\",\n\"host name aliases\", and \"tunnel data connections through SSH\" sections.\n\nIf you are using a VPN (including GlobalProtect VPN), try turning it off.\n\nIf you use multi-factor authentication (MFA4) with a smartcard or yubikey\nwhen connecting to our systems, this does not work with VisIt. VisIt only\naccepts RSA PIN+tokencodes, so you will have to change your SSH config\nsettings (typically within a .ssh/config file) and temporarily turn\noff MFA4.\n\n\n\nAdditional Resources\n\nThe OLCF VisIt Tutorial on Andes is a\nbeginner friendly tutorial for getting started on Andes with example datasets.\n\nThe VisIt User Manual\ncontains all information regarding the CLI and the GUI.\n\nPast VisIt Tutorials\nare available on the Visit User's Wiki along with a set of\nUpdated Tutorials\navailable in the VisIt User Manual.\n\nSample data not pre-packaged with VisIt can be found in the\nVisIt Data Archive.\n\nOlder VisIt Versions\nwith their release notes can be found on the old VisIt website, and\nNewer Versions\ncan be found on the new VisIt website with release notes found on the\nVisIt Blog\nor VisIt Github Releases page.\n\nNon-ORNL related bugs and issues in VisIt can be found and reported on\nGithub."}
