text1,text2,label
"what is the type of CPU on Frontier? 
","On Frontier, there are two major types of nodes you will encounter: Login and Compute. While these are similar in terms of hardware (see: https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes), they differ considerably in their intended use.",4.29184561621361
"what is the type of CPU on Frontier? 
",Programming Environment  Frontier utilizes the Cray Programming Environment.  Many aspects including LMOD are similar to Summit's environment.  But Cray's compiler wrappers and the Cray and AMD compilers are worth noting.  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more information.  AMD GPUs  Each frontier node has 4 AMD MI250X accelerators with two Graphic Compute Dies (GCDs) in each accelerator. The system identifies each GCD as an independent device (so for simplicity we use the term GPU when we talk about a GCD) for a total of 8,4.265235858130821
"what is the type of CPU on Frontier? 
","Frontier has a total of 9,408 AMD compute nodes, with each node consisting of [1x] 64-core AMD “Optimized 3rd Gen EPYC” CPU with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X accelerators, each with 2 Graphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can think of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth memory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU, allowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of 36+36 GB/s. The 2 GCDs on the same MI250X are connected with",4.265177505301664
"show an example job script for Frontier.
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.318887843557825
"show an example job script for Frontier.
","Unlike Summit and Titan, there are no launch/batch nodes on Frontier. This means your batch script runs on a node allocated to you rather than a shared node. You still must use the job launcher (srun) to run parallel jobs across all of your nodes, but serial tasks need not be launched with srun.



To easily visualize job examples (see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-mapping further below), the compute node diagram has been simplified to the picture shown below.

Simplified Frontier node architecture diagram",4.243172212398559
"show an example job script for Frontier.
","Frontier

.. code-block:: bash

    #!/bin/bash
    #SBATCH -A ABC123
    #SBATCH -J job_name
    #SBATCH -N 1
    #SBATCH -t 0:05:00
    #SBATCH -p batch

    unset SLURM_EXPORT_ENV

    cd $SLURM_SUBMIT_DIR
    date",4.229779536720953
"Please provide an example job script for Summit.
","As is the case on other OLCF systems, computational work on Summit is performed within jobs. A typical job consists of several components:

A submission script

An executable

Input files needed by the executable

Output files created by the executable

In general, the process for running a job is to:

Prepare executables and input files

Write the batch script

Submit the batch script

Monitor the job's progress before and during execution",4.333092887807401
"Please provide an example job script for Summit.
","Summit has a node hierarchy that can be very confusing for the uninitiated. The Summit User Guide explains this in depth. This has some consequences that may be unusual for R programmers. A few important ones to note are:

You must have a script that you can run in batch, e.g. with Rscript or R CMD BATCH

All data that needs to be visible to the R process (including the script and your packages) must be on gpfs (not your home directory!).

You must launch your script from the launch nodes with jsrun.

We'll start with something very simple. The script is:

""hello world""",4.2761330617804445
"Please provide an example job script for Summit.
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.270943662517826
"what is the job scheduling policy on Frontier? 
","agreed to by the following persons as a condition of access to or use of

OLCF computational resources:



-  Principal Investigators (Non-Profit)

-  Principal Investigators (Industry)

-  All Users



**Title:** Titan Scheduling Policy **Version:** 13.02



In a simple batch queue system, jobs run in a first-in, first-out (FIFO)

order. This often does not make effective use of the system. A large job

may be next in line to run. If the system is using a strict FIFO queue,

many processors sit idle while the large job waits to run. *Backfilling*",4.240700823071761
"what is the job scheduling policy on Frontier? 
","To override this default layout (not recommended), set -S 0 at job allocation.



Frontier uses SchedMD's Slurm Workload Manager for scheduling and managing jobs. Slurm maintains similar functionality to other schedulers such as IBM's LSF, but provides unique control of Frontier's resources through custom commands and options specific to Slurm. A few important commands can be found in the conversion table below, but please visit SchedMD's Rosetta Stone of Workload Managers for a more complete conversion reference.",4.226095209900119
"what is the job scheduling policy on Frontier? 
","Users may have only 100 jobs queued in the batch queue at any time (this includes jobs in all states). Additional jobs will be rejected at submit time.

The debug quality of service (QOS) class can be used to access Frontier's compute resources for short non-production debug tasks. The QOS provides a higher priority compare to jobs of the same job size bin in production queues. Production work and job chaining using the debug QOS is prohibited. Each user is limited to one job in any state at any one point. Attempts to submit multiple jobs to this QOS will be rejected upon job submission.",4.225448931944388
"What is the file system on Frontier? 
","For more detailed information about center-wide file systems and data archiving available on Frontier, please refer to the pages on https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers. The subsections below give a quick overview of NFS, Lustre,and HPSS storage spaces as well as the on node NVMe ""Burst Buffers"" (SSDs).",4.323623490652494
"What is the file system on Frontier? 
","Frontier is connected to Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Data will not be automatically transferred from Alpine to Orion. Frontier also has access to the center-wide NFS-based filesystem (which provides user and project home areas). Each compute node has two 1.92TB Non-Volatile Memory storage devices. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-data-storage for more",4.308937908112104
"What is the file system on Frontier? 
","Frontier connects to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#dtn-user-guide to move data to/from HPSS.

Frontier is running Cray OS 2.4 based on SUSE Linux Enterprise Server (SLES) version 15.4.",4.263050998347607
"Can you recommend any tutorials to learn deep learning on Frontier? 
","| (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/Joo-FrontierTipsAndTricks.pdf https://vimeo.com/803633277 | | 2023-02-17 | GPU Debugging | Mark Stock, HPC Applications Engineer, HPE | Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2-17-23_GPU_Debugging_distribute-2.pdf https://vimeo.com/840552596 | | 2023-02-17 | GPU Profiling | Alessandro Fanfarillo, Senior Member of Technical Staff, Exascale Application Performance, AMD | Frontier Training Workshop",4.077380595858767
"Can you recommend any tutorials to learn deep learning on Frontier? 
","Scientist, ORNL | Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2-15-23_Frontier_Programming_Environment.pdf https://vimeo.com/803620593 | | 2023-02-15 | Epyc CPU and Instinct GPU | Nick Malaya, Principal Member of Technical Staff, Exascale Application Performance, AMD | Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/ | (slides | recording)",4.075475321854087
"Can you recommend any tutorials to learn deep learning on Frontier? 
","| Blender on Frontier | Michael Sandoval (OLCF) | Blender on Frontier https://www.olcf.ornl.gov/calendar/userconcall-june2023/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/Blender_on_Frontier_published.pdf https://vimeo.com/840891737 | | 2023-06-15 | AI Training Series: AI for Science at Scale - Introduction | Sajal Dash, Junqi Yin, Wes Brewer (OLCF) | AI for Science at Scale - Introduction https://www.olcf.ornl.gov/calendar/ai-for-science-at-scale-intro/ | (slides | recording | tutorial site)",4.075158123482021
"If I used the OLCF resources for my paper, who should I acknowledge? 
","Publications using resources provided by the OLCF are requested to include the following acknowledgment statement: “This research used resources of the Oak Ridge Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC05-00OR22725.”",4.364077849625872
"If I used the OLCF resources for my paper, who should I acknowledge? 
","Users should acknowledge the OLCF in all publications and presentations that speak to work performed on OLCF resources:

This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725.

Many OLCF projects make use of optional subprojects. Subprojects provide a useful means for

dividing allocations among different applications, groups, or individuals

controlling priority

monitoring progress",4.331790095198622
"If I used the OLCF resources for my paper, who should I acknowledge? 
","Users should acknowledge the OLCF in all publications and presentations that speak to work performed on OLCF resources:

This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725.



To request software installation, please contact help@olcf.ornl.gov with a description of the software, including the following details:

Software name.

Description of the software and its purpose. Is it export controlled?",4.329708296074323
"what is the mixed precision capability of Frontier? 
","The Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.235199520733404
"what is the mixed precision capability of Frontier? 
","Each Frontier compute node contains 4 AMD MI250X. The AMD MI250X has a peak performance of 53 TFLOPS in double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 26.5 TFLOPS (double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in each direction simultaneously).

To connect to Frontier, ssh to frontier.olcf.ornl.gov. For example:

$ ssh <username>@frontier.olcf.ornl.gov",4.19854282052584
"what is the mixed precision capability of Frontier? 
",Programming Environment  Frontier utilizes the Cray Programming Environment.  Many aspects including LMOD are similar to Summit's environment.  But Cray's compiler wrappers and the Cray and AMD compilers are worth noting.  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more information.  AMD GPUs  Each frontier node has 4 AMD MI250X accelerators with two Graphic Compute Dies (GCDs) in each accelerator. The system identifies each GCD as an independent device (so for simplicity we use the term GPU when we talk about a GCD) for a total of 8,4.19384768223504
"How can I port my code in CUDA to HIP on Frontier? 
",work to learn HIP. See here for a series of tutorials on programming with HIP and also converting existing CUDA code to HIP with the hipify tools .,4.394925509378363
"How can I port my code in CUDA to HIP on Frontier? 
","As mentioned above, HIP can be used on systems running on either the ROCm or CUDA platform, so OLCF users can start preparing their applications for Frontier today on Summit. To use HIP on Summit, you must load the HIP module:

$ module load hip-cuda

CUDA 11.4.0 or later must be loaded in order to load the hip-cuda module. hipBLAS, hipFFT, hipSOLVER, hipSPARSE, and hipRAND have also been added to hip-cuda.",4.394772936374441
"How can I port my code in CUDA to HIP on Frontier? 
","The HIP API is very similar to CUDA, so if you are already familiar with using CUDA, the transition to using HIP should be fairly straightforward. Whether you are already familiar with CUDA or not, the best place to start learning about HIP is this Introduction to HIP webinar that was recently given by AMD:

Introduction to AMD GPU Programming with HIP: (slides | recording)

More useful resources, provided by AMD, can be found here:

HIP Programming Guide

HIP API Documentation

HIP Porting Guide

The OLCF is currently adding some simple HIP tutorials here as well:",4.305356902566503
"What is the difference between Summit node and Frontier node?
","On Frontier, there are two major types of nodes you will encounter: Login and Compute. While these are similar in terms of hardware (see: https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes), they differ considerably in their intended use.",4.398445335795858
"What is the difference between Summit node and Frontier node?
","On Summit, there are three major types of nodes you will encounter: Login, Launch, and Compute. While all of these are similar in terms of hardware (see: https://docs.olcf.ornl.gov/systems/summit_user_guide.html#summit-nodes), they differ considerably in their intended use.",4.34877088482969
"What is the difference between Summit node and Frontier node?
","The login nodes listed above mirrors the Summit and Frontier login nodes in hardware and software.  The login node also provides access to the same compute resources as are accessible from Summit and Frontier's non-SPI workflows.

The Citadel login nodes cannot access the external network and are only accessible from whitelisted IP addresses.",4.337596955891146
"Where can I find information on compiling for AMD GPUs on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.559565415504277
"Where can I find information on compiling for AMD GPUs on Frontier?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers. Please see the following https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more detailed inforation on how to compile using these modules.",4.344634964477707
"Where can I find information on compiling for AMD GPUs on Frontier?
",Programming Environment  Frontier utilizes the Cray Programming Environment.  Many aspects including LMOD are similar to Summit's environment.  But Cray's compiler wrappers and the Cray and AMD compilers are worth noting.  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more information.  AMD GPUs  Each frontier node has 4 AMD MI250X accelerators with two Graphic Compute Dies (GCDs) in each accelerator. The system identifies each GCD as an independent device (so for simplicity we use the term GPU when we talk about a GCD) for a total of 8,4.302117552603172
"What types of upstream applications can I install with Helm?
","It is also possible to write your own charts for helm, if you have an application that can be deployed to many namespaces or that could benefit from templating objects. How to write charts is outside the scope of this documentation, but the upstream docs are a great place to start.",4.395852869256595
"What types of upstream applications can I install with Helm?
","Helm is the OLCF preferred package manager for Kubernetes. There are a variety of upstream applications that can be installed with helm, such as databases, web frontends, and monitoring tools. Helm has ""packages"" called ""charts"", which can essentially be thought of as kubernetes object templates. You can pass in values which helm uses to fill in these templates, and create the objects (pods, deployments, services, etc)

Follow https://docs.olcf.ornl.gov/systems/helm_example.html#helm_prerequisite for installing Helm.",4.39331864658504
"What types of upstream applications can I install with Helm?
","The following items need to be established before deploying applications on Slate systems (Marble or Onyx OpenShift clusters).

Follow https://docs.olcf.ornl.gov/systems/helm_prerequisite.html#slate_getting_started if you do not already have a project/namespace established on Onyx or Marble.

It is recommended to use Helm version 3.

Helm enables application deployment via Helm Charts. The high level Helm Architecture docs are also a great reference for understanding Helm.

Like oc, Helm is a single binary executable.

This can be installed on macOS with Homebrew :

$ brew install helm",4.214034864686354
"What is the default username for VisIt on Andes?
","The first time you launch VisIt (after installing), you will be prompted for a remote host preference. Unfortunately, ORNL does not maintain that list often, so the ORNL entry may be outdated. Click the “None” option instead. Restart VisIt, and go to Options→Host Profiles. Select “New Host”

Andes

**For Andes:**",4.207511604921063
"What is the default username for VisIt on Andes?
","Click ""Apply""

At the top menu click on ""Options""→""Save Settings""



Once you have VisIt installed and set up on your local computer:

Open VisIt on your local computer.

Go to: ""File→Open file"" or click the ""Open"" button on the GUI.

Click the ""Host"" dropdown menu on the ""File open"" window that popped up and choose ""ORNL_Andes"".

This will prompt you for your OLCF password, and connect you to Andes.

Navigate to the appropriate file.",4.206146719242565
"What is the default username for VisIt on Andes?
","Navigate to the appropriate file.

Once you choose a file, you will be prompted for the number of nodes and processors you would like to use (remember that each node of Andes contains 32 processors, or 28 if using the high-memory GPU partition) and the Project ID, which VisIt calls a ""Bank"" as shown below.



Once specified, the server side of VisIt will be launched, and you can interact with your data.",4.203540510212301
"What is the name of the command used to switch to a Project in the Slate tutorial?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.115426535346497
"What is the name of the command used to switch to a Project in the Slate tutorial?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",4.084280770676072
"What is the name of the command used to switch to a Project in the Slate tutorial?
","switch projects dropdown menu

The left navigation menu also includes a number of expandable items, each with links to project-centric pages for the current project context.",4.0834042053001465
"How do I compile HIP + OpenMP CPU threading hybrid codes?
","This section describes how to compile HIP + OpenMP CPU threading hybrid codes. For all compiler toolchains, HIP kernels and kernel launch calls (ie hipLaunchKernelGGL) cannot be implemented in the same file that requires -fopenmp. HIP API calls (hipMalloc, hipMemcpy) are allowed in files that require -fopenmp. HIP source files should be compiled into object files using the instructions in the HIP section, with the -c flag added to generate an object file. OpenMP and other non-HIP source files should be compiled into object files using the instructions in the OpenMP section. Then these object",4.540230644068981
"How do I compile HIP + OpenMP CPU threading hybrid codes?
","Information about compiling code for different XNACK modes (which control page migration between GPU and CPU memory) can be found in the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#compiling-hip-kernels-for-xnack-modes section.

This section shows how to compile HIP + OpenMP CPU threading hybrid codes.

Make sure the craype-accel-amd-gfx90a module is loaded when compiling HIP with the Cray compiler wrappers.

| Vendor | Compiler | Compile/Link Flags, Header Files, and Libraries | | --- | --- | --- | | AMD/Cray | CC |  | | hipcc |  | | GNU | CC |  |",4.430766568916081
"How do I compile HIP + OpenMP CPU threading hybrid codes?
","| Vendor | Module | Language | Compiler | OpenMP flag (GPU) | | --- | --- | --- | --- | --- | | Cray | cce | C C++ |  | -fopenmp | | Fortran | ftn (wraps crayftn) |  | | AMD | rocm |  |  | -fopenmp |

If invoking amdclang, amdclang++, or amdflang directly, or using hipcc you will need to add: -fopenmp -target x86_64-pc-linux-gnu -fopenmp-targets=amdgcn-amd-amdhsa -Xopenmp-target=amdgcn-amd-amdhsa -march=gfx90a.

This section shows how to compile HIP codes using the Cray compiler wrappers and hipcc compiler driver.",4.373387984917101
"Is HTTPS required for routes in Slate?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.362318979202772
"Is HTTPS required for routes in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.272915275104804
"Is HTTPS required for routes in Slate?
","By default, a route will only expose your https://docs.olcf.ornl.gov/systems/route.html#slate_services to NCCS networks. If you need your service exposed to the world outside ORNL, you will first need to get your project approved for external routes. To do this, submit a systems ticket. In the description, give us your project name and a brief reasoning for why exposing externally is needed.

We will let you know once your project is able to set up external routes.",4.198993486052277
"What is the name of the route that can be used to access the ArgoCD server?
","This enables access to the ArgoCD instance once deployed via the web browser more easily. In the above images, notice that the instance name is argocd. By default, the route name to the web UI will be <<instanceName>>-server-<<projectName>>.apps.<<clusterName>>.ccs.ornl.gov. If a different host name is desired to access the instance, enter the desired name in the Host parameter while maintaining the pattern new-name.apps.<cluster>.ccs.ornl.gov. For example,

Image of the form view with a custom host name set.",4.426562442411822
"What is the name of the route that can be used to access the ArgoCD server?
","Next Click the ""Create ArgoCD"" button. You will be presented with a form view similar to:

Image of the form view for ArgoCD instance creation.

Starting with the form view of the process, make the following changes allowing for access to the ArgoCD web UI via a route:

Server -> Insecure -> true

Server -> Route -> Enabled -> true

Server -> Route -> Tls -> Termination -> edge

Image of the form view for ArgoCD instance creation with the route settings configured.",4.400520632261223
"What is the name of the route that can be used to access the ArgoCD server?
","NAME                                     HOST/PORT                                PATH   SERVICES        PORT   TERMINATION   WILDCARD
route.route.openshift.io/argocd-server   argocd-stf042.apps.marble.ccs.ornl.gov          argocd-server   http   edge          None

When one navigates to the route in a web browser, the ArgoCD login screen will be presented:

Image  of the ArgoCD login screen.

For ArgoCD authentication, the default user is admin with the password stored in the <<instanceName>>-cluster secret in the project. Following login, the instance is ready for configuration:",4.379173849258928
"What happens if my software is classified under the wrong ECCN?
","Export Control Classification Number (ECCN)
  US Department of Commerce Export Control Classification Number. If an
  application software package is export controlled, list the applicable ECCN.
  This is requested for all software that is `https://docs.olcf.ornl.gov/systems/glossary.html#Not Open
  Source<glossary-closed-source>`. If the code is subject to a different
  export control jurisdiction (e.g. Department of State ITAR, Nuclear
  Regulatory Commission Controls) please indicate an appropriate
  categorization.",4.318453988652369
"What happens if my software is classified under the wrong ECCN?
","Closed Source Software / Not Open Source Software
  Software with source code which is not publicly available for general
  distribution. For this type of software, the `https://docs.olcf.ornl.gov/systems/glossary.html#Export Control
  Classification Number<glossary-eccn-number>` (ECCN) is requested. If the
  code is subject to a different export control jurisdiction (e.g. Department
  of State, ITAR) please indicate an appropriate categorization.",4.205302541083107
"What happens if my software is classified under the wrong ECCN?
","these prohibited data types or information that falls under Export Control. For questions, contact help@nccs.gov.",4.054355040412853
"What are some examples of data science tasks that can benefit from using CuPy?
","As noted in AMD+CuPy limitations, data sizes explored here hang. So, this section currently does not apply to Frontier.

Now that you know how to use CuPy, time to see the actual benefits that CuPy provides for large datasets. More specifically, let's see how much faster CuPy can be than NumPy on Summit. You won't need to fix any errors; this is mainly a demonstration on what CuPy is capable of.

There are a few things to consider when running on GPUs, which also apply to using CuPy:

Higher precision means higher cost (time and space)

The structuring of your data is important",4.35083447156266
"What are some examples of data science tasks that can benefit from using CuPy?
","CuPy is a library that implements NumPy arrays on NVIDIA GPUs by utilizing CUDA Toolkit libraries like cuBLAS, cuRAND, cuSOLVER, cuSPARSE, cuFFT, cuDNN and NCCL. Although optimized NumPy is a significant step up from Python in terms of speed, performance is still limited by the CPU (especially at larger data sizes) -- this is where CuPy comes in. Because CuPy's interface is nearly a mirror of NumPy, it acts as a replacement to run existing NumPy/SciPy code on NVIDIA CUDA platforms, which helps speed up calculations further. CuPy supports most of the array operations that NumPy provides,",4.316007523235811
"What are some examples of data science tasks that can benefit from using CuPy?
","datasets, but represents what CuPy is capable of at this scale.",4.289540369981063
"How can I include the phase called phase 1 in the instrumentation of my application?
","export TAU_OPTIONS=“-optTauSelectFile=phase.tau”

Now when you instrument your application, the phase called phase 1 are the lines 300-327 of the file miniWeather_mpi.cpp. Every call will be instrumented. This could create signiificant overhead, thus you should be careful when you use it.

Create a file called phases.tau.

BEGIN_INSTRUMENT_SECTION
static phase name=""phase1"" file=""miniWeather_mpi.cpp"" line=300 to line=327
static phase name=""phase2"" file=""miniWeather_mpi.cpp"" line=333 to line=346
END_INSTRUMENT_SECTION

Declare the TAU_OPTIONS variable.",4.12461793813426
"How can I include the phase called phase 1 in the instrumentation of my application?
","To instrument your code, you need to compile the code using the Score-P instrumentation command (scorep), which is added as a prefix to your compile statement. In most cases the Score-P instrumentor is able to automatically detect the programming paradigm from the set of compile and link options given to the compiler. Some cases will, however, require some additional link options within the compile statement e.g. CUDA instrumentation.

Below are some basic examples of the different instrumentation scenarios:",4.079065009558722
"How can I include the phase called phase 1 in the instrumentation of my application?
","Declare the TAU_OPTIONS variable

export TAU_OPTIONS=“-optTauSelectFile=select.tau”

Now, the routine sort*(int *) is excluded from the instrumentation.

Create a file called phase.tau.

BEGIN_INSTRUMENT_SECTION
dynamic phase name=“phase1” file=“miniWeather_mpi.cpp” line=300 to line=327
END_INSTRUMENT_SECTION

Declare the TAU_OPTIONS variable.

export TAU_OPTIONS=“-optTauSelectFile=phase.tau”",3.9529893920353576
"How does RAPIDS optimize data processing performance on GPUs?
","RAPIDS is a suite of libraries to execute end-to-end data science and analytics pipelines on GPUs. RAPIDS utilizes NVIDIA CUDA primitives for low-level compute optimization through user-friendly Python interfaces. An overview of the RAPIDS libraries available at OLCF is given next.

cuDF is a Python GPU DataFrame library (built on the Apache Arrow columnar memory format) for loading, joining, aggregating, filtering, and otherwise manipulating data.",4.415510310170393
"How does RAPIDS optimize data processing performance on GPUs?
","Note the ""RAPIDS basic execution"" option is for illustrative purposes and not recommended to run RAPIDS on Summit since it underutilizes resources. If your RAPIDS code is single GPU, consider Jupyter or the concurrent job steps option.

<string>:3: (INFO/1) Duplicate explicit target name: ""simultaneous job steps"".

In cases when a set of time steps need to be processed by single-GPU RAPIDS codes and each time step fits comfortably in GPU memory, it is recommended to execute simultaneous job steps.

The following script provides a general pattern to run job steps simultaneously with RAPIDS:",4.294814421689927
"How does RAPIDS optimize data processing performance on GPUs?
","Running RAPIDS multi-gpu/multi-node workloads requires a dask-cuda cluster. Setting up a dask-cuda cluster on Summit requires two components:

dask-scheduler.

dask-cuda-workers.

Once the dask-cluster is running, the RAPIDS script should perform four main tasks. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

Reference of multi-gpu/multi-node operation with cuDF, cuML, cuGraph is available in the next links:

10 Minutes to cuDF and Dask-cuDF.

cuML's Multi-Node, Multi-GPU Algorithms.",4.235286842852507
"How can we build the new image?
","Building an image in Openshift is the act of transferring a set of input parameters into an object. That object is typically an image. Openshift contains all of the necessary components to build a Docker image or a piece of source code an image that will run in a container.

A Docker build is achieved by linking to a source repository that contains a docker image and all of the necessary Docker artifacts. A build is then triggered through the standard $ docker build command. More specific documentation on docker builds can be found here.",4.176771893488775
"How can we build the new image?
","Source to image is a tool to build docker formatted container images. This is accomplished by injecting source code into a container image and then building a new image. The new image will contain the source code ready to run, via the $ docker run command inside the newly built image.

Using the S2I model of building images, it is possible to have your source hosted on a git repository and then pulled and built by Openshift.

Click on the newly created project and then on the blue Add to Project button on the center of the page.",4.150837223272159
"How can we build the new image?
","OpenShift has an integrated container image build service that users interact with through BuildConfig objects. BuildConfig's are very powerful, builds can be triggered by git repo or image tag pushes and connected into a pipeline to do automated deployments of newly built images. While powerful, these mechanisms can be cumbersome when starting out so we will be using a BuildConfig in a slightly simpler setup.

We will create a BuildConfig that will take a Binary (Local) source which will stream the contents of our local filesystem to the builder.",4.143748416470444
"What is the difference between SCOREP_USER_REGION_TYPE_COMMON and SCOREP_USER_REGION_TYPE_FINE?
","In this case, ""my_region"" is the handle name of the region which has to be defined with SCOREP_USER_REGION_DEFINE. Additionally, ""foo"" is the string containing the region's unique name (this is the name that will show up in Vampir) and SCOREP_USER_REGION_TYPE_COMMON identifies the type of the region. Make note of the header files seen in the above example that are needed to include the Score-P macros. See the Score-P User Adapter page for more user configuration options.

Below are some examples of manually instrumented regions using phase and loop types:

#include <scorep/SCOREP_User.h>",4.259118413483829
"What is the difference between SCOREP_USER_REGION_TYPE_COMMON and SCOREP_USER_REGION_TYPE_FINE?
",".. code::

   #include <scorep/SCOREP_User.inc>

   subroutine foo
      SCOREP_USER_REGION_DEFINE(my_region)
      SCOREP_USER_REGION_BEGIN(my_region, ""foo"", SCOREP_USER_REGION_TYPE_COMMON)
      ! do something
      SCOREP_USER_REGION_END(my_region)
   end subroutine foo",4.135103050603142
"What is the difference between SCOREP_USER_REGION_TYPE_COMMON and SCOREP_USER_REGION_TYPE_FINE?
","The regions ""sum"" and ""my_calculations"" in the above examples would then be included in the profiling and tracing runs and can be analysed with Vampir. For more details, refer to the Advanced Score-P training in the https://docs.olcf.ornl.gov/systems/Scorep.html#training-archive.

Please see the provided video below to watch a brief demo of using Score-P provided by TU-Dresden and presented by Ronny Brendel.",4.033400183250119
"What are some examples of tasks performed in CI?
","Taking the GitLab CI/CD concepts documentation as a start point, Continuous Integration (CI) completes tasks necessary to test and build software resulting in a container image. Example tasks performed could be code linting, test coverage, unit testing, functional testing, code compiling or integration testing. Tasks would be triggered whenever code is pushed into a repository.",4.4130627319627
"What are some examples of tasks performed in CI?
","CD could be either Continuous Delivery or Continuous Deployment. Both take an application following CI and make it available for use. In Continuous Delivery, an application deployment is triggered manually whereas in Continuous Deployment the process occurs automatically without the involvement of a person.

A more in depth discussion may be found with Martin Fowler's Continuous Integration article.

On Slate, there are three primary CI/CD style tools in use:

GitLab Runners

Jenkins

OpenShift Pipelines

GitLab Runners",4.092879951526649
"What are some examples of tasks performed in CI?
","Most users will find batch jobs an easy way to use the system, as they allow you to ""hand off"" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.",4.014117471824146
"How do I set the output directory for the Swift/T workflow?
","The suggested directory structure is to have a outer directory say swift-work that has the swift source and shell scripts. Inside of swift-work create a new directory called data.

Additionally, we will need two terminals open. In the first terminal window, navigate to the swift-work directory and invoke the data generation script like so:

$ ./gendata.sh

In the second terminal, we will run the swift workflow as follows (make sure to change the project name per your allocation):",4.3303568422430105
"How do I set the output directory for the Swift/T workflow?
","$ module load imagemagick # for convert utility
$ export WALLTIME=00:10:00
$ export PROJECT=STF019
$ export TURBINE_OUTPUT=/gpfs/alpine/scratch/ketan2/stf019/swift-work/cross-facility/data
$ swift-t -O0 -m lsf workflow.swift

If all goes well, and when the job starts running, the output will be produced in the data directory output.txt file.",4.171401115576284
"How do I set the output directory for the Swift/T workflow?
","This example demonstrates a continuously running cross-facility workflow. The idea is that there is a science facility (eg. SNS at ORNL) that produces scientific data to be processed by the remote compute facility (eg. OLCF at ORNL). The data is continuously arriving in a designated directory at the compute facility from science facility. The workflow picks data from that directory and does the processing to the data to produce some output. The Swift source file workflow.swift looks as follows:

import files;
import io;",4.15271261303729
"How do I generate random strings for the SECRET_TOKEN values in the secret-token.yaml file?
","Replace <name-of-your-app> with the name value you put in your values.yaml file.

Replace <your-choice> with strings of your choice (the access-key length should be at least 3, and the secret-key must be at least 8 characters). These will be the SECRET_TOKEN values.

Once your secret-token.yaml file is set, you can apply it to your Marble project/namespace with this command (assumes you are logged into Marble's CLI):

$ oc apply -f secret-token.yaml

You should get output similar to this:

secret ""rprout-test-minio-access-key"" created
secret ""rprout-test-minio-secret-key"" created",4.198171508507088
"How do I generate random strings for the SECRET_TOKEN values in the secret-token.yaml file?
","name: <name-of-your-app>-secret-key
  stringData:
    SECRET_TOKEN: <your_choice>",4.154327678677739
"How do I generate random strings for the SECRET_TOKEN values in the secret-token.yaml file?
","These are the root credentials referenced here.

To establish these credentials in our Marble project, allowing our MinIO deployment to use them, we need to create a secret-token.yaml file and apply it to our project.

Create this example secret-token.yaml file locally:",4.114279045196193
"What is the main purpose of the Red Hat OpenShift GitOps operator?
","From the release notes:

Red Hat OpenShift GitOps is a declarative way to implement continuous deployment for cloud native applications. Red Hat OpenShift GitOps ensures consistency in applications when you deploy them to different clusters in different environments, such as: development, staging, and production.",4.521309536716159
"What is the main purpose of the Red Hat OpenShift GitOps operator?
","In addition to multiple operators, Red Hat OpenShift GitOps provides RBAC roles and bindings, default resource request limits, integration with Red Hat SSO, integration with OpenShift cluster logging as well as cluster metrics, ability to manage resources across multiple OpenShift clusters with a single OpenShift GitOps instance, automatic remediation if resource configuration changes from desired configuration, and promotion of configurations from dev to test/staging to production.

Resources

Installation

Configuration

Multiple Project Management

Application Deployment",4.466453319162415
"What is the main purpose of the Red Hat OpenShift GitOps operator?
","To foster collaboration, discussion, and knowledge sharing, the CNCF GitOps Working Group held GitOpsCon North America 2021 with sessions concerning GitOps in general practice as well as specific tools. Additionally, there are two awesome lists where one may find more information concerning GitOps and tools:

Awesome Argo

Awesome GitOps

The former is curated by one of the committers to the ArgoCD project while the latter is curated by Weaveworks. The remainder of this document is focused solely on the use of the Red Hat OpenShift GitOps operator.",4.284352377101384
"How does Crusher handle page faults in GPU kernels?
","On Pascal-generation GPUs and later, this automatic migration is enhanced with hardware support. A page migration engine enables GPU page faulting, which allows the desired pages to be migrated to the GPU ""on demand"" instead of the entire ""managed"" allocation. In addition, 49-bit virtual addressing allows programs using unified memory to access the full system memory size. The combination of GPU page faulting and larger virtual addressing allows programs to oversubscribe the system memory, so very large data sets can be processed. In addition, new CUDA API functions introduced in CUDA8 allow",4.298277778896245
"How does Crusher handle page faults in GPU kernels?
","The accessibility of memory from GPU kernels and whether pages may migrate depends three factors: how the memory was allocated; the XNACK operating mode of the GPU; whether the kernel was compiled to support page migration. The latter two factors are intrinsically linked, as the MI250X GPU operating mode restricts the types of kernels which may run.",4.240774064398295
"How does Crusher handle page faults in GPU kernels?
","The accessibility of memory from GPU kernels and whether pages may migrate depends three factors: how the memory was allocated; the XNACK operating mode of the GPU; whether the kernel was compiled to support page migration. The latter two factors are intrinsically linked, as the MI250X GPU operating mode restricts the types of kernels which may run.",4.240774064398295
"What is the advantage of using --gpu-bind=map_gpu over --gpu-bind=closest?
","--gpu-bind=closest binds each task to the GPU which is closest.

To further clarify, --gpus-per-task does not actually bind GPUs to MPI ranks. It allocates GPUs to the job step. The --gpu-bind=closest is what actually maps a specific GPU to each rank; namely, the ""closest"" one, which is the GPU on the same NUMA domain as the CPU core the MPI rank is running on (see the https://docs.olcf.ornl.gov/systems/spock_quick_start_guide.html#spock-compute-nodes section).

Without these additional flags, all MPI ranks would have access to all GPUs (which is the default behavior).",4.4188641780855455
"What is the advantage of using --gpu-bind=map_gpu over --gpu-bind=closest?
","The output shows the round-robin (cyclic) distribution of MPI ranks to GPUs. In fact, it is a round-robin distribution of MPI ranks to L3 cache regions (the default distribution). The GPU mapping is a consequence of where the MPI ranks are distributed; --gpu-bind=closest simply maps the GPU in an L3 cache region to the MPI ranks in the same L3 region.

Example 6: 32 MPI ranks - where 2 ranks share a GPU (round-robin, multi-node)

This example is an extension of Example 5 to run on 2 nodes.",4.281440306072561
"What is the advantage of using --gpu-bind=map_gpu over --gpu-bind=closest?
","This example will be very similar to Example 1, but instead of using --gpu-bind=closest to map each MPI rank to the closest GPU, --gpu-bind=map_gpu will be used to map each MPI rank to a specific GPU. The map_gpu option takes a comma-separated list of GPU IDs to specify how the MPI ranks are mapped to GPUs, where the form of the comma-separated list is <gpu_id_for_task_0>, <gpu_id_for_task_1>,....

$ export OMP_NUM_THREADS=2
$ srun -N1 -n8 -c2 --gpus-per-node=8 --gpu-bind=map_gpu:4,5,2,3,6,7,0,1 ./hello_jobstep | sort",4.275356871906642
"How do I load GDB on Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.239683795037818
"How do I load GDB on Summit?
","GDB, the GNU Project Debugger, is a command-line debugger useful for traditional debugging and investigating code crashes. GDB lets you debug programs written in Ada, C, C++, Objective-C, Pascal (and many other languages).

GDB is available on Summit under all compiler families:

module load gdb

To use GDB to debug your application run:

gdb ./path_to_executable

Additional information about GDB usage can befound on the GDB Documentation Page.",4.215917060533683
"How do I load GDB on Summit?
","GDB, the GNU Project Debugger, is a command-line debugger useful for traditional debugging and investigating code crashes. GDB lets you debug programs written in Ada, C, C++, Objective-C, Pascal (and many other languages).

GDB is availableon Summit under all compiler families:

module load gdb

To use GDB to debug your application run:

gdb ./path_to_executable

Additional information about GDB usage can befound on the GDB Documentation Page.",4.212830380857067
"What is the difference between Open and Moderate Enclaves in Slate?
","- Existing OLCF Project ID

- Project PI

- Enclave (i.e. Open or Moderate Enclave - Onyx or Marble respectively)

NOTE: Summit is in Moderate

- Description (i.e. How you will use Slate)

- Resource Request (i.e. CPU/Memory/Storage requirements - Default is

8CPU/16GB/50GB respectively)

The web UI for OpenShift is available from all of ORNL (you should be able to reach it from your laptop on ORNL WiFi as well as the VPN).",4.0966821609207935
"What is the difference between Open and Moderate Enclaves in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",3.99060309906464
"What is the difference between Open and Moderate Enclaves in Slate?
","It is assumed you have already gone through https://docs.olcf.ornl.gov/systems/minio.html#slate_getting_started and established the https://docs.olcf.ornl.gov/systems/minio.html#helm_prerequisite. Please do that before attempting to deploy anything on Slate's clusters.

This example uses Helm version 3 to deploy a MinIO standalone Helm chart on Slate's Marble Cluster. This is the cluster in OLCF's Moderate enclave, the same enclave as Summit.

To start, clone the slate helm examples repository , containing the MinIO standalone Helm chart, and navigate into the charts directory:",3.976041073590377
"What is the advantage of using hipHostMalloc for CPU-local memory allocation in Crusher?
","+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+
| hipHostMalloc                               | CPU DDR4                  | Local read/write                           | Zero copy read/write over Infinity Fabric          |
+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+",4.344104925917009
"What is the advantage of using hipHostMalloc for CPU-local memory allocation in Crusher?
","+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+
| hipHostMalloc                               | CPU DDR4                  | Local read/write                           | Zero copy read/write over Infinity Fabric          |
+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+",4.344104925917009
"What is the advantage of using hipHostMalloc for CPU-local memory allocation in Crusher?
","The AMD MI250X and operating system on Crusher supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Crusher. In the following section we'll discuss how the heterogenous memory space on a Crusher node is surfaced within your application.",4.200888125483436
"How does HPCToolkit measure the performance of my program?
","HPCToolkit is an integrated suite of tools for measurement and analysis of program performance on computers ranging from multicore desktop systems to the nation's largest supercomputers. HPCToolkit provides accurate measurements of a program's work, resource consumption, and inefficiency, correlates these metrics with the program's source code, works with multilingual, fully optimized binaries, has very low measurement overhead, and scales to large parallel systems. HPCToolkit's measurements provide support for analyzing a program execution cost, inefficiency, and scaling characteristics both",4.5336950399782365
"How does HPCToolkit measure the performance of my program?
","HPCToolkit is an integrated suite of tools for measurement and analysis of program performance on computers ranging from multicore desktop systems to the nation's largest supercomputers. HPCToolkit provides accurate measurements of a program's work, resource consumption, and inefficiency, correlates these metrics with the program's source code, works with multilingual, fully optimized binaries, has very low measurement overhead, and scales to large parallel systems. HPCToolkit's measurements provide support for analyzing a program execution cost, inefficiency, and scaling characteristics both",4.5336950399782365
"How does HPCToolkit measure the performance of my program?
","HPCToolkit is an integrated suite of tools for measurement and analysis of program performance on computers ranging from multicore desktop systems to the nation’s largest supercomputers. HPCToolkit provides accurate measurements of a program’s work, resource consumption, and inefficiency, correlates these metrics with the program’s source code, works with multilingual, fully optimized binaries, has very low measurement overhead, and scales to large parallel systems. HPCToolkit’s measurements provide support for analyzing a program execution cost, inefficiency, and scaling characteristics both",4.522245459768382
"What type of module files can Lmod interpret?
",Typical use of Lmod is very similar to that of interacting with modulefiles on other OLCF systems. The interface to Lmod is provided by the module command:,4.374996524553069
"What type of module files can Lmod interpret?
",Typical use of Lmod is very similar to that of interacting with modulefiles on other OLCF systems. The interface to Lmod is provided by the module command:,4.374996524553069
"What type of module files can Lmod interpret?
","General usage

Typical use of Lmod is very similar to that of interacting with modulefiles on other OLCF systems. The interface to Lmod is provided by the module command:",4.342153258348599
"What is the purpose of the ""ldd"" command in the sbcast file?
","Notice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable. Once libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.

As mentioned above, you can alternatively use --exclude=NONE on sbcast to send all libraries along with the binary. Using --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time. A job script for the previous example, modified for sending all libraries is shown below.",4.107275678652089
"What is the purpose of the ""ldd"" command in the sbcast file?
","Slurm contains a utility called sbcast. This program takes a file and broadcasts it to each node's node-local storage (ie, /tmp, NVMe). This is useful for sharing large input files, binaries and shared libraries, while reducing the overhead on shared file systems and overhead at startup. This is highly recommended at scale if you have multiple shared libraries on Lustre/NFS file systems.

Here is a simple example of a file sbcast from a user's scratch space on Lustre to each node's NVMe drive:",4.099926456158273
"What is the purpose of the ""ldd"" command in the sbcast file?
","Slurm contains a utility called sbcast. This program takes a file and broadcasts it to each node's node-local storage (ie, /tmp, NVMe). This is useful for sharing large input files, binaries and shared libraries, while reducing the overhead on shared file systems and overhead at startup. This is highly recommended at scale if you have multiple shared libraries on Lustre/NFS file systems.

Here is a simple example of a file sbcast from a user's scratch space on Lustre to each node's NVMe drive:",4.099926456158273
"Under what circumstances can OLCF employees discuss data files with unauthorized entities?
","OLCF resources are federal computer systems, and as such, users should have no explicit or implicit expectation of privacy. OLCF employees and authorized vendor personnel with “root” privileges have access to all data on OLCF systems. Such employees can also login to OLCF systems as other users. As a general rule, OLCF employees will not discuss your data with any unauthorized entities nor grant access to data files to any person other than the UNIX “owner” of the data file, except in the following situations:",4.502399311253492
"Under what circumstances can OLCF employees discuss data files with unauthorized entities?
","The OLCF uses a standard file system structure to assist users with data organization on OLCF systems. Complete details about all file systems available to OLCF users can be found in the Data Management Policy section.

Additional file systems and file protections may be employed for sensitive data. If you are a user on a project producing sensitive data, further instructions will be given by the OLCF. The following guidelines apply to sensitive data:

Only store sensitive data in designated locations. Do not store sensitive data in your User Home directory.",4.36595198250087
"Under what circumstances can OLCF employees discuss data files with unauthorized entities?
","Users are prohibited from taking unauthorized actions to intentionally modify or delete information or programs.

The OLCF reserves the right to remove any data at any time and/or transfer data to other users working on the same or similar project once a user account is deleted or a person no longer has a business association with the OLCF. After a sensitive project has ended or has been terminated, all data related to the project must be purged from all OLCF computing resources within 30 days.",4.355427278619649
"How do I create a new directory in my Project Home area?
","The -p flag specifies the desired path and name of your new virtual environment. The directory structure is case sensitive, so be sure to insert <YOUR_PROJECT_ID> as lowercase. Directories will be created if they do not exist already (provided you have write-access in that location). Instead, one can solely use the --name <your_env_name> flag which will automatically use your $HOME directory.",4.1197827760054455
"How do I create a new directory in my Project Home area?
","When all of the above steps are completed, your user account will be created and you will be notified by email. Now that you have a user account and it has been associated with a project, you're ready to get to work.",4.116089023703459
"How do I create a new directory in my Project Home area?
","Image of ArgoCD new application source and destination settings.

The last section entitled Directory most likely will be left at the defaults.

Image of ArgoCD new application directory settings.

Once everything is set, scroll to the top and click the Create button. An application tile should be created on the ArgoCD Applications page:

Image of ArgoCD new application tile.

Clicking on the tile in this case revealed that there was an error on deployment:

Image of ArgoCD namespace error message.",4.084471112835728
"What is the default value of the PBD_CPU_COUNT environment variable in pbdR?
","If you want to do GPU computing on Summit with R, we would love to collaborate with you (see contact details at the bottom of this document).

For more information about using R and pbdR effectively in an HPC environment such as Summit, please see the R and pbdR articles. These are long-form articles that introduce HPC concepts like MPI programming in much more detail than we do here.

Also, if you want to use R and/or pbdR on Summit, please feel free to contact us directly:

Mike Matheson - mathesonma AT ornl DOT gov

George Ostrouchov - ostrouchovg AT ornl DOT gov",3.999590794413642
"What is the default value of the PBD_CPU_COUNT environment variable in pbdR?
","| Partition | Node Count | Memory | GPU | CPU | | --- | --- | --- | --- | --- | | batch (default) | 704 | 256 GB | N/A | [2x] AMD EPYC 7302 16Core Processor 3.0 GHz, 16 cores (total 32 cores per node) | | gpu | 9 | 1 TB | [2x] NVIDIA K80 | [2x] Intel      Xeon      E5-2695 @2.3 GHz - 14 cores, 28 HT (total 28 cores, 56 HT per node) |

Batch Partition",3.995114630225834
"What is the default value of the PBD_CPU_COUNT environment variable in pbdR?
","| Partition | Node Count | Memory | GPU | CPU | | --- | --- | --- | --- | --- | | batch (default) | 704 | 256 GB | N/A | [2x] AMD EPYC 7302 16Core Processor 3.0 GHz, 16 cores (total 32 cores per node) | | gpu | 9 | 1 TB | [2x] NVIDIA K80 | [2x] Intel      Xeon      E5-2695 @2.3 GHz - 14 cores, 28 HT (total 28 cores, 56 HT per node) |

Batch Partition",3.995114630225834
"Is it possible to programmatically map any combination of GPUs to MPI ranks?
","As mentioned previously, all GPUs are accessible by all MPI ranks by default, so it is possible to programatically map any combination of MPI ranks to GPUs. However, there is currently no way to use Slurm to map multiple GPUs to a single MPI rank. If this functionality is needed for an application, please submit a ticket by emailing help@olcf.ornl.gov.

There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_jobstep program and test whether or not processes and threads are running where intended.",4.57384862516886
"Is it possible to programmatically map any combination of GPUs to MPI ranks?
","In general, GPU mapping can be accomplished in different ways. For example, an application might map MPI ranks to GPUs programmatically within the code using, say, hipSetDevice. In this case, since all GPUs on a node are available to all MPI ranks on that node by default, there might not be a need to map to GPUs using Slurm (just do it in the code). However, in another application, there might be a reason to make only a subset of GPUs available to the MPI ranks on a node. It is this latter case that the following examples refer to.",4.463492373365247
"Is it possible to programmatically map any combination of GPUs to MPI ranks?
","In general, GPU mapping can be accomplished in different ways. For example, an application might map MPI ranks to GPUs programmatically within the code using, say, hipSetDevice. In this case, since all GPUs on a node are available to all MPI ranks on that node by default, there might not be a need to map to GPUs using Slurm (just do it in the code). However, in another application, there might be a reason to make only a subset of GPUs available to the MPI ranks on a node. It is this latter case that the following examples refer to.",4.463492373365247
"How can I install a package in my Spack environment?
","Alternatively, a user may install a package and its dependencies manually by:

$ spack install <my_app_dependencies@version%compiler>

## This may or may not add the spec to the spack.yaml depending on the Spack version being used.

For more information regarding Spack and its usage, please see the Spack documentation.

<string>:3: (INFO/1) Duplicate explicit target name: ""the spack 101 tutorial"".

For an extensive tutorial concerning Spack, go to the Spack 101 tutorial.

For more information concerning external packages, please see here.

Spack - package management tool",4.4216658760093965
"How can I install a package in my Spack environment?
","Spack - package management tool

Spack 101 tutorial - Spack tutorial",4.397985293086976
"How can I install a package in my Spack environment?
","Traditionally, the user environment is modified by module files.  For example, a user would add use  module load cmake/3.18.2 to load CMake version 3.18.2 into their environment.  Using a Spack environment, a user can add an OLCF provided package and build against it using Spack without having to load the module file separately.

The information presented here is a subset of what can be found at the Spack documentation site.",4.326902289096638
"What is the difference between a Role and a RoleBinding in Kubernetes RBAC?
","apiVersion: rbac.authorization.k8s.io/v1
# This role binding allows user ""2jl"" to read pods in the ""default"" namespace.
# You need to already have a Role named ""pod-reader"" in that namespace.
kind: RoleBinding
metadata:
  # Name of the RoleBinding
  name: read-pods
  # Namespace for the RoleBinding
  namespace: default
subjects:
# You can specify more than one ""subject""
- kind: User
  name: 2jl
  apiGroup: rbac.authorization.k8s.io
roleRef:
  # kind is what your binding is to. In this case a Role
  kind: Role
  # The Role you are binding the user to
  name: pod-reader",4.179105978690059
"What is the difference between a Role and a RoleBinding in Kubernetes RBAC?
",Examples of basic Kubernetes objects meant to be used as a reference for those familiar with Kubernetes.,4.085099482393514
"What is the difference between a Role and a RoleBinding in Kubernetes RBAC?
","The yaml that defines the PVC that is being mounted by the above pod can be found in the Volumes section

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
# Role Name
  name: pod-reader
rules:
# """" indicates the core API group
- apiGroups: [""""]
# What object the verbs apply to
  resources: [""pods""]
# The API requests allowed on the above object
  verbs: [""get"", ""watch"", ""list""]

The verbs match to HTTP verbs against the API. A list of that matching can be found here.",4.0701778893674
"What is the difference in execution time between GPFS and NVMe for the given application?
","The following table shows the differences of executing an application on GPFS, NVMe, and NVMe with Spectral. This example is using one compute node. We copy the executable and input file for the NVMe cases but this is not always necessary. Depending on the application, you could execute the binary from the GPFS and save the output files on NVMe. Adjust your parameters to copy, if necessary, the executable and input files onto all the NVMe devices.",4.539293423452689
"What is the difference in execution time between GPFS and NVMe for the given application?
","application as it is not always necessary, we can execute the binary on the GPFS and write/read the data from NVMe if it is supported by the application.",4.373202800567012
"What is the difference in execution time between GPFS and NVMe for the given application?
","To run this example: bsub ./check_nvme.lsf.   We could include all the commands in a script and call this file as a jsrun argument in an interactive job, in order to avoid changing numbers of processes for all the jsrun calls. You can see in the table below an example of the differences in a submission script for executing an application on GPFS and NVMe. In the example, a binary ./btio reads input from an input file and generates output files. In this particular case we copy the binary and the input file onto the NVMe, but this depends on the application as it is not always necessary, we can",4.192089637936178
"What is the High Performance Storage System (HPSS) at OLCF and what is its purpose?
",The High Performance Storage System (HPSS) is the tape-archive storage system at the OLCF and is the storage technology that supports the User Archive areas. HPSS is intended for data that do not require day-to-day access.,4.688870493698108
"What is the High Performance Storage System (HPSS) at OLCF and what is its purpose?
","System Overview





The High Performance Storage System (HPSS) provides tape storage for large amounts of data created on OLCF systems. The HPSS can be accessed from any OLCF system through the hsi utility. More information about using HPSS can be found in the https://docs.olcf.ornl.gov/systems/hpss_user_guide.html#data-hpss section of the https://docs.olcf.ornl.gov/systems/hpss_user_guide.html#data-storage-and-transfers page.",4.655608327493081
"What is the High Performance Storage System (HPSS) at OLCF and what is its purpose?
","The High Performance Storage System (HPSS) at the OLCF provides longer-term storage for the large amounts of data created on the OLCF compute systems. The mass storage facility consists of tape and disk storage components, servers, and the HPSS software. After data is uploaded, it persists on disk for some period of time. The length of its life on disk is determined by how full the disk caches become. When data is migrated to tape, it is done so in a first-in, first-out fashion.",4.636791844470517
"What is the purpose of unloading Darshan on Summit?
",For Summit:,4.119539686603439
"What is the purpose of unloading Darshan on Summit?
","You will need to unload the darshan-runtime module. In some instances you may need to unload the xalt and xl modules.  $ module unload darshan-runtime

Serial

..  C

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load scorep
        $ module load gcc
        $ scorep gcc -c test.c
        $ scorep gcc -o test test.o

..  C++

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load scorep
        $ module load gcc
        $ scorep g++ -c test.cpp main.cpp
        $ scorep g++ -o test test.o main.o

..  Fortran",4.054229848683912
"What is the purpose of unloading Darshan on Summit?
","<p style=""font-size:20px""><b>Frontier: Darshan Runtime 3.4.0 (May 10, 2023)</b></p>

The Darshan Runtime modulefile darshan-runtime/3.4.0 on Frontier is now loaded by default. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>.

Unloading darshan-runtime modulefile is recommended for users profiling their applications with other profilers to prevent conflicts.",4.014263916027293
"What is the purpose of the --sort option in srun?
",The following srun options will be used in the examples below. See man srun for a complete list of options and more information.,4.334441369404502
"What is the purpose of the --sort option in srun?
",The following srun options will be used in the examples below. See man srun for a complete list of options and more information.,4.334441369404502
"What is the purpose of the --sort option in srun?
","Using srun

By default, commands will be executed on the job’s primary compute node, sometimes referred to as the job’s head node. The srun command is used to execute an MPI binary on one or more compute nodes in parallel.

srun accepts the following common options:

| -N | Minimum number of nodes | | --- | --- | | -n | Total number of MPI tasks | | --cpu-bind=no | Allow code to control thread affinity | | -c | Cores per MPI task | | --cpu-bind=cores | Bind to cores |

If you do not specify the number of MPI tasks to srun via -n, the system will default to using only one task per node.",4.280708681266054
"Can you explain the reason behind the hardware design choice that leads to slower FP32 LDS atomicAdd() operations in some cases?
","Hardware FP atomic operations performed in LDS memory are usually always faster than an equivalent CAS loop, in particular when contention on LDS memory locations is high. Because of a hardware design choice, FP32 LDS atomicAdd() operations can be slower than equivalent FP64 LDS atomicAdd(), in particular when contention on memory locations is low (e.g. random access pattern). The aforementioned behavior is only true for FP atomicAdd() operations. Hardware atomic operations for CAS/Min/Max on FP32 are usually faster than the FP64 counterparts. In cases when contention is very low, a FP32 CAS",4.5217430991027
"Can you explain the reason behind the hardware design choice that leads to slower FP32 LDS atomicAdd() operations in some cases?
","Hardware FP atomic operations performed in LDS memory are usually always faster than an equivalent CAS loop, in particular when contention on LDS memory locations is high. Because of a hardware design choice, FP32 LDS atomicAdd() operations can be slower than equivalent FP64 LDS atomicAdd(), in particular when contention on memory locations is low (e.g. random access pattern). The aforementioned behavior is only true for FP atomicAdd() operations. Hardware atomic operations for CAS/Min/Max on FP32 are usually faster than the FP64 counterparts. In cases when contention is very low, a FP32 CAS",4.5217430991027
"Can you explain the reason behind the hardware design choice that leads to slower FP32 LDS atomicAdd() operations in some cases?
","In cases when contention is very low, a FP32 CAS loop implementing an atomicAdd() operation could be faster than an hardware FP32 LDS atomicAdd(). Applications using single precision FP atomicAdd() are encouraged to experiment with the use of double precision to evaluate the trade-off between high atomicAdd() performance vs. potential lower occupancy due to higher LDS usage.",4.504864769864916
"How do I retrieve the registration token for a specific runner in GitLab?
","Prior to installation of the GitLab Runner, a registration token for the runner is needed from the GitLab server. This token will allow a GitLab runner to register to the server in the needed location for running CI/CD jobs. Runners themselves may be registered to either a group as a shared runner or a project as a repository specific runner.",4.53140016613465
"How do I retrieve the registration token for a specific runner in GitLab?
","of the CI/CD settings. In the ""Specific Runners"" area, the registration token should be available for retrieval.",4.487970667847775
"How do I retrieve the registration token for a specific runner in GitLab?
","If the runner is to be a group shared runner, navigate to the group in GitLab and then go to Settings -> CI/CD. Expand the Runners section of the CI/CD Settings panel. Ensure that the ""Enable shared runners for this group"" toggle is enabled. The registration token should also be available for retrieval from ""Group Runners"" area.",4.449214673136046
"How do I enable GPU-aware MPI on Frontier?
","module load craype-accel-amd-gfx90a

export MPICH_GPU_SUPPORT_ENABLED=1

If using PrgEnv-cray:

module load craype-accel-amd-gfx90a
module load amd-mixed

export MPICH_GPU_SUPPORT_ENABLED=1

There are extra steps needed to enable GPU-aware MPI on Frontier, which depend on the compiler that is used (see 1. and 2. below).

When using GPU-aware Cray MPICH with the Cray compiler wrappers, most of the needed libraries are automatically linked through the environment variables.

Though, the following header files and libraries must be included explicitly:",4.443960873821901
"How do I enable GPU-aware MPI on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.402573764246535
"How do I enable GPU-aware MPI on Frontier?
","As has been pointed out previously in the Frontier documentation, notice that GPUs are NOT mapped to MPI ranks in sequential order (e.g., MPI rank 0 is mapped to physical CPU cores 1-7 and GPU 4, MPI rank 1 is mapped to physical CPU cores 9-15 and GPU 5), but this IS expected behavior. It is simply a consequence of the Frontier node architectures as shown in the Frontier Node Diagram and subsequent https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Note on NUMA domains <numa-note>.

Example 2: 1 MPI rank with 7 CPU cores and 1 GPU (single-node)",4.369118172718918
"Can I use the hipcc compiler to compile kernels for other GPU architectures?
","hipcc --amdgpu-target=gfx90a:xnack+ or CC --offload-arch=gfx90a:xnack+ -x hip

Kernels are compiled in ""xnack plus"" mode and will only be able to run on GPUs with HSA_XNACK=1 to enable XNACK. Performance may be better than ""xnack any"", but attempts to run with XNACK disabled will fail.

hipcc --amdgpu-target=gfx90a:xnack- or CC --offload-arch=gfx90a:xnack- -x hip

Kernels are compiled in ""xnack minus"" mode and will only be able to run on GPUs with HSA_XNACK=0 and XNACK disabled. Performance may be better than ""xnack any"", but attempts to run with XNACK enabled will fail.",4.3280150480634365
"Can I use the hipcc compiler to compile kernels for other GPU architectures?
","hipcc --amdgpu-target=gfx90a:xnack+ or CC --offload-arch=gfx90a:xnack+ -x hip

Kernels are compiled in ""xnack plus"" mode and will only be able to run on GPUs with HSA_XNACK=1 to enable XNACK. Performance may be better than ""xnack any"", but attempts to run with XNACK disabled will fail.

hipcc --amdgpu-target=gfx90a:xnack- or CC --offload-arch=gfx90a:xnack- -x hip

Kernels are compiled in ""xnack minus"" mode and will only be able to run on GPUs with HSA_XNACK=0 and XNACK disabled. Performance may be better than ""xnack any"", but attempts to run with XNACK enabled will fail.",4.3280150480634365
"Can I use the hipcc compiler to compile kernels for other GPU architectures?
","| Vendor | Module | Language | Compiler | OpenMP flag (GPU) | | --- | --- | --- | --- | --- | | Cray | cce | C C++ |  | -fopenmp | | Fortran | ftn |  | | AMD | rocm |  |  |  |

This section shows how to compile HIP codes using the Cray compiler wrappers and hipcc compiler driver.

Make sure the craype-accel-amd-gfx908 module is loaded when using HIP.

| Compiler | Compile/Link Flags, Header Files, and Libraries | | --- | --- | | CC |  | | hipcc |  |",4.269112704542629
"What is the advantage of building on a login node compared to a non-homogeneous system?
","Although the nodes are logically organized into different types, they all contain similar hardware. As a result of this homogeneous architecture there is not a need to cross-compile when building on a login node. Since login nodes have similar hardware resources as compute nodes, any tests that are run by your build process (especially by utilities such as autoconf and cmake) will have access to the same type of hardware that is on compute nodes and should not require intervention that might be required on non-homogeneous systems.",4.43555945260463
"What is the advantage of building on a login node compared to a non-homogeneous system?
","Login nodes should be used for basic tasks such as file editing, code compilation, data backup, and job submission. Login nodes should not be used for memory- or compute-intensive tasks. Users should also limit the number of simultaneous tasks performed on the login resources. For example, a user should not run (10) simultaneous tar processes on a login node.

Compute-intensive, memory-intensive, or otherwise disruptive processes running on login nodes may be killed without warning.",4.273111191790889
"What is the advantage of building on a login node compared to a non-homogeneous system?
","When you log into an OLCF cluster, you are placed on a login node.  Login node resources are shared by all users of the system. Because of this, users should be mindful when performing tasks on a login node.

Login nodes should be used for basic tasks such as file editing, code compilation, data backup, and job submission. Login nodes should not be used for memory- or compute-intensive tasks. Users should also limit the number of simultaneous tasks performed on the login resources. For example, a user should not run (10) simultaneous tar processes on a login node.",4.260096673605859
"Can anyone access a service that the project runs in Slate?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",4.2214619832652165
"Can anyone access a service that the project runs in Slate?
","By default, a route will only expose your https://docs.olcf.ornl.gov/systems/route.html#slate_services to NCCS networks. If you need your service exposed to the world outside ORNL, you will first need to get your project approved for external routes. To do this, submit a systems ticket. In the description, give us your project name and a brief reasoning for why exposing externally is needed.

We will let you know once your project is able to set up external routes.",4.199965735761539
"Can anyone access a service that the project runs in Slate?
","This section will describe the project allocation process for Slate. This is applicable to both the Onyx and Marble OLCF OpenShift clusters.

In addition, we will go over the process to log into Onyx and Marble, and how namespaces work within the OpenShift context.

Please fill out the Slate Request Form in order to use Slate resources. This must be done in order to proceed.

If you have any question please contact User Assistance, via a Help Ticket Submission or by emailing help@olcf.ornl.gov.

Required information:

- Existing OLCF Project ID

- Project PI",4.195555840618212
"What is the next screen that will be displayed after clicking ""Next""?
","The next screen will show you some information about the project, you don't need to change anything, just click ""Next"".

Fill in your personal information and then click ""Next"".

Fill in your shipping information and then click ""Next"".",4.323458159455907
"What is the next screen that will be displayed after clicking ""Next""?
","On the ""Policies & Agreements"" page click the links to read the polices and then answer ""Yes"" to affirm you have read each. Certify your application by selecting ""Yes"" as well. Then Click ""Submit.""

<string>:5: (INFO/1) Enumerated list start value not ordinal-1: ""8"" (ordinal 8)

After submitting your application, it will need to pass through the approval process. Depending on when you submit, approval might not occur until the next business day.",4.00255497528713
"What is the next screen that will be displayed after clicking ""Next""?
","<string>:5: (INFO/1) Enumerated list start value not ordinal-1: ""3"" (ordinal 3)



On the user account page, selected ""yes"" or ""no"" for the questions asking about any pre-existing account names. If this is your first account with us, leave those questions set to ""no"". Also enter your preferred shell. If you do not know which shell to use, select ""/bin/bash"". We can change this later if needed. Click ""Next"".",3.999538048580782
"What is the name of the CPU used in a Frontier compute node?
","On Frontier, there are two major types of nodes you will encounter: Login and Compute. While these are similar in terms of hardware (see: https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes), they differ considerably in their intended use.",4.401261596481888
"What is the name of the CPU used in a Frontier compute node?
","Frontier has a total of 9,408 AMD compute nodes, with each node consisting of [1x] 64-core AMD “Optimized 3rd Gen EPYC” CPU with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X accelerators, each with 2 Graphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can think of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth memory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU, allowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of 36+36 GB/s. The 2 GCDs on the same MI250X are connected with",4.3662216837088765
"What is the name of the CPU used in a Frontier compute node?
","Simplified Frontier node architecture diagram

In the diagram, each physical core on a Frontier compute node is composed of two logical cores that are represented by a pair of blue and grey boxes. For a given physical core, the blue box represents the logical core of the first hardware thread, where the grey box represents the logical core of the second hardware thread.",4.350243081335525
"What is the annotation for the batchScheduler?
","Batch job submission from containers is designed to work exactly like on a login node for the cluster. The workload will need to be annotated in order to get the necessary configuration injected at runtime.

| Cluster | Annotation | Value | Schedulers | | --- | --- | --- | --- | | Marble | ccs.ornl.gov/batchScheduler | true | Slurm, LSF | | Onyx | ccs.ornl.gov/batchScheduler | true | LSF |",4.210464512275046
"What is the annotation for the batchScheduler?
","If the Registration Token is left blank, the runner installation will not succeed.

If the GitLab Runner needs to be able to use batch scheduler command, toggle the ""Enable Batch Scheduler"" option to true. Then, select the necessary filesystem annotation to use for the cluster the chart is being deployed. For Marble, the ""olcf"" annotation would be used whereas whereas for Onyx the ""ccsopen"" annotation would be used.",4.209108886478736
"What is the annotation for the batchScheduler?
","You can add the required annotations to any workload object such as a Pod, Deployment, or a DeploymentConfig. Submitting a batch job from a container requires access to the OLCF shared filesystems so that annotation is also included.

metadata:
  annotations:
    ccs.ornl.gov/batchScheduler: ""true""
    ccs.ornl.gov/fs: olcf

Full example of a deployment using a base image provided by OLCF.

Batch job submission from containers uses SSH to access the submission host. If you use your own image you must install the openssh client package in your image.",4.1439255292113
"What happens if I violate the OLCF policy?
","authorized individual or investigative agency is in effect during the period of your access to information on a DOE computer and for a period of three years thereafter. OLCF personnel and users are required to address, safeguard against, and report misuse, abuse and criminal activities. Misuse of OLCF resources can lead to temporary or permanent disabling of accounts, loss of DOE allocations, and administrative or legal actions. Users who have not accessed a OLCF computing resource in at least 6 months will be disabled. They will need to reapply to regain access to their account. All users",4.294232494414244
"What happens if I violate the OLCF policy?
","The requirements outlined in this document apply to all individuals who have an OLCF account. It is your responsibility to ensure that all individuals have the proper need-to-know before allowing them access to the information on OLCF computing resources. This document will outline the main security concerns.

OLCF computing resources are for business use only. Installation or use of software for personal use is not allowed. Incidents of abuse will result in account termination. Inappropriate uses include, but are not limited to:

Sexually oriented information",4.262193057511689
"What happens if I violate the OLCF policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to or use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title:Security Policy Version: 12.10",4.258942368608064
"How many file accesses per second can be improved with the Metada operations in the Alpine parallel filesystem?
","I/O and 2.2 TB/s for random I/O under FPP mode, which means each process, writes its own file. Metada operations are improved with around to minimum 50,000 file access per sec and aggregated up to 2.6 million accesses of 32KB small files.",4.360865813025673
"How many file accesses per second can be improved with the Metada operations in the Alpine parallel filesystem?
","Summit mounts a POSIX-based IBM Spectrum Scale parallel filesystem called Alpine. Alpine's maximum capacity is 250 PB. It is consisted of 77 IBM Elastic Storage Server (ESS) GL4 nodes running IBM Spectrum Scale 5.x which are called Network Shared Disk (NSD) servers. Each IBM ESS GL4 node, is a scalable storage unit (SSU), constituted by two dual-socket IBM POWER9 storage servers, and a 4X EDR InfiniBand network for up to 100Gbit/sec of networking bandwidth.  The maximum performance of the final production system will be about 2.5 TB/s for sequential I/O and 2.2 TB/s for random I/O under FPP",4.2608016096716295
"How many file accesses per second can be improved with the Metada operations in the Alpine parallel filesystem?
","File systems

The OLCF's center-wide https://docs.olcf.ornl.gov/systems/your_file.html#data-alpine-ibm-spectrum-scale-filesystem name Alpine is available on Andes for computational work.  An NFS-based file system provides https://docs.olcf.ornl.gov/systems/your_file.html#data-user-home-directories-nfs and https://docs.olcf.ornl.gov/systems/your_file.html#data-project-home-directories-nfs. Additionally, the OLCF's https://docs.olcf.ornl.gov/systems/your_file.html#data-hpss provides archival spaces.

Shell and programming environments",4.173325676711514
"How do I select a job layout for my application on Frontier?
","To override this default layout (not recommended), set -S 0 at job allocation.



Frontier uses SchedMD's Slurm Workload Manager for scheduling and managing jobs. Slurm maintains similar functionality to other schedulers such as IBM's LSF, but provides unique control of Frontier's resources through custom commands and options specific to Slurm. A few important commands can be found in the conversion table below, but please visit SchedMD's Rosetta Stone of Workload Managers for a more complete conversion reference.",4.250286018075194
"How do I select a job layout for my application on Frontier?
","node layout <frontier-simple> should also be considered when selecting job layout.  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-srun section for more srun information, and see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-mapping for srun examples on Frontier.  OLCF Support  If you encounter any issues or have questions, please contact the OLCF via the following:  Email us at help@olcf.ornl.gov  Contact your OLCF liaison  Sign-up to attend OLCF Office Hours",4.208892498924511
"How do I select a job layout for my application on Frontier?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.16313222790136
"How can I get started with using CuPy for data science tasks?
","As noted in AMD+CuPy limitations, data sizes explored here hang. So, this section currently does not apply to Frontier.

Now that you know how to use CuPy, time to see the actual benefits that CuPy provides for large datasets. More specifically, let's see how much faster CuPy can be than NumPy on Summit. You won't need to fix any errors; this is mainly a demonstration on what CuPy is capable of.

There are a few things to consider when running on GPUs, which also apply to using CuPy:

Higher precision means higher cost (time and space)

The structuring of your data is important",4.365916713937626
"How can I get started with using CuPy for data science tasks?
","CuPy is a library that implements NumPy arrays on NVIDIA GPUs by utilizing CUDA Toolkit libraries like cuBLAS, cuRAND, cuSOLVER, cuSPARSE, cuFFT, cuDNN and NCCL. Although optimized NumPy is a significant step up from Python in terms of speed, performance is still limited by the CPU (especially at larger data sizes) -- this is where CuPy comes in. Because CuPy's interface is nearly a mirror of NumPy, it acts as a replacement to run existing NumPy/SciPy code on NVIDIA CUDA platforms, which helps speed up calculations further. CuPy supports most of the array operations that NumPy provides,",4.350139486058061
"How can I get started with using CuPy for data science tasks?
","You have now discovered what CuPy can provide! Now you can try speeding up your own codes by swapping CuPy and NumPy where you can.

CuPy User Guide

CuPy Website

CuPy API Reference

CuPy Release Notes",4.314058775483932
"What is the difference between Rank and RankCore in Summit?
","Rank:   20; NumRanks: 24; RankCore:  96; Hostname: a01n01; GPU: 4
Rank:   21; NumRanks: 24; RankCore: 100; Hostname: a01n01; GPU: 4

Rank:   22; NumRanks: 24; RankCore: 104; Hostname: a01n01; GPU: 5
Rank:   23; NumRanks: 24; RankCore: 108; Hostname: a01n01; GPU: 5

summit>",4.173379601589733
"What is the difference between Rank and RankCore in Summit?
","Rank:   18; NumRanks: 24; RankCore:  88; Hostname: a33n05; GPU: 3, 4, 5
Rank:   19; NumRanks: 24; RankCore:  92; Hostname: a33n05; GPU: 3, 4, 5
Rank:   20; NumRanks: 24; RankCore:  96; Hostname: a33n05; GPU: 3, 4, 5
Rank:   21; NumRanks: 24; RankCore: 100; Hostname: a33n05; GPU: 3, 4, 5
Rank:   22; NumRanks: 24; RankCore: 104; Hostname: a33n05; GPU: 3, 4, 5
Rank:   23; NumRanks: 24; RankCore: 108; Hostname: a33n05; GPU: 3, 4, 5
summit>",4.135381321390697
"What is the difference between Rank and RankCore in Summit?
","Rank: 11; RankCore: 120; Thread: 0; ThreadCore: 120; Hostname: a33n05; OMP_NUM_PLACES: {120},{124},{128},{132}
Rank: 11; RankCore: 120; Thread: 1; ThreadCore: 124; Hostname: a33n05; OMP_NUM_PLACES: {120},{124},{128},{132}
Rank: 11; RankCore: 120; Thread: 2; ThreadCore: 128; Hostname: a33n05; OMP_NUM_PLACES: {120},{124},{128},{132}
Rank: 11; RankCore: 120; Thread: 3; ThreadCore: 132; Hostname: a33n05; OMP_NUM_PLACES: {120},{124},{128},{132}

summit>",4.11181990670416
"What is the host nickname for Frontier?
","Frontier

**For Frontier:**

- **Host nickname**: ``Frontier`` (this is arbitrary)
- **Remote hostname**: ``frontier.olcf.ornl.gov`` (required)
- **Host name aliases**: ``login#`` (required)
- **Maximum Nodes**: Unchecked
- **Maximum processors**: Unchecked (arbitrary)
- **Path to VisIt Installation**: ``/sw/frontier/ums/ums022/linux-sles15-zen3/gcc-11.2.0/visit-3.3.3-zfoh2caq5tbshlvtujditymjizstvewe/`` (required)
- **Username**: Your OLCF Username (required)
- **Tunnel data connections through SSH**: Checked (required)",4.277750714969714
"What is the host nickname for Frontier?
","$ ssh <username>@frontier.olcf.ornl.gov

For more information on connecting to OLCF resources, see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#connecting-to-olcf.

By default, connecting to Frontier will automatically place the user on a random login node. If you need to access a specific login node, you will ssh to that node after your intial connection to Frontier.

[<username>@login12.frontier ~]$ ssh <username>@login01.frontier.olcf.ornl.gov

Users can connect to any of the 17 Frontier login nodes by replacing login01 with their login node of choice.",4.07902444032705
"What is the host nickname for Frontier?
",The Frontier nodes are connected with [4x] HPE Slingshot 200 Gbps (25 GB/s) NICs providing a node-injection bandwidth of 800 Gbps (100 GB/s).,4.0762514410243424
"How can I use TAU to analyze the memory usage of my applications?
","TAU is installed with Program Database Toolkit (PDT) on Summit. PDT is a framework for analyzing source code written in several programming languages. Moreover, Performance Application Programming Interface (PAPI) is supported. PAPI counters are used to assess the CPU performance. In this section, some approaches for profiling and tracing will be presented.

In most cases, we need to use wrappers to recompile the application:

For C: replace the compiler with the TAU wrapper tau_cc.sh

For C++: replace the compiler with the TAU wrapper tau_cxx.sh",4.3201867019779385
"How can I use TAU to analyze the memory usage of my applications?
","TAU is a portable profiling and tracing toolkit that supports many programming languages. The instrumentation can be done by inserting in the source code using an automatic tool based on the Program Database Toolkit (PDT), on the compiler instrumentation, or manually using the instrumentation API.

Webpage: https://www.cs.uoregon.edu/research/tau/home.php",4.286159729213811
"How can I use TAU to analyze the memory usage of my applications?
","Below, we'll look at using TAU to profile each case.

Edit the cmake_summit_pgi.sh and replace mpic++ with tau_cxx.sh. This applies only for the non-GPU versions.

TAU works with special TAU makefiles to declare what programming models are expected from the application:

The available makefiles are located inside TAU installation:",4.253390937337743
"How do I access my project files on HPSS?
","Project Archive directories may only be accessed via utilities called HSI and HTAR. For more information on using HSI or HTAR, see the https://docs.olcf.ornl.gov/systems/project_centric.html#data-hpss section.",4.411629956082532
"How do I access my project files on HPSS?
","Member Work, Project Work, and World Work directories are not backed up. Project members are responsible for backing up these files, either to Project Archive areas (HPSS) or to an off-site location.

Moderate projects without export control restrictions are also allocated project-specific archival space on the High Performance Storage System (HPSS). The default quota is shown on the table below. If a higher quota is needed, contact the User Assistance Center.

There is no HPSS storage for Moderate Enhanced Projects, Moderate Projects subject to export control, or Open projects.",4.318804683216718
"How do I access my project files on HPSS?
","Project members get an individual Member Archive directory for each associated project; these reside on the High Performance Storage System (HPSS), OLCF's tape-archive storage system. Member Archive areas are not shared with other users of the system and are intended for project data that the user does not want to make available to other users.  HPSS is intended for data that do not require day-to-day access. Users should not store data unrelated to OLCF projects on HPSS. Users should periodically review files and remove unneeded ones. See the section",4.304587304176223
"What happens if GPU HBM becomes full on Frontier?
","The page migration behavior summarized by the following tables represents the current, observable behavior. Said behavior will likely change in the near future.  CPU accesses to migratable memory may behave differently than other platforms you're used to. On Frontier, pages will not migrate from GPU HBM to CPU DDR4 based on access patterns alone. Once a page has migrated to GPU HBM it will remain there even if the CPU accesses it, and all accesses which do not resolve in the CPU cache will occur over the Infinity Fabric between the AMD ""Optimized 3rd Gen EPYC"" CPU and AMD MI250X GPU. Pages",4.157082813539449
"What happens if GPU HBM becomes full on Frontier?
","The 110 CUs in each GPU deliver peak performance of 26.5 TFLOPS in double precision. Also, each GPU contains 64 GB of high-bandwidth memory (HBM2) accessible at a peak bandwidth of 1.6 TB/s. The 2 GPUs in an MI250X are connected with [4x] GPU-to-GPU Infinity Fabric links providing 200+200 GB/s of bandwidth. (Consult the diagram in the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes section for information on how the accelerators are connected to each other, to the CPU, and to the network.",4.142491014369129
"What happens if GPU HBM becomes full on Frontier?
","| (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/Joo-FrontierTipsAndTricks.pdf https://vimeo.com/803633277 | | 2023-02-17 | GPU Debugging | Mark Stock, HPC Applications Engineer, HPE | Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2-17-23_GPU_Debugging_distribute-2.pdf https://vimeo.com/840552596 | | 2023-02-17 | GPU Profiling | Alessandro Fanfarillo, Senior Member of Technical Staff, Exascale Application Performance, AMD | Frontier Training Workshop",4.135529550299826
"Is there a way to specify a maximum wall-clock time for my job?
","Preemptable job limits:

| Bin | Min Nodes | Max Nodes | Max Walltime (Hours) | Guaranteed Walltime | | --- | --- | --- | --- | --- | | 4 | 46 | 91 | 24.0 | 6.0 (hours) | | 5 | 1 | 45 | 24.0 | 2.0 (hours) |

If a job in the killable queue does not reach its requested walltime, it will continue to use allocation time with each automatic resubmission until it either reaches the requested walltime during a single continuous run, or is manually killed by the user. Allocations are always charged based on actual compute time used by all jobs.",4.357879337100136
"Is there a way to specify a maximum wall-clock time for my job?
",you can optionally specify a +time value for each jobid.,4.217148374023603
"Is there a way to specify a maximum wall-clock time for my job?
","The basic priority-setting mechanism for jobs waiting in the queue is the time a job has been waiting relative to other jobs in the queue.

If your jobs require resources outside these queue policies such as higher priority or longer walltimes, please contact help@olcf.ornl.gov.

Jobs are aged according to the job's requested processor count (older age equals higher queue priority). Each job's requested processor count places it into a specific bin. Each bin has a different aging parameter, which all jobs in the bin receive.",4.2028968732228
"How does one ensure that sensitive information is transmitted securely on OLCF systems?
","Never allow access to your sensitive data to anyone outside of your group.

Transfer of sensitive data must be through the use encrypted methods (scp, sftp, etc).

All sensitive data must be removed from all OLCF resources when your project has concluded.",4.4705300339474014
"How does one ensure that sensitive information is transmitted securely on OLCF systems?
","3. Prior to your project being initialized, we must have source IP addresses for devices in your organization that are authorized to transfer sensitive/controlled information to/from your organization, or for use in accessing that information. Encryption is necessary for transferring sensitive and/or controlled information to and from the OLCF.",4.416309460943064
"How does one ensure that sensitive information is transmitted securely on OLCF systems?
","The OLCF systems provide protections to maintain the confidentiality, integrity, and availability of user data. Measures include: the availability of file permissions, archival systems with access control lists, and parity/CRC checks on data paths/files. It is the user’s responsibility to set access controls appropriately for data. In the event of system failure or malicious actions, the OLCF makes no guarantee against loss of data nor makes a guarantee that a user’s data could not be potentially accessed, changed, or deleted by another individual. It is the user’s responsibility to insure",4.404382381148443
"Can I prevent my job from being preempted in the killable queue?
","The killable queue is a preemptable queue that allows jobs in bins 4 and 5 to request walltimes up to 24 hours. Jobs submitted to the killable queue will be preemptable once the job reaches the guaranteed runtime limit as shown in the table below. For example, a job in bin 5 submitted to the killable queue can request a walltime of 24 hours. The job will be preemptable after two hours of run time. Similarly, a job in bin 4 will be preemptable after six hours of run time. Once a job is preempted, the job will be resubmitted by default with the original limits as requested in the job script and",4.37275800700457
"Can I prevent my job from being preempted in the killable queue?
","To submit a job to the killable queue, add the -q killable option to your bsub command or #BSUB -q killable to your job script.

To prevent a preempted job from being automatically requeued, the BSUB -rn flag can be used at submit time.",4.267912999946029
"Can I prevent my job from being preempted in the killable queue?
","-------------------------



At the start of a scheduled system outage, a *queue reservation* is used

to ensure that no jobs are running. In the ``batch`` queue, the

scheduler will not start a job if it expects that the job would not

complete (based on the job's user-specified max walltime) before the

reservation's start time. In constrast, the ``killable`` queue allows

the scheduler to start a job even if it will *not* complete before a

scheduled reservation. It enforces the following policies:



-  Jobs will be killed if still running when a system outage begins.",4.254689380284487
"How do I set the walltime for my high-throughput computing environment in Parsl?
","Preemptable job limits:

| Bin | Min Nodes | Max Nodes | Max Walltime (Hours) | Guaranteed Walltime | | --- | --- | --- | --- | --- | | 4 | 46 | 91 | 24.0 | 6.0 (hours) | | 5 | 1 | 45 | 24.0 | 2.0 (hours) |

If a job in the killable queue does not reach its requested walltime, it will continue to use allocation time with each automatic resubmission until it either reaches the requested walltime during a single continuous run, or is manually killed by the user. Allocations are always charged based on actual compute time used by all jobs.",4.122504349693365
"How do I set the walltime for my high-throughput computing environment in Parsl?
","Parsl is a flexible and scalable parallel programming library for Python which is being developed at the University of Chicago. It augments Python with simple constructs for encoding parallelism. For more information about Parsl, please refer to its documentation.

Parsl can be installed with Conda for use on Summit by running the following from a login node:

$ module load workflows
$ module load parsl/1.1.0

The following instructions illustrate how to run a ""Hello world"" program with Parsl on Summit.",4.113573203151219
"How do I set the walltime for my high-throughput computing environment in Parsl?
","Parsl needs to be able to write to the working directory from compute nodes, so we will work from within the member work directory and assume a project ID ABC123:

$ mkdir -p ${MEMBERWORK}/abc123/parsl-demo/
$ cd ${MEMBERWORK}/abc123/parsl-demo/

To run an example ""Hello world"" program with Parsl on Summit, create a file called hello-parsl.py with the following contents, but with your own project ID in the line specified:",4.10662458141876
"How often do export control regulations change?
","Will undergo Export Control Review.  Need enough information to pass ECR in the application.
Will be given ""modest"" allocation of hours.  Monitored but not explicit.
Added to RUC for approval.
Quota in RATS?",4.149647375689959
"How often do export control regulations change?
","Export Control: The project request will be reviewed by ORNL Export Control to determine whether sensitive or proprietary data will be generated or used. The results of this review will be forwarded to the PI. If the project request is deemed sensitive and/or proprietary, the OLCF Security Team will schedule a conference call with the PI to discuss the data protection needs.",4.083345411457385
"How often do export control regulations change?
","these prohibited data types or information that falls under Export Control. For questions, contact help@nccs.gov.",4.075647246500241
"What information do I need to provide to run VisIt on Andes?
","Navigate to the appropriate file.

Once you choose a file, you will be prompted for the number of nodes and processors you would like to use (remember that each node of Andes contains 32 processors, or 28 if using the high-memory GPU partition) and the Project ID, which VisIt calls a ""Bank"" as shown below.



Once specified, the server side of VisIt will be launched, and you can interact with your data.",4.350403794719226
"What information do I need to provide to run VisIt on Andes?
","VisIt is now available on Frontier through the UMS022 module.

VisIt 3.3.3 is now available on Andes.

VisIt is an interactive, parallel analysis and visualization tool for scientific data. VisIt contains a rich set of visualization features so you can view your data in a variety of ways. It can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes. Further information regarding VisIt can be found at the links provided in the https://docs.olcf.ornl.gov/systems/visit.html#visit-resources section.",4.343815650377826
"What information do I need to provide to run VisIt on Andes?
","For sample data and additional examples, explore the VisIt Data Archive and various VisIt Tutorials. Supplementary test data can be found in your local installation in the data directory:

Linux: /path/to/visit/data

macOS: /path/to/VisIt.app/Contents/Resources/data

Windows: C:\path\to\LLNL\VisIt x.y.z\data

Additionally, check out our beginner friendly OLCF VisIt Tutorial which uses Andes to visualize example datasets.",4.325627330091262
"Can I use Miniconda to manage environments for other package managers?
","Cloning the base environment:

It is not recommended to try to install new packages into the base environment. Instead, you can clone the base environment for yourself and install packages into the clone. To clone an environment, you must use the --clone <env_to_clone> flag when creating a new conda environment. An example for cloning the base environment into your Project Home directory on Summit is provided below:",4.204723974441672
"Can I use Miniconda to manage environments for other package managers?
","Exporting (sharing) an environment:

You may want to share your environment with someone else. One way to do this is by creating your environment in a shared location where other users can access it. A different way (the method described below) is to export a list of all the packages and versions of your environment (an environment.yml file). If a different user provides conda the list you made, conda will install all the same package versions and recreate your environment for them -- essentially ""sharing"" your environment. To export your environment list:",4.1891685253685536
"Can I use Miniconda to manage environments for other package managers?
","$ conda env list

# conda environments:
#
                      *  /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/py3711-summit
base                     /sw/summit/python/3.8/anaconda3/2020.07-rhel8

Next, let's install a package (NumPy). There are a few different approaches.",4.186699593819642
"How do I create a network policy for my namespace?
","Network policies are specifications of how groups of pods are allowed to communicate with each other and other network endpoints. A pod is selected in a Network Policy based on a label so the Network Policy can define rules to specify traffic to that pod.

When you create a namespace in Slate with the Quota Dashboard some Network Policies are added to your newly created namespace. This means that pods inside your project are isolated from connections not defined in these Network Policies.",4.447284805187545
"How do I create a network policy for my namespace?
","Network Policies do not conflict, they are additive. This means that if two policies match a pod the pod will be restricted to what is allowed by the union of those policies' ingress/egress rules.

To create a Network policy using the GUI click the Networking tab followed by the Network Policy tab:

Creating Network Policies

This will place you in an editor with some boiler plate YAML. From here you can define the network policy that you need for your namespace. Below is an example Network Policy that you would use to allow all external traffic to access a nodePort in your namespace.",4.4067830327265725
"How do I create a network policy for my namespace?
","to view object's YAML.

To create a Network Policy, define one in YAML similar to the output of the previous command and run:

oc create -f FILENAME

For a more complex example of a Network Policy please see the Kubernetes doc.

A full reference of Network Policies can be found here.",4.342073651342719
"How do I get information about the workers connected to the dask-cuda-cluster using Nvidia Rapids?
","Running RAPIDS multi-gpu/multi-node workloads requires a dask-cuda cluster. Setting up a dask-cuda cluster on Summit requires two components:

dask-scheduler.

dask-cuda-workers.

Once the dask-cluster is running, the RAPIDS script should perform four main tasks. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

Reference of multi-gpu/multi-node operation with cuDF, cuML, cuGraph is available in the next links:

10 Minutes to cuDF and Dask-cuDF.

cuML's Multi-Node, Multi-GPU Algorithms.",4.277679606678336
"How do I get information about the workers connected to the dask-cuda-cluster using Nvidia Rapids?
","# 3. Do computation
    # ...
    # ...

    # 4. Shutting down the dask-cuda-cluster
    print(""Shutting down the cluster"")
    workers_list = list(workers_info)
    disconnect (client, workers_list)

The RAPIDS environment is read-only. Therefore, users cannot install any additional packages that may be needed. If users need any additional conda or pip packages, they can clone the RAPIDS environment into their preferred directory and then add any packages they need.

Cloning the RAPIDS environment can be done with the next commands:",4.192643094207305
"How do I get information about the workers connected to the dask-cuda-cluster using Nvidia Rapids?
","As mentioned earlier, the RAPIDS code should perform four main tasks as shown in the following script. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

import sys
from dask.distributed import Client

def disconnect(client, workers_list):
    client.retire_workers(workers_list, close_workers=True)
    client.shutdown()

if __name__ == '__main__':

    sched_file = str(sys.argv[1]) #scheduler file
    num_workers = int(sys.argv[2]) # number of workers to wait for",4.166768585869534
"What is the purpose of the ""-n"" flag in the jsrun command?
","In addition, to debug slow startup JSM provides the option to create a progress file. The file will show information that can be helpful to pinpoint if a specific node is hanging or slowing down the job step launch. To enable it, you can use: jsrun --progress ./my_progress_file_output.txt.

When using file-based specification of resource sets with jsrun -U, the -a flag (number of tasks per resource set) is ignored. This has been reported to IBM and they are investigating. It is generally recommended to use jsrun explicit resource files (ERF) with --erf_input and --erf_output instead of -U.",4.250440160895175
"What is the purpose of the ""-n"" flag in the jsrun command?
","As on any system, it is useful to keep in mind the hardware underneath every execution. This is particularly true when laying out resource sets.

jsrun    [ -n #resource sets ]   [tasks, threads, and GPUs within each resource set]   program [ program args ]

Below are common jsrun options. More flags and details can be found in the jsrun man page. The defaults listed in the table below are the OLCF defaults and take precedence over those mentioned in the man page.",4.238790366863904
"What is the purpose of the ""-n"" flag in the jsrun command?
","This section describes tools that users might find helpful to better understand the jsrun job launcher.

hello_jsrun is a ""Hello World""-type program that users can run on Summit nodes to better understand how MPI ranks and OpenMP threads are mapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A screencast showing how to use Hello_jsrun is also available: https://vimeo.com/261038849

Job Step Viewer provides a graphical view of an application's runtime layout on Summit. It allows users to preview and quickly iterate with multiple jsrun options to understand and optimize job launch.",4.200717079479574
"What is the difference between the EGL and OSMesa versions of the ParaView module?
","We offer two rendering modes of the ParaView API on our systems: OSMesa and EGL.  OSMesa is intended for use on regular compute nodes, whereas EGL is intended for use on GPU enabled nodes. When running interactively, you do not need to download or install anything special to use the EGL or OSMesa versions, as you'll be able to choose between those options when connecting to the system (see https://docs.olcf.ornl.gov/systems/paraview.html#paraview-gui below). If instead you're running in batch mode on the command line (see https://docs.olcf.ornl.gov/systems/paraview.html#paraview-command-line",4.509375981549678
"What is the difference between the EGL and OSMesa versions of the ParaView module?
","If problems persist and you do not need EGL, try using the OSMesa version of the module instead (e.g., paraview/5.9.1-osmesa instead of paraview/5.9.1-egl).

A command not found error occurs when trying to execute either PvBatch or PvPython after loading the default ParaView module on Andes. To fix this, you must load the equivalent ParaView module ending in ""pyapi"" instead (i.e., module load paraview/5.9.1-py3-pyapi instead of module load paraview/5.9.1-py3).",4.325366420666544
"What is the difference between the EGL and OSMesa versions of the ParaView module?
","[user@login4.summit ~]$ module -t avail paraview

/sw/summit/modulefiles/core:
paraview/5.9.1-egl
paraview/5.9.1-osmesa
paraview/5.10.0-egl
paraview/5.10.0-osmesa
paraview/5.11.0-egl
paraview/5.11.0-osmesa

[user@login4.summit ~]$ module load paraview/5.9.1-egl

It is highly recommended to only use the modules located in /sw/andes/modulefiles/core or /sw/summit/modulefiles/core.

The EGL mode seems to work better with larger datasets and is generally recommended over OSMesa on our systems. However, we encourage users to try both options and see which version works best for their data.",4.312051383932724
"Will the system configuration updates on Crusher affect my current jobs?
","The Alpine GPFS file system remains available but will be permanently unmounted from Crusher on Tuesday, April 18, 2023. Please begin moving your data to the Orion file system as soon as possible.

On Thursday, December 29, 2022 the following system configuration settings will be updated on Crusher:

Low-Noise Mode will be enabled: as a result, system processes will be constrained to core 0 on every node.",4.130159390230091
"Will the system configuration updates on Crusher affect my current jobs?
","Crusher users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.102204165285715
"Will the system configuration updates on Crusher affect my current jobs?
","This page lists significant changes to software provided on OLCF systems. The most recent changes are listed first.



<p style=""font-size:20px""><b>Frontier and Crusher: System Software Upgrade (July 18, 2023)</b></p>

The Crusher TDS and Frontier systems were upgraded to a new version of the system software stack. This stack introduces ROCm 5.5.1 and HPE/Cray Programming Environment 23.05. For more information, please see:

Crusher System Updates.

Frontier System Updates.

Please contact help@olcf.ornl.gov with any issues or questions.",4.089220755929686
"How do I know which software packages and scientific libraries are available on Summit?
","The E4S software list is installed along side the existing software on Summit and can be access via lmod modulefiles.

To access the installed software, load the desired compiler via:

module load < compiler/version >
.. ie ..
module load gcc/9.1.0
.. or ..
module load gcc/7.5.0

Then use module avail to see the installed list of packages.

List of installed packages on Summit for E4S release 21.08:",4.287195561844829
"How do I know which software packages and scientific libraries are available on Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.266973860126741
"How do I know which software packages and scientific libraries are available on Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.25923098326129
"How do I use the compiler wrappers provided by the PrgEnv-<compiler> module?
","Cray provides PrgEnv-<compiler> modules (e.g., PrgEnv-cray) that load compatible components of a specific compiler toolchain. The components include the specified compiler as well as MPI, LibSci, and other libraries. Loading the PrgEnv-<compiler> modules also defines a set of compiler wrappers for that compiler toolchain that automatically add include paths and link in libraries for Cray software. Compiler wrappers are provided for C (cc), C++ (CC), and Fortran (ftn).

For example, to load the AMD programming environment, do:

module load PrgEnv-amd",4.325929637283467
"How do I use the compiler wrappers provided by the PrgEnv-<compiler> module?
","Cray provides PrgEnv-<compiler> modules (e.g., PrgEnv-cray) that load compatible components of a specific compiler toolchain. The components include the specified compiler as well as MPI, LibSci, and other libraries. Loading the PrgEnv-<compiler> modules also defines a set of compiler wrappers for that compiler toolchain that automatically add include paths and link in libraries for Cray software. Compiler wrappers are provided for C (cc), C++ (CC), and Fortran (ftn).",4.315497798369167
"How do I use the compiler wrappers provided by the PrgEnv-<compiler> module?
","Cray provides PrgEnv-<compiler> modules (e.g., PrgEnv-cray) that load compatible components of a specific compiler toolchain. The components include the specified compiler as well as MPI, LibSci, and other libraries. Loading the PrgEnv-<compiler> modules also defines a set of compiler wrappers for that compiler toolchain that automatically add include paths and link in libraries for Cray software. Compiler wrappers are provided for C (cc), C++ (CC), and Fortran (ftn).",4.315497798369167
"Can I create a secured HTTPS route in Slate that uses a different TLS termination method than Edge Termination?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.297696186285962
"Can I create a secured HTTPS route in Slate that uses a different TLS termination method than Edge Termination?
","Edge Termination terminates TLS at the router, before sending traffic to the service. We have a wildcard certificate on the routers for each cluster. These will be used by default if no certificate is provided, and this is the preferred method for securing a route.

$ oc create route edge --service=my-project \
  --hostname=my-project.apps.<cluster>.ccs.ornl.gov

If you would like to use your own keys with edge termination, this can be done with a command similar to this example.",4.254996767235294
"Can I create a secured HTTPS route in Slate that uses a different TLS termination method than Edge Termination?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.177370278598294
"Who should I contact if I encounter the cudaManagedMemory driver bug on Summit?
","There is a (very rare) driver bug involving cudaManagedMemory that can cause a kernel panic. If you encounter this bug, please contact the OLCF User Support team. The easiest mitigation is for the user code to initialize a context on every GPU with which it intends to interact (for example by calling cudaFree(0) while each device is active).",4.271662435008395
"Who should I contact if I encounter the cudaManagedMemory driver bug on Summit?
","If you want to do GPU computing on Summit with R, we would love to collaborate with you (see contact details at the bottom of this document).

For more information about using R and pbdR effectively in an HPC environment such as Summit, please see the R and pbdR articles. These are long-form articles that introduce HPC concepts like MPI programming in much more detail than we do here.

Also, if you want to use R and/or pbdR on Summit, please feel free to contact us directly:

Mike Matheson - mathesonma AT ornl DOT gov

George Ostrouchov - ostrouchovg AT ornl DOT gov",4.162588706261926
"Who should I contact if I encounter the cudaManagedMemory driver bug on Summit?
","Using the hip-cuda/5.1.0 module on Summit requires CUDA v11.4.0 or later. However, only CUDA v11.0.3 and earlier currently support GPU-aware MPI, so GPU-aware MPI is currently not available when using HIP on Summit.

Any codes compiled with post 11.0.3 CUDA (cuda/11.0.3) will not work with MPS enabled (-alloc_flags ""gpumps"") on Summit. The code will hang indefinitely. CUDA v11.0.3 is the latest version that is officially supported by IBM's software stack installed on Summit. We are continuing to look into this issue.",4.162540127900736
"What is the basic principle of the simple batch queue system used by the OLCF?
","The queue structure was designed based on user feedback and analysis of batch jobs over the recent years. However, we understand that the structure may not meet the needs of all users. If this structure limits your use of the system, please let us know. We want Andes to be a useful OLCF resource and will work with you providing exceptions or even changing the queue structure if necessary.

If your jobs require resources outside these queue policies such as higher priority or longer walltimes, please contact help@olcf.ornl.gov.",4.296609783302173
"What is the basic principle of the simple batch queue system used by the OLCF?
","The queue structure was designed based on user feedback and analysis of batch jobs over the recent years. However, we understand that the structure may not meet the needs of all users. If this structure limits your use of the system, please let us know. We want Andes to be a useful OLCF resource and will work with you providing exceptions or even changing the queue structure if necessary.

If your jobs require resources outside these queue policies such as higher priority or longer walltimes, please contact help@olcf.ornl.gov.

Allocation Overuse Policy",4.290273543190545
"What is the basic principle of the simple batch queue system used by the OLCF?
","In a simple batch queue system, jobs run in a first-in, first-out (FIFO) order. This often does not make effective use of the system. A large job may be next in line to run. If the system is using a strict FIFO queue, many processors sit idle while the large job waits to run. Backfilling would allow smaller, shorter jobs to use those otherwise idle resources, and with the proper algorithm, the start time of the large job would not be delayed. While this does make more effective use of the system, it indirectly encourages the submission of smaller jobs.",4.284185290650347
"How can I list all the environments available in Conda at OLCF?
","This guide introduces a user to the basic workflow of using conda environments, as well as providing an example of how to create a conda environment that uses a different version of Python than the base environment uses on Summit. Although Summit is being used in this guide, all of the concepts still apply to other OLCF systems. If using Frontier, this assumes you already have installed your own version of conda (e.g., by https://docs.olcf.ornl.gov/software/python/miniconda.html).

OLCF Systems this guide applies to:

Summit

Andes

Frontier (if using conda)",4.348504653066236
"How can I list all the environments available in Conda at OLCF?
","$ conda config --show envs_dirs

On OLCF systems, the default location is your $HOME directory. If you plan to frequently create environments in a different location other than the default (such as /ccs/proj/...), then there is an option to add directories to the envs_dirs list.

For example, to track conda environments in a subdirectory called summit in Project Home you would execute:

$ conda config --append envs_dirs /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit",4.279702846991937
"How can I list all the environments available in Conda at OLCF?
","This also is a great way to keep track of the locations and names of all other environments that have been created. The current environment is indicated by *.

To see what packages are installed in the active environment, use conda list:

$ conda list",4.27094656060619
"What is the difference between pprof and paraprof?
","$ paraprof --pack name.ppk
$ paraprof name.ppk &

The first window that opens when the paraprof name.ppk command is executed shows the experiment and the used metrics, for this case, TIME, PAPI_FP_OPS, PAPI_TOT_INS, PAPI_TOT_CYC.



The user is responsible for understanding which PAPI metrics should be used",4.074372785039616
"What is the difference between pprof and paraprof?
","If you do not declare the TAU_METRICS variable, then by default TIME is used and the profiling files are not in a directory. When the execution ends, there will be one file per process, called profile.X.Y.Z.

In order to use paraprof to visualize the data, your ssh connection should support X11 forwarding.

Pack the profiling data with a name that you prefer and start the paraprof GUI

$ paraprof --pack name.ppk
$ paraprof name.ppk &",4.07064155840559
"What is the difference between pprof and paraprof?
","There are three programming interfaces available: (1) Perftools-lite, (2) Perftools, and (3) Perftools-preload.

Below are two examples that generate an instrumented executable using Perftools, which is an advanced interface that provides full-featured data collection and analysis capability, including full traces with timeline displays.

The first example generates an instrumented executable using a PrgEnv-amd build:

module load PrgEnv-amd
module load craype-accel-amd-gfx90a
module load rocm
module load perftools",4.063720065536554
"What are the available tools for visualization on Andes?
","Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-user-guide.

For a full list of software available at the OLCF, please see the Software section (coming soon).",4.483512205261958
"What are the available tools for visualization on Andes?
","VisIt is now available on Frontier through the UMS022 module.

VisIt 3.3.3 is now available on Andes.

VisIt is an interactive, parallel analysis and visualization tool for scientific data. VisIt contains a rich set of visualization features so you can view your data in a variety of ways. It can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes. Further information regarding VisIt can be found at the links provided in the https://docs.olcf.ornl.gov/systems/visit.html#visit-resources section.",4.349533343196559
"What are the available tools for visualization on Andes?
","ParaView was developed to analyze extremely large datasets using distributed memory computing resources. The OLCF provides ParaView server installs on Andes and Summit to facilitate large scale distributed visualizations. The ParaView server running on Andes and Summit may be used in a headless batch processing mode or be used to drive a ParaView GUI client running on your local machine.

For a tutorial of how to get started with ParaView on Andes, see our ParaView at OLCF Tutorial.",4.289582576951466
"How can I view the profile data generated by rocprof?
","Integer instructions and cache levels are currently not documented here.

To get started, you will need to make an input file for rocprof, to be passed in through rocprof -i <input_file> --timestamp on -o my_output.csv <my_exe>. Below is an example, and contains the information needed to roofline profile GPU 0, as seen by each rank:",4.2434742221160295
"How can I view the profile data generated by rocprof?
","Integer instructions and cache levels are currently not documented here.

To get started, you will need to make an input file for rocprof, to be passed in through rocprof -i <input_file> --timestamp on -o my_output.csv <my_exe>. Below is an example, and contains the information needed to roofline profile GPU 0, as seen by each rank:",4.2434742221160295
"How can I view the profile data generated by rocprof?
","rocprof gathers metrics on kernels run on AMD GPU architectures. The profiler works for HIP kernels, as well as offloaded kernels from OpenMP target offloading, OpenCL, and abstraction layers such as Kokkos. For a simple view of kernels being run, rocprof --stats --timestamp on is a great place to start. With the --stats option enabled, rocprof will generate a file that is named results.stats.csv by default, but named <output>.stats.csv if the -o flag is supplied. This file will list all kernels being run, the number of times they are run, the total duration and the average duration (in",4.195518996598761
"Can I use the confirmation code to access my account?
","When our accounts team begins processing your application, you will receive an automated email containing an unique 36-character confirmation code. Make note of it; you can use it to check the status of your application at any time.

The principal investigator (PI) of the project must approve your account and system access. We will make the project PI aware of your request.",4.253057057127156
"Can I use the confirmation code to access my account?
","First-time users should apply for an account using the Account Request Form. You will need the correct 6 character project ID from your PI.

When our accounts team begins processing your application, you will receive an automated email containing a unique 36-character confirmation code. Make note of it; you can use it to check the status of your application at any time.

The principal investigator (PI) of the project must approve your account and system access. We will make the project PI aware of your request.",4.172511718361763
"Can I use the confirmation code to access my account?
","Form. You will need your Application Confirmation Number that was emailed to you by our accounts team to schedule in this manner. If you do not possess a confirmation number (you are verifying a replacement token, for example), please email us at help@olcf.ornl.gov to schedule.",4.129284456076026
"How does the hipMallocManaged function differ from the System Allocator in terms of GPU access?
","| System Allocator (malloc,new,allocate, etc) | Determined by first touch | Zero copy read/write                       | Migrate to GPU HBM on touch, then local read/write |
+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+
| hipMallocManaged                            | GPU HBM                   | Zero copy read/write                       | Populate in  GPU HBM, then local read/write        |",4.417396786316271
"How does the hipMallocManaged function differ from the System Allocator in terms of GPU access?
","| System Allocator (malloc,new,allocate, etc) | Determined by first touch | Zero copy read/write                       | Migrate to GPU HBM on touch, then local read/write |
+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+
| hipMallocManaged                            | GPU HBM                   | Zero copy read/write                       | Populate in  GPU HBM, then local read/write        |",4.417396786316271
"How does the hipMallocManaged function differ from the System Allocator in terms of GPU access?
","| Allocator | Initial Physical Location | CPU Access after GPU First Touch | Default Behavior for GPU Access | | --- | --- | --- | --- | | System Allocator (malloc,new,allocate, etc) | CPU DDR4 | Migrate to CPU DDR4 on touch | Migrate to GPU HBM on touch | | hipMallocManaged | CPU DDR4 | Migrate to CPU DDR4 on touch | Migrate to GPU HBM on touch | | hipHostMalloc | CPU DDR4 | Local read/write | Zero copy read/write over Infinity Fabric | | hipMalloc | GPU HBM | Zero copy read/write over Inifinity Fabric | Local read/write |",4.306472453665162
"Why do I need to disable the cache when creating a Singularity sif file?
","The reason we include the --disable-cache flag is because Singularity's caching can fill up your home directory without you realizing it. And if the home directory is full, Singularity builds will fail. If you wish to make use of the cache, you can set the environment variable SINGULARITY_CACHEDIR=/tmp/containers/<user>/singularitycache or something like that so that the NVMe storage is used as the cache.

As a simple example, we will run hostname with the Singularity container.

Create a file submit.lsf with the contents below.",4.3498686115297645
"Why do I need to disable the cache when creating a Singularity sif file?
","You can now create a Singularity sif file with singularity build --disable-cache --docker-login simple.sif docker://docker.io/<username>/simple.

This will ask you to enter your Docker username and password again for Singularity to download the image from Dockerhub and convert it to a sif file.",4.342439493926728
"Why do I need to disable the cache when creating a Singularity sif file?
","If you already have a ""image.tar"" file created with podman save from earlier that you are trying to replace, you will need to delete it first before running any other podman save to replace it. podman save won't overwrite the tar file for you.

Not using the --disable-cache flag in your singularity build commands could cause your home directory to get quickly filled by singularity caching image data. You can set the cache to a location in /tmp/containers with export SINGULARITY_CACHEDIR=/tmp/containers/<username>/singularitycache if you want to avoid using the --disable-cache flag.",4.236291527836523
"What is the title of the policy that outlines the penalties for late submission of quarterly progress reports?
","Principal Investigators of current OLCF projects must submit a quarterly progress report. The quarterly reports are essential as the OLCF must diligently track the use of the center's resources. In keeping with this, the OLCF (and DOE Leadership Computing Facilities in general) imposes the following penalties for late submission:

| Timeframe | Penalty | | --- | --- | | 1 Month Late | Job submissions against offending project will be suspended. | | 3 Months Late | Login privileges will be suspended for all OLCF resources for all users associated with offending project. |",4.363814431650288
"What is the title of the policy that outlines the penalties for late submission of quarterly progress reports?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  Title: Project Reporting Policy Version: 12.10",4.107583839413577
"What is the title of the policy that outlines the penalties for late submission of quarterly progress reports?
","Increased disk quota

Purge exemption for User/Group/World Work areas

Software requests



Closeout Report Template Use this template if you have been asked to submit a closeout report for your project.  Note this form does not apply to INCITE projects.  If you have been provided a template via email, only that template applies.

Industry Quarterly Report Template Use this template if you have an industry project to submit a quarterly report.



Director's Discretion Review Form For internal use only.",4.106432924070572
"How can TAU_OPTIONS be used to optimize the performance of the MPI version of MiniWeather?
","export TAU_OPTIONS=“-optTauSelectFile=phase.tau”

Now when you instrument your application, the phase called phase 1 are the lines 300-327 of the file miniWeather_mpi.cpp. Every call will be instrumented. This could create signiificant overhead, thus you should be careful when you use it.

Create a file called phases.tau.

BEGIN_INSTRUMENT_SECTION
static phase name=""phase1"" file=""miniWeather_mpi.cpp"" line=300 to line=327
static phase name=""phase2"" file=""miniWeather_mpi.cpp"" line=333 to line=346
END_INSTRUMENT_SECTION

Declare the TAU_OPTIONS variable.",4.374128992548244
"How can TAU_OPTIONS be used to optimize the performance of the MPI version of MiniWeather?
","Next, we will look at using the paraprof tool for the MPI version of MiniWeather.

For the MPI version, we should use a makefile with MPI. The compilation could fail if the makefile supports MPI+OpenMP, but the code doesn't include any OpenMP calls. Moreover, with TAU_OPTIONS declared below, we will add options to the linker.

$ module load tau
$ export TAU_MAKEFILE=/sw/summit/tau/tau2/ibm64linux/lib/Makefile.tau-pgi-papi-mpi-cupti-pdt-pgi
$ export TAU_OPTIONS='-optLinking=-lpnetcdf -optVerbose'
$ make mpi",4.352375173463115
"How can TAU_OPTIONS be used to optimize the performance of the MPI version of MiniWeather?
","#Activate tracing
#export TAU_TRACE=1

export OMP_NUM_THREADS=4
jsrun -n 16 -r 8 -a 1 -c 4 -b packed:4 ./miniWeather_mpi_openmp

For the current TAU version, you should use the tau_exec and not the TAU wrappers only for the compilation.

Use the mpic++ compiler in the Makefile, do not use TAU wrapper.

Build the MPI+OpenACC version by running make openacc.

Add the following in your submission file:",4.331915548207505
"What is the name of the function used for all-reduce operations in TAU?
","755          8          8          8          0  Message size for all-reduce
       302  2.621E+05          4  1.302E+05  1.311E+05  Message size for broadcast
---------------------------------------------------------------------------------------",4.049110739289629
"What is the name of the function used for all-reduce operations in TAU?
","Below, we'll look at using TAU to profile each case.

Edit the cmake_summit_pgi.sh and replace mpic++ with tau_cxx.sh. This applies only for the non-GPU versions.

TAU works with special TAU makefiles to declare what programming models are expected from the application:

The available makefiles are located inside TAU installation:",4.009113507537927
"What is the name of the function used for all-reduce operations in TAU?
","A similar approach for other metrics, not all of them can be used. TAU provides a tool called tau_cupti_avail, where we can see the list of available metrics, then we have to figure out which CUPTI metrics use these ones.

Activate tracing and declare the data format to OTF2. OTF2 format is supported only by MPI and OpenSHMEM applications.

$ export TAU_TRACE=1
$ export TAU_TRACE_FORMAT=otf2

Use Vampir for visualization.

For example, do not instrument routine sort*(int *)

Create a file select.tau

BEGIN_EXCLUDE_LIST
void sort_#(int *)
END_EXCLUDE_LIST

Declare the TAU_OPTIONS variable",4.004214344292151
"Can I use Pyquil to simulate a quantum circuit without running it on a QVM?
","A recommended workflow for running on Quantinuum's quantum computers is to utilize the syntax checker first, run on the emulator, then run on one of the quantum computers. This is highlighted in the examples.",4.15620389823659
"Can I use Pyquil to simulate a quantum circuit without running it on a QVM?
","Quil is the Rigetti-developed quantum instruction/assembly language. PyQuil is a Python library for writing and running quantum programs using Quil.

Installing pyQuil requires installing the Forest SDK. To quote Rigetti: ""pyQuil, along with quilc, the QVM, and other libraries, make up what is called the Forest SDK"". Because we don't have Docker functionality and due to normal users not having sudo privileges, this means that you will have to install the SDK via the ""bare-bones"" method. The general info below came from:

https://pyquil-docs.rigetti.com/en/stable/start.html",4.142657383601639
"Can I use Pyquil to simulate a quantum circuit without running it on a QVM?
","IBM Quantum provides Qiskit (Quantum Information Software Kit for Quantum Computation) for working with OpenQASM and the IBM Q quantum processors. Qiskit allows users to build quantum circuits, compile them for a particular backend, and run the compiled circuits as jobs. Additional information on using Qiskit is available at https://qiskit.org/learn/ and in our https://docs.olcf.ornl.gov/systems/ibm_quantum.html#Software Section <ibm-soft> below.",4.130148240346805
"What is the benefit of using external routes on Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.33747450739249
"What is the benefit of using external routes on Slate?
","By default, a route will only expose your https://docs.olcf.ornl.gov/systems/route.html#slate_services to NCCS networks. If you need your service exposed to the world outside ORNL, you will first need to get your project approved for external routes. To do this, submit a systems ticket. In the description, give us your project name and a brief reasoning for why exposing externally is needed.

We will let you know once your project is able to set up external routes.",4.28547319552503
"What is the benefit of using external routes on Slate?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.245304026229512
"What is the advantage of using an interactive job when building the Singularity image?
","A common use of interactive batch is to aid in debugging efforts.  interactive access to compute resources allows the ability to run a process to the point of failure; however, unlike a batch job, the process can be restarted after brief changes are made without losing the compute resource pool; thus speeding up the debugging effort.

Because interactive jobs must sit in the queue until enough resources become available to allocate, it is useful to know when a job can start.",4.2135356468515885
"What is the advantage of using an interactive job when building the Singularity image?
","Debugging

A common use of interactive batch is to aid in debugging efforts.  interactive access to compute resources allows the ability to run a process to the point of failure; however, unlike a batch job, the process can be restarted after brief changes are made without losing the compute resource pool; thus speeding up the debugging effort.

Choosing a Job Size

Because interactive jobs must sit in the queue until enough resources become available to allocate, it is useful to know when a job can start.",4.209560650984044
"What is the advantage of using an interactive job when building the Singularity image?
","Most users will find batch jobs to be the easiest way to interact with the system, since they permit you to hand off a job to the scheduler and then work on other tasks; however, it is sometimes preferable to run interactively on the system. This is especially true when developing, modifying, or debugging a code.",4.201942079262475
"How do I get my IP whitelisted?
","An easy way to locate your IP or range of IP addresses is to contact your local network administration team.  Your network administrator will be able to provide your individual IP or the ranges of IP addresses that you will use on the network.

Another way to find your IP is to use tools such as ‘whats my ip’. But please note, the tools may only return your internal IP. The IP you provide for the whitelist must be your external IP. The following are internal rages that cannot be used to whitelist your IP:

10.0. 0.0 - 10.255. 255.255 (10.0. 0.0/8 prefix)",4.348515486965624
"How do I get my IP whitelisted?
","https://docs.olcf.ornl.gov/systems/index.html#Whitelist your IPs<spi-whitelisting-ip>.  Access to the SPI resources is limited to IPs that have been whitelisted by the OLCF.  The only exception is for projects using KDI resources.  If your project also uses KDI resources, you will use the KDI access procedures and do not need to provide your IP to the OLCF.",4.229323976403969
"How do I get my IP whitelisted?
","10.0. 0.0 - 10.255. 255.255 (10.0. 0.0/8 prefix)

172.16. 0.0 - 172.31. 255.255 (172.16. 0.0/12 prefix)

192.168. 0.0 - 192.168. 255.255 (192.168. 0.0/16 prefix)

The tool may also return you current IP which may change if not static. For these reasons, reaching out to your IT department may be the best option. Your IT department can provide a range of externally facing IP addresses that can be whitelisted.",4.151768887356944
"How can I display the full link lines for the mpicc compiler without actually compiling?
","Fortran: mpifort, mpif77, mpif90

While these wrappers conveniently abstract away linking of Spectrum MPI, it's sometimes helpful to see exactly what's happening when invoked. The --showme flag will display the full link lines, without actually compiling:",4.245309432537495
"How can I display the full link lines for the mpicc compiler without actually compiling?
","Use the -craype-verbose flag to display the full include and link information used by the Cray compiler wrappers. This must be called on a file to see the full output (e.g., CC -craype-verbose test.cpp).

The MPI implementation available on Crusher is Cray's MPICH, which is ""GPU-aware"" so GPU buffers can be passed directly to MPI calls.



This section covers how to compile for different programming models using the different compilers covered in the previous section.

<string>:230: (INFO/1) Duplicate implicit target name: ""mpi"".",4.156624812477686
"How can I display the full link lines for the mpicc compiler without actually compiling?
","Use the -craype-verbose flag to display the full include and link information used by the Cray compiler wrappers. This must be called on a file to see the full output (e.g., CC -craype-verbose test.cpp).

The MPI implementation available on Spock is Cray's MPICH, which is ""GPU-aware"" so GPU buffers can be passed directly to MPI calls.



This section covers how to compile for different programming models using the different compilers covered in the previous section.

<string>:266: (INFO/1) Duplicate implicit target name: ""mpi"".",4.141396865156652
"What is the size of the L2 cache in Summit?
","Most Summit nodes contain 512 GB of DDR4 memory for use by the POWER9 processors, 96 GB of High Bandwidth Memory (HBM2) for use by the accelerators, and 1.6TB of non-volatile memory that can be used as a burst buffer. A small number of nodes (54) are configured as ""high memory"" nodes. These nodes contain 2TB of DDR4 memory, 192GB of HBM2, and 6.4TB of non-volatile memory.",4.252787163424743
"What is the size of the L2 cache in Summit?
",For Summit:,4.205051586881096
"What is the size of the L2 cache in Summit?
","Summit is connected to an IBM Spectrum Scale™ filesystem providing 250PB of storage capacity with a peak write speed of 2.5 TB/s. Summit also has access to the center-wide NFS-based filesystem (which provides user and project home areas) and has access to the center’s High Performance Storage System (HPSS) for user and project archival storage.

Summit is running Red Hat Enterprise Linux (RHEL) version 8.2.",4.196704151096814
"Can I request a temporary user account extension for all of my data?
","The user data retention policy exists to reclaim storage space after a user account is deactivated, e.g., after the user’s involvement on all OLCF projects concludes. By default, the OLCF will retain data in user-centric storage areas only for a designated amount of time after the user’s account is deactivated. During this time, a user can request a temporary user account extension for data access. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for details on retention timeframes for each user-centric storage area.",4.107791655178545
"Can I request a temporary user account extension for all of my data?
","The project data retention policy exists to reclaim storage space after a project ends. By default, the OLCF will retain data in project-centric storage areas only for a designated amount of time after the project end date. During this time, a project member can request a temporary user account extension for data access. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for details on purge and retention timeframes for each project-centric storage area.",4.060823324722256
"Can I request a temporary user account extension for all of my data?
","If you need an exception to the limits listed in the table above, such as a higher quota in your User/Project Home or a purge exemption in a Member/Project/World Work area, contact help@olcf.ornl.gov with a summary of the exception that you need.

By default, the OLCF does not guarantee lifetime data retention on any OLCF resources. Following a user account deactivation or project end, user and project data in non-purged areas will be retained for 90 days. After this timeframe, the OLCF retains the right to delete data. Data in purged areas remains subject to normal purge policies.",4.058368629464477
"How many nodes have a RankCore of 0?
","Rank: 1; RankCore: 16; Thread: 0; ThreadCore: 16; Hostname: a33n06; OMP_NUM_PLACES: {16},{20},{24},{28}
Rank: 1; RankCore: 16; Thread: 1; ThreadCore: 20; Hostname: a33n06; OMP_NUM_PLACES: {16},{20},{24},{28}
Rank: 1; RankCore: 16; Thread: 2; ThreadCore: 24; Hostname: a33n06; OMP_NUM_PLACES: {16},{20},{24},{28}
Rank: 1; RankCore: 16; Thread: 3; ThreadCore: 28; Hostname: a33n06; OMP_NUM_PLACES: {16},{20},{24},{28}

...",4.095981725238194
"How many nodes have a RankCore of 0?
","Rank:   12; NumRanks: 24; RankCore:   0; Hostname: a33n05; GPU: 0, 1, 2
Rank:   13; NumRanks: 24; RankCore:   4; Hostname: a33n05; GPU: 0, 1, 2
Rank:   14; NumRanks: 24; RankCore:   8; Hostname: a33n05; GPU: 0, 1, 2
Rank:   15; NumRanks: 24; RankCore:  12; Hostname: a33n05; GPU: 0, 1, 2
Rank:   16; NumRanks: 24; RankCore:  16; Hostname: a33n05; GPU: 0, 1, 2
Rank:   17; NumRanks: 24; RankCore:  20; Hostname: a33n05; GPU: 0, 1, 2",4.093956647539247
"How many nodes have a RankCore of 0?
","Rank:    6; NumRanks: 12; RankCore:   0; Hostname: h41n03; GPU: 0
Rank:    7; NumRanks: 12; RankCore:   4; Hostname: h41n03; GPU: 1
Rank:    8; NumRanks: 12; RankCore:   8; Hostname: h41n03; GPU: 2
Rank:    9; NumRanks: 12; RankCore:  88; Hostname: h41n03; GPU: 3
Rank:   10; NumRanks: 12; RankCore:  92; Hostname: h41n03; GPU: 4
Rank:   11; NumRanks: 12; RankCore:  96; Hostname: h41n03; GPU: 5",4.093229103189676
"Can project delegates and overseers delegate their responsibilities to others?
","controlling priority

monitoring progress

Similar to individual user accounts being granted resources by their association with a project, individual user accounts are granted the resources of a subproject upon association with the subproject.

Subprojects do not inherit the accesses of their primary projects, and users can be associated with a subproject without association with the primary project.",4.034060727929051
"Can project delegates and overseers delegate their responsibilities to others?
",holding up its resources). And the same argument can be made for the other job steps.,4.010726011887559
"Can project delegates and overseers delegate their responsibilities to others?
","DD – Director’s Discretion (DD) projects are dedicated to leadership computing preparation, INCITE and ALCC scaling, and application performance to maximize scientific application efficiency and productivity on leadership computing platforms. The OLCF Resource Utilization Council, as well as independent referees, review and approve all DD requests. Applications are accepted year-round via the OLCF Director's Discretion Project Application. Select ""OLCF Director's Discretionary Project"" from the drop down menu to begin.",3.9935887158915047
"How do I move data from a NumPy array to a CuPy array?
","As is the standard with NumPy being imported as ""np"", CuPy is often imported in a similar fashion:

>>> import numpy as np
>>> import cupy as cp

Similar to NumPy arrays, CuPy arrays can be declared with the cupy.ndarray class. NumPy arrays will be created on the CPU (the ""host""), while CuPy arrays will be created on the GPU (the ""device""):

>>> x_cpu = np.array([1,2,3])
>>> x_gpu = cp.array([1,2,3])

Manipulating a CuPy array can also be done in the same way as manipulating NumPy arrays:",4.287123975954315
"How do I move data from a NumPy array to a CuPy array?
","CuPy is a library that implements NumPy arrays on NVIDIA GPUs by utilizing CUDA Toolkit libraries like cuBLAS, cuRAND, cuSOLVER, cuSPARSE, cuFFT, cuDNN and NCCL. Although optimized NumPy is a significant step up from Python in terms of speed, performance is still limited by the CPU (especially at larger data sizes) -- this is where CuPy comes in. Because CuPy's interface is nearly a mirror of NumPy, it acts as a replacement to run existing NumPy/SciPy code on NVIDIA CUDA platforms, which helps speed up calculations further. CuPy supports most of the array operations that NumPy provides,",4.269888853502351
"How do I move data from a NumPy array to a CuPy array?
","most of the array operations that NumPy provides, including array indexing, math, and transformations. Most operations provide an immediate speed-up out of the box, and some operations are sped up by over a factor of 100 (see CuPy benchmark timings below, from the Single-GPU CuPy Speedups article).",4.220188482911941
"How does CITADEL manage private data?
","Although the facility already adheres to the National Institute of Standards and Technology’s security and privacy controls for moderate Official Use Only data, CITADEL was crafted to enforce security measures for handling vast datasets that encompass types of data necessitating heightened privacy safeguards on systems overseen by the Oak Ridge Leadership Computing Facility (OLCF). Extra precautions have been taken to manage private data such that it cannot be accessed by other researchers or used by other projects. For example, HIPAA-protected data for a project sponsored by the VA will be",4.328702577713124
"How does CITADEL manage private data?
","With the CITADEL framework, researchers can use the OLCF’s large HPC resources including Frontier and Summit to compute data containing protected health information (PHI), personally identifiable information (PII), data protected under International Traffic in Arms Regulations, and other types of data that require privacy.",4.30959863329545
"How does CITADEL manage private data?
","The NCCS CITADEL security framework was originally conceived to facilitate the large-scale analysis of protected health information (PHI) data from the US Department of Veterans Affairs' (VA) Million Veteran Program. The NCCS SPI team, with assistance from ORNL Risk Management and ORNL’s Information Technology Services Division (ITSD), refined the initial prototype and expanded CITADEL's capabilities to accommodate a diverse array of programs, projects, and sponsors.",4.211118859311055
"What is the role of the PI in granting access to a project?
","Upon completion of the above steps, the PI will be notified that the project has been created, and provided with the project ID and system allocation details. At this time, project participants may apply for user accounts.",4.393703890086002
"What is the role of the PI in granting access to a project?
","4. For codes being used, please make sure that the proper number of licenses have been obtained from the vendors of the respective software. It is also the responsibility of the PI to ensure that all project members have the appropriate authorization to access any sensitive data and/or codes used as required by relevant data use agreements.",4.354688561210714
"What is the role of the PI in granting access to a project?
","When our accounts team begins processing your application, you will receive an automated email containing an unique 36-character confirmation code. Make note of it; you can use it to check the status of your application at any time.

The principal investigator (PI) of the project must approve your account and system access. We will make the project PI aware of your request.",4.341033288916747
"How can I restore a specific saved user collection in Andes?
",For Andes:,4.091342448204261
"How can I restore a specific saved user collection in Andes?
","| Command | Description | | --- | --- | | module restore NAME | Recalls a specific saved user collection titled ""NAME"" | | module restore | Recalls the user-defined defaults | | module reset | Resets loaded modules to system defaults | | module restore system | Recalls the system defaults | | module savelist | Shows the list user-defined saved collections |

You should use unique names when creating collections to specify the application (and possibly branch) you are working on. For example, app1-development, app1-production, and app2-production.",4.001357776296312
"How can I restore a specific saved user collection in Andes?
","| Command | Description | | --- | --- | | module restore NAME | Recalls a specific saved user collection titled ""NAME"" | | module restore | Recalls the user-defined defaults | | module reset | Resets loaded modules to system defaults | | module restore system | Recalls the system defaults | | module savelist | Shows the list user-defined saved collections |

You should use unique names when creating collections to specify the application (and possibly branch) you are working on. For example, app1-development, app1-production, and app2-production.",4.001282701555957
"What happens if HSA_XNACK=0 in Crusher?
","XNACK (pronounced X-knack) refers to the AMD GPU's ability to retry memory accesses that fail due to a page fault. The XNACK mode of an MI250X can be changed by setting the environment variable HSA_XNACK before starting a process that uses the GPU. Valid values are 0 (disabled) and 1 (enabled), and all processes connected to a GPU must use the same XNACK setting. The default MI250X on Crusher is HSA_XNACK=0.",4.30778919987549
"What happens if HSA_XNACK=0 in Crusher?
","XNACK (pronounced X-knack) refers to the AMD GPU's ability to retry memory accesses that fail due to a page fault. The XNACK mode of an MI250X can be changed by setting the environment variable HSA_XNACK before starting a process that uses the GPU. Valid values are 0 (disabled) and 1 (enabled), and all processes connected to a GPU must use the same XNACK setting. The default MI250X on Frontier is HSA_XNACK=0.",4.22092035257484
"What happens if HSA_XNACK=0 in Crusher?
",HSA_XNACK=1 Automatic Page Migration Enabled,4.20632276278414
"How can I visualize my data on Frontier's Andes cluster?
","Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-user-guide.

For a full list of software available at the OLCF, please see the Software section (coming soon).",4.415950486530315
"How can I visualize my data on Frontier's Andes cluster?
","The above procedure can also be followed to connect to Summit or Frontier, with the main difference being the number of available processors. The time limit syntax for Andes, Summit, and Frontier also differ. Summit uses the format HH:MM while Andes and Frontier follow HH:MM:SS.

Please do not run VisIt's GUI client from an OLCF machine. You will get much better performance if you install a client on your workstation and launch locally. You can directly connect to OLCF machines from inside VisIt and access your data remotely.",4.303980547838958
"How can I visualize my data on Frontier's Andes cluster?
","VisIt is now available on Frontier through the UMS022 module.

VisIt 3.3.3 is now available on Andes.

VisIt is an interactive, parallel analysis and visualization tool for scientific data. VisIt contains a rich set of visualization features so you can view your data in a variety of ways. It can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes. Further information regarding VisIt can be found at the links provided in the https://docs.olcf.ornl.gov/systems/visit.html#visit-resources section.",4.282237323053921
"How can I run the converted image using Singularity?
","Users can build container images with Podman, convert it to a SIF file, and run the container using the Singularity runtime. This page is intended for users with some familiarity with building and running containers.

Users will make use of two applications on Summit - Podman and Singularity - in their container build and run workflow. Both are available without needing to load any modules.",4.285878724721312
"How can I run the converted image using Singularity?
","Singularity is a container framework from Sylabs. On Summit, we will use Singularity solely as the runtime. We will convert the tar files of the container images Podman creates into sif files, store those sif files on GPFS, and run them with Jsrun. Singularity also allows building images but ordinary users cannot utilize that on Summit due to additional permissions not allowed for regular users.",4.244141020379831
"How can I run the converted image using Singularity?
","You can now create a Singularity sif file with singularity build --disable-cache --docker-login simple.sif docker://docker.io/<username>/simple.

This will ask you to enter your Docker username and password again for Singularity to download the image from Dockerhub and convert it to a sif file.",4.197752388558939
"How do I know which version of SLURM is running on Crusher?
","This section describes how to run programs on the Crusher compute nodes, including a brief overview of Slurm and also how to map processes and threads to CPU cores and GPUs.

Slurm is the workload manager used to interact with the compute nodes on Crusher. In the following subsections, the most commonly used Slurm commands for submitting, running, and monitoring jobs will be covered, but users are encouraged to visit the official documentation and man pages for more information.",4.361065383201907
"How do I know which version of SLURM is running on Crusher?
","Slurm provides 3 ways of submitting and launching jobs on Crusher's compute nodes: batch scripts, interactive, and single-command. The Slurm commands associated with these methods are shown in the table below and examples of their use can be found in the related subsections. Please note that regardless of the submission method used, the job will launch on compute nodes, with the first compute in the allocation serving as head-node.

| sbatch |  | | --- | --- | | salloc |  | | srun |  |",4.288436440279001
"How do I know which version of SLURM is running on Crusher?
","Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.",4.166213892544135
"What is the purpose of the t7 and t8 variables?
",Environment variables to be used during compilation through the environment variable TAU_OPTIONS,3.946948148416208
"What is the purpose of the t7 and t8 variables?
",performance data. Score-P and TAU generate OTF2 trace files for Vampir to visualize.,3.9335275375130414
"What is the purpose of the t7 and t8 variables?
",performance data. Score-P and TAU generate OTF2 trace files for Vampir to visualize.,3.9335275375130414
"How can I specify the username and host for a Paraview session?
","After installing, you must give ParaView the relevant server information to be able to connect to OLCF systems (comparable to VisIt's system of host profiles). The following provides an example of doing so. Although several methods may be used, the one described should work in most cases.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.

For Windows clients, it is necessary to install PuTTY to create an ssh connection in step 2.

Step 1: Save the following servers.pvsc file to your local computer",4.270588096310698
"How can I specify the username and host for a Paraview session?
","You can modify settings relevant to this host machine. For example, you can change the ""Username"" field if your OLCF username differs from your local computer username.

Once you have made your changes, press the ""Apply"" button, and then save the settings (Options/Save Settings).

Each host can have several launch profiles. A launch profile specifies how VisIt runs on a given host computer. To make changes to a host's launch profile, do the following:

Go to ""Options→Host Profiles"".

Select the host in the left side of the window.",4.255202695676558
"How can I specify the username and host for a Paraview session?
","<String default=""YOURUSERNAME""/>
        </Option>
        <Switch name=""PV_CLIENT_PLATFORM"">
          <Case value=""Apple"">
            <Set name=""TERM_PATH"" value=""/opt/X11/bin/xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />
            <Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Linux"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.190319108459073
"How do I expose my route on the external router in Slate?
","By default, a route will only expose your https://docs.olcf.ornl.gov/systems/route.html#slate_services to NCCS networks. If you need your service exposed to the world outside ORNL, you will first need to get your project approved for external routes. To do this, submit a systems ticket. In the description, give us your project name and a brief reasoning for why exposing externally is needed.

We will let you know once your project is able to set up external routes.",4.3571327526195525
"How do I expose my route on the external router in Slate?
","Under metadata, add a label for ccs.ornl.gov/externalRoute: 'true' as shown below and click the Save button at the bottom of the page.

Route After

After saving, your route will be exposed on two routers, default and external. This means your service is now accessible from outside ORNL. Note that if your project has not yet been approved for external routing, this second router will not expose your route.

Route Exposed",4.318762057255307
"How do I expose my route on the external router in Slate?
","Once your project has been approved, you only need to give your route a label to tell the OpenShift router to expose this service externally. You can do this in the CLI or in the web interface.

<string>:249: (INFO/1) Duplicate implicit target name: ""cli"".

On the CLI, run oc label route {ROUTE_NAME} ccs.ornl.gov/externalRoute=true.

In the web interface, from the side menu, select Networking, then Routes.

Routes Menu

This will show a list of your routes. Click the route you want to expose, and click the YAML tab.",4.2999343962272825
"Can you explain the concept of managed memory in Summit?
",https://vimeo.com/431954101 | | 2020-06-18 | CUDA Managed Memory | Bob Crovella (NVIDIA) | CUDA Managed Memory https://www.olcf.ornl.gov/calendar/cuda-managed-memory/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/06/06_Managed_Memory.pdf https://vimeo.com/431616420 | | 2020-06-03 | Summit Tips & Tricks | Tom Papatheodore (OLCF) | 2020 OLCF User Meeting (Summit New User Training) https://www.olcf.ornl.gov/calendar/2020-olcf-user-meeting/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/Summit_Tips_and_Tricks_2020-06-03.pdf,4.212623282321829
"Can you explain the concept of managed memory in Summit?
","Unified memory is a single virtual address space that is accessible to any processor in a system (within a node). This means that programmers only need to allocate a single unified-memory pointer (e.g. using cudaMallocManaged) that can be accessed by both the CPU and GPU, instead of requiring separate allocations for each processor. This ""managed memory"" is automatically migrated to the accessing processor, which eliminates the need for explicit data transfers.",4.209179866243637
"Can you explain the concept of managed memory in Summit?
","Most Summit nodes contain 512 GB of DDR4 memory for use by the POWER9 processors, 96 GB of High Bandwidth Memory (HBM2) for use by the accelerators, and 1.6TB of non-volatile memory that can be used as a burst buffer. A small number of nodes (54) are configured as ""high memory"" nodes. These nodes contain 2TB of DDR4 memory, 192GB of HBM2, and 6.4TB of non-volatile memory.",4.185375670543231
"Can I use Home.ccs.ornl.gov for machine learning tasks?
","home.ccs.ornl.gov (Home) is a general purpose system that can be used to log into other OLCF systems that are not directly accessible from outside the OLCF network. For example, running the screen or tmux utility is one common use of Home. Compiling, data transfer, or executing long-running or memory-intensive tasks should never be performed on Home.



Home access is automatically granted to all enabled OLCF users.

To connect to Home, SSH to home.ccs.ornl.gov. For example:

ssh username@home.ccs.ornl.gov",4.323629761577776
"Can I use Home.ccs.ornl.gov for machine learning tasks?
","ssh username@home.ccs.ornl.gov

For more information on connecting to OLCF resources, see https://docs.olcf.ornl.gov/systems/home_user_guide.html#connecting-to-olcf.

The Home system should only be used to access systems within the OLCF network. The following are examples of appropriate uses of Home:

RSA SecurID Token setup

SSH

Vi and other non-GUI editors

Screen or other terminal multiplexers

The following are examples of inappropriate uses of Home:

Compiling

Data Transfers

Long-running or memory-intensive processes",4.186120342158616
"Can I use Home.ccs.ornl.gov for machine learning tasks?
",| (slides | recording | tutorial site) https://www.olcf.ornl.gov/wp-content/uploads/AI-For-Science-at-Scale-Introduction.pdf https://vimeo.com/836918490 https://github.com/olcf/ai-training-series/tree/main/ai_at_scale | | 2023-05-31 | OLCF Storage and Orion Best Practices | Suzanne Parete-Koon and Jesse Hanley (OLCF) | May 2023 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-may2023/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/May2023_Usercall_OLCFStorage.pdf https://vimeo.com/833369509 | | 2023-05-24 | Julia for High Performance Computing,4.123182947602532
"How can I get the latest list of additional steps required for using SBCAST with certain libraries?
","Notice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable. Once libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.

As mentioned above, you can alternatively use --exclude=NONE on sbcast to send all libraries along with the binary. Using --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time. A job script for the previous example, modified for sending all libraries is shown below.",4.115230559471918
"How can I get the latest list of additional steps required for using SBCAST with certain libraries?
","Notice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable. Once libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.

As mentioned above, you can use --exclude=NONE on sbcast to send all libraries along with the binary. Using --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time. A job script for the previous example, modified for sending all libraries is shown below.",4.107115239091782
"How can I get the latest list of additional steps required for using SBCAST with certain libraries?
","# If you SBCAST **all** your libraries (ie, `--exclude=NONE`), you may use the following line:
#export LD_LIBRARY_PATH=""/mnt/bb/$USER/${exe}_libs:$(pkg-config --variable=libdir libfabric)""
# Use with caution -- certain libraries may use ``dlopen`` at runtime, and that is NOT covered by sbcast
# If you use this option, we recommend you contact OLCF Help Desk for the latest list of additional steps required",4.0978944548614695
"What is the purpose of the `cray-mpich` command on Frontier?
","| Implementation | Module | Compiler | Header Files & Linking | | --- | --- | --- | --- | | Cray MPICH | cray-mpich | cc, CC, ftn (Cray compiler wrappers) | MPI header files and linking is built into the Cray compiler wrappers | | hipcc |  |

hipcc requires the ROCm Toolclain, See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#exposing-the-rocm-toolchain-to-your-programming-environment

To use GPU-aware Cray MPICH, with Frontier's PrgEnv modules, users must set the following modules and environment variables:

If using PrgEnv-amd:

module load craype-accel-amd-gfx90a",4.355026280698756
"What is the purpose of the `cray-mpich` command on Frontier?
","module load craype-accel-amd-gfx90a

export MPICH_GPU_SUPPORT_ENABLED=1

If using PrgEnv-cray:

module load craype-accel-amd-gfx90a
module load amd-mixed

export MPICH_GPU_SUPPORT_ENABLED=1

There are extra steps needed to enable GPU-aware MPI on Frontier, which depend on the compiler that is used (see 1. and 2. below).

When using GPU-aware Cray MPICH with the Cray compiler wrappers, most of the needed libraries are automatically linked through the environment variables.

Though, the following header files and libraries must be included explicitly:",4.345516393117071
"What is the purpose of the `cray-mpich` command on Frontier?
","| Implementation | Module | Compiler | Header Files & Linking | | --- | --- | --- | --- | | Cray MPICH | cray-mpich | cc, CC, ftn (Cray compiler wrappers) | MPI header files and linking is built into the Cray compiler wrappers | | hipcc |  |

To use GPU-aware Cray MPICH, users must set the following modules and environment variables:

module load craype-accel-amd-gfx90a
module load rocm

export MPICH_GPU_SUPPORT_ENABLED=1

There are extra steps needed to enable GPU-aware MPI on Crusher, which depend on the compiler that is used (see 1. and 2. below).",4.276894879502923
"How do I get started with Parsl on Summit?
","Parsl is a flexible and scalable parallel programming library for Python which is being developed at the University of Chicago. It augments Python with simple constructs for encoding parallelism. For more information about Parsl, please refer to its documentation.

Parsl can be installed with Conda for use on Summit by running the following from a login node:

$ module load workflows
$ module load parsl/1.1.0

The following instructions illustrate how to run a ""Hello world"" program with Parsl on Summit.",4.33476133811128
"How do I get started with Parsl on Summit?
","Parsl needs to be able to write to the working directory from compute nodes, so we will work from within the member work directory and assume a project ID ABC123:

$ mkdir -p ${MEMBERWORK}/abc123/parsl-demo/
$ cd ${MEMBERWORK}/abc123/parsl-demo/

To run an example ""Hello world"" program with Parsl on Summit, create a file called hello-parsl.py with the following contents, but with your own project ID in the line specified:",4.27489406399436
"How do I get started with Parsl on Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.239657969199472
"How do I know which Node ID to use for port forwarding?
","Port Forwarding

ssh -L <localport>:<Node ID>:<Remote port>  <USERID>@summit.olcf.ornl.gov

The local port number can be any unused port number on your local machine...try a number between 30000-30030.  To check if the port you picked is open run:    $ netstat -ab | grep ""<selected port number>""  #This can take a minute to return anything. If nothing is returned, your selected port is open

After submitting the port forward command as seen above, it will ask for your login password to access Summit. Leave this terminal window open!

Launch the Vampir GUI on your local machine",4.16995014820403
"How do I know which Node ID to use for port forwarding?
","In general, using FQDNs to access a service is more convenient. If your service communicates over HTTP or HTTPS, you can set up a https://docs.olcf.ornl.gov/systems/services.html#slate_routes to achieve this. If your service uses another protocol, you can use https://docs.olcf.ornl.gov/systems/services.html#slate_nodeports.

NodePort is a type of Service that reserves a port across the cluster that can route traffic to your pods. This is more flexible than a Route and can handle any TCP or UDP traffic.",4.153148242134827
"How do I know which Node ID to use for port forwarding?
","Note that a NodePort value will automatically be given by the service controller.

Your service can then be accessed by the scheme apps.{cluster}.ccs.ornl.gov:{nodePort}.

In this example, if the service was running on marble, I could access it with apps.marble.ccs.ornl.gov:30298",4.14511448437146
"What is the significance of December 18th in the 2023 OLCF System Changes?
",| Date | Event | | --- | --- | | https://docs.olcf.ornl.gov/systems/2023_olcf_system_changes.html#Sep 19<summit_proposals_open> | Submission system opens for 2024 Summit proposals. | | https://docs.olcf.ornl.gov/systems/2023_olcf_system_changes.html#Dec 18<summit_last_day_batch> | Last day to execute batch jobs on Summit. | | https://docs.olcf.ornl.gov/systems/2023_olcf_system_changes.html#Dec 18<andes_last_day_batch> | Last day to use Alpine from Andes. | | https://docs.olcf.ornl.gov/systems/2023_olcf_system_changes.html#Dec 19<alpine_readonly> | Alpine becomes read-only and available only,4.252374575448256
"What is the significance of December 18th in the 2023 OLCF System Changes?
","| | 2021-12-08 | Analysis Tools at OLCF | Benjamin Hernandez (OLCF) | December 2021 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-dec2021/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/08/Dec_2021_Analysis_Tools_at_OLCF.pdf https://vimeo.com/654969964 | | 2021-11-12 | Introduction to Leadership Computing | Bronson Messer, Tom Papatheodore | Introduction to Leadership Computing https://www.olcf.ornl.gov/introduction-to-leadership-computing/ | (slides | recording) https://www.olcf.ornl.gov/introduction-to-leadership-computing/",4.178831891569589
"What is the significance of December 18th in the 2023 OLCF System Changes?
","Timeline

Proposals accepted beginning September 19

Proposals will undergo review and the OLCF will notify awardees in mid-to-late November.

Projects are anticipated to start in mid-to-late January 2024.



Your project may continue to submit jobs on Summit through your current project's end date (which varies by allocation program) or December 18th (whichever comes first).  The last day batch jobs from current projects will run on Summit is December 18, 2023.",4.167280176625145
"Can a user have more than 2 jobs in the ""running"" state at any given time if they have completed jobs that have not yet been removed from the queue?
","Limit of (4) eligible-to-run jobs per user.

Jobs in excess of the per user limit above will be placed into a held state, but will change to eligible-to-run at the appropriate time.

Users may have only (100) jobs queued in the batch queue at any state at any time. Additional jobs will be rejected at submit time.

The eligible-to-run state is not the running state. Eligible-to-run jobs have not started and are waiting for resources. Running jobs are actually executing.",4.442358607463227
"Can a user have more than 2 jobs in the ""running"" state at any given time if they have completed jobs that have not yet been removed from the queue?
","following policies:



-  Limit of (4) *eligible-to-run* jobs per user.

-  Jobs in excess of the per user limit above will be placed into a

*held* state, but will change to eligible-to-run at the appropriate

time.

-  Users may have only (2) jobs in bin 5 *running* at any time. Any

additional jobs will be blocked until one of the running jobs

completes.



.. note::

The *eligible-to-run* state is not the *running* state.

Eligible-to-run jobs have not started and are waiting for resources.

Running jobs are actually executing.



``killable`` Queue Policy

-------------------------",4.283699510784511
"Can a user have more than 2 jobs in the ""running"" state at any given time if they have completed jobs that have not yet been removed from the queue?
","same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. Additionally, projects that are at 125% of their allocated time will be limited to only 3 running jobs at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.218125708205228
"How can I get the token for the Docker registry?
","There might be an image built locally that you would like to have in your OpenShift project. It is possible to add this image to your project by adding it to the Docker registry of the cluster that your project is on.

First, copy your login token. We will need this for the next step.

oc login https://api.<cluster>.ccs.ornl.gov --token=<COPY THIS TOKEN>

Next, log into the Docker registry. Use your copied token when prompted for your password. Upon succesful login, a message saying so will appear.

docker login -u <NCCS USERNAME> registry.apps.<cluster>.ccs.ornl.gov",4.271721909935515
"How can I get the token for the Docker registry?
","docker push registry.apps.<cluster>.ccs.ornl.gov/<namespace>/<image>:<tag>

OpenShift has an integrated container registry that can be accessed from outside the cluster to push and pull images as well as run containers.

This assumes that you have Docker installed locally. Installing Docker is outside of the scope of this documentation.

First you have to log into OpenShift

oc login https://api.<cluster>.ccs.ornl.gov

Next you can use your token to log into the integrated registry.

docker login -u user -p $(oc whoami -t) registry.apps.<cluster>.ccs.ornl.gov",4.203092178982483
"How can I get the token for the Docker registry?
","of the CI/CD settings. In the ""Specific Runners"" area, the registration token should be available for retrieval.",4.152171545517877
"What is the purpose of the BEGIN_INSTRUMENT_SECTION and END_INSTRUMENT_SECTION in TAU?
","TAU is a portable profiling and tracing toolkit that supports many programming languages. The instrumentation can be done by inserting in the source code using an automatic tool based on the Program Database Toolkit (PDT), on the compiler instrumentation, or manually using the instrumentation API.

Webpage: https://www.cs.uoregon.edu/research/tau/home.php",4.164063600721129
"What is the purpose of the BEGIN_INSTRUMENT_SECTION and END_INSTRUMENT_SECTION in TAU?
","export TAU_OPTIONS=“-optTauSelectFile=phase.tau”

Now when you instrument your application, the phase called phase 1 are the lines 300-327 of the file miniWeather_mpi.cpp. Every call will be instrumented. This could create signiificant overhead, thus you should be careful when you use it.

Create a file called phases.tau.

BEGIN_INSTRUMENT_SECTION
static phase name=""phase1"" file=""miniWeather_mpi.cpp"" line=300 to line=327
static phase name=""phase2"" file=""miniWeather_mpi.cpp"" line=333 to line=346
END_INSTRUMENT_SECTION

Declare the TAU_OPTIONS variable.",4.161516765623762
"What is the purpose of the BEGIN_INSTRUMENT_SECTION and END_INSTRUMENT_SECTION in TAU?
","Declare the TAU_OPTIONS variable

export TAU_OPTIONS=“-optTauSelectFile=select.tau”

Now, the routine sort*(int *) is excluded from the instrumentation.

Create a file called phase.tau.

BEGIN_INSTRUMENT_SECTION
dynamic phase name=“phase1” file=“miniWeather_mpi.cpp” line=300 to line=327
END_INSTRUMENT_SECTION

Declare the TAU_OPTIONS variable.

export TAU_OPTIONS=“-optTauSelectFile=phase.tau”",4.11620367956058
"Can I use the Burst Buffer for temporary storage during my application's execution?
","the time that applications wait for I/O. Using an SSD drive per compute node, the burst buffer will be used to transfer data to or from the drive before the application reads a file or after it writes a file.  The result will be that the application benefits from native SSD performance for a portion of its I/O requests. Users are not required to use the NVMes.  Data can also be written directly to the parallel filesystem.",4.24278524941673
"Can I use the Burst Buffer for temporary storage during my application's execution?
",NVMes will need to be copied back to the parallel filesystem before the job ends. This largely manual mode of usage will not be the recommended way to use the burst buffer for most applications because tools are actively being developed to automate and improve the NVMe transfer and data management process. Here are the basic steps for using the BurstBuffers in their current limited mode of usage:,4.187272111030941
"Can I use the Burst Buffer for temporary storage during my application's execution?
","Tools for using the burst buffers are still under development.  Currently, the user will have access to a writeable directory on each node's NVMe and then explicitly move data to and from the NVMes with posix commands during a job. This mode of usage only supports writing file-per-process or file-per-node. It does not support automatic ""n to 1"" file writing, writing from multiple nodes to a single file.  After a job completes the NVMes are trimmed, a process that irreversibly deletes data from the devices, so all desired data from the NVMes will need to be copied back to the parallel",4.157028075173624
"How do I know which libraries are required by my application on Frontier?
",This section of the Summit User Guide is intended to show current OLCF users how to start preparing their applications to run on the upcoming Frontier system. We will continue to add more topics to this section in the coming months. Please see the topics below to get started.,4.137482725604929
"How do I know which libraries are required by my application on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.109854191065633
"How do I know which libraries are required by my application on Frontier?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers. Please see the following https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more detailed inforation on how to compile using these modules.",4.067487015669957
"Can sensitive data be shared with individuals outside of the user's group?
","Never allow access to your sensitive data to anyone outside of your group.

Transfer of sensitive data must be through the use encrypted methods (scp, sftp, etc).

All sensitive data must be removed from all OLCF resources when your project has concluded.",4.345497422492476
"Can sensitive data be shared with individuals outside of the user's group?
","Portions of data and/or software used in your project require extra protections due to requirements for protecting HIPAA/ITAR or other sensitive or controlled information. There are countries from which citizens are restricted from accessing sensitive/controlled information and therefore cannot be a part of your project. When you request users to be added to your project, our user assistance center will check the nationality of those users for conflict.",4.180637287182499
"Can sensitive data be shared with individuals outside of the user's group?
","Sensitive or Restricted Information
  Principal Investigators are responsible for knowing whether their project
  uses or generates sensitive or restricted information. Department of Energy
  systems contain data only related to scientific research.

  Sensitive Information: This includes, but is not limited to, personally-identifiable
  information (PII). PII is information that can be used to distinguish or trace an
  individual's identity, either alone or when combined with other information
  that is linked or linkable to a specific individual.",4.141471630975735
"What is the purpose of the FAULT_TOLERANCE parameter in the ~/.jsm.conf file?
","If you get an error message that looks like:

A received msg header indicates a size that is too large:
 Requested size: 25836785
 Size limit: 16777216
If you believe this msg is legitimate, please increase the
max msg size via the ptl_base_max_msg_size parameter.

This can be resolved by setting export PMIX_MCA_ptl_base_max_msg_size=18 where the value is size in MB. Setting it to 18 or higher usually works. The default if its not explicitly set is around 16 MB.

Adding FAULT_TOLERANCE=1 in your individual ~/.jsm.conf file, will result in LSF jobs failing to successfully start.",4.09831008664948
"What is the purpose of the FAULT_TOLERANCE parameter in the ~/.jsm.conf file?
","In addition, to debug slow startup JSM provides the option to create a progress file. The file will show information that can be helpful to pinpoint if a specific node is hanging or slowing down the job step launch. To enable it, you can use: jsrun --progress ./my_progress_file_output.txt.

When using file-based specification of resource sets with jsrun -U, the -a flag (number of tasks per resource set) is ignored. This has been reported to IBM and they are investigating. It is generally recommended to use jsrun explicit resource files (ERF) with --erf_input and --erf_output instead of -U.",4.017949818857757
"What is the purpose of the FAULT_TOLERANCE parameter in the ~/.jsm.conf file?
","Explicit Resource Files provide even more fine-granied control over how processes are mapped onto compute nodes. Users have reported errors when using ERF on Summit:

Failed to bind process to ERF smt array, err: Invalid argument

This is a known issue with the current version of jsrun. A workaround is to add the following lines in your job script.

export JSM_ROOT=/gpfs/alpine/stf007/world-shared/vgv/inbox/jsm_erf/jsm-10.4.0.4/opt/ibm/jsm
$JSM_ROOT/bin/jsm &
$JSM_ROOT/bin/jsrun --erf_input=Your_erf ./Your_app

If you get an error message that looks like:",4.009529654335346
"Set the proxy settings for the compute nodes to access the internet.
","export all_proxy=socks://proxy.ccs.ornl.gov:3128/
export ftp_proxy=ftp://proxy.ccs.ornl.gov:3128/
export http_proxy=http://proxy.ccs.ornl.gov:3128/
export https_proxy=http://proxy.ccs.ornl.gov:3128/
export no_proxy='localhost,127.0.0.0/8,*.ccs.ornl.gov'

These settings currently do not work for pyQuil; thus, when running pyQuil on the compute nodes, you are unable to connect to Rigetti's machines and can only run local simulators. To be able to connect to Rigetti's machines, you'll have to run on the login nodes instead.",4.27153380181031
"Set the proxy settings for the compute nodes to access the internet.
","cd $SLURM_SUBMIT_DIR
    date

    # Set proxy settings so compute nodes can reach internet (required when not using a simulator)
    # Currently, does not work properly with pyQuil
    export all_proxy=socks://proxy.ccs.ornl.gov:3128/
    export ftp_proxy=ftp://proxy.ccs.ornl.gov:3128/
    export http_proxy=http://proxy.ccs.ornl.gov:3128/
    export https_proxy=http://proxy.ccs.ornl.gov:3128/
    export no_proxy='localhost,127.0.0.0/8,*.ccs.ornl.gov'

    # Load python module and virtual environment
    module load python
    source activate base
    conda activate ENV_NAME",4.101877558275844
"Set the proxy settings for the compute nodes to access the internet.
","cd $SLURM_SUBMIT_DIR
    date

    # Set proxy settings so compute nodes can reach internet (required when not using a simulator)
    # Currently, does not work properly with pyQuil
    export all_proxy=socks://proxy.ccs.ornl.gov:3128/
    export ftp_proxy=ftp://proxy.ccs.ornl.gov:3128/
    export http_proxy=http://proxy.ccs.ornl.gov:3128/
    export https_proxy=http://proxy.ccs.ornl.gov:3128/
    export no_proxy='localhost,127.0.0.0/8,*.ccs.ornl.gov'

    # Load python module and virtual environment
    module load cray-python
    source $HOME/ENV_NAME/bin/activate",4.088334012602236
"Can Tensor Cores in Summit be used for other applications beyond iterative refinement techniques?
","Tensor Cores provide the potential for an enormous performance boost over full-precision operations, but when their use is appropriate is highly application and even problem independent. Iterative Refinement techniques can suffer from slow or possible a complete lack of convergence if the condition number of the matrix is very large. By using Tensor Cores, which support 32-bit accumulation, rather than strict 16-bit math operations, iterative refinement becomes a viable option in a much larger number of cases, so it should be attempted when an application is already using a supported solver.",4.463530663717538
"Can Tensor Cores in Summit be used for other applications beyond iterative refinement techniques?
","Some non-traditional uses of Tensor Cores can come from places where integers that fall within the FP16 range are used in an application. For instance, in “Attacking the Opioid Epidemic: Determining the Epistatic and Pleiotropic Genetic Architectures for Chronic Pain and Opioid Addiction,” a 2018 Gordon Bell Prize-winning paper, the authors used Tensor Cores in place of small integers, allowing them very high performance over performing the same calculation in integer space. This technique is certainly not applicable to all applications, but does show that Tensor Cores may be used in",4.35557462101046
"Can Tensor Cores in Summit be used for other applications beyond iterative refinement techniques?
",| (recording) https://vimeo.com/306440151 | | 2018-12-04 | Using V100 Tensor Cores | Jeff Larkin (NVIDIA) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Tensor-Cores.pdf https://vimeo.com/306437682 | | 2018-12-04 | NVIDIA Profilers | Jeff Larkin (NVIDIA) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Profilers.pdf,4.319059270793312
"How can I run Paraview in a terminal on a Mac?
","After installing, if you see a ""Can't open display"" or a ""DISPLAY is not set"" error, try restarting your computer. Sometimes XQuartz doesn't function properly if the computer was never restarted after installing.

If ParaView crashes when using the EGL version of the ParaView module via the command line and raises errors about OpenGL drivers or features, this is most likely due to not being connected to any GPUs.

Double check that you are either running on the GPU partition on Andes (i.e., -p gpu), or that you have -g set to a value greater than zero in your jsrun command on Summit.",4.284207452784038
"How can I run Paraview in a terminal on a Mac?
","After installing, you must give ParaView the relevant server information to be able to connect to OLCF systems (comparable to VisIt's system of host profiles). The following provides an example of doing so. Although several methods may be used, the one described should work in most cases.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.

For Windows clients, it is necessary to install PuTTY to create an ssh connection in step 2.

Step 1: Save the following servers.pvsc file to your local computer",4.254920125843803
"How can I run Paraview in a terminal on a Mac?
","If ParaView is unable to connect to our systems after trying to initiate a connection via the GUI and you see a ""The process failed to start. Either the invoked program is missing, or you may have insufficient permissions to invoke the program"" error, make sure that you have XQuartz (X11) installed.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.",4.229416124043452
"How can I build a Singularity image file for use on Summit?
","Singularity is a container framework from Sylabs. On Summit, we will use Singularity solely as the runtime. We will convert the tar files of the container images Podman creates into sif files, store those sif files on GPFS, and run them with Jsrun. Singularity also allows building images but ordinary users cannot utilize that on Summit due to additional permissions not allowed for regular users.",4.424468577587788
"How can I build a Singularity image file for use on Summit?
","Users can build container images with Podman, convert it to a SIF file, and run the container using the Singularity runtime. This page is intended for users with some familiarity with building and running containers.

Users will make use of two applications on Summit - Podman and Singularity - in their container build and run workflow. Both are available without needing to load any modules.",4.361559855483904
"How can I build a Singularity image file for use on Summit?
","You can now create a Singularity sif file with singularity build --disable-cache --docker-login simple.sif docker://docker.io/<username>/simple.

This will ask you to enter your Docker username and password again for Singularity to download the image from Dockerhub and convert it to a sif file.",4.224044544978038
"How can I set the total number of MPI tasks in Paraview?
","Submitting one of the above scripts will submit a job to the batch partition for five minutes using 28 MPI tasks across 1 node. As rendering speeds and memory issues widely vary for different datasets and MPI tasks, users are encouraged to find the optimal amount of MPI tasks to use for their data. Users with large datasets may also find a slight increase in performance by using the gpu partition on Andes, or by utilizing the GPUs on Summit. Once the batch job makes its way through the queue, the script will launch the loaded ParaView module (specified with module load) and execute a python",4.206519846356211
"How can I set the total number of MPI tasks in Paraview?
","In addition to specifying the SMT level, you can also control the number of threads per MPI task by exporting the OMP_NUM_THREADS environment variable. If you don't export it yourself, Jsrun will automatically set the number of threads based on the number of cores requested (-c) and the binding (-b) option. It is better to be explicit and set the OMP_NUM_THREADS value yourself rather than relying on Jsrun constructing it for you. Especially when you are using Job Step Viewer which relies on the presence of that environment variable to give you visual thread assignment information.",4.120784446988755
"How can I set the total number of MPI tasks in Paraview?
","The following example will create 12 resource sets each with 1 task, 4 threads, and 1 GPU. Each MPI task will start 4 threads and have access to 1 GPU. Rank 0 will have access to GPU 0 and start 4 threads on the first socket of the first node ( red resource set). Rank 2 will have access to GPU 1 and start 4 threads on the second socket of the first node ( green resource set). This pattern will continue until 12 resource sets have been created. The following jsrun command will create 12 resource sets (-n12). Each resource set will contain 1 MPI task (-a1), 1 GPU (-g1), and 4 cores (-c4).",4.107116512297086
"How can I ensure that the container in my deployment has the correct ownership?
","We will use OpenShift to build a new image based on the upstream one and change owner of the directories that need to be writable during container execution. Here is an example Dockerfile which derives from an upstream image and changes ownership of directories to the user id that the container will run as in the cluster.

For example, if we are using the UID 63114 for our NCCS project user and we need to write to /opt/application-data during the runtime of the container image we could do this:

FROM upstream-image:tag
USER 0
RUN chown -R 63114 /opt/application-data
USER 63114",4.209295307343436
"How can I ensure that the container in my deployment has the correct ownership?
","We will use this Dockerfile to generate a BuildConfig and then build a new image in our project that has the correct permissions.

cat Dockerfile | oc new-build --dockerfile=- --to=my-image:tag

The build should start automatically, monitor it with oc logs bc/my-image -f.

Now that we have a new image with our /opt/application-data directory owned by the right user we can either update an existing deployment or create a new one with the image.",4.206118927638026
"How can I ensure that the container in my deployment has the correct ownership?
","Sometimes you may want to do a full purge of your container storage area. Your user should own all the files in your /tmp/containers location. Recursively add write permissions to all files by running chmod -R +w /tmp/containers/<username> and then run rm -r /tmp/containers/<username>.

Sometimes you may need to kill your podman process because you may have gotten killed due to hitting cgroup limit. You can do so with pkill podman, then log out and log back in to reset your cgroup usage.",4.150219311454872
"What is the benefit of using XNACK operating mode for GPU memory migration on Frontier?
",Memory can be automatically migrated to GPU from CPU on a page fault if XNACK operating mode is set.  No need to explicitly migrate data or provide managed memory. This is useful if you're migrating code from a programming model that relied on 'unified' or 'managed' memory. See more in https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#enabling-gpu-page-migration. Information about how memory is accessed based on the allocator used and the XNACK mode can be found in https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#migration-of-memory-allocator-xnack.,4.543000412760345
"What is the benefit of using XNACK operating mode for GPU memory migration on Frontier?
","Most applications that use ""managed"" or ""unified"" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Frontier. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.",4.492216728348262
"What is the benefit of using XNACK operating mode for GPU memory migration on Frontier?
","The accessibility of memory from GPU kernels and whether pages may migrate depends three factors: how the memory was allocated; the XNACK operating mode of the GPU; whether the kernel was compiled to support page migration. The latter two factors are intrinsically linked, as the MI250X GPU operating mode restricts the types of kernels which may run.",4.427026877783117
"How do I ensure that the craype-accel-amd-gfx90a module is loaded when compiling HIP with the Cray compiler wrappers?
","Make sure the craype-accel-amd-gfx90a module is loaded when compiling HIP with the Cray compiler wrappers.

| Compiler | Compile/Link Flags, Header Files, and Libraries | | --- | --- | |  |  | | hipcc |  |",4.733013265100056
"How do I ensure that the craype-accel-amd-gfx90a module is loaded when compiling HIP with the Cray compiler wrappers?
","Make sure the craype-accel-amd-gfx90a module is loaded when compiling HIP with the Cray compiler wrappers.

| Compiler | Compile/Link Flags, Header Files, and Libraries | | --- | --- | |  |  | | hipcc |  |

hipcc requires the ROCm Toolclain, See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#exposing-the-rocm-toolchain-to-your-programming-environment",4.70586092097572
"How do I ensure that the craype-accel-amd-gfx90a module is loaded when compiling HIP with the Cray compiler wrappers?
","| Vendor | Module | Language | Compiler | OpenMP flag (GPU) | | --- | --- | --- | --- | --- | | Cray | cce | C C++ |  | -fopenmp | | Fortran | ftn |  | | AMD | rocm |  |  |  |

This section shows how to compile HIP codes using the Cray compiler wrappers and hipcc compiler driver.

Make sure the craype-accel-amd-gfx908 module is loaded when using HIP.

| Compiler | Compile/Link Flags, Header Files, and Libraries | | --- | --- | | CC |  | | hipcc |  |",4.456883650260036
"What is the purpose of the --shell='/bin/bash' flag in the oc rsh command?
","Finally, to get a remote shell in the pod we run the oc rsh <POD_NAME> command. This will default to using /bin/sh in the pod. If a different shell is required, we can provide the optional --shell=/path/to/shell flag. For example, if we wanted to open a bash shell in the pod we would run the following command:

oc rsh --shell='/bin/bash' <POD_NAME>

If you have multiple containers in your pod, the oc rsh <POD_NAME> command will default to the first container. If you would like to start a remote shell in one of the other containers, you can use the optional -c <CONTAINER_NAME> flag.",4.403170384385685
"What is the purpose of the --shell='/bin/bash' flag in the oc rsh command?
","If we have a pod that is crash looping then it is exiting too quickly to spawn a shell inside the container. We can use oc debug to start a pod with that same image but instead of the image entrypoint we use /bin/sh instead.

$ oc debug misbehaving-pod-1
Defaulting container name to bad.
Use 'oc describe pod/misbehaving-pod-1' to see all of the containers in this pod.

Debugging with pod/misbehaving-pod-1, original command: <image entrypoint>
Waiting for pod to start ...
If you don't see a command prompt, try pressing enter.
/ $

What if we want to get a shell inside of the container to debug?",4.082376646415389
"What is the purpose of the --shell='/bin/bash' flag in the oc rsh command?
","OLCF systems provide many software packages and scientific libraries pre-installed at the system-level for users to take advantage of. To facilitate this, environment management tools are employed to handle necessary changes to the shell. The sections below provide information about using these management tools on Summit.

A user’s default shell is selected when completing the User Account Request form. The chosen shell is set across all OLCF resources, and is the shell interface a user will be presented with upon login to any OLCF system. Currently, supported shells include:

bash

tcsh",4.058737267771628
"How can I optimize my application's performance on Frontier?
",This section of the Summit User Guide is intended to show current OLCF users how to start preparing their applications to run on the upcoming Frontier system. We will continue to add more topics to this section in the coming months. Please see the topics below to get started.,4.243787505638302
"How can I optimize my application's performance on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.211752605930847
"How can I optimize my application's performance on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.181443493612282
"What is the purpose of the ""image"" field in a container's spec?
","Source to image is a tool to build docker formatted container images. This is accomplished by injecting source code into a container image and then building a new image. The new image will contain the source code ready to run, via the $ docker run command inside the newly built image.

Using the S2I model of building images, it is possible to have your source hosted on a git repository and then pulled and built by Openshift.

Click on the newly created project and then on the blue Add to Project button on the center of the page.",4.081286245173553
"What is the purpose of the ""image"" field in a container's spec?
","Building an image in Openshift is the act of transferring a set of input parameters into an object. That object is typically an image. Openshift contains all of the necessary components to build a Docker image or a piece of source code an image that will run in a container.

A Docker build is achieved by linking to a source repository that contains a docker image and all of the necessary Docker artifacts. A build is then triggered through the standard $ docker build command. More specific documentation on docker builds can be found here.",4.031872067698947
"What is the purpose of the ""image"" field in a container's spec?
","First, we will replace the openshift/hello-openshift value after the image tag with value image-registry.openshift-image-registry.svc:5000/openshift/ccs-rhel7-base-amd64. This is the  Image that the  Pod will be using. We will be using the ccs-base image; a bare-bones image provided by the platforms team that is usually used as the foundation to build more complex custom images on top of.",4.013164666644991
"Why should users not use JSMD_LAUNCH_MODE=csm in their ~/.jsm.conf file at this time?
","$ jsrun  nvprof --openmp-profiling off

Users should not use JSMD_LAUNCH_MODE=csm in their ~/.jsm.conf file at this time. A bug has been filed with IBM to address this issue.



In some cases with large number of MPI processes when there is not enough memory available on the compute node, the Abstract-Device Interface for I/O (ADIO) driver can break with this error:

Out of memory in file ../../../../../../../opensrc/ompi/ompi/mca/io/romio321/romio/adio/ad_gpfs/ad_gpfs_rdcoll.c, line 1178

The solution is to declare in your submission script:

export GPFSMPIO_COMM=1",4.233963418923413
"Why should users not use JSMD_LAUNCH_MODE=csm in their ~/.jsm.conf file at this time?
","Users should not use USE_SPINDLE=1 or LOAD_SPINDLE=1 in their ~/.jsm.conf file at this time. A bug has been filed with IBM to address this issue.

By default, Spectrum MPI is configured for minimum latency. If your application needs maximum bandwidth, the following settings are recommended:

$ export PAMI_ENABLE_STRIPING=1
$ export PAMI_IBV_ADAPTER_AFFINITY=1
$ export PAMI_IBV_DEVICE_NAME=""mlx5_0:1,mlx5_3:1""
$ export PAMI_IBV_DEVICE_NAME_1=""mlx5_3:1,mlx5_0:1""

In order for debugging and profiling tools to work, you need to unload Darshan

$ module unload darshan-runtime",4.15650050528083
"Why should users not use JSMD_LAUNCH_MODE=csm in their ~/.jsm.conf file at this time?
","In addition, to debug slow startup JSM provides the option to create a progress file. The file will show information that can be helpful to pinpoint if a specific node is hanging or slowing down the job step launch. To enable it, you can use: jsrun --progress ./my_progress_file_output.txt.

When using file-based specification of resource sets with jsrun -U, the -a flag (number of tasks per resource set) is ignored. This has been reported to IBM and they are investigating. It is generally recommended to use jsrun explicit resource files (ERF) with --erf_input and --erf_output instead of -U.",4.0882325381708
"What is the purpose of isolating a socket's system services to a single core?
","One core per socket is set aside for system service tasks. The cores are not available to jsrun. When listing available resources through jsrun, you will not see cores with hyperthreads 84-87 and 172-175. Isolating a socket's system services to a single core helps to reduce jitter and improve performance of tasks performed on the socket's remaining cores.

The isolated core always operates at SMT4 regardless of the batch job's SMT level.",4.439906751182004
"What is the purpose of isolating a socket's system services to a single core?
","1 node

2 sockets (grey)

42 physical cores* (dark blue)

168 hardware cores (light blue)

6 GPUs (orange)

2 Memory blocks (yellow)

*Core Isolation: 1 core on each socket has been set aside for overhead and is not available for allocation through jsrun. The core has been omitted and is not shown in the above image.",4.050657344070794
"What is the purpose of isolating a socket's system services to a single core?
","As you can see in the node diagram above, this results in the 7 MPI tasks (outlined in different colors) being distributed ""horizontally"" within a socket, rather than being spread across different L3 sockets like with the previous example. However, if an 8th task was requested it would be assigned to the next L3 region on core 009.",3.9969758021821455
"What is the benefit of using OpenMP in Summit?
",http://vimeo.com/306890606 | | 2018-12-05 | Targeting GPUs Using GPU Directives on Summit with GenASiS: A Simple and Effective Fortran Experience | Reuben Budiardja (OLCF) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_budiardja.pdf https://vimeo.com/306890448 | | 2018-12-05 | Experiences Using the Volta Tensor Cores | Wayne Joubert (OLCF) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (recording),4.220416142790182
"What is the benefit of using OpenMP in Summit?
","If you want to do GPU computing on Summit with R, we would love to collaborate with you (see contact details at the bottom of this document).

For more information about using R and pbdR effectively in an HPC environment such as Summit, please see the R and pbdR articles. These are long-form articles that introduce HPC concepts like MPI programming in much more detail than we do here.

Also, if you want to use R and/or pbdR on Summit, please feel free to contact us directly:

Mike Matheson - mathesonma AT ornl DOT gov

George Ostrouchov - ostrouchovg AT ornl DOT gov",4.214102120480733
"What is the benefit of using OpenMP in Summit?
",https://vimeo.com/306437439 | | 2018-12-04 | GPU-Accelerated Libraries | Jeff Larkin (NVIDIA) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Libraries.pdf https://vimeo.com/306437127 | | 2018-12-04 | Targeting Summit's Multi-GPU Nodes | Steve Abbott (NVIDIA) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_MultiGPU-nodes.pdf,4.212107629296876
"How do I access the compute nodes on Frontier?
","On Frontier, there are two major types of nodes you will encounter: Login and Compute. While these are similar in terms of hardware (see: https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes), they differ considerably in their intended use.",4.413722083811468
"How do I access the compute nodes on Frontier?
","Recall from the System Overview that Frontier contains two node types: Login and Compute. When you connect to the system, you are placed on a login node. Login nodes are used for tasks such as code editing, compiling, etc. They are shared among all users of the system, so it is not appropriate to run tasks that are long/computationally intensive on login nodes. Users should also limit the number of simultaneous tasks on login nodes (e.g. concurrent tar commands, parallel make",4.365753613402444
"How do I access the compute nodes on Frontier?
","$ ssh <username>@frontier.olcf.ornl.gov

For more information on connecting to OLCF resources, see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#connecting-to-olcf.

By default, connecting to Frontier will automatically place the user on a random login node. If you need to access a specific login node, you will ssh to that node after your intial connection to Frontier.

[<username>@login12.frontier ~]$ ssh <username>@login01.frontier.olcf.ornl.gov

Users can connect to any of the 17 Frontier login nodes by replacing login01 with their login node of choice.",4.330578010954095
"How do I add a git repository to ArgoCD?
","Image of the repositories area.

and then add the ""Repository URL"", ""Username"" for the deploy token, and the deploy token itself as the password. If Git-LFS support is needed, click the ""Enable LFS support"" at the bottom of the page. Once entries look correct:

Image of the connect to repo using https parameters.

click the ""CONNECT"" button in the upper left. Once entered and ArgoCD is able to access the server, the connection should have a status of ""Successful"" with a green check mark:

Image of a successful git repository configuration.",4.4703612334191485
"How do I add a git repository to ArgoCD?
","Image of the ArgoCD applications tab.

Prior to using ArgoCD, a git repository containing Kubernetes custom resources needs to be added for use.

The git repository containing Kubernetes custom resources is typically not the same git repository used to build the application.

There are three ways for ArgoCD to connect to a git repository:

connect using ssh

connect using https

connect using GitHub App",4.467637301553401
"How do I add a git repository to ArgoCD?
","With a git repository defined in the ArgoCD Repositories settings that has kustomize environments ready for use, one can start using ArgoCD to deploy and manage kubernetes resources.",4.4137291493600594
"How can a Project PI ensure that their product meets the requirements for providing modules and a statement of support?
","Products must provide a statement of support, to be displayed via the module system and in other appropriate contexts/locations.

The statement should clearly indicate that the product is not supported or maintained by the OLCF, but is supported by the UMS project applicant and/or the UMS project team.

The statement should clearly indicate the organization that is providing support and maintenance, and clearly indicate the preferred method(s) of reporting issues or requesting support.

Product modules will be grouped under project-level modules.",4.415718177618485
"How can a Project PI ensure that their product meets the requirements for providing modules and a statement of support?
","Users will be advised to do a module load <UMS project>, which will expose modules for the individual products associated with that project, accessed by a second module load <product>.

Project PI must ensure that support is provided for users of the product, as documented in the statement of support.

Project PI must ensure that the product is updated in response to changes in the system software environment (e.g. updates to OS, compiler, library, or other key tools) and in a timely fashion.  If this commitment is not met, the OLCF may remove the software from UMS.",4.409458274637021
"How can a Project PI ensure that their product meets the requirements for providing modules and a statement of support?
","4. For codes being used, please make sure that the proper number of licenses have been obtained from the vendors of the respective software. It is also the responsibility of the PI to ensure that all project members have the appropriate authorization to access any sensitive data and/or codes used as required by relevant data use agreements.",4.253723948932912
"What is the myOLCF self-service portal?
","Project members

The myOLCF site is provided to aid in the utilization and management of OLCF allocations. See the https://docs.olcf.ornl.gov/services_and_applications/myolcf/index.html for more information.

If you have any questions or have a request for additional data, please contact the OLCF User Assistance Center.",4.4355842672306505
"What is the myOLCF self-service portal?
","You can check the general status of your application at any time using the myOLCF self-service portal's account status page. For more information, see the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#myOLCF self-service portal documentation<myolcf-overview>. If you need to make further inquiries about your application, you may email our Accounts Team at accounts@ccs.ornl.gov.",4.37250687668977
"What is the myOLCF self-service portal?
","myOLCF is currently available to OLCF Moderate user accounts; i.e., users that authenticate to OLCF systems with an RSA SecurID token. Visit https://my.olcf.ornl.gov and authenticate with your OLCF Moderate username and RSA SecurID PASSCODE (PIN followed by the 6-digit tokencode).

The myOLCF login page

OLCF Open user accounts, i.e., users that authenticate to OLCF systems with a password, cannot access myOLCF at this time, as we are still investigating the feasibility of supporting password-only authentication.",4.33188375594019
"What is the purpose of the ""-4L"" option in the ""ssh -4L 5901:localhost:5901 andes-gpu5"" command?
","In a new terminal, open a tunneling connection with andes-gpu5.olcf.ornl.gov and                                                                              port 5901
example:
     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov
     andes: ssh -4L 5901:localhost:5901 andes-gpu5

**************************************************************************",4.405537541280851
"What is the purpose of the ""-4L"" option in the ""ssh -4L 5901:localhost:5901 andes-gpu5"" command?
","In a new terminal, open a tunneling connection with andes-gpu5.olcf.ornl.gov and                                                                              port 5901
example:
     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov
     andes: ssh -4L 5901:localhost:5901 andes-gpu5

**************************************************************************",4.405537541280851
"What is the purpose of the ""-4L"" option in the ""ssh -4L 5901:localhost:5901 andes-gpu5"" command?
","In a new terminal, open a tunneling connection with andes79.olcf.ornl.gov and port 5901
example:
     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov
     andes: ssh -4L 5901:localhost:5901 andes79

**************************************************************************

MATLAB is selecting SOFTWARE OPENGL rendering.

In a second terminal on your local system open a tunneling connection following the instructions given by the vnc start-up script:

localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov

andes: ssh -4L 5901:localhost:5901 andes79",4.339361144856892
"What is the advantage of using Lmod on Summit?
",For Summit:,4.1968055219430855
"What is the advantage of using Lmod on Summit?
","Environment management with lmod

The modules software package allows you to dynamically modify your user environment by using pre-written modulefiles. Environment modules are provided through Lmod, a Lua-based module system for dynamically altering shell environments.  By managing changes to the shell’s environment variables (such as path, ld_library_path, and pkg_config_path), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.",4.182340696963995
"What is the advantage of using Lmod on Summit?
","below), you can choose between the rendering options by loading its corresponding module on the system you're connected to. For example, to see these modules on Summit:",4.169208833112257
"What is the benefit of using templating objects in Slate charts?
","It is also possible to write your own charts for helm, if you have an application that can be deployed to many namespaces or that could benefit from templating objects. How to write charts is outside the scope of this documentation, but the upstream docs are a great place to start.",4.160650285782527
"What is the benefit of using templating objects in Slate charts?
","The following items need to be established before deploying applications on Slate systems (Marble or Onyx OpenShift clusters).

Follow https://docs.olcf.ornl.gov/systems/helm_prerequisite.html#slate_getting_started if you do not already have a project/namespace established on Onyx or Marble.

It is recommended to use Helm version 3.

Helm enables application deployment via Helm Charts. The high level Helm Architecture docs are also a great reference for understanding Helm.

Like oc, Helm is a single binary executable.

This can be installed on macOS with Homebrew :

$ brew install helm",3.984508944364151
"What is the benefit of using templating objects in Slate charts?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",3.965569613297663
"How can I ensure that my jsrun command is executed in a specific queue?
","summit>



Jsrun provides the ability to launch multiple jsrun job launches within a single batch job allocation. This can be done within a single node, or across multiple nodes.

By default, multiple invocations of jsrun in a job script will execute serially in order. In this configuration, jobs will launch one at a time and the next one will not start until the previous is complete. The batch node allocation is equal to the largest jsrun submitted, and the total walltime must be equal to or greater then the sum of all jsruns issued.",4.184422649862753
"How can I ensure that my jsrun command is executed in a specific queue?
","To execute multiple job steps concurrently, standard UNIX process backgrounding can be used by adding a & at the end of the command. This will return control to the job script and execute the next command immediately, allowing multiple job launches to start at the same time. The jsruns will not share core/gpu resources in this configuration. The batch node allocation is equal to the sum of those of each jsrun, and the total walltime must be equal to or greater than that of the longest running jsrun task.",4.183390136529847
"How can I ensure that my jsrun command is executed in a specific queue?
","(either batch or interactive) on a launch node. Otherwise, you will not have any compute nodes allocated and your parallel job will run on the login node. If this happens, your job will interfere with (and be interfered with by) other users' login node tasks. jsrun is covered in-depth in the Job Launcher (jsrun) section.",4.16207155928347
"What is the retention policy for files in the Member Work directories on Slate?
","Footnotes

1

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

2

Retention is not applicable as files will follow purge cycle.",4.357878324006654
"What is the retention policy for files in the Member Work directories on Slate?
","Retention - Period of time, post-account-deactivation or post-project-end, after which data will be marked as eligible for permanent deletion.

Important! Files within ""Work"" directories (i.e., Member Work, Project Work, World Work) are not backed up and are purged on a regular basis according to the timeframes listed above.

Footnotes

1

This entry is for legacy User Archive directories which contained user data on January 14, 2020. There is also a quota/limit of 2,000 files on this directory.

2",4.337190920754141
"What is the retention policy for files in the Member Work directories on Slate?
","3

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

4

Retention is not applicable as files will follow purge cycle.",4.32228192545775
"What is the advantage of using fakeroot in package installations?
",a apt-get -y fakeroot to get you started. Other distributions haven't been tested. Using centos for this case for now is the most user friendly option).,4.272269187318935
"What is the advantage of using fakeroot in package installations?
","You will notice the use of the fakeroot command when doing package installs with dnf. This is necessary as some some package installations require root permissions on container which the container builder does not have. So fakeroot allows dnf to think it is running as root and allows the installation to succeed.

Singularity requires libevent installed in any container you build in order for it to work correctly with the jsrun job launcher.

Build the container image with podman build -t simple -f simple.dockerfile ..",4.229604897748297
"What is the advantage of using fakeroot in package installations?
","You will also notice that we use centos:stream8 as our base image in the example. If you're planning on building a container image from scratch instead of using the OLCF MPI base image , use a centos:stream8 image with fakeroot installed as demonstrated above as your starting point (we talk about the OLCF MPI base image later in the https://docs.olcf.ornl.gov/systems/containers_on_summit.html#olcf-mpi-base-image section). Ubuntu would be difficult to use as a starting point since apt-get requires root from the get-go, and you can't even do a apt-get -y fakeroot to get you started. Other",3.9673516840675416
"How do you ensure that each MPI rank is assigned to the closest available GPU?
","The output shows the round-robin (cyclic) distribution of MPI ranks to GPUs. In fact, it is a round-robin distribution of MPI ranks to L3 cache regions (the default distribution). The GPU mapping is a consequence of where the MPI ranks are distributed; --gpu-bind=closest simply maps the GPU in an L3 cache region to the MPI ranks in the same L3 region.

Example 6: 32 MPI ranks - where 2 ranks share a GPU (round-robin, multi-node)

This example is an extension of Example 5 to run on 2 nodes.",4.436079668310091
"How do you ensure that each MPI rank is assigned to the closest available GPU?
","The output shows the round-robin (cyclic) distribution of MPI ranks to GPUs. In fact, it is a round-robin distribution of MPI ranks to NUMA domains (the default distribution). The GPU mapping is a consequence of where the MPI ranks are distributed; --gpu-bind=closest simply maps the GPU in a NUMA domain to the MPI ranks in the same NUMA domain.

Example 6: 16 MPI ranks - where 2 ranks share a GPU (round-robin, multi-node)

This example is an extension of Example 5 to run on 2 nodes.",4.42227842781517
"How do you ensure that each MPI rank is assigned to the closest available GPU?
","In the following examples, each MPI rank (and its OpenMP threads) will be mapped to a single GPU.

Example 0: 1 MPI rank with 1 OpenMP thread and 1 GPU (single-node)

Somewhat counterintuitively, this common test case is currently among the most difficult. Slurm ignores GPU bindings for nodes with only a single task, so we do not use --gpu-bind here. We must allocate only a single GPU to ensure that only one GPU is available to the task, and since we get the first GPU available we should bind the task to the CPU closest to the allocated GPU.",4.41699335121023
"How do I run pyQuil on my computer?
","Quil is the Rigetti-developed quantum instruction/assembly language. PyQuil is a Python library for writing and running quantum programs using Quil.

Installing pyQuil requires installing the Forest SDK. To quote Rigetti: ""pyQuil, along with quilc, the QVM, and other libraries, make up what is called the Forest SDK"". Because we don't have Docker functionality and due to normal users not having sudo privileges, this means that you will have to install the SDK via the ""bare-bones"" method. The general info below came from:

https://pyquil-docs.rigetti.com/en/stable/start.html",4.37974772624326
"How do I run pyQuil on my computer?
","With the way pyQuil works, you need to launch its compiler server, launch the virtual machine / simulator QVM server, and then launch your pyQuil Python program on the same host. Running a Python script will ping and utilize both the compiler and QVM servers. As a proof of concept, this has been done on a single login node and the steps are outlined below.

Using your already created ENV_NAME virtual environment (outlined above):

(ENV_NAME)$ quilc -P -S > quilc.log 2>&1 & qvm -S > qvm.log 2>&1 & python3 script.py ; kill $(jobs -p)",4.342053592644551
"How do I run pyQuil on my computer?
","Rigetti provides system access via a cloud-based JupyterLab development environment: https://docs.rigetti.com/qcs/getting-started/jupyterlab-ide.  From there, users can access a JupyterLab server loaded with Rigetti's PyQuil programming framework, Rigetti's Forest Software Development Kit, and associated program examples and tutorials.  This is the method that allows access to Rigetti's QPU's directly, as opposed to simulators.",4.172303774878672
"How do I track my usage of IonQ resources?
","Information on submitting jobs to IonQ systems, system availability, checking job status, and tracking usage can be found via the IonQ Cloud Console.

A recommended workflow for running on IonQ's quantum computers is to utilize the emulator first, then run on one of the quantum computers. This is highlighted in the examples.",4.300664372925557
"How do I track my usage of IonQ resources?
","IonQ backends are available via the IonQ cloud interface via the API and also via many quantum Software Development Kits (SDK’s)



Users can access information about IonQ’s systems, view submitted jobs, look up machine availability, and update job notification preferences via the IonQ Cloud Console.

Jupyter at OLCF: Access to the IonQ queues can also be obtained via OLCF JupyterHub, a web-based interactive computing environment. See examples of common use case notebooks at IonQ Notebook Samples.",4.214663506788861
"How do I track my usage of IonQ resources?
","After submitting the OLCF quantum account application and receiving approval, you will receive an email from IonQ inviting you to create your quantum account. Once logged in, users will have access to IonQ's User Interface, https://cloud.ionq.com/, their online platform for managing jobs and accessing the available quantum systems, including the Harmony and Aria-1 systems, as well as the simulator, via the cloud. From the UI, users can view system status and upcoming system availability, as well as monitor batch submissions and job history.",4.175268896611835
"What is the purpose of the #SBATCH -t directive in the job script?
","The execution section of a script will be interpreted by a shell and can contain multiple lines of executables, shell commands, and comments.  when the job's queue wait time is finished, commands within this section will be executed on the primary compute node of the job's allocated resources. Under normal circumstances, the batch job will exit the queue after the last line of the script is executed.

#!/bin/bash
#SBATCH -A XXXYYY
#SBATCH -J test
#SBATCH -N 2
#SBATCH -t 1:00:00

cd $SLURM_SUBMIT_DIR
date
srun -n 8 ./a.out",4.296835256254542
"What is the purpose of the #SBATCH -t directive in the job script?
","The shell commands follow the last #SBATCH option and represent the executable content of the batch job. If any #SBATCH lines follow executable statements, they will be treated as comments only.",4.293714598133645
"What is the purpose of the #SBATCH -t directive in the job script?
","cd $SLURM_SUBMIT_DIR
date
srun -n 8 ./a.out

This batch script shows examples of the three sections outlined above:

<string>:509: (INFO/1) Duplicate implicit target name: ""interpreter line"".

1: This line is optional and can be used to specify a shell to interpret the script. In this example, the bash shell will be used.

2: The job will be charged to the “XXXYYY” project.

3: The job will be named test.

4: The job will request (2) nodes.

5: The job will request (1) hour walltime.

<string>:526: (INFO/1) Duplicate implicit target name: ""shell commands"".",4.281728267564721
"How do I specify the version of CMake to use in Spack?
","Traditionally, the user environment is modified by module files.  For example, a user would add use  module load cmake/3.18.2 to load CMake version 3.18.2 into their environment.  Using a Spack environment, a user can add an OLCF provided package and build against it using Spack without having to load the module file separately.

The information presented here is a subset of what can be found at the Spack documentation site.",4.279353372725911
"How do I specify the version of CMake to use in Spack?
","packages:
  # This example is included in the template file
  cmake:
    version: [3.23.2]
    buildable: false
    externals:
    - spec: cmake@3.23.2
      modules:
      - cmake/3.23.2

As a reminder, to find modules:

## Using cmake as an example again.

$ module -t av cmake
/sw/summit/spack-envs/base/modules/spack/linux-rhel8-ppc64le/Core:
cmake/3.18.4
cmake/3.20.2
cmake/3.21.3
cmake/3.22.2
cmake/3.23.1
cmake/3.23.2",4.22551412170026
"How do I specify the version of CMake to use in Spack?
","A dependency that is not already installed will be built via Spack once the environment is concretized and installed. These can be added to the spack.yaml by adding to the specs section.

specs:
- cmake@3.18.2                            ## example from above
- my_apps_dependency1@version%compiler    ## other explicitly defined specs
- my_apps_dependency2@version%compiler

When in the Spack environment, any packages that are added to the environment file can be installed via:

$ spack concretize -f  ## The -f flag here forces a reconcretization of the entire environment
$ spack install",4.1845716883106885
"What is the primary CUDA C/C++ compiler in Summit?
","The following compilers are available on Summit:

XL: IBM XL Compilers (loaded by default)

LLVM: LLVM compiler infrastructure

PGI: Portland Group compiler suite

NVHPC: Nvidia HPC SDK compiler suite

GNU: GNU Compiler Collection

NVCC: CUDA C compiler

PGI was bought out by Nvidia and have rebranded their compilers, incorporating them into the NVHPC compiler suite. There will be no more new releases of the PGI compilers.",4.342968376186029
"What is the primary CUDA C/C++ compiler in Summit?
","Although there are newer CUDA modules on Summit, cuda/11.0.3 is the latest version that is officially supported by the version of IBM's software stack installed on Summit. When loading the newer CUDA modules, a message is printed to the screen stating that the module is for “testing purposes only”. These newer unsupported CUDA versions might work with some users’ applications, but importantly, they are known not to work with GPU-aware MPI.",4.257523678819889
"What is the primary CUDA C/C++ compiler in Summit?
","https://vimeo.com/306436688 | | 2018-12-04 | GPU Direct, RDMA, CUDA-Aware MPI | Steve Abbott (NVIDIA) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_CUDA-Aware-MPI.pdf https://vimeo.com/306436248 | | 2018-12-04 | CUDA Unified Memory | Jeff Larkin (NVIDIA) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_UVM.pdf",4.250199341483015
"Can I use the srun command to launch multiple job steps in a single Slurm submission script?
","Slurm provides 3 ways of submitting and launching jobs on Spock's compute nodes: batch  scripts, interactive, and single-command. The Slurm commands associated with these methods are shown in the table below and examples of their use can be found in the related subsections.

| sbatch |  | | --- | --- | | salloc |  | | srun |  |

A batch script can be used to submit a job to run on the compute nodes at a later time. In this case, stdout and stderr will be written to a file(s) that can be opened after the job completes. Here is an example of a simple batch script:",4.413401813782778
"Can I use the srun command to launch multiple job steps in a single Slurm submission script?
","cd $SLURM_SUBMIT_DIR
date
srun -n 8 ./a.out

This batch script shows examples of the three sections outlined above:

Interpreter Line

<string>:509: (INFO/1) Duplicate implicit target name: ""interpreter line"".

1: This line is optional and can be used to specify a shell to interpret the script. In this example, the bash shell will be used.

Slurm Options

2: The job will be charged to the “XXXYYY” project.

3: The job will be named test.

4: The job will request (2) nodes.

5: The job will request (1) hour walltime.

Shell Commands",4.409967767960224
"Can I use the srun command to launch multiple job steps in a single Slurm submission script?
","cd $SLURM_SUBMIT_DIR
date
srun -n 8 ./a.out

This batch script shows examples of the three sections outlined above:

<string>:509: (INFO/1) Duplicate implicit target name: ""interpreter line"".

1: This line is optional and can be used to specify a shell to interpret the script. In this example, the bash shell will be used.

2: The job will be charged to the “XXXYYY” project.

3: The job will be named test.

4: The job will request (2) nodes.

5: The job will request (1) hour walltime.

<string>:526: (INFO/1) Duplicate implicit target name: ""shell commands"".",4.409095330083309
"Where can I store compute job I/O on SPI?
",Use of the SPI queue will trigger configuration changes to the compute nodes to allow enhanced data protection. Compute nodes will be booted before and after each SPI batch job. Compute nodes will be booted into an image that mounts only the Arx filesystem. The image will also restrict connections. Please note: the reboot process may cause a slight delay in job startup.,4.307961114310413
"Where can I store compute job I/O on SPI?
","https://docs.olcf.ornl.gov/systems/index.html#SPI resources mount SPI filesystems<spi-file-systems>.  The SPI resources do not mount the non-SPI's scratch filesystems, home areas, or mass storage.

https://docs.olcf.ornl.gov/systems/index.html#SPI compute resources cannot access external resources<spi-data-transfer>.  Needed data must be transferred to the SPI resources through the SPI's DTN.

https://docs.olcf.ornl.gov/systems/index.html#The Citadel login nodes<spi-compute-citadel> and batch queues must be used to access Summit and Frontier for SPI workflows.",4.234511843855399
"Where can I store compute job I/O on SPI?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.

The SPI provides access to the OLCF's Summit resource for compute.  To safely separate SPI and non-SPI workflows, SPI workflows must use a separate login node named https://docs.olcf.ornl.gov/systems/citadel_user_guide.html#Citadel<spi-compute-citadel>.  Citadel provides a login node specifically for SPI workflows.",4.18728421477618
"What is a recommended way to debug a large problem size on Summit?
","Summit

Andes

Frontier

Scientific simulations generate large amounts of data on Summit (about 100 Terabytes per day for some applications). Because of how large some datafiles may be, it is important that writing and reading these files is done as fast as possible. Less time spent doing input/output (I/O) leaves more time for advancing a simulation or analyzing data.",4.23101931463466
"What is a recommended way to debug a large problem size on Summit?
","Using VisIt on Summit is also an option, as the scalable rendering problem is currently not an issue on Summit (as of Sept. 2021).

As of February 2022, this issue on Andes has been fixed (must use VisIt 3.2.2 or higher).",4.196602822764087
"What is a recommended way to debug a large problem size on Summit?
","For bug reports or suggestions, please email help@olcf.ornl.gov.

Request a Summit allocation

bsub -W 10 -nnodes 2 -P $OLCF_PROJECT_ID -Is $SHELL

Load the job-step-viewer module

module load job-step-viewer

Test out a jsrun line by itself, or provide an executable as normal

jsrun -n12 -r6 -c7 -g1 -a1 -EOMP_NUM_THREADS=7 -brs

Visit the provided URL

https://jobstepviewer.olcf.ornl.gov/summit/871957-1",4.171727208540787
"How can I create an LSF batch script for Fireworks?
","module load workflows
module load fireworks/2.0.2

# Edit the following line to match your own MongoDB connection string.
export MONGODB_URI=""mongodb://admin:password@apps.marble.ccs.ornl.gov:32767/test""

jsrun -n 1 python3 demo.py

Finally, submit the batch job to LSF by executing the following command from a Summit login node:

$ bsub fireworks_demo.lsf

Congratulations! Once the batch job completes, you will find new directories beginning with launcher_ and containing FW.json files that detail exactly what happened.",4.300300631528527
"How can I create an LSF batch script for Fireworks?
","# Create the individual FireWorks and Workflow.
fw1 = Firework(ScriptTask.from_str('echo ""hello""'), name = ""hello"")
fw2 = Firework(ScriptTask.from_str('echo ""goodbye""'), name = ""goodbye"")
wf = Workflow([fw1, fw2], {fw1: fw2}, name = ""test workflow"")

# Store workflow and launch it locally.
launchpad.add_wf(wf)
rapidfire(launchpad)

Finally, create an LSF batch script called fireworks_demo.lsf, and change abc123 to match your own project identifier:

#BSUB -P abc123
#BSUB -W 10
#BSUB -nnodes 1

#BSUB -J fireworks_demo
#BSUB -o fireworks_demo.o%J
#BSUB -e fireworks_demo.e%J",4.273742504385659
"How can I create an LSF batch script for Fireworks?
","Run the following command to verify that FireWorks is available:

$ rlaunch -v
rlaunch v2.0.2

To run this FireWorks demo on Summit, you will create a Python file and then submit it as a batch job to LSF from a Summit node.

The contents for demo.py follow:

import os

from fireworks import Firework, Workflow, LaunchPad, ScriptTask
from fireworks.core.rocket_launcher import rapidfire

# Set up and reset the LaunchPad using MongoDB URI string.
launchpad = LaunchPad(host = os.getenv(""MONGODB_URI""), uri_mode = True)
launchpad.reset('', require_password=False)",4.245110030523208
"Is there a limit to the amount of data I can transfer to/from the NVMe storage devices on Frontier?
","Each compute node on Frontier has [2x] 1.92TB Non-Volatile Memory (NVMe) storage devices (SSDs), colloquially known as a ""Burst Buffer"" with a peak sequential performance of 5500 MB/s (read) and 2000 MB/s (write). The purpose of the Burst Buffer system is to bring improved I/O performance to appropriate workloads. Users are not required to use the NVMes. Data can also be written directly to the parallel filesystem.



The NVMes on Frontier are local to each node.",4.364625331281864
"Is there a limit to the amount of data I can transfer to/from the NVMe storage devices on Frontier?
","Frontier is connected to Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Data will not be automatically transferred from Alpine to Orion. Frontier also has access to the center-wide NFS-based filesystem (which provides user and project home areas). Each compute node has two 1.92TB Non-Volatile Memory storage devices. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-data-storage for more",4.196336293983673
"Is there a limit to the amount of data I can transfer to/from the NVMe storage devices on Frontier?
","For more detailed information about center-wide file systems and data archiving available on Frontier, please refer to the pages on https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers. The subsections below give a quick overview of NFS, Lustre,and HPSS storage spaces as well as the on node NVMe ""Burst Buffers"" (SSDs).",4.168832076906377
"How do I run my application on Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.368420316852878
"How do I run my application on Summit?
",For Summit:,4.353842960568372
"How do I run my application on Summit?
","Summit has a node hierarchy that can be very confusing for the uninitiated. The Summit User Guide explains this in depth. This has some consequences that may be unusual for R programmers. A few important ones to note are:

You must have a script that you can run in batch, e.g. with Rscript or R CMD BATCH

All data that needs to be visible to the R process (including the script and your packages) must be on gpfs (not your home directory!).

You must launch your script from the launch nodes with jsrun.

We'll start with something very simple. The script is:

""hello world""",4.2780764105584
"Can you provide more information on the Frontier nodes and how they are connected?
","On Frontier, there are two major types of nodes you will encounter: Login and Compute. While these are similar in terms of hardware (see: https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes), they differ considerably in their intended use.",4.359345465686809
"Can you provide more information on the Frontier nodes and how they are connected?
",The Frontier nodes are connected with [4x] HPE Slingshot 200 Gbps (25 GB/s) NICs providing a node-injection bandwidth of 800 Gbps (100 GB/s).,4.319351611040229
"Can you provide more information on the Frontier nodes and how they are connected?
","Simplified Frontier node architecture diagram

In the diagram, each physical core on a Frontier compute node is composed of two logical cores that are represented by a pair of blue and grey boxes. For a given physical core, the blue box represents the logical core of the first hardware thread, where the grey box represents the logical core of the second hardware thread.",4.310682933399324
"Can users submit jobs to Rigetti's systems using the QCS dashboard?
","All jobs run on Rigetti's systems are submitted via system reservation.  This can be done either by using Rigetti's QCS dashboard to schedule the reservation, or via interacting with the QCS via the Command Line Interface (CLI).  Scheduled reservations can be viewed and/or cancelled via either method, either in the dashboard or from the CLI.

To submit a reservation via the QCS dashboard: https://docs.rigetti.com/qcs/guides/reserving-time-on-a-qpu#using-the-qcs-dashboard",4.585845585513817
"Can users submit jobs to Rigetti's systems using the QCS dashboard?
","To submit a reservation via the QCS CLI, and installation instructions: https://docs.rigetti.com/qcs/guides/using-the-qcs-cli

Accessing the Rigetti QPU systems can only be done during a user's reservation window.  To submit a job, users must have JupyterHub access to the system.  QVM jobs can be run without network access in local environments.  Jobs are compiled via Quilc and submitted via pyQuil (see https://docs.olcf.ornl.gov/systems/rigetti.html#Software Section <rigetti-soft> below) in a python environment or Jupyter notebook.",4.5180960053128025
"Can users submit jobs to Rigetti's systems using the QCS dashboard?
","accessing the hybrid infrastructure of available quantum processors and classical computational framework via the cloud. From the QCS, users can view system status and availability, initiate and manage quantum infrastructure reservations (either executing programs manually or adding them to the queue). Information on using this resource is available on the Rigetti's Documentation or our https://docs.olcf.ornl.gov/quantum/quantum_systems/rigetti.html.",4.436027506440635
"What is the purpose of the measurement run in Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations. It works in combination with Periscope, Scalasca, Vampir, and Tau.

Steps in a typical Score-P workflow to run on https://docs.olcf.ornl.gov/systems/Scorep.html#summit-user-guide:",4.314310348166003
"What is the purpose of the measurement run in Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi-processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations.

For detailed information about using Score-P on Summit and the builds available, please see the Score-P Software Page.",4.298958107517185
"What is the purpose of the measurement run in Score-P?
","Once the code has been instrumented, it is time to begin the measurement run of the newly compiled code. The measurement calls will gather information during the runtime of the code where this information will be stored for later analysis.

By default Score-P is configured to run with profiling set to true and tracing set to false. Measurement types are configured via environment variables.

##Environment variable setup examples

export SCOREP_ENABLE_TRACING=true

You can check what current Score-P environment variables are set:

$ scorep-info config-vars --full

#Output",4.291230237333968
"How can I view the JSM log files for my job on Summit?
","This section describes tools that users might find helpful to better understand the jsrun job launcher.

hello_jsrun is a ""Hello World""-type program that users can run on Summit nodes to better understand how MPI ranks and OpenMP threads are mapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A screencast showing how to use Hello_jsrun is also available: https://vimeo.com/261038849

Job Step Viewer provides a graphical view of an application's runtime layout on Summit. It allows users to preview and quickly iterate with multiple jsrun options to understand and optimize job launch.",4.2592036027506905
"How can I view the JSM log files for my job on Summit?
","For bug reports or suggestions, please email help@olcf.ornl.gov.

Request a Summit allocation

bsub -W 10 -nnodes 2 -P $OLCF_PROJECT_ID -Is $SHELL

Load the job-step-viewer module

module load job-step-viewer

Test out a jsrun line by itself, or provide an executable as normal

jsrun -n12 -r6 -c7 -g1 -a1 -EOMP_NUM_THREADS=7 -brs

Visit the provided URL

https://jobstepviewer.olcf.ornl.gov/summit/871957-1",4.2478359602896125
"How can I view the JSM log files for my job on Summit?
","In addition, to debug slow startup JSM provides the option to create a progress file. The file will show information that can be helpful to pinpoint if a specific node is hanging or slowing down the job step launch. To enable it, you can use: jsrun --progress ./my_progress_file_output.txt.

When using file-based specification of resource sets with jsrun -U, the -a flag (number of tasks per resource set) is ignored. This has been reported to IBM and they are investigating. It is generally recommended to use jsrun explicit resource files (ERF) with --erf_input and --erf_output instead of -U.",4.238906365254562
"How can I sort the output of a job step in Crusher?
","Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.",4.085470272093916
"How can I sort the output of a job step in Crusher?
","The output shows that each independent process ran on its own CPU core and GPU on the same single node. To show that the ranks ran simultaneously, date was called before each job step and a 20 second sleep was added to the end of the hello_jobstep program. So the output also shows that the first job step was submitted at :45 and the subsequent job steps were all submitted between :46 and :52. But because each hello_jobstep sleeps for 20 seconds, the subsequent job steps must have all been running while the first job step was still sleeping (and holding up its resources). And the same argument",4.034375158397594
"How can I sort the output of a job step in Crusher?
","This section describes how to run programs on the Crusher compute nodes, including a brief overview of Slurm and also how to map processes and threads to CPU cores and GPUs.

Slurm is the workload manager used to interact with the compute nodes on Crusher. In the following subsections, the most commonly used Slurm commands for submitting, running, and monitoring jobs will be covered, but users are encouraged to visit the official documentation and man pages for more information.",4.022408384886519
"How can I uninstall a package named package_name using Conda at OLCF?
","$ pip uninstall numpy

The traditional, and more basic, approach to installing/uninstalling packages into a conda environment is to use the commands conda install and conda remove. Installing packages with this method checks the Anaconda Distribution Repository for pre-built binary packages to install. Let's do this to install NumPy:

$ conda install numpy

Because NumPy depends on other packages for optimization, this will also install all of its dependencies. You have just installed an optimized version of NumPy, now let's test it.",4.143639390071769
"How can I uninstall a package named package_name using Conda at OLCF?
","This page describes how a user can successfully install specific quantum software tools to run on select OLCF HPC systems. This allows a user to utilize our systems to run a simulator, or even as a ""frontend"" to a vendor's quantum machine (""backend"").

For most of these instructions, we use conda environments. For more detailed information on how to use Python modules and conda environments on OLCF systems, see our :doc:`Python on OLCF Systems page</software/python/index>`, and our https://docs.olcf.ornl.gov/software/python/conda_basics.html.",4.1421672131060845
"How can I uninstall a package named package_name using Conda at OLCF?
","In addition, the following packages will be upgraded to newer versions and the specific versions listed below will be removed from the system. If you need any of the specific versions scheduled to be removed, please contact help@olcf.ornl.gov.",4.137944840572108
"Can I use the sbatch --test-only command to check the estimated start time for a job on a specific partition?
","Use the sbatch --test-only command to see when a job of a specific size could be scheduled. For example, the snapshot below shows that a (2) node job would start at 10:54.

$ sbatch --test-only -N2 -t1:00:00 batch-script.slurm

  sbatch: Job 1375 to start at 2019-08-06T10:54:01 using 64 processors on nodes andes[499-500] in partition batch

The queue is fluid, the given time is an estimate made from the current queue state and load. Future job submissions and job completions will alter the estimate.



The following table summarizes frequently-used options to Slurm:",4.47832189394162
"Can I use the sbatch --test-only command to check the estimated start time for a job on a specific partition?
","Use the sbatch --test-only command to see when a job of a specific size could be scheduled. For example, the snapshot below shows that a (2) node job would start at 10:54.

$ sbatch --test-only -N2 -t1:00:00 batch-script.slurm

  sbatch: Job 1375 to start at 2019-08-06T10:54:01 using 64 processors on nodes andes[499-500] in partition batch

The queue is fluid, the given time is an estimate made from the current queue state and load. Future job submissions and job completions will alter the estimate.



Common Batch Options to Slurm",4.474340379710318
"Can I use the sbatch --test-only command to check the estimated start time for a job on a specific partition?
","$ srun -A <project_id> -t 00:05:00 -p <partition> -N 2 -n 4 --ntasks-per-node=2 ./a.out
<output printed to terminal>

The job name and output options have been removed since stdout/stderr are typically desired in the terminal window in this usage mode.

The table below summarizes commonly-used Slurm job submission options:",4.152088706740193
"How can I load the ESSL module on Summit?
","]])

When this module is loaded, the $OLCF_ESSL_ROOT environment variable holds the path to the ESSL installation, which contains the lib64/ and include/ directories:

summit$ module load essl
summit$ echo $OLCF_ESSL_ROOT
/sw/summit/essl/6.1.0-1/essl/6.1
summit$ ls $OLCF_ESSL_ROOT
FFTW3  READMES  REDIST.txt  include  iso-swid  ivps  lap  lib64  man  msg

The following screencast shows an example of linking two libraries into a simple program on Summit. https://vimeo.com/292015868",4.392561728029186
"How can I load the ESSL module on Summit?
","summit$ module show essl
------------------------------------------------------------------------------------
   /sw/summit/modulefiles/core/essl/6.1.0-1:
------------------------------------------------------------------------------------
whatis(""ESSL 6.1.0-1 "")
prepend_path(""LD_LIBRARY_PATH"",""/sw/summit/essl/6.1.0-1/essl/6.1/lib64"")
append_path(""LD_LIBRARY_PATH"",""/sw/summit/xl/16.1.1-beta4/lib"")
prepend_path(""MANPATH"",""/sw/summit/essl/6.1.0-1/essl/6.1/man"")
setenv(""OLCF_ESSL_ROOT"",""/sw/summit/essl/6.1.0-1/essl/6.1"")
help([[ESSL 6.1.0-1

]])",4.356736175446193
"How can I load the ESSL module on Summit?
","The E4S software list is installed along side the existing software on Summit and can be access via lmod modulefiles.

To access the installed software, load the desired compiler via:

module load < compiler/version >
.. ie ..
module load gcc/9.1.0
.. or ..
module load gcc/7.5.0

Then use module avail to see the installed list of packages.

List of installed packages on Summit for E4S release 21.08:",4.3332486221756845
"How can I run a job step on multiple nodes in Frontier?
","Unlike Summit and Titan, there are no launch/batch nodes on Frontier. This means your batch script runs on a node allocated to you rather than a shared node. You still must use the job launcher (srun) to run parallel jobs across all of your nodes, but serial tasks need not be launched with srun.



To easily visualize job examples (see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-mapping further below), the compute node diagram has been simplified to the picture shown below.

Simplified Frontier node architecture diagram",4.298597947365122
"How can I run a job step on multiple nodes in Frontier?
","Due to the unique architecture of Frontier compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.

Before jumping into the examples, it is helpful to understand the output from the hello_jobstep program:",4.291032206371063
"How can I run a job step on multiple nodes in Frontier?
","To execute multiple job steps concurrently, standard UNIX process backgrounding can be used by adding a & at the end of the command. This will return control to the job script and execute the next command immediately, allowing multiple job launches to start at the same time. The jsruns will not share core/gpu resources in this configuration. The batch node allocation is equal to the sum of those of each jsrun, and the total walltime must be equal to or greater than that of the longest running jsrun task.",4.233548247517938
"How can you move a NumPy array from the CPU to a GPU using Cupy?
","As is the standard with NumPy being imported as ""np"", CuPy is often imported in a similar fashion:

>>> import numpy as np
>>> import cupy as cp

Similar to NumPy arrays, CuPy arrays can be declared with the cupy.ndarray class. NumPy arrays will be created on the CPU (the ""host""), while CuPy arrays will be created on the GPU (the ""device""):

>>> x_cpu = np.array([1,2,3])
>>> x_gpu = cp.array([1,2,3])

Manipulating a CuPy array can also be done in the same way as manipulating NumPy arrays:",4.307120047637243
"How can you move a NumPy array from the CPU to a GPU using Cupy?
","CuPy is a library that implements NumPy arrays on NVIDIA GPUs by utilizing CUDA Toolkit libraries like cuBLAS, cuRAND, cuSOLVER, cuSPARSE, cuFFT, cuDNN and NCCL. Although optimized NumPy is a significant step up from Python in terms of speed, performance is still limited by the CPU (especially at larger data sizes) -- this is where CuPy comes in. Because CuPy's interface is nearly a mirror of NumPy, it acts as a replacement to run existing NumPy/SciPy code on NVIDIA CUDA platforms, which helps speed up calculations further. CuPy supports most of the array operations that NumPy provides,",4.2381232633995465
"How can you move a NumPy array from the CPU to a GPU using Cupy?
","most of the array operations that NumPy provides, including array indexing, math, and transformations. Most operations provide an immediate speed-up out of the box, and some operations are sped up by over a factor of 100 (see CuPy benchmark timings below, from the Single-GPU CuPy Speedups article).",4.229488987052543
"How can I send signal 9 to a job using bkill?
","You can send signals to jobs with the bkill command. While the command name suggests its only purpose is to terminate jobs, this is not the case. Similar to the kill command found in Unix-like operating systems, this command can be used to send various signals (not just SIGTERM and SIGKILL) to jobs. The command can accept both numbers and names for signals. For a list of accepted signal names, run bkill -l. Common ways to invoke the command include:",4.472966951051412
"How can I send signal 9 to a job using bkill?
","| Command | Description | | --- | --- | | bkill 12345 | Force a job to stop by sending SIGINT, SIGTERM, and SIGKILL. These signals are sent in that order, so users can write applications such that they will trap SIGINT and/or SIGTERM and exit in a controlled manner. | | bkill -s USR1 12345 | Send SIGUSR1 to job 12345 NOTE: When specifying a signal by name, omit SIG from the name. Thus, you specify USR1 and not SIGUSR1 on the bkill command line. | | bkill -s 9 12345 | Send signal 9 to job 12345 |",4.328823098741336
"How can I send signal 9 to a job using bkill?
","Thus, if you want to send SIGUSR1 to a job, you would use scancel -s 10 12345 or scancel -s USR1 12345.",4.284481098439819
"What is the significance of the $MEMBERWORK/<YOUR_PROJECT_ID> directory?
","3

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

4

Retention is not applicable as files will follow purge cycle.",4.266974884093909
"What is the significance of the $MEMBERWORK/<YOUR_PROJECT_ID> directory?
","3

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

4

Retention is not applicable as files will follow purge cycle.",4.266974884093909
"What is the significance of the $MEMBERWORK/<YOUR_PROJECT_ID> directory?
","Footnotes

1

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

2

Retention is not applicable as files will follow purge cycle.",4.256627363030495
"Can I use --stdio_stdout and --stdio_stderr with relative paths in Summit?
","When using --stdio_stdout or --stdio_stderr users must use absolute paths. Using relative paths (e.g. ./my_stdout) will not successfully create the file in the user's current working directory. An bug has been filed with IBM to fix this issue and allow relative paths.

In some cases users will encounter a segmentation fault when running job steps that have uneven number of resource sets per node. For example:",4.298008988646438
"Can I use --stdio_stdout and --stdio_stderr with relative paths in Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.127444720394252
"Can I use --stdio_stdout and --stdio_stderr with relative paths in Summit?
","<string>:17: (INFO/1) Duplicate explicit target name: ""summit"".

<string>:17: (INFO/1) Duplicate explicit target name: ""x11 forwarding"".

After logging onto Summit (with X11 forwarding), execute the series of commands below:

$ module load vampir

$ vampir &

Once the GUI pops up (might take a few seconds), you can load a file resident on the file system by selecting Local File for file selection.",4.126501021742124
"Are there any limitations on the number of minutes that can be reserved on each device per month?
","There is a limited number of minutes per month that can be reserved on each device. Reservations are supported on these devices with these monthly allocations:

ibmq_kolkata, 2400 minutes per month

ibmq_jakarta, 480 minutes per month

In order to make the most efficient use of reservation allocations:

Reservations requests must be submitted to the project Principle Investigator (PI) to help@olcf.ornl.gov

Requests for reservations must include technical justification.

Once submitted, requests will be sent to the Quantum Resource Utilization Council (QRUC) for consideration.",4.422690715873979
"Are there any limitations on the number of minutes that can be reserved on each device per month?
","To prevent reserved resources from remaining idle for an extended period of time, reservations are monitored for inactivity. If activity falls below 50% of the reserved resources for more than (30) minutes, the reservation will be canceled and the system will be returned to normal scheduling. A new reservation must be requested if this occurs.",4.163846658267104
"Are there any limitations on the number of minutes that can be reserved on each device per month?
","reservations are monitored for inactivity. If activity falls below 50% of the reserved resources for more than (30) minutes, the reservation will be canceled and the system will be returned to normal scheduling. A new reservation must be requested if this occurs.",4.162070436759792
"How does the script handle partial files left behind by a failed `sbcast` command?
","Slurm contains a utility called sbcast. This program takes a file and broadcasts it to each node's node-local storage (ie, /tmp, NVMe). This is useful for sharing large input files, binaries and shared libraries, while reducing the overhead on shared file systems and overhead at startup. This is highly recommended at scale if you have multiple shared libraries on Lustre/NFS file systems.

Here is a simple example of a file sbcast from a user's scratch space on Lustre to each node's NVMe drive:",4.112469076134775
"How does the script handle partial files left behind by a failed `sbcast` command?
","Slurm contains a utility called sbcast. This program takes a file and broadcasts it to each node's node-local storage (ie, /tmp, NVMe). This is useful for sharing large input files, binaries and shared libraries, while reducing the overhead on shared file systems and overhead at startup. This is highly recommended at scale if you have multiple shared libraries on Lustre/NFS file systems.

Here is a simple example of a file sbcast from a user's scratch space on Lustre to each node's NVMe drive:",4.112469076134775
"How does the script handle partial files left behind by a failed `sbcast` command?
","sbcast --send-libs -pf ${exe} /mnt/bb/$USER/${exe}
if [ ! ""$?"" == ""0"" ]; then
    # CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,
    # your application may pick up partially complete shared library files, which would give you confusing errors.
    echo ""SBCAST failed!""
    exit 1
fi",4.061817168406221
"How do I specify the number of nodes to use in VisIt?
","Navigate to the appropriate file.

Once you choose a file, you will be prompted for the number of nodes and processors you would like to use (remember that each node of Andes contains 32 processors, or 28 if using the high-memory GPU partition) and the Project ID, which VisIt calls a ""Bank"" as shown below.



Once specified, the server side of VisIt will be launched, and you can interact with your data.",4.367608492748981
"How do I specify the number of nodes to use in VisIt?
","the -s flag will launch the CLI in the form of a Python shell, which can be useful for interactive batch jobs.  The -np and -nn flags represent the number of processors and nodes VisIt will use to execute the Python script, while the -l flag specifies the specific parallel method to do so (required). Execute visit -fullhelp to get a list of all command line options.",4.200725957228767
"How do I specify the number of nodes to use in VisIt?
","VisIt developers have been notified, and at this time the current workaround is to disable Scalable Rendering from being used. To do this, go to Options→Rendering→Advanced and set the ""Use Scalable Rendering"" option to ""Never"".

However, this workaround has been reported to affect VisIt's ability to save images, as scalable rendering is utilized to save plots as image files (which can result in another compute engine crash). To avoid this, screen capture must be enabled. Go to File→""Set save options"" and check the box labeled ""Screen capture"".",4.1150066955391305
"Can you describe the AMP technique in a few words?
","Lastly, when performing the training step of a deep learning application it is often beneficial to do at least some of the layer calculations in reduced precision. The AMP technique described above can be tried with little to know code changes, making it highly advisable to attempt in any machine learning application.

NVIDIA has provided several example codes for using Tensor Cores from a variety of the APIs listed above. These examples can be found on GitHub.

NVIDIA Tensor Core Workshop (August 2018): slides, recording (coming soon)",4.171993107751333
"Can you describe the AMP technique in a few words?
","NVIDIA has also integrated a technology called Automatic Mixed Precision (AMP) into several common frameworks, TensorFlow, PyTorch, and MXNet at time of writing. In most cases AMP can be enabled via a small code change or via setting and environment variable. AMP does not strictly replace all matrix multiplication operations with half precision, but uses graph optimization techniques to determine whether a given layer is best run in full or half precision.

Examples are provided for using AMP, but the following sections summarize the usage in the three supported frameworks.",4.12950482957779
"Can you describe the AMP technique in a few words?
","With TensorFlow AMP can be enabled using one of the following techniques.

os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'

OR

export TF_ENABLE_AUTO_MIXED_PRECISION=1

Explicit optimizer wrapper available in NVIDIA Container 19.07+, TF 1.14+, TF 2.0:

opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)

Adding the following to a PyTorch model will enable AMP:

model, optimizer = amp.initialize(model, optimizer, opt_level=""O1"")
with amp.scale_loss(loss, optimizer) as scaled_loss:
  scaled_loss.backward()

The code below will enable AMP for MXNet:",3.965798944979056
"Is there a way to view the dependencies of a specific module using the spider sub-command?
","Searching for modules

Modules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the spider sub-command can be used as summarized in the following table.",4.433891421125456
"Is there a way to view the dependencies of a specific module using the spider sub-command?
","Modules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the spider sub-command can be used as summarized in the following table.",4.402455619177065
"Is there a way to view the dependencies of a specific module using the spider sub-command?
","Modules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the spider sub-command can be used as summarized in the following table.",4.401976509230983
"How do I list all the environments I have created?
","This also is a great way to keep track of the locations and names of all other environments that have been created. The current environment is indicated by *.

To see what packages are installed in the active environment, use conda list:

$ conda list",4.324630074161146
"How do I list all the environments I have created?
","Exporting (sharing) an environment:

You may want to share your environment with someone else. One way to do this is by creating your environment in a shared location where other users can access it. A different way (the method described below) is to export a list of all the packages and versions of your environment (an environment.yml file). If a different user provides conda the list you made, conda will install all the same package versions and recreate your environment for them -- essentially ""sharing"" your environment. To export your environment list:",4.14178048523889
"How do I list all the environments I have created?
","$ source activate my_env
$ conda env export > environment.yml

You can then email or otherwise provide the environment.yml file to the desired person. The person would then be able to create the environment like so:

$ conda env create -f environment.yml



List environments:

$ conda env list

List installed packages in current environment:

$ conda list

Creating an environment with Python version X.Y:

For a specific path:

$ conda create -p /path/to/your/my_env python=X.Y

For a specific name:

$ conda create -n my_env python=X.Y

Deleting an environment:

For a specific path:",4.11565292307318
"Can data be copied directly from Alpine (GPFS) to Orion (Lustre)?
","Standard tools such as rsync and scp can also be used through the DTN, but may be slower and require more manual intervention than Globus

Copying data directly from Alpine (GPFS) to Orion (Lustre)

Globus is the suggested tool to transfer needed data from Alpine to Orion.

Globus should be used when transfer large amounts of data.

Standard tools such as rsync and cp can also be used. The DTN mounts both filesystems and should be used when transferring with rsync and cp tools. These methods should not be used to transfer large amounts of data.

Copying data to the HPSS archive system",4.505549715261897
"Can data be copied directly from Alpine (GPFS) to Orion (Lustre)?
",The following example is intended to help users who are making the transition from Summit to Frontier to move their data between Alpine GPFS and Orion Lustre. We strongly recommend using Globus for this transfer as it is the method that is most efficient for users and that causes the least contention on filesystems and data transfer nodes.,4.483821500209466
"Can data be copied directly from Alpine (GPFS) to Orion (Lustre)?
",Data will not be automatically transferred from Alpine to Orion. Users should consider the data needed from Alpine and transfer it. Globus is the preferred method and there is access to both Orion and Alpine through the Globus OLCF DTN endpoint. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-transferring-data-globus. Use of HPSS to specifically stage data for the Alpine to Orion transfer is discouraged.,4.4014566140063
"Are files in the Member Work and Member Archive directories considered project data?
","World Archive Directory: 775

For example, if you have data that must be restricted only to yourself, keep them in your Member Archive directory for that project (and leave the default permissions unchanged). If you have data that you intend to share with researchers within your project, keep them in the project’s Project Archive directory. If you have data that you intend to share with researchers outside of a project, keep them in the project’s World Archive directory.",4.439522845833013
"Are files in the Member Work and Member Archive directories considered project data?
","As with the three project work areas, the difference between these three areas lies in the accessibility of data to project members and to researchers outside of the project. Member Archive directories are accessible only by an individual project member by default, Project Archive directories are accessible by all project members, and World Archive directories are readable by any user on the system.

<string>:194: (INFO/1) Duplicate implicit target name: ""permissions"".",4.38002386093247
"Are files in the Member Work and Member Archive directories considered project data?
","Footnotes

1

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

2

Retention is not applicable as files will follow purge cycle.",4.364841025368737
"How can I optimize the use of GPUs in Frontier with MPI ranks?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.496634694501392
"How can I optimize the use of GPUs in Frontier with MPI ranks?
","As has been pointed out previously in the Frontier documentation, notice that GPUs are NOT mapped to MPI ranks in sequential order (e.g., MPI rank 0 is mapped to physical CPU cores 1-7 and GPU 4, MPI rank 1 is mapped to physical CPU cores 9-15 and GPU 5), but this IS expected behavior. It is simply a consequence of the Frontier node architectures as shown in the Frontier Node Diagram and subsequent https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Note on NUMA domains <numa-note>.

Example 2: 1 MPI rank with 7 CPU cores and 1 GPU (single-node)",4.433176286127103
"How can I optimize the use of GPUs in Frontier with MPI ranks?
","While this level of control over mapping MPI ranks to GPUs might be useful for some applications, it is always important to consider the implication of the mapping. For example, if the order of the GPU IDs in the map_gpu option is reversed, the MPI ranks and the GPUs they are mapped to would be in different L3 cache regions, which could potentially lead to poorer performance.

Example 4: 16 MPI ranks - each with 2 OpenMP threads and 1 *specific* GPU (multi-node)",4.339214649423727
"What options are available for submitted jobs on Frontier?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.338326758825797
"What options are available for submitted jobs on Frontier?
","Users may have only 100 jobs queued in the batch queue at any time (this includes jobs in all states). Additional jobs will be rejected at submit time.

The debug quality of service (QOS) class can be used to access Frontier's compute resources for short non-production debug tasks. The QOS provides a higher priority compare to jobs of the same job size bin in production queues. Production work and job chaining using the debug QOS is prohibited. Each user is limited to one job in any state at any one point. Attempts to submit multiple jobs to this QOS will be rejected upon job submission.",4.261723598133112
"What options are available for submitted jobs on Frontier?
","The table below summarizes options for submitted jobs. Unless otherwise noted, they can be used for either batch scripts or interactive batch jobs. For scripts, they can be added on the sbatch command line or as a #BSUB directive in the batch script. (If they're specified in both places, the command line takes precedence.) This is only a subset of all available options. Check the Slurm Man Pages for a more complete list.",4.238899970970014
"What is the name of the PersistentVolumeClaim?
","Once the Persistent Volume is created its allocated memory is available to be claimed by a project. This is done through a Persistent Volume Claim. PVC's are created using a YAML file in the same manner that PV's are created. An example of a PVC that claims the PV created in the previous section follows:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: storage-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

The claim is then placed using:

oc create -f storage-1.yaml",4.438586892298391
"What is the name of the PersistentVolumeClaim?
","apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  # The name of the claim
  name: test-pod-pvc
spec:
  # The type of storage being requested. This can be blank and it will be
  # set to the default value, which is netapp-nfs, but it is good practice
  # to explictly declare it.
  storageClassName: netapp-nfs
  # how the volume can be accessed. ReadWriteMany, or RWX as it is abbreviated,
  # means the volume can be mounted as Read Write by multiple nodes
  accessModes:
  - ReadWriteMany
  resources:
    # the amount of storage being requested
    requests:
      storage: 1Gi",4.412475386302101
"What is the name of the PersistentVolumeClaim?
",a desired size for a PersistentVolume. The cluster administrator or some automated mechanism will provision the storage on the backend and make it available to the cluster via the PersistentVolumeClaim.,4.410713539989834
"How much storage space is available for project home directories on Crusher?
",Crusher is connected to the center-wide IBM Spectrum Scale™ filesystem providing 250 PB of storage capacity with a peak write speed of 2.5 TB/s. Crusher also has access to the center-wide NFS-based filesystem (which provides user and project home areas). While Crusher does not have direct access to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#dtn-user-guide to move data to/from HPSS.,4.269395209234491
"How much storage space is available for project home directories on Crusher?
","Although there are no hard quota limits for the project storage, an upper storage limit should be reported in the project request. The available space of a project can be modified upon request.",4.233977699361589
"How much storage space is available for project home directories on Crusher?
","Open and Moderate Projects are provided with a Project Home storage area in the NFS-mounted filesystem. This area is intended for storage of data, code, and other files that are of interest to all members of a project. Since Project Home is an NFS-mounted filesystem, its performance will not be as high as other filesystems.

Moderate Enhanced projects are not provided with Project Home spaces, just Project Work spaces.

Project Home area is accessible at /ccs/proj/abc123 (where abc123 is your project ID).",4.223587099933633
"What is the command-line option used to specify the number of resource sets to create?
","The following table provides a quick reference for creating resource sets of various common use cases. The -n flag can be altered to specify the number of resource sets needed.

| Resource Sets | MPI Tasks | Threads | Physical Cores | GPUs | jsrun Command | | --- | --- | --- | --- | --- | --- | | 1 | 42 | 0 | 42 | 0 | jsrun -n1 -a42 -c42 -g0 | | 1 | 1 | 0 | 1 | 1 | jsrun -n1 -a1 -c1 -g1 | | 1 | 2 | 0 | 2 | 1 | jsrun -n1 -a2 -c2 -g1 | | 1 | 1 | 0 | 1 | 2 | jsrun -n1 -a1 -c1 -g2 | | 1 | 1 | 21 | 21 | 3 | jsrun -n1 -a1 -c21 -g3 -bpacked:21 |",4.324377148422032
"What is the command-line option used to specify the number of resource sets to create?
","The following example will create 12 resource sets each with 1 task, 4 threads, and 1 GPU. Each MPI task will start 4 threads and have access to 1 GPU. Rank 0 will have access to GPU 0 and start 4 threads on the first socket of the first node ( red resource set). Rank 2 will have access to GPU 1 and start 4 threads on the second socket of the first node ( green resource set). This pattern will continue until 12 resource sets have been created. The following jsrun command will create 12 resource sets (-n12). Each resource set will contain 1 MPI task (-a1), 1 GPU (-g1), and 4 cores (-c4).",4.274622539231978
"What is the command-line option used to specify the number of resource sets to create?
","The following jsrun command will request 12 resource sets (-n12). Each resource set will contain 2 MPI tasks (-a2), 1 GPU (-g1), and 2 cores (-c2). 2 MPI tasks will have access to a single GPU. Ranks 0 - 1 will have access to GPU 0 on the first node ( red resource set). Ranks 2 - 3 will have access to GPU 1 on the first node ( green resource set). This pattern will continue until 12 resource sets have been created.",4.250257337958193
"How can I see the status of my build?
","What happens is that oc pulls in the provided repository, in this example Django, and automatically configures everything needed to build the image. You should now be able to go to the Openshift web GUI and under the builds tab see your newly built build.

Now, since everything has been configured, you can click the Start Build button in the upper right hand side of the Web GUI anytime that you need to make another build. You can also start a another build from the command line with either:

oc start-build <buildconfig_name>

Or, if you would like to receive logs from the build:",4.057623712760404
"How can I see the status of my build?
","Openshift will automatically pull and build your source. If you make a change and need an updated build simply navigate to the build option on the menu to the left and select your project and click Start Build in the upper right.

The first step to creating the the build is to create a build config. This is done in one of three ways depending on how your code is structured. If you have a git repository already configured and it is public then the command

oc new-build .

will create your build config from that repository.",4.044059040518658
"How can I see the status of my build?
","You should now be on the 'Pod' screen with the 'Overview' tab selected From here you can get a quick idea of the amount of resources (memory, CPU etc) that your  Pod is using.

Click on the 'Logs' tab to get the logs from your pod. This will display ""Hello World!"" in our example because of our echo command. There will be a dropdown here that for our example will contain only one item named 'hello-openshift'. This is the name of the container that you are viewing the logs for inside your pod.",4.029630020539773
"What is the name of the tool that Ronny Brendel introduces in the video?
","This recording is from the 2018 Score-P / Vampir workshop that took place at ORNL on August 17, 2018. In the video, Ronny Brendel gives an introduction to the Score-P and Vampir tools, which are often used together to collect performance profiles/traces from an application and visualize the results.",4.231693262853037
"What is the name of the tool that Ronny Brendel introduces in the video?
","<p>This recording is from the 2018 Score-P / Vampir workshop that took place at ORNL on August 17, 2018. In the video, Ronny Brendel gives an introduction to the Score-P and Vampir tools, which are often used together to collect performance profiles/traces from an application and visualize the results.</p>",4.189537584906404
"What is the name of the tool that Ronny Brendel introduces in the video?
",| (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/04/CrayToolsAndDebuggers_v1.0_pdfVersion.pdf https://vimeo.com/554873364 | | 2021-05-20 | Spock Tips & Information | Tom Papatheodore (OLCF) | Spock Training https://www.olcf.ornl.gov/spock-training/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/04/Spock_Tips.pdf https://vimeo.com/554875783 | | 2021-03-31 | NVIDIA RAPIDS | Joe Eaton (NVIDIA) and Benjamin Hernandez (OLCF) | March 2021 OLCF User Conference Call https://www.olcf.ornl.gov/calendar/userconcall-mar2021/ | (recording),3.9198764878067416
"What command can I use to interact with a batch job after it has been submitted?
","Sometimes it’s necessary to interact with a batch job after it has been submitted. LSF provides several commands for interacting with already-submitted jobs.

Many of these commands can operate on either one job or a group of jobs. In general, they only operate on the most recently submitted job that matches other criteria provided unless “0” is specified as the job id.",4.503508700803893
"What command can I use to interact with a batch job after it has been submitted?
","Most users will find batch jobs to be the easiest way to interact with the system, since they permit you to hand off a job to the scheduler and then work on other tasks; however, it is sometimes preferable to run interactively on the system. This is especially true when developing, modifying, or debugging a code.",4.350336085505381
"What command can I use to interact with a batch job after it has been submitted?
","The most common way to interact with the batch system is via batch scripts. A batch script is simply a shell script with added directives to request various resoruces from or provide certain information to the scheduling system.  Aside from these directives, the batch script is simply the series of commands needed to set up and run your job.

To submit a batch script, use the command sbatch myjob.sl

Consider the following batch script:

#!/bin/bash
#SBATCH -A ABC123
#SBATCH -J RunSim123
#SBATCH -o %x-%j.out
#SBATCH -t 1:00:00
#SBATCH -p batch
#SBATCH -N 1024",4.345242455165183
"What modules do I need to load for this job?
","Users will be advised to do a module load <UMS project>, which will expose modules for the individual products associated with that project, accessed by a second module load <product>.

Project PI must ensure that support is provided for users of the product, as documented in the statement of support.

Project PI must ensure that the product is updated in response to changes in the system software environment (e.g. updates to OS, compiler, library, or other key tools) and in a timely fashion.  If this commitment is not met, the OLCF may remove the software from UMS.",4.191261594861512
"What modules do I need to load for this job?
","below), you can choose between the rendering options by loading its corresponding module on the system you're connected to. For example, to see these modules on Summit:",4.157859509991617
"What modules do I need to load for this job?
","module load open-ce

Loading a specific version of the module is recommended to future-proof scripts against software updates. The following commands can be used to find and load specific module versions:

[user@login2.summit ~]$ module avail open-ce",4.1385182641001546
"How do I analyze the performance data generated by PAT?
","module load valgrind4hpc

Additional information about Valgrind4hpc usage can be found on the HPE Cray Programming Environment User Guide Page.

The Performance Analysis Tools (PAT), formerly CrayPAT, are a suite of utilities that enable users to capture and analyze performance data generated during program execution. These tools provide an integrated infrastructure for measurement, analysis, and visualization of computation, communication, I/O, and memory utilization to help users optimize programs for faster execution and more efficient computing resource usage.",4.298817983417769
"How do I analyze the performance data generated by PAT?
","Some libraries still resolved to paths outside of /mnt/bb, and the reason for that is that the executable may have several paths in RPATH.



The Performance Analysis Tools (PAT), formerly CrayPAT, are a suite of utilities that enable users to capture and analyze performance data generated during program execution. These tools provide an integrated infrastructure for measurement, analysis, and visualization of computation, communication, I/O, and memory utilization to help users optimize programs for faster execution and more efficient computing resource usage.",4.2797861127148416
"How do I analyze the performance data generated by PAT?
","pat_report hello_jobstep+pat+39545-2t

The resulting report includes profiles of functions, profiles of maximum function times, details on load imbalance, details on program energy and power usages, details on memory high water mark, and more.

More detailed information on the HPE Performance Analysis Tools can be found in the HPE Performance Analysis Tools User Guide.

When using perftools-lite-gpu, there is a known issue causing ld.lld not to be found. A workaround this issue can be found here.",4.150881085623468
"What is the impact of non-optimal I/O patterns on Summit?
","The I/O performance can be lower than the optimal one when you save one single shared file with non-optimal I/O pattern. Moreover, the previous performance results are achieved under an ideal system, the system is dedicated, and a specific number of compute nodes are used. The file system is shared across many users; the I/O performance can vary because other users that perform heavy I/O as also executing large scale jobs and stress the interconnection network. Finally, if the I/O pattern is not aligned, then the I/O performance can be significantly lower than the ideal one.  Similar, related",4.289933104216017
"What is the impact of non-optimal I/O patterns on Summit?
","When a user occupies more than one compute node, then they are using more NVMes and the I/O can scale linearly. For example in the following plot you can observe the scalability of the IOR benchmark on 2048 compute nodes on Summit where the write performance achieves 4TB/s and the read 11.3 TB/s",4.195752268858374
"What is the impact of non-optimal I/O patterns on Summit?
","Summit

Andes

Frontier

Scientific simulations generate large amounts of data on Summit (about 100 Terabytes per day for some applications). Because of how large some datafiles may be, it is important that writing and reading these files is done as fast as possible. Less time spent doing input/output (I/O) leaves more time for advancing a simulation or analyzing data.",4.194413061275792
"What are the KDI access procedures that I need to follow?
","Login to https://kdivdi.ornl.gov with your KDI issued credentials

Launch the Putty Application

Enter the hostname ""citadel.ccs.ornl.gov"" and click Open

You will then be in an ssh terminal to authenticate with your OLCF credentials as detailed above.

Projects using ORNL's KDI must following KDI access procedures and cannot access SPI resources directly.  If your project uses both KDI and SPI, you will not access the SPI resources directly.",4.134630963468807
"What are the KDI access procedures that I need to follow?
","Similar to the non-SPI resources, SPI resources require two-factor authentication.  If you are new to the center, you will receive a SecurID fob during the account approval/creation process.  If you are an existing user of non-SPI resources, you can use the same SecurID fob and PIN used on your non-SPI account.

Also similar to non-SPI resources, you will connect directly to the SPI resources through ssh.

ORNL's KDI users are an exception and cannot, by policy, log directly into SPI resources.  KDI users, please follow the KDI documented procedures:",4.124272194625497
"What are the KDI access procedures that I need to follow?
","https://docs.olcf.ornl.gov/systems/index.html#Whitelist your IPs<spi-whitelisting-ip>.  Access to the SPI resources is limited to IPs that have been whitelisted by the OLCF.  The only exception is for projects using KDI resources.  If your project also uses KDI resources, you will use the KDI access procedures and do not need to provide your IP to the OLCF.",4.091149356501365
"What is the annual call for INCITE proposals?
",|  | INCITE | Director's Discretion | ALCC | | --- | --- | --- | --- | | Allocations | Large | Small | Large | | Call for Proposals | Once per year | At any time | Once per year | | Duration | 1 year | Up to 12 months | 1 year | | Priority | High | Medium | High | | Closeout Report | yes | yes | yes | | Quarterly Reports | yes | no* | yes | | Where to apply | Apply for INCITE https://doeleadershipcomputing.org/proposal/call-for-proposals/ | Apply for DD https://my.olcf.ornl.gov/project-application-new | Apply for ALCC http://science.energy.gov/ascr/facilities/accessing-ascr-facilities/alcc/ |,4.428870200093973
"What is the annual call for INCITE proposals?
","INCITE – The Novel Computational Impact on Theory and Experiment (INCITE) program invites proposals for large-scale, computationally intensive research projects to run at the OLCF. The INCITE program awards sizeable allocations (typically, millions of processor-hours per project) on some of the world’s most powerful supercomputers to address grand challenges in science and engineering. There is an annual call for INCITE proposals and awards are made on an annual basis. Please visit the Department of Energy Leadership Computing website for more information and to submit a proposal.",4.382415494085848
"What is the annual call for INCITE proposals?
",The OLCF has a pull-back policy for under-utilization of INCITE allocations. Under-utilized INCITE project allocations will have core-hours removed from their outstanding core-hour project balance at specific times during the INCITE calendar year. The following table summarizes the current under-utilization policy:,4.204191392743172
"How can I specify 42 CPU cores per resource set?
","Based on how your code expects to interact with the system, you can create resource sets containing the needed GPU and core resources. If a code expects to utilize one GPU per task, a resource set would contain one core and one GPU. If a code expects to pass work to a single GPU from two tasks, a resource set would contain two cores and one GPU.

Decide on the number of resource sets needed

Once you understand tasks, threads, and GPUs in a resource set, you simply need to decide the number of resource sets needed.",4.330645119884886
"How can I specify 42 CPU cores per resource set?
","The following example will create 12 resource sets each with 1 task, 4 threads, and 1 GPU. Each MPI task will start 4 threads and have access to 1 GPU. Rank 0 will have access to GPU 0 and start 4 threads on the first socket of the first node ( red resource set). Rank 2 will have access to GPU 1 and start 4 threads on the second socket of the first node ( green resource set). This pattern will continue until 12 resource sets have been created. The following jsrun command will create 12 resource sets (-n12). Each resource set will contain 1 MPI task (-a1), 1 GPU (-g1), and 4 cores (-c4).",4.262663438602228
"How can I specify 42 CPU cores per resource set?
","Because jsrun sees 8 cores and the -brs flag, it assigns all 8 cores to each of the 2 tasks in the resource set. Jsrun will set up OMP_NUM_THREADS as 32 (8 cores with 4 threads per core) which will apply to all the tasks in the resource set. This means that each task sees that it can have 32 threads (which means 64 threads for the 2 tasks combined) which will oversubscribe the cores and may decrease efficiency as a result.",4.236679225813952
"How can I install mpi4py from source on Summit?
","Andes

.. code-block:: bash

   $ MPICC=""mpicc -shared"" pip install --no-cache-dir --no-binary=mpi4py mpi4py

Frontier

.. code-block:: bash

   $ MPICC=""cc -shared"" pip install --no-cache-dir --no-binary=mpi4py mpi4py

The MPICC flag ensures that you are using the correct C wrapper for MPI on the system. Building from source typically takes longer than a simple conda install, so the download and installation may take a couple minutes. If everything goes well, you should see a ""Successfully installed mpi4py"" message.

Next, install h5py from source.

Summit

.. code-block:: bash",4.461619637475388
"How can I install mpi4py from source on Summit?
","Andes

.. code-block:: bash

   $ source activate /ccs/proj/<project_id>/<user_id>/envs/andes/h5pympi-andes

Frontier

.. code-block:: bash

   $ source activate /ccs/proj/<project_id>/<user_id>/envs/frontier/h5pympi-frontier

Now that you have a fresh environment, you will next install mpi4py from source into your new environment. To make sure that you are building from source, and not a pre-compiled binary, use pip:

Summit

.. code-block:: bash

   $ MPICC=""mpicc -shared"" pip install --no-cache-dir --no-binary=mpi4py mpi4py

Andes

.. code-block:: bash",4.403642442616127
"How can I install mpi4py from source on Summit?
","Summit

.. code-block:: bash

   $ HDF5_MPI=""ON"" CC=mpicc pip install --no-cache-dir --no-binary=h5py h5py

Andes

.. code-block:: bash

   $ HDF5_MPI=""ON"" CC=mpicc pip install --no-cache-dir --no-binary=h5py h5py

Frontier

.. code-block:: bash

   $ HDF5_MPI=""ON"" CC=cc HDF5_DIR=${OLCF_HDF5_ROOT} pip install --no-cache-dir --no-binary=h5py h5py",4.378267714577154
"How can I analyze the binary of executables and its dependent libraries on Summit?
","# 2. Analyze the binary of executables and its dependent libraries
hpcstruct <measurement_dir>

# 3. Combine measurements with program structure information and generate a database
hpcprof -o <database_dir> <measurement_dir>

# 4. Understand performance issues by analyzing profiles and traces with the GUI
hpcviewer <database_dir>

More detailed information on HPCToolkit can be found in the HPCToolkit User's Manual.

HPCToolkit does not require a recompile to profile the code. It is recommended to use the -g optimization flag for attribution to source lines.",4.192509732562785
"How can I analyze the binary of executables and its dependent libraries on Summit?
","TAU is installed with Program Database Toolkit (PDT) on Summit. PDT is a framework for analyzing source code written in several programming languages. Moreover, Performance Application Programming Interface (PAPI) is supported. PAPI counters are used to assess the CPU performance. In this section, some approaches for profiling and tracing will be presented.

In most cases, we need to use wrappers to recompile the application:

For C: replace the compiler with the TAU wrapper tau_cc.sh

For C++: replace the compiler with the TAU wrapper tau_cxx.sh",4.110507878894819
"How can I analyze the binary of executables and its dependent libraries on Summit?
","Login to https://docs.olcf.ornl.gov/systems/Scorep.html#Summit <connecting-to-olcf>: ssh <user_id>@summit.olcf.ornl.gov

Instrument your code with Score-P

Perform a measurement run with profiling enabled

Perform a profile analysis with CUBE or cube_stat

Use scorep-score to define a filter

Perform a measurement run with tracing enabled and the filter applied

Perform in-depth analysis on the trace data with Vampir",4.099066678917504
"What is the main goal of the QCUP program?
","There are several broad aims of the Quantum Computing User Program at OLCF, which are as follows:

Enable Research

The QCUP aims to provide a broad spectrum of user access to the best available quantum computing systems. Once a user’s intended research has been reviewed for merit and user agreements have been established, we seek to provide users with the opportunity to become familiar with the unique aspects and challenges of quantum computing, as well as to implement and test quantum algorithms on the available systems.

Evaluate Technology",4.345371510330615
"What is the main goal of the QCUP program?
","The QCUP aims to aid in the evaluation of technology by monitoring the breadth and performance of early quantum computing applications. How users integrate quantum computing with scientific computing is a question constrained by both application, infrastructure constraints, and the use cases expected for the associated computational system. Through the QCUP program, users can explore new potential computational research applications, and potentially accelerate existing scientific applications using quantum processors and architectures. Research projects supported include advanced scientific",4.318434570935868
"What is the main goal of the QCUP program?
","The QCUP aims to engage the quantum computing community and support the growth of the quantum information science ecosystems. Our quantum computing users range in quantum computing experience from novice to expert; users are from US national labs, universities, government, and industry.  User groups utilize quantum computing expertise to investigate diverse application interests, using multiple programming languages, quantum-classical programming, and multiple software environments. Most projects focus on proof-of-principle demonstrations and/or new method development. Some projects focus on",4.305444385800171
"What is the purpose of the SPI Data Transfer Nodes (DTNs)?
","The SPI provides separate https://docs.olcf.ornl.gov/systems/index.html#Data Transfer Nodes<spi-data-transfer> configured specifically for SPI workflows.  The nodes are not directly accessible for login but are accessible through the Globus tool.  The SPI DTNs mount the same Arx filesystem available on the SPI compute resources.  Globus is the preferred method to transfer data into and out of the SPI resources.

Please see the https://docs.olcf.ornl.gov/systems/index.html#Data Transfer Nodes<spi-data-transfer> section for more details.",4.367615199829963
"What is the purpose of the SPI Data Transfer Nodes (DTNs)?
",The Data Transfer Nodes (DTNs) are hosts specifically designed to provide optimized data transfer between OLCF systems and systems outside of the OLCF network. These nodes perform well on local-area transfers as well as the wide-area data transfers for which they are tuned. The OLCF recommends that users use these nodes to improve transfer speed and reduce load on computational systems’ login and service nodes. OLCF provides two sets of DTNs: one for systems in our moderate enclave and a second for systems in the open enclave.,4.361309383295086
"What is the purpose of the SPI Data Transfer Nodes (DTNs)?
","https://docs.olcf.ornl.gov/systems/index.html#Transfer needed data<spi-data-transfer> to the SPI filesystems.  The SPI resources mount filesystems unique to the SPI.  Needed data, code, and libraries must be transferred into the SPI using the SPI's Data Transfer Nodes.",4.189096946580969
"What is the role of the `OMP_NUM_THREADS` environment variable in the program?
","In addition to specifying the SMT level, you can also control the number of threads per MPI task by exporting the OMP_NUM_THREADS environment variable. If you don't export it yourself, Jsrun will automatically set the number of threads based on the number of cores requested (-c) and the binding (-b) option. It is better to be explicit and set the OMP_NUM_THREADS value yourself rather than relying on Jsrun constructing it for you. Especially when you are using Job Step Viewer which relies on the presence of that environment variable to give you visual thread assignment information.",4.382788415708666
"What is the role of the `OMP_NUM_THREADS` environment variable in the program?
","There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.",4.278339580966069
"What is the role of the `OMP_NUM_THREADS` environment variable in the program?
","summit> setenv OMP_NUM_THREADS 4
summit> jsrun -n12 -a1 -c4 -g1 -b packed:4 -d packed ./a.out
Rank: 0; RankCore: 0; Thread: 0; ThreadCore: 0; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}
Rank: 0; RankCore: 0; Thread: 1; ThreadCore: 4; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}
Rank: 0; RankCore: 0; Thread: 2; ThreadCore: 8; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}
Rank: 0; RankCore: 0; Thread: 3; ThreadCore: 12; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}",4.240296880084628
"What is the purpose of the OLCF Policy directory?
","The OLCF uses a standard file system structure to assist users with data organization on OLCF systems. Complete details about all file systems available to OLCF users can be found in the Data Management Policy section.

Additional file systems and file protections may be employed for sensitive data. If you are a user on a project producing sensitive data, further instructions will be given by the OLCF. The following guidelines apply to sensitive data:

Only store sensitive data in designated locations. Do not store sensitive data in your User Home directory.",4.322612934162787
"What is the purpose of the OLCF Policy directory?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to or use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title:Security Policy Version: 12.10",4.304518426957341
"What is the purpose of the OLCF Policy directory?
","The OLCF provides a comprehensive suite of hardware and software resources for the creation, manipulation, and retention of scientific data. This document comprises guidelines for acceptable use of those resources. It is an official policy of the OLCF, and as such, must be agreed to by relevant parties as a condition of access to and use of OLCF computational resources.",4.293916186809484
"Write a pyQuil program that implements a quantum circuit learning algorithm.
","Quil is the Rigetti-developed quantum instruction/assembly language. PyQuil is a Python library for writing and running quantum programs using Quil.

Installing pyQuil requires installing the Forest SDK. To quote Rigetti: ""pyQuil, along with quilc, the QVM, and other libraries, make up what is called the Forest SDK"". Because we don't have Docker functionality and due to normal users not having sudo privileges, this means that you will have to install the SDK via the ""bare-bones"" method. The general info below came from:

https://pyquil-docs.rigetti.com/en/stable/start.html",4.276415680888912
"Write a pyQuil program that implements a quantum circuit learning algorithm.
","Quil-T: an extension of Quil with enhanced control of microwave input signals, gate definitions and pulse parameters: https://pyquil-docs.rigetti.com/en/stable/quilt.html

Forest SDK: Rigetti-provided software tools for writing quantum programs in Quil, compiling and running them.

PyQuil: PyQuil is a Python library for writing and running quantum programs using Quil: https://pyquil-docs.rigetti.com/en/stable/

Quilc: Quilc is an optional optimizing compiler for Rigetti QPU code deployment: https://pyquil-docs.rigetti.com/en/v2.1.1/quilc-man.html

Rigetti's Documention",4.249757207880668
"Write a pyQuil program that implements a quantum circuit learning algorithm.
","# Set up your Quantum Quil Program (in this case, a ""Bell State"")
program = Program(
    Declare(""ro"", ""BIT"", 2),
    H(0),
    CNOT(0, 1),
    MEASURE(0, (""ro"", 0)),
    MEASURE(1, (""ro"", 1)),
).wrap_in_numshots_loop(10)

# Set up your QVM
qc = get_qc(""2q-qvm"") # Ask for a QVM with two qubits and generic topology

# Compile and Run (pings your Quilc and QVM servers)
print(qc.run(qc.compile(program)).readout_data.get(""ro""))

After running the above script, you should see something similar to this:

[[1 1]
 [0 0]
 [1 1]
 [0 0]
 [1 1]
 [0 0]
 [1 1]
 [1 1]
 [1 1]
 [0 0]]",4.2308243120206726
"Can I use comments in the serial portion of a batch script?
","The serial portion of the batch script may contain comments, shell commands, executable scripts, and compiled executables. These can be used in combination to, for example, navigate file systems, set up job execution, run serial executables, and even submit other batch jobs.

Andes Compute Node Description

The following image represents a high level compute node that will be used below to display layout options.



Using srun",4.283641385359691
"Can I use comments in the serial portion of a batch script?
","The serial portion of the batch script may contain comments, shell commands, executable scripts, and compiled executables. These can be used in combination to, for example, navigate file systems, set up job execution, run serial executables, and even submit other batch jobs.

The following image represents a high level compute node that will be used below to display layout options.



By default, commands will be executed on the job’s primary compute node, sometimes referred to as the job’s head node. The srun command is used to execute an MPI binary on one or more compute nodes in parallel.",4.21170111672344
"Can I use comments in the serial portion of a batch script?
","The shell commands follow the last #SBATCH option and represent the executable content of the batch job. If any #SBATCH lines follow executable statements, they will be treated as comments only.",4.143520881039471
"Can I use the htar utility to create a new archive?
","To extract all files from the project1/src directory in the archive file called project1.tar, and use the time of extraction as the modification time, use the following command:

htar -xm -f  /hpss/prod/[projid]/users/[userid]/project1.tar project1/src

The htar utility has several limitations.

You cannot add or append files to an existing archive.

File path names within an htar archive of the form prefix/name are limited to 154 characters for the prefix and 99 characters for the file name. Link names cannot exceed 99 characters.",4.434746243747327
"Can I use the htar utility to create a new archive?
","htar will overwrite files of the same name in the target directory.  When possible, extract only the files you need from large archives. To display the names of the files in the project1.tar archive file within the HPSS home directory:

htar -vtf  /hpss/prod/[projid]/users/[userid]/project1.tar

To extract only one file, executable.out, from the project1 directory in the Archive file called `` /hpss/prod/[projid]/users/[userid]/project1.tar``:

htar -xm -f project1.tar project1/ executable.out",4.400449593171259
"Can I use the htar utility to create a new archive?
","As with the standard Unix tar utility the -c, -x, and -t options, respectively, function to create, extract, and list tar archive files. The -K option verifies an existing tarfile in HPSS and the -X option can be used to re-create the index file for an existing archive. For example, to store all files in the directory dir1 to a file named /hpss/prod/[projid]/users/[userid]/allfiles.tar on HPSS, use the command:

htar -cvf /hpss/prod/[projid]/users/[userid]/allfiles.tar dir1/*

To retrieve these files:

htar -xvf  /hpss/prod/[projid]/users/[userid]/allfiles.tar",4.382261063047625
"Can I submit jobs to Quantinuum systems from a local python development environment?
","Jupyter at OLCF: Access to the Quantinuum queues can also be obtained via OLCF JupyterHub, a web-based interactive computing environment.



Users are able to submit jobs that run remotely on Quantinuum's systems from a local python development environment. Directions for setting up the python environment and getting started in a notebook locally as well as additional examples utilizing conditional logic and mid-circuit measurement are found under the Examples tab on the Quantinuum User Portal.",4.374441909288518
"Can I submit jobs to Quantinuum systems from a local python development environment?
","To submit a reservation via the QCS CLI, and installation instructions: https://docs.rigetti.com/qcs/guides/using-the-qcs-cli

Accessing the Rigetti QPU systems can only be done during a user's reservation window.  To submit a job, users must have JupyterHub access to the system.  QVM jobs can be run without network access in local environments.  Jobs are compiled via Quilc and submitted via pyQuil (see https://docs.olcf.ornl.gov/systems/rigetti.html#Software Section <rigetti-soft> below) in a python environment or Jupyter notebook.",4.30247378046619
"Can I submit jobs to Quantinuum systems from a local python development environment?
","Jobs are compiled and submitted via Qiskit in a Python virtual environment or Jupyter notebook (see https://docs.olcf.ornl.gov/systems/ibm_quantum.html#Cloud Access <ibm-cloud> and https://docs.olcf.ornl.gov/systems/ibm_quantum.html#Local Access <ibm-local> sections above).

Circuit jobs comprise jobs of constructed quantum circuits and algorithms submitted to backends in IBM Quantum fair-share queue.

Program jobs utilize a pre-compiled quantum program utilizing the Qiskit Runtime framework.",4.25634606709417
"What is the purpose of the -t1:00:00 option in the sbatch command?
","Use the sbatch --test-only command to see when a job of a specific size could be scheduled. For example, the snapshot below shows that a (2) node job would start at 10:54.

$ sbatch --test-only -N2 -t1:00:00 batch-script.slurm

  sbatch: Job 1375 to start at 2019-08-06T10:54:01 using 64 processors on nodes andes[499-500] in partition batch

The queue is fluid, the given time is an estimate made from the current queue state and load. Future job submissions and job completions will alter the estimate.



The following table summarizes frequently-used options to Slurm:",4.332567114665554
"What is the purpose of the -t1:00:00 option in the sbatch command?
","Use the sbatch --test-only command to see when a job of a specific size could be scheduled. For example, the snapshot below shows that a (2) node job would start at 10:54.

$ sbatch --test-only -N2 -t1:00:00 batch-script.slurm

  sbatch: Job 1375 to start at 2019-08-06T10:54:01 using 64 processors on nodes andes[499-500] in partition batch

The queue is fluid, the given time is an estimate made from the current queue state and load. Future job submissions and job completions will alter the estimate.



Common Batch Options to Slurm",4.319803975122374
"What is the purpose of the -t1:00:00 option in the sbatch command?
","to request an interactive batch job with the same resources that the batch script above requests, you would use salloc -A ABC123 -J RunSim123 -t 1:00:00 -p batch -N 1024. Note there is no option for an output file...you are running interactively, so standard output and standard error will be displayed to the terminal.",4.263849455973759
"How does Crusher handle memory regions that are not automatically migrated to GPU HBM?
","The AMD MI250X and operating system on Crusher supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Crusher. In the following section we'll discuss how the heterogenous memory space on a Crusher node is surfaced within your application.",4.326246642786295
"How does Crusher handle memory regions that are not automatically migrated to GPU HBM?
","If HSA_XNACK=0, page faults in GPU kernels are not handled and will terminate the kernel. Therefore all memory locations accessed by the GPU must either be resident in the GPU HBM or mapped by the HIP runtime. Memory regions may be migrated between the host DDR4 and GPU HBM using explicit HIP library functions such as hipMemAdvise and hipPrefetchAsync, but memory will not be automatically migrated based on access patterns alone.",4.309895721694394
"How does Crusher handle memory regions that are not automatically migrated to GPU HBM?
","If HSA_XNACK=0, page faults in GPU kernels are not handled and will terminate the kernel. Therefore all memory locations accessed by the GPU must either be resident in the GPU HBM or mapped by the HIP runtime. Memory regions may be migrated between the host DDR4 and GPU HBM using explicit HIP library functions such as hipMemAdvise and hipPrefetchAsync, but memory will not be automatically migrated based on access patterns alone.",4.309895721694394
"How do I monitor the performance of my job on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.225516197718092
"How do I monitor the performance of my job on Summit?
",For Summit:,4.223041900113156
"How do I monitor the performance of my job on Summit?
","Summit is connected to an IBM Spectrum Scale™ filesystem providing 250PB of storage capacity with a peak write speed of 2.5 TB/s. Summit also has access to the center-wide NFS-based filesystem (which provides user and project home areas) and has access to the center’s High Performance Storage System (HPSS) for user and project archival storage.

Summit is running Red Hat Enterprise Linux (RHEL) version 8.2.",4.18140205367571
"How can I create an HTAR archive of a directory with a very large number of files?
","There are limits to the size and number of files that can be placed in an HTAR archive.

| Individual File Size Maximum | 68GB, due to POSIX limit | | --- | --- | | Maximum Number of Files per Archive | 1 million |

For example, when attempting to HTAR a directory with one member file larger that 64GB, the following error message will appear:

$ htar -cvf  /hpss/prod/[projid]/users/[userid]/hpss_test.tar hpss_test/",4.445859668166061
"How can I create an HTAR archive of a directory with a very large number of files?
","To extract all files from the project1/src directory in the archive file called project1.tar, and use the time of extraction as the modification time, use the following command:

htar -xm -f  /hpss/prod/[projid]/users/[userid]/project1.tar project1/src

The htar utility has several limitations.

You cannot add or append files to an existing archive.

File path names within an htar archive of the form prefix/name are limited to 154 characters for the prefix and 99 characters for the file name. Link names cannot exceed 99 characters.",4.368713666918457
"How can I create an HTAR archive of a directory with a very large number of files?
","htar will overwrite files of the same name in the target directory.  When possible, extract only the files you need from large archives. To display the names of the files in the project1.tar archive file within the HPSS home directory:

htar -vtf  /hpss/prod/[projid]/users/[userid]/project1.tar

To extract only one file, executable.out, from the project1 directory in the Archive file called `` /hpss/prod/[projid]/users/[userid]/project1.tar``:

htar -xm -f project1.tar project1/ executable.out",4.337389063899045
"Where can I find more information on connecting ArgoCD to a git repository?
","Image of the ArgoCD applications tab.

Prior to using ArgoCD, a git repository containing Kubernetes custom resources needs to be added for use.

The git repository containing Kubernetes custom resources is typically not the same git repository used to build the application.

There are three ways for ArgoCD to connect to a git repository:

connect using ssh

connect using https

connect using GitHub App",4.416730112036678
"Where can I find more information on connecting ArgoCD to a git repository?
","connect using https

connect using GitHub App

Each of these methods are described in the ArgoCD Private Repositories document. For example, to connect to an OLCF or NCCS GitLab instance, create a deploy token per the GitLab documentation for use by ArgoCD copying the username and token value. Then, in ArgoCD, navigate to ""Manage your repositories, projects, settings"" tab and select ""Repositories"".

Image of the Manage your repositories, project, settings tab.

Once into the ""Repositories"" area, select ""CONNECT REPO USING HTTPS"":

Image of the repositories area.",4.3886700340869735
"Where can I find more information on connecting ArgoCD to a git repository?
","Image of the repositories area.

and then add the ""Repository URL"", ""Username"" for the deploy token, and the deploy token itself as the password. If Git-LFS support is needed, click the ""Enable LFS support"" at the bottom of the page. Once entries look correct:

Image of the connect to repo using https parameters.

click the ""CONNECT"" button in the upper left. Once entered and ArgoCD is able to access the server, the connection should have a status of ""Successful"" with a green check mark:

Image of a successful git repository configuration.",4.362122389750187
"What should I do after I have successfully logged in to the Docker registry?
","There might be an image built locally that you would like to have in your OpenShift project. It is possible to add this image to your project by adding it to the Docker registry of the cluster that your project is on.

First, copy your login token. We will need this for the next step.

oc login https://api.<cluster>.ccs.ornl.gov --token=<COPY THIS TOKEN>

Next, log into the Docker registry. Use your copied token when prompted for your password. Upon succesful login, a message saying so will appear.

docker login -u <NCCS USERNAME> registry.apps.<cluster>.ccs.ornl.gov",4.198061959930541
"What should I do after I have successfully logged in to the Docker registry?
","Now, find the repository and tag information of the local image you want to add to the registry and tag it accordingly.

$ docker images
REPOSITORY                                TAG                 IMAGE ID            CREATED             SIZE
example:5000/streams                      v3.1.4              fd7673fdbe30        3 weeks ago         1.95GB

The command to tag your image is:

docker tag example:5000/streams:v3.1.4 registry.apps.<cluster>.ccs.ornl.gov/<namespace>/<image>:<tag>

Lastly, the image needs to be pushed to the registry.",4.195830121804255
"What should I do after I have successfully logged in to the Docker registry?
","docker push registry.apps.<cluster>.ccs.ornl.gov/<namespace>/<image>:<tag>

OpenShift has an integrated container registry that can be accessed from outside the cluster to push and pull images as well as run containers.

This assumes that you have Docker installed locally. Installing Docker is outside of the scope of this documentation.

First you have to log into OpenShift

oc login https://api.<cluster>.ccs.ornl.gov

Next you can use your token to log into the integrated registry.

docker login -u user -p $(oc whoami -t) registry.apps.<cluster>.ccs.ornl.gov",4.14170823460982
"How can I transfer only the files that are smaller than 1MB using rsync?
","Transfer data and show progress while transferring

rsync -avz --progress mydir/ $USER@dtn.ccs.ornl.gov:/path/

Include files or directories starting with T and exclude all others

rsync -avz --progress --include 'T*' --exclude '*' mydir/ $USER@dtn.ccs.ornl.gov:/path/

If the file or directory exists at the target but not on the source, then delete it

rsync -avz --delete $USER@dtn.ccs.ornl.gov:/path/ .

Transfer only the files that are smaller than 1MB

rsync -avz --max-size='1m' mydir/ $USER@dtn.ccs.ornl.gov:/path/

If you want to verify the behavior is as intended, execute a dry-run",4.310442011098795
"How can I transfer only the files that are smaller than 1MB using rsync?
","rsync -avz --dry-run mydir/ $USER@dtn.ccs.ornl.gov:/path/

See the manual pages for more information:

$ man scp
$ man rsync

Differences:

scp cannot continue if it is interrupted. rsync can.

rsync is optimized for performance.

By default, rsync checks if the transfer of the data was successful.

Standard file transfer protocol (FTP) and remote copy (RCP) should not be used to transfer files to the NCCS high-performance computing (HPC) systems due to security concerns.",4.145560759460818
"How can I transfer only the files that are smaller than 1MB using rsync?
","transfers across all the users. Each user has a limit of 3 active transfers, so it is required to transfer a lot of data on each transfer than less data across many transfers.  If a folder is constituted with mixed files including thousands of small files (less than 1MB each one), it would be better to tar the small files.  Otherwise, if the files are larger, Globus will handle them.",4.083285076110677
"How is utilization calculated on Summit?
",For Summit:,4.286088851459901
"How is utilization calculated on Summit?
","The requesting project's allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Summit would be equivalent to using 16,000 Summit node-hours of a project's allocation.



As is the case with many other queuing systems, it is possible to place dependencies on jobs to prevent them from running until other jobs have started/completed/etc. Several possible dependency settings are described in the table below:",4.283103791750312
"How is utilization calculated on Summit?
","Utilization is calculated daily using batch jobs which complete between 00:00 and 23:59 of the previous day. For example, if a job moves into a run state on Tuesday and completes Wednesday, the job's utilization will be recorded Thursday. Only batch jobs which write an end record are used to calculate utilization. Batch jobs which do not write end records due to system failure or other reasons are not used when calculating utilization. Jobs which fail because of run-time errors (e.g. the user's application causes a segmentation fault) are counted against the allocation.",4.226042900150463
"What is the purpose of the `jsrun` command when running a container on Summit?
","This section describes tools that users might find helpful to better understand the jsrun job launcher.

hello_jsrun is a ""Hello World""-type program that users can run on Summit nodes to better understand how MPI ranks and OpenMP threads are mapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A screencast showing how to use Hello_jsrun is also available: https://vimeo.com/261038849

Job Step Viewer provides a graphical view of an application's runtime layout on Summit. It allows users to preview and quickly iterate with multiple jsrun options to understand and optimize job launch.",4.357544553335404
"What is the purpose of the `jsrun` command when running a container on Summit?
","summit>



Jsrun provides the ability to launch multiple jsrun job launches within a single batch job allocation. This can be done within a single node, or across multiple nodes.

By default, multiple invocations of jsrun in a job script will execute serially in order. In this configuration, jobs will launch one at a time and the next one will not start until the previous is complete. The batch node allocation is equal to the largest jsrun submitted, and the total walltime must be equal to or greater then the sum of all jsruns issued.",4.332000778298252
"What is the purpose of the `jsrun` command when running a container on Summit?
","to service nodes on that system). All commands within your job script (or the commands you run in an interactive job) will run on a launch node. Like login nodes, these are shared resources so you should not run multiprocessor/threaded programs on Launch nodes. It is appropriate to launch the jsrun command from launch nodes. | | Compute | Most of the nodes on Summit are compute nodes. These are where your parallel job executes. They're accessed via the jsrun command. |",4.292503396939994
"How can I customize a Helm deployment?
","It is also possible to write your own charts for helm, if you have an application that can be deployed to many namespaces or that could benefit from templating objects. How to write charts is outside the scope of this documentation, but the upstream docs are a great place to start.",4.386918106942762
"How can I customize a Helm deployment?
","For an example, let's install a basic mysql database, with a release named mysql.

You could simply run helm install mysql stable/mysql, and an basic mysql deployment would be created with default values. However, we probably want to customize this deployment a bit. Let's take a look at the documentation for the mysql helm chart.",4.363405273547125
"How can I customize a Helm deployment?
","Helm is the OLCF preferred package manager for Kubernetes. There are a variety of upstream applications that can be installed with helm, such as databases, web frontends, and monitoring tools. Helm has ""packages"" called ""charts"", which can essentially be thought of as kubernetes object templates. You can pass in values which helm uses to fill in these templates, and create the objects (pods, deployments, services, etc)

Follow https://docs.olcf.ornl.gov/systems/helm_example.html#helm_prerequisite for installing Helm.",4.303541769024185
"Can I use the Ascent system for machine learning workloads?
","System Overview



Ascent is a stand-alone 18-node system with the same architecture and design as Summit. It's most often utilized as a resource for OLCF training events, workshops, and conferences. Ascent exists in the NCCS Open Security Enclave, which is subject to fewer restrictions than the Moderate Security Enclave that houses systems such as Summit. This means that participants in training events can go through a streamlined version of the approval process before being granted access.",4.317790891416011
"Can I use the Ascent system for machine learning workloads?
","As the Ascent user environment is almost identical to Summit, additional information can be found in the https://docs.olcf.ornl.gov/systems/ascent_user_guide.html#training-system-ascent section of the https://docs.olcf.ornl.gov/systems/ascent_user_guide.html#summit-user-guide.",4.302486750941089
"Can I use the Ascent system for machine learning workloads?
","Ascent is an 18-node stand-alone system with the same architecture as Summit (see https://docs.olcf.ornl.gov/systems/summit_user_guide.html#summit-nodes section above), so most of this Summit User Guide can be referenced for Ascent as well. However, aside from the number of compute nodes, there are other differences between the two systems. Most notably, Ascent sits in the NCCS Open Security Enclave, which is subject to fewer restrictions than the Moderate Security Enclave that systems such as Summit belong to. This means that participants in OLCF training events can go through a streamlined",4.301840280273996
"What is the estimated number of flushes required for the trace?
","The first line of the output gives an estimation of the total size of the trace, aggregated over all processes. This information is useful for estimating the space required on disk. In the given example, the estimated total size of the event trace is 40GB. The second line prints an estimation of the memory space required by a single process for the trace. Since flushes heavily disturb measurements, the memory space that Score-P reserves on each process at application start must be large enough to hold the process’ trace in memory in order to avoid flushes during runtime.",4.341100743383075
"What is the estimated number of flushes required for the trace?
","Estimated aggregate size of event trace:                   40GB
Estimated requirements for largest trace buffer (max_buf): 10GB
Estimated memory requirements (SCOREP_TOTAL_MEMORY):       10GB
(warning: The memory requirements can not be satisfied by Score-P to avoid
intermediate flushes when tracing. Set SCOREP_TOTAL_MEMORY=4G to get the
maximum supported memory or reduce requirements using USR regions filters.)",4.190076950593836
"What is the estimated number of flushes required for the trace?
","In addition to the trace, Score-P requires some additional memory to maintain internal data structures. Thus, it provides also an estimation for the total amount of required memory on each process. The memory size per process that Score-P reserves is set via the environment variable SCOREP_TOTAL_MEMORY. In the given example the per process memory is about 10GB. When defining a filter, it is recommended to exclude short, frequently called functions from measurement since they require a lot of buffer space (represented by a high value under max_tbc) but incur a high measurement overhead. MPI",4.00074361584219
"How can I monitor the progress of a job on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.279304320707213
"How can I monitor the progress of a job on Summit?
",For Summit:,4.21307346279662
"How can I monitor the progress of a job on Summit?
","This method on Summit requires the user to be present until the job completes. For users who have long scripts or are unable to monitor the job, you can submit the above lines in a batch script. However, you will wait in the queue twice, so this is not recommended. Alternatively, one can use Andes.

For Andes/Frontier (Slurm Script):

Andes

.. code-block:: bash


   #!/bin/bash
   #SBATCH -A XXXYYY
   #SBATCH -J visit_test
   #SBATCH -N 1
   #SBATCH -p gpu
   #SBATCH -t 0:05:00

   cd $SLURM_SUBMIT_DIR
   date

   module load visit",4.2035096423517615
"Can you explain what Zero Copy Read/Write means in the context of Frontier?
","Each compute node on Frontier has [2x] 1.92TB Non-Volatile Memory (NVMe) storage devices (SSDs), colloquially known as a ""Burst Buffer"" with a peak sequential performance of 5500 MB/s (read) and 2000 MB/s (write). The purpose of the Burst Buffer system is to bring improved I/O performance to appropriate workloads. Users are not required to use the NVMes. Data can also be written directly to the parallel filesystem.



The NVMes on Frontier are local to each node.",4.137686748583361
"Can you explain what Zero Copy Read/Write means in the context of Frontier?
","The 2 GCDs on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The OLCF Director's Discretionary (DD) program allocates approximately 10% of the available Frontier hours in a calendar year. Frontier is allocated in *node* hours, and a typical DD project is awarded between 15,000 - 20,000 *node* hours. For more information about Frontier, please visit the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#frontier-user-guide.",4.077525614115347
"Can you explain what Zero Copy Read/Write means in the context of Frontier?
","BytesMoved = BytesWritten + BytesRead

where

BytesWritten = 64 * TCC\_EA\_WRREQ\_64B\_sum + 32 * (TCC\_EA\_WRREQ\_sum - TCC\_EA\_WRREQ\_64B\_sum)

BytesRead = 32 * TCC\_EA\_RDREQ\_32B\_sum + 64 * (TCC\_EA\_RDREQ\_sum - TCC\_EA\_RDREQ\_32B\_sum)





This section details 'tips and tricks' and information of interest to users when porting from Summit to Frontier.",4.075454160761306
"Can I access my project files on HPSS from a remote location?
","Project Archive directories may only be accessed via utilities called HSI and HTAR. For more information on using HSI or HTAR, see the https://docs.olcf.ornl.gov/systems/project_centric.html#data-hpss section.",4.311437379490924
"Can I access my project files on HPSS from a remote location?
","Member Work, Project Work, and World Work directories are not backed up. Project members are responsible for backing up these files, either to Project Archive areas (HPSS) or to an off-site location.

Moderate projects without export control restrictions are also allocated project-specific archival space on the High Performance Storage System (HPSS). The default quota is shown on the table below. If a higher quota is needed, contact the User Assistance Center.

There is no HPSS storage for Moderate Enhanced Projects, Moderate Projects subject to export control, or Open projects.",4.288246082895964
"Can I access my project files on HPSS from a remote location?
",Please note that the HPSS is not mounted directly onto Frontier nodes. There are two main methods for accessing and moving data to/from the HPSS. The first is to use the command line utilities hsi and htar. The second is to use the Globus data transfer service. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-hpss for more information on both of these methods.,4.262687513735321
"How many GCDs are available on a single MI250X on a Crusher compute node?
","Each Crusher compute node consists of [1x] 64-core AMD EPYC 7A53 ""Optimized 3rd Gen EPYC"" CPU (with 2 hardware threads per physical core) with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X, each with 2 Graphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can think of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth memory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU, allowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of 36+36 GB/s. The 2 GCDs on the same MI250X are connected with",4.478110006039397
"How many GCDs are available on a single MI250X on a Crusher compute node?
","The 2 GCDs on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The GCDs on different MI250X are connected with Infinity Fabric GPU-GPU in the arrangement shown in the Crusher Node Diagram below, where the peak bandwidth ranges from 50-100 GB/s based on the number of Infinity Fabric connections between individual GCDs.",4.47415790868252
"How many GCDs are available on a single MI250X on a Crusher compute node?
","TERMINOLOGY:  The 8 GCDs contained in the 4 MI250X will show as 8 separate GPUs according to Slurm, ROCR_VISIBLE_DEVICES, and the ROCr runtime, so from this point forward in the quick-start guide, we will simply refer to the GCDs as GPUs.

Crusher node architecture diagram",4.438889251041982
"What are the accessModes that we should specify in the PersistentVolumeClaim?
","apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  # The name of the claim
  name: test-pod-pvc
spec:
  # The type of storage being requested. This can be blank and it will be
  # set to the default value, which is netapp-nfs, but it is good practice
  # to explictly declare it.
  storageClassName: netapp-nfs
  # how the volume can be accessed. ReadWriteMany, or RWX as it is abbreviated,
  # means the volume can be mounted as Read Write by multiple nodes
  accessModes:
  - ReadWriteMany
  resources:
    # the amount of storage being requested
    requests:
      storage: 1Gi",4.29720314989222
"What are the accessModes that we should specify in the PersistentVolumeClaim?
",a desired size for a PersistentVolume. The cluster administrator or some automated mechanism will provision the storage on the backend and make it available to the cluster via the PersistentVolumeClaim.,4.225589704091793
"What are the accessModes that we should specify in the PersistentVolumeClaim?
","Once the Persistent Volume is created its allocated memory is available to be claimed by a project. This is done through a Persistent Volume Claim. PVC's are created using a YAML file in the same manner that PV's are created. An example of a PVC that claims the PV created in the previous section follows:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: storage-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

The claim is then placed using:

oc create -f storage-1.yaml",4.205515805718387
"Can you use CPU regular malloc() routine with hipMemAdvise()?
","Finally, the following table summarizes the nature of the memory returned based on the flag passed as argument to hipMallocManaged() and the use of CPU regular malloc() routine with the possible use of hipMemAdvise().

| API | MemAdvice | Result | | --- | --- | --- | | hipMallocManaged() |  | Fine grained | | hipMallocManaged() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained | | malloc() |  | Fine grained | | malloc() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained |",4.425091893333397
"Can you use CPU regular malloc() routine with hipMemAdvise()?
","Finally, the following table summarizes the nature of the memory returned based on the flag passed as argument to hipMallocManaged() and the use of CPU regular malloc() routine with the possible use of hipMemAdvise().

| API | MemAdvice | Result | | --- | --- | --- | | hipMallocManaged() |  | Fine grained | | hipMallocManaged() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained | | malloc() |  | Fine grained | | malloc() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained |",4.425091893333397
"Can you use CPU regular malloc() routine with hipMemAdvise()?
","Users applying floating point atomic operations (e.g., atomicAdd) on memory regions allocated via regular hipMalloc() can safely apply the -munsafe-fp-atomics flags to their codes to get the best possible performance and leverage hardware supported floating point atomics. Atomic operations supported in hardware on non-FP datatypes  (e.g., INT32) will work correctly regardless of the nature of the memory region used.",4.19001893048616
"Can I use nvprof to profile my MPI job on Summit?
","Users on Summit can have MPI calls automatically annotated in nvprof timelines using the nvprof --annotate-mpi openmpi option. If the user calls MPI_Init_thread instead of MPI_Init, nvprof may segfault, as MPI_Init_thread is currently not being wrapped by nvprof. The current alternative is to build and follow the instructions from https://github.com/NVIDIA/cuda-profiler/tree/mpi_init_thread.",4.472374337845722
"Can I use nvprof to profile my MPI job on Summit?
","summit> module load cuda

A simple ""Hello, World!"" run using nvprof can be done by adding ""nvprof"" to the jsrun (see: https://docs.olcf.ornl.gov/systems/summit_user_guide.html#job-launcher-jsrun) line in your batch script (see https://docs.olcf.ornl.gov/systems/summit_user_guide.html#batch-scripts).

...
jsrun -n1 -a1 -g1 nvprof ./hello_world_gpu
...

Although nvprof doesn't provide aggregated MPI data, the %h and %p output file modifiers can be used to create separate output files for each host and process.

...
jsrun -n1 -a1 -g1 nvprof -o output.%h.%p ./hello_world_gpu
...",4.410040917524391
"Can I use nvprof to profile my MPI job on Summit?
","Nsight Systems can be used for MPI runs with multiple ranks, but it is not a parallel profiler and cannot combine output from multiple ranks. Instead, each rank must be profiled and analyzed independently. The file name should be unique for every rank. Nsight Systems knows how to parse environment variables with the syntax %q{ENV_VAR}, and since Spectrum MPI provides an environment variable for every process with its MPI rank, you can do

summit> jsrun -n6 -a1 -g1 nsys profile -o vectorAdd_%q{OMPI_COMM_WORLD_RANK} ./vectorAdd",4.298055756478032
"How do I instruct BSUB to reserve nodes in the same rack for a deep learning job on Summit?
","When making node reservations for DDL jobs, it can sometimes improve performance to reserve nodes in a rack-contiguous manner.

In order to instruct BSUB to reserve nodes in the same rack, expert mode must be used (-csm y), and the user needs to explicitly specify the reservation string. For more information on Expert mode see: https://docs.olcf.ornl.gov/systems/ibm-wml-ce.html#easy_mode_v_expert_mode

The following BSUB arguments and reservation string instruct bsub to reserve 2 compute nodes within the same rack:",4.4651686306114815
"How do I instruct BSUB to reserve nodes in the same rack for a deep learning job on Summit?
","Dependency expressions can be combined with logical operators. For example, if you want a job held until job 12345 is DONE and job 12346 has started, you can use #BSUB -w ""done(12345) && started(12346)""



The default job launcher for Summit is jsrun. jsrun was developed by IBM for the Oak Ridge and Livermore Power systems. The tool will execute a given program on resources allocated through the LSF batch scheduler; similar to mpirun and aprun functionality.

The following compute node image will be used to discuss jsrun resource sets and layout.



1 node

2 sockets (grey)",4.239655948169288
"How do I instruct BSUB to reserve nodes in the same rack for a deep learning job on Summit?
","As pointed out in https://docs.olcf.ornl.gov/systems/summit_user_guide.html#login-launch-and-compute-nodes, you will be placed on a launch (a.k.a. ""batch"") node upon launching an interactive job and as usual need to use jsrun to access the compute node(s):

$ bsub -Is -W 0:10 -nnodes 1 -P STF007 $SHELL
Job <779469> is submitted to default queue <batch>.
<<Waiting for dispatch ...>>
<<Starting on batch2>>

$ hostname
batch2

$ jsrun -n1 hostname
a35n03",4.232944257040032
"Can export control regulations apply to information that is shared between researchers in different countries?
","Export Control: The project request will be reviewed by ORNL Export Control to determine whether sensitive or proprietary data will be generated or used. The results of this review will be forwarded to the PI. If the project request is deemed sensitive and/or proprietary, the OLCF Security Team will schedule a conference call with the PI to discuss the data protection needs.",4.206862540476219
"Can export control regulations apply to information that is shared between researchers in different countries?
","these prohibited data types or information that falls under Export Control. For questions, contact help@olcf.ornl.gov.",4.18391438022692
"Can export control regulations apply to information that is shared between researchers in different countries?
","Portions of data and/or software used in your project require extra protections due to requirements for protecting HIPAA/ITAR or other sensitive or controlled information. There are countries from which citizens are restricted from accessing sensitive/controlled information and therefore cannot be a part of your project. When you request users to be added to your project, our user assistance center will check the nationality of those users for conflict.",4.172129996857704
"How do I know which version of the open-ce module is best for my workload?
","module load open-ce

Loading a specific version of the module is recommended to future-proof scripts against software updates. The following commands can be used to find and load specific module versions:

[user@login2.summit ~]$ module avail open-ce",4.348583304706758
"How do I know which version of the open-ce module is best for my workload?
","As seen above, there are also different Python versions of each Open-CE release available on Summit (indicated by -pyXY- in the module name, where ""X"" and ""Y"" are the major and minor Python version numbers, respectively.)

For more information on loading modules, including loading specific verions, see: https://docs.olcf.ornl.gov/systems/ibm-wml-ce.html#environment-management-with-lmod

Loading an Open-CE module will activate a conda environment which is pre-loaded with the following packages, and their dependencies:",4.172172919643061
"How do I know which version of the open-ce module is best for my workload?
","[user@login2.summit ~]$ module avail open-ce

---------------------------------- /sw/summit/modulefiles/core ---------------------------------
open-ce/1.2.0-py36-0        open-ce/1.4.0-py37-0    open-ce/1.5.0-py37-0    open-ce/1.5.2-py37-0
open-ce/1.2.0-py37-0        open-ce/1.4.0-py38-0    open-ce/1.5.0-py38-0    open-ce/1.5.2-py38-0
open-ce/1.2.0-py38-0 (D)    open-ce/1.4.0-py39-0    open-ce/1.5.0-py39-0    open-ce/1.5.2-py39-0

[user@login2.summit ~]$ module load open-ce/1.5.0-py39-0",4.1439213019640295
"How can I run a command on a specific node in the allocation?
","After running this command, the job will wait until enough compute nodes are available, just as any other batch job must. However, once the job starts, the user will be given an interactive prompt on the primary compute node within the allocated resource pool. Commands may then be executed directly (instead of through a batch script).",4.222987321201547
"How can I run a command on a specific node in the allocation?
","After running this command, the job will wait until enough compute nodes are available, just as any other batch job must. However, once the job starts, the user will be given an interactive prompt on the primary compute node within the allocated resource pool. Commands may then be executed directly (instead of through a batch script).

Debugging",4.218470717212068
"How can I run a command on a specific node in the allocation?
","$ salloc -A <project_id> -J <job_name> -t 00:05:00 -p <partition> -N 2
salloc: Granted job allocation 4258
salloc: Waiting for resource configuration
salloc: Nodes spock[10-11] are ready for job

$ srun -n 4 --ntasks-per-node=2 ./a.out
<output printed to terminal>

$ srun -n 2 --ntasks-per-node=1 ./a.out
<output printed to terminal>

Here, salloc is used to request an allocation of 2 MI100 compute nodes for 5 minutes. Once the resources become available, the user is granted access to the compute nodes (spock10 and spock11 in this case) and can launch job steps on them using srun.",4.186769588267991
"Can I use darshan-runtime for profiling applications on other HPC systems?
","On Tuesday, May 9, 2023, the darshan-runtime modulefile was added to DefApps and is now loaded by default on Frontier. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>. Unloading darshan-runtime is recommended for users profiling their applications with other profilers to prevent conflicts.



JIRA_CONTENT_HERE",4.287797747890228
"Can I use darshan-runtime for profiling applications on other HPC systems?
","<p style=""font-size:20px""><b>Frontier: Darshan Runtime 3.4.0 (May 10, 2023)</b></p>

The Darshan Runtime modulefile darshan-runtime/3.4.0 on Frontier is now loaded by default. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>.

Unloading darshan-runtime modulefile is recommended for users profiling their applications with other profilers to prevent conflicts.",4.285092683951418
"Can I use darshan-runtime for profiling applications on other HPC systems?
","Programming models supported by HPCToolkit include MPI, OpenMP, OpenACC, CUDA, OpenCL, DPC++, HIP, RAJA, Kokkos, and others.

Below is an example that generates a profile and loads the results in their GUI-based viewer.

module use /gpfs/alpine/csc322/world-shared/modulefiles/x86_64
module load hpctoolkit

# 1. Profile and trace an application using CPU time and GPU performance counters
srun <srun_options> hpcrun -o <measurement_dir> -t -e CPUTIME -e gpu=amd <application>

# 2. Analyze the binary of executables and its dependent libraries
hpcstruct <measurement_dir>",4.195716987030666
"What is the command to run Job Step Viewer?
","This section describes tools that users might find helpful to better understand the jsrun job launcher.

hello_jsrun is a ""Hello World""-type program that users can run on Summit nodes to better understand how MPI ranks and OpenMP threads are mapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A screencast showing how to use Hello_jsrun is also available: https://vimeo.com/261038849

Job Step Viewer provides a graphical view of an application's runtime layout on Summit. It allows users to preview and quickly iterate with multiple jsrun options to understand and optimize job launch.",4.232142231451113
"What is the command to run Job Step Viewer?
","For bug reports or suggestions, please email help@olcf.ornl.gov.

Request a Summit allocation

bsub -W 10 -nnodes 2 -P $OLCF_PROJECT_ID -Is $SHELL

Load the job-step-viewer module

module load job-step-viewer

Test out a jsrun line by itself, or provide an executable as normal

jsrun -n12 -r6 -c7 -g1 -a1 -EOMP_NUM_THREADS=7 -brs

Visit the provided URL

https://jobstepviewer.olcf.ornl.gov/summit/871957-1",4.173671619898961
"What is the command to run Job Step Viewer?
","Provides additional details of given job.

The sview tool provide a graphical queue monitoring tool. To use, you will need an X server running on your local system. You will also need to tunnel X traffic through your ssh connection:

local-system> ssh -Y username@andes.ccs.ornl.gov
andes-login> sview",4.115997412271905
"How does the Crusher system's coherent host interface enable advanced memory capabilities?
","The Crusher system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.399013410206071
"How does the Crusher system's coherent host interface enable advanced memory capabilities?
","The AMD MI250X and operating system on Crusher supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Crusher. In the following section we'll discuss how the heterogenous memory space on a Crusher node is surfaced within your application.",4.335215299238539
"How does the Crusher system's coherent host interface enable advanced memory capabilities?
","The Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.240851561441088
"How do I set the environment variables for Crusher?
","Crusher users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.257147144890772
"How do I set the environment variables for Crusher?
","First, create a setup file setup.bash that will load the correct modules and define environment variables.

module load workflows
module load entk/1.13.0

export RADICAL_PILOT_DBURL=""mongodb://admin:password@apps.marble.ccs.ornl.gov:32767/test""
export RMQ_HOSTNAME=""apps.marble.ccs.ornl.gov""
export RMQ_PORT=""30256""
export RMQ_USERNAME=""admin""
export RMQ_PASSWORD=""password""

Replace, without renaming, the RADICAL_PILOT_DBURL environment variable in setup.bash with the MongoDB connection string that corresponds to your own service running on Slate. EnTK uses this environment variable directly.",4.132660678034948
"How do I set the environment variables for Crusher?
","Before setting up your environment, you must exit and log back in so that you have a fresh login shell. This is to ensure that no previously activated environments exist in your $PATH environment variable. Additionally, you should execute module reset.

Building CuPy from source is highly sensitive to the current environment variables set in your profile. Because of this, it is extremely important that all the modules and environments you plan to load are done in the correct order, so that all the environment variables are set correctly.",4.060166091613379
"What is the purpose of the ""xnack"" option?
","XNACK (pronounced X-knack) refers to the AMD GPU's ability to retry memory accesses that fail due to a page fault. The XNACK mode of an MI250X can be changed by setting the environment variable HSA_XNACK before starting a process that uses the GPU. Valid values are 0 (disabled) and 1 (enabled), and all processes connected to a GPU must use the same XNACK setting. The default MI250X on Frontier is HSA_XNACK=0.",4.257197087202664
"What is the purpose of the ""xnack"" option?
","XNACK (pronounced X-knack) refers to the AMD GPU's ability to retry memory accesses that fail due to a page fault. The XNACK mode of an MI250X can be changed by setting the environment variable HSA_XNACK before starting a process that uses the GPU. Valid values are 0 (disabled) and 1 (enabled), and all processes connected to a GPU must use the same XNACK setting. The default MI250X on Crusher is HSA_XNACK=0.",4.256685118880294
"What is the purpose of the ""xnack"" option?
","Two versions of each kernel will be generated, one that runs with XNACK disabled and one that runs if XNACK is enabled. This is different from ""xnack any"" in that two versions of each kernel are compiled and HIP picks the appropriate one at runtime, rather than there being a single version compatible with both. A ""fat binary"" compiled in this way will have the same performance of ""xnack+"" with HSA_XNACK=1 and as ""xnack-"" with HSA_XNACK=0, but the final executable will be larger since it contains two copies of every kernel.",4.2364625104276525
"How can I verify the integrity of an HTAR archive?
","Project Archive directories may only be accessed via utilities called HSI and HTAR. For more information on using HSI or HTAR, see the https://docs.olcf.ornl.gov/systems/project_centric.html#data-hpss section.",4.2108558248395775
"How can I verify the integrity of an HTAR archive?
","htar will overwrite files of the same name in the target directory.  When possible, extract only the files you need from large archives. To display the names of the files in the project1.tar archive file within the HPSS home directory:

htar -vtf  /hpss/prod/[projid]/users/[userid]/project1.tar

To extract only one file, executable.out, from the project1 directory in the Archive file called `` /hpss/prod/[projid]/users/[userid]/project1.tar``:

htar -xm -f project1.tar project1/ executable.out",4.1542338170307005
"How can I verify the integrity of an HTAR archive?
","To extract all files from the project1/src directory in the archive file called project1.tar, and use the time of extraction as the modification time, use the following command:

htar -xm -f  /hpss/prod/[projid]/users/[userid]/project1.tar project1/src

The htar utility has several limitations.

You cannot add or append files to an existing archive.

File path names within an htar archive of the form prefix/name are limited to 154 characters for the prefix and 99 characters for the file name. Link names cannot exceed 99 characters.",4.154199272693991
"What is the benefit of using ROCm 5.0 for machine learning?
","The system was upgraded to Cray OS 2.5, Slingshot Host Software 2.0.2-112, and the AMD GPU 6.0.5 device driver (ROCm 5.5.1 release).

ROCm 5.5.1 is now available via the rocm/5.5.1 modulefile.

HPE/Cray Programming Environments (PE) 23.05 is now available via the cpe/23.05 modulefile.

HPE/Cray PE 23.05 introduces support for ROCm 5.5.1. However, due to issues identified during testing, ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.

On Wednesday, April 5, 2023, the Crusher TDS was upgraded to a new software stack.  A summary of the changes is included below.",4.185758915624548
"What is the benefit of using ROCm 5.0 for machine learning?
","AMD has provided a solution in ROCm 5.0 which modifies the behavior of Tensorflow, PyTorch, and rocBLAS. This modification starts with FP16 input values, casting the intermediate FP16 values to BF16, and then casting back to FP16 output after the accumulate FP32 operations. In this way, the input and output types are unchanged. The behavior is enabled by default in machine learning frameworks. This behavior requires user action in rocBLAS, via a special enum type. For more information, see the rocBLAS link below.",4.134724118280486
"What is the benefit of using ROCm 5.0 for machine learning?
","AMD has provided a solution in ROCm 5.0 which modifies the behavior of Tensorflow, PyTorch, and rocBLAS. This modification starts with FP16 input values, casting the intermediate FP16 values to BF16, and then casting back to FP16 output after the accumulate FP32 operations. In this way, the input and output types are unchanged. The behavior is enabled by default in machine learning frameworks. This behavior requires user action in rocBLAS, via a special enum type. For more information, see the rocBLAS link below.",4.134724118280486
"How do I manage my environment on Andes?
",For Andes:,4.285005363362283
"How do I manage my environment on Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.208251233405292
"How do I manage my environment on Andes?
","Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-user-guide.

For a full list of software available at the OLCF, please see the Software section (coming soon).",4.149943242394109
"What is the purpose of the --gpu_per_rs flag in Summit?
","jsrun --nrs 1 --tasks_per_rs 1 --cpu_per_rs 1 --gpu_per_rs 1 --smpiargs=""-disable_gpu_hooks"" \
      python /my_path/my_rapids_script.py dataset_part01 &
jsrun --nrs 1 --tasks_per_rs 1 --cpu_per_rs 1 --gpu_per_rs 1 --smpiargs=""-disable_gpu_hooks"" \
      python /my_path/my_rapids_script.py dataset_part02 &
jsrun --nrs 1 --tasks_per_rs 1 --cpu_per_rs 1 --gpu_per_rs 1 --smpiargs=""-disable_gpu_hooks"" \
      python /my_path/my_rapids_script.py dataset_part03 &
...
wait

Be aware of different OLCF's queues and scheduling policies to make best use of regular and high memory Summit nodes.",4.208535140586552
"What is the purpose of the --gpu_per_rs flag in Summit?
","If you plan on using the EGL version of the ParaView module (e.g., paraview/5.11.0-egl), then you must be connected to the GPUs. On Andes, this is done by using the gpu partition via #SBATCH -p gpu, while on Summit the -g flag in the jsrun command must be greater than zero.",4.195060075668841
"What is the purpose of the --gpu_per_rs flag in Summit?
","It's recommended to explicitly specify jsrun options and not rely on the default values. This most often includes --nrs,--cpu_per_rs, --gpu_per_rs, --tasks_per_rs, --bind, and --launch_distribution.

The below examples were launched in the following 2 node interactive batch job:

summit> bsub -nnodes 2 -Pprj123 -W02:00 -Is $SHELL

The following example will create 12 resource sets each with 1 MPI task and 1 GPU. Each MPI task will have access to a single GPU.",4.192010595706539
"What does the -z option do when using rsync?
","rsync -avz --dry-run mydir/ $USER@dtn.ccs.ornl.gov:/path/

See the manual pages for more information:

$ man scp
$ man rsync

Differences:

scp cannot continue if it is interrupted. rsync can.

rsync is optimized for performance.

By default, rsync checks if the transfer of the data was successful.

Standard file transfer protocol (FTP) and remote copy (RCP) should not be used to transfer files to the NCCS high-performance computing (HPC) systems due to security concerns.",4.187180650205445
"What does the -z option do when using rsync?
","Transfer data and show progress while transferring

rsync -avz --progress mydir/ $USER@dtn.ccs.ornl.gov:/path/

Include files or directories starting with T and exclude all others

rsync -avz --progress --include 'T*' --exclude '*' mydir/ $USER@dtn.ccs.ornl.gov:/path/

If the file or directory exists at the target but not on the source, then delete it

rsync -avz --delete $USER@dtn.ccs.ornl.gov:/path/ .

Transfer only the files that are smaller than 1MB

rsync -avz --max-size='1m' mydir/ $USER@dtn.ccs.ornl.gov:/path/

If you want to verify the behavior is as intended, execute a dry-run",4.119415896509379
"What does the -z option do when using rsync?
","As with the standard Unix tar utility the -c, -x, and -t options, respectively, function to create, extract, and list tar archive files. The -K option verifies an existing tarfile in HPSS and the -X option can be used to re-create the index file for an existing archive. For example, to store all files in the directory dir1 to a file named /hpss/prod/[projid]/users/[userid]/allfiles.tar on HPSS, use the command:

htar -cvf /hpss/prod/[projid]/users/[userid]/allfiles.tar dir1/*

To retrieve these files:

htar -xvf  /hpss/prod/[projid]/users/[userid]/allfiles.tar",3.9653252247420623
"How can I monitor the performance of my job step on Frontier?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.2077730778351485
"How can I monitor the performance of my job step on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.118988289645596
"How can I monitor the performance of my job step on Frontier?
",One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Frontier from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Frontier. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.,4.10489742781255
"Can I use the OLCF's SPI for simulations that require large amounts of data?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads. Because of this, many processes used within the SPI are very similar to those used for standard non-SPI. This page lists the differences you may see when using OLCF resources to execute SPI resources.",4.30484486129956
"Can I use the OLCF's SPI for simulations that require large amounts of data?
","CITADEL, having undergone comprehensive technical-, legal-, and policy-oriented reviews and received third-party accreditation, has presented new possibilities for research projects that previously could not access utilize OLCF systems due to the nature of their data.

If you are new to the OLCF or the OLCF's SPI, this page can help get you started.  Below are some of the high-level steps needed to begin use of the OLCF's SPI:",4.2865912947572005
"Can I use the OLCF's SPI for simulations that require large amounts of data?
","The OLCF's Scalable Protected Infrastructure (SPI) provides resources and protocols that enable researchers to process protected data at scale.  The SPI is built around a framework of security protocols that allows researchers to process large datasets containing private information.  Using this framework researchers can use the center's large HPC resources to compute data containing protected health information (PHI), personally identifiable information (PII), data protected under International Traffic in Arms Regulations, and other types of data that require privacy.",4.281101854085964
"How do I run a dask-cuda cluster on two compute nodes?
","Running RAPIDS multi-gpu/multi-node workloads requires a dask-cuda cluster. Setting up a dask-cuda cluster on Summit requires two components:

dask-scheduler.

dask-cuda-workers.

Once the dask-cluster is running, the RAPIDS script should perform four main tasks. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

Reference of multi-gpu/multi-node operation with cuDF, cuML, cuGraph is available in the next links:

10 Minutes to cuDF and Dask-cuDF.

cuML's Multi-Node, Multi-GPU Algorithms.",4.4032265426393336
"How do I run a dask-cuda cluster on two compute nodes?
","cuML's Multi-Node, Multi-GPU Algorithms.

Multi-GPU with cuGraph.

The following script will run a dask-cuda cluster on two compute nodes, then it executes a Python script.

#BSUB -P <PROJECT>
#BSUB -W 0:05
#BSUB -alloc_flags ""gpumps smt4 NVME""
#BSUB -nnodes 2
#BSUB -J rapids_dask_test_tcp
#BSUB -o rapids_dask_test_tcp_%J.out
#BSUB -e rapids_dask_test_tcp_%J.out

PROJ_ID=<project>

module load ums
module load ums-gen119
module load nvidia-rapids/21.08

SCHEDULER_DIR=$MEMBERWORK/$PROJ_ID/dask
WORKER_DIR=/mnt/bb/$USER

if [ ! -d ""$SCHEDULER_DIR"" ]
then
    mkdir $SCHEDULER_DIR
fi",4.394189038121575
"How do I run a dask-cuda cluster on two compute nodes?
","The following script will run a dask-cuda cluster on two compute nodes, then it executes a Python script running BlazingSQL.

#BSUB -P ABC123
#BSUB -W 0:05
#BSUB -alloc_flags ""gpumps smt4 NVME""
#BSUB -nnodes 2
#BSUB -q batch
#BSUB -J bsql_dask
#BSUB -o bsql_dask_%J.out
#BSUB -e bsql_dask_%J.out

PROJ_ID=abc123

module load ums
module load ums-gen119
module load nvidia-rapids/21.08

SCHEDULER_DIR=$MEMBERWORK/$PROJ_ID/dask
BSQL_LOG_DIR=$MEMBERWORK/$PROJ_ID/bsql
WORKER_DIR=/mnt/bb/$USER

mkdir -p $SCHEDULER_DIR
mkdir -p $BSQL_LOG_DIR

SCHEDULER_FILE=$SCHEDULER_DIR/my-scheduler.json",4.379051161521636
"How do you roll out a new version of a Deployment in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.22538938946974
"How do you roll out a new version of a Deployment in Slate?
","Blue-green deployments are defined as running two versions of an application at the same time, then moving traffic from the old production version (the green version) to the new production version (the blue version). You could use a Rolling Deployment Strategy for this, but for the sake of showing how route-based deployments work, we'll use a route.",4.184636041377191
"How do you roll out a new version of a Deployment in Slate?
","A/B Deployments are a popular way to try a new version of an application with a small subset of users in the production environment. With this strategy, you can specify that the older version gets most of the user requests while a limited fraction of users get sent to the new version. Since you can control the amount of users which get sent to the new version, you can gradually increase the volume of requests to the new version and eventually stop using the old version. Remember that deployment configurations don't do any autoscaling of pods, so you may have to adjust the number of pod",4.175618230485762
"Can I collaborate with other researchers on an INCITE proposal?
","INCITE – The Novel Computational Impact on Theory and Experiment (INCITE) program invites proposals for large-scale, computationally intensive research projects to run at the OLCF. The INCITE program awards sizeable allocations (typically, millions of processor-hours per project) on some of the world’s most powerful supercomputers to address grand challenges in science and engineering. There is an annual call for INCITE proposals and awards are made on an annual basis. Please visit the Department of Energy Leadership Computing website for more information and to submit a proposal.",4.267639341644476
"Can I collaborate with other researchers on an INCITE proposal?
",|  | INCITE | Director's Discretion | ALCC | | --- | --- | --- | --- | | Allocations | Large | Small | Large | | Call for Proposals | Once per year | At any time | Once per year | | Duration | 1 year | Up to 12 months | 1 year | | Priority | High | Medium | High | | Closeout Report | yes | yes | yes | | Quarterly Reports | yes | no* | yes | | Where to apply | Apply for INCITE https://doeleadershipcomputing.org/proposal/call-for-proposals/ | Apply for DD https://my.olcf.ornl.gov/project-application-new | Apply for ALCC http://science.energy.gov/ascr/facilities/accessing-ascr-facilities/alcc/ |,4.177321713514117
"Can I collaborate with other researchers on an INCITE proposal?
","SummitPLUS is one of the new allocation programs that will be used to allocate a significant portion of the system for 2024. The program is open to researchers from academia, government laboratories, federal agencies, and industry. We welcome proposals for computationally ready projects from investigators who are new to Summit, as well as from previous INCITE, ALCC, DD, ECP awardees and projects. We encourage proposals on emerging paradigms for computational campaigns including data-intensive science and AI/ML.",4.1700861463244845
"How can I collect performance counters for my GPU kernels on Summit?
","that data motion. But if that bottleneck is a GPU kernel, then Nsight Compute can be used to collect performance counters to understand whether the kernel is running efficiently and if there's anything you can do to improve.",4.343195582280806
"How can I collect performance counters for my GPU kernels on Summit?
","The first step to GPU profiling is collecting a timeline of your application. (This operation is also sometimes called ""tracing,"" that is, finding the start and stop timestamps of all activities that occurred on the GPU or involved the GPU, such as copying data back and forth.) To do this, we can collect a timeline using the command-line interface, nsys. To use this tool, load the nsight-systems module.

summit> module load nsight-systems

For example, we can profile the vectorAdd CUDA sample (the CUDA samples can be found in $OLCF_CUDA_ROOT/samples if the cuda module is loaded.)",4.251497085360935
"How can I collect performance counters for my GPU kernels on Summit?
","If you want to collect information on just a specific performance measurement, for example the number of bytes written to DRAM, you can do so with the --metrics option:

summit> jsrun -n1 -a1 -g1 nv-nsight-cu-cli -k vectorAdd --metrics dram__bytes_write.sum ./vectorAdd

The list of available metrics can be obtained with nv-nsight-cu-cli --query-metrics. Most metrics have both a base name and suffix. Together these  make up the full metric name to pass to nv-nsight-cu-cli. To list the full names for a collection of metrics, use --query-metrics-mode suffix --metrics <metrics list>.",4.237180612267756
"How can I create a scheduler file for Dask scheduler on Nvidia Rapids?
","Running RAPIDS multi-gpu/multi-node workloads requires a dask-cuda cluster. Setting up a dask-cuda cluster on Summit requires two components:

dask-scheduler.

dask-cuda-workers.

Once the dask-cluster is running, the RAPIDS script should perform four main tasks. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

Reference of multi-gpu/multi-node operation with cuDF, cuML, cuGraph is available in the next links:

10 Minutes to cuDF and Dask-cuDF.

cuML's Multi-Node, Multi-GPU Algorithms.",4.2520279412567294
"How can I create a scheduler file for Dask scheduler on Nvidia Rapids?
","As mentioned earlier, the RAPIDS code should perform four main tasks as shown in the following script. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

import sys
from dask.distributed import Client

def disconnect(client, workers_list):
    client.retire_workers(workers_list, close_workers=True)
    client.shutdown()

if __name__ == '__main__':

    sched_file = str(sys.argv[1]) #scheduler file
    num_workers = int(sys.argv[2]) # number of workers to wait for",4.201892962025494
"How can I create a scheduler file for Dask scheduler on Nvidia Rapids?
","SCHEDULER_FILE=$SCHEDULER_DIR/my-scheduler.json

echo 'Running scheduler'
jsrun --nrs 1 --tasks_per_rs 1 --cpu_per_rs 1 --smpiargs=""-disable_gpu_hooks"" \
      dask-scheduler --interface ib0 \
                     --scheduler-file $SCHEDULER_FILE \
                     --no-dashboard --no-show &

#Wait for the dask-scheduler to start
sleep 10",4.19532838097444
"What is the advantage of using the --follow flag when starting a build?
","oc start-build <buildconfig_name> --follow

It is perfectly normal for a build to take a few minutes to complete.

Using a Dockerfile inside of Openshift works in the same way that $ docker build  works outside of Openshift. If all that is needed for your build is a Dockerfile. From within the directory containing the Dockerfile you can run:

$ oc new-build . --name example
--> Found image 224765a (3 months old) in image stream ""buildexample/openjdk"" under tag ""8-alpine"" for ""openjdk:8-alpine""",4.049327678323574
"What is the advantage of using the --follow flag when starting a build?
","In addition, to debug slow startup JSM provides the option to create a progress file. The file will show information that can be helpful to pinpoint if a specific node is hanging or slowing down the job step launch. To enable it, you can use: jsrun --progress ./my_progress_file_output.txt.

When using file-based specification of resource sets with jsrun -U, the -a flag (number of tasks per resource set) is ignored. This has been reported to IBM and they are investigating. It is generally recommended to use jsrun explicit resource files (ERF) with --erf_input and --erf_output instead of -U.",4.028117490626597
"What is the advantage of using the --follow flag when starting a build?
","$ oc start-build example --from-file=./Dockerfile
  Uploading file ""Dockerfile"" as binary input for the build ...
  build ""example-1"" started

In the above example example was the name of the build config.

Additionally, if there are artifacts that need to be included in your build, a directory containing those artifacts can be used by passing the --from-dir flag to the start-build command like so:

$ oc start-build example --from-dir=./sampledir
  Uploading directory ""sampledir"" as binary input for the build ...
  build ""django-5"" started",4.021675872356596
"How can we create a Deployment object in Slate that has a single replica?
","On top of Kubernetes ReplicaSets, OpenShift gives us even more support for software lifecycle with Deployments. A Deployment creates a ReplicaSet and has the added benefit of controlling how new deployments get triggered and deployed.

Deployments are sufficient for deploying a production service

Deployments manages ReplicaSets which in turn manages a set of Pods

Below is an example Deployment:",4.095422828063365
"How can we create a Deployment object in Slate that has a single replica?
","Below is an example Deployment:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 5
  selector:
    matchLabels:
      name: my-app
  template: { ... }
  strategy:
    type: RollingUpdate
  revisionHistoryLimit: 2
  minReadySeconds: 0
  paused: false

Let's look at the individual parts of this definition, under spec.

replicas - the number of replicas to be passed down to the ReplicaSet

selector - the selector to determine which pods are managed by the ReplicaSet.",4.095303989859247
"How can we create a Deployment object in Slate that has a single replica?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.08777214456984
"How do I optimize MPS performance in Summit?
",For Summit:,4.213777783260008
"How do I optimize MPS performance in Summit?
","NVIDIA recommends using the EXCLUSIVE_PROCESS compute mode (the default on Summit) when using the Multi-Process Service, but both MPS and the compute mode can be changed by providing both values: -alloc_flags ""gpumps gpudefault"".",4.210148698986283
"How do I optimize MPS performance in Summit?
","Using the hip-cuda/5.1.0 module on Summit requires CUDA v11.4.0 or later. However, only CUDA v11.0.3 and earlier currently support GPU-aware MPI, so GPU-aware MPI is currently not available when using HIP on Summit.

Any codes compiled with post 11.0.3 CUDA (cuda/11.0.3) will not work with MPS enabled (-alloc_flags ""gpumps"") on Summit. The code will hang indefinitely. CUDA v11.0.3 is the latest version that is officially supported by IBM's software stack installed on Summit. We are continuing to look into this issue.",4.199407067818786
"How does the use of MFMA instructions affect the theoretical peak floating-point FLOPS/s for Crusher?
","TheoreticalFLOPS = 128 FLOP/cycle/CU * 110 CU * 1700000000 cycles/second = 23.9 TFLOP/s

However, when using MFMA instructions, the theoretical peak floating-point FLOPS/s is calculated by:

TheoreticalFLOPS = flop\_per\_cycle(precision) FLOP/cycle/CU * 110 CU * 1700000000 cycles/second

where flop_per_cycle(precision) is the published floating-point operations per clock cycle, per compute unit. Those values are:

| Data Type | Flops/Clock/CU | | --- | --- | | FP64 | 256 | | FP32 | 256 | | FP16 | 1024 | | BF16 | 1024 | | INT8 | 1024 |",4.427240180926132
"How does the use of MFMA instructions affect the theoretical peak floating-point FLOPS/s for Crusher?
","TheoreticalFLOPS = 128 FLOP/cycle/CU * 110 CU * 1700000000 cycles/second = 23.9 TFLOP/s

However, when using MFMA instructions, the theoretical peak floating-point FLOPS/s is calculated by:

TheoreticalFLOPS = flop\_per\_cycle(precision) FLOP/cycle/CU * 110 CU * 1700000000 cycles/second

where flop_per_cycle(precision) is the published floating-point operations per clock cycle, per compute unit. Those values are:

| Data Type | Flops/Clock/CU | | --- | --- | | FP64 | 256 | | FP32 | 256 | | FP16 | 1024 | | BF16 | 1024 | | INT8 | 1024 |",4.427240180926132
"How does the use of MFMA instructions affect the theoretical peak floating-point FLOPS/s for Crusher?
","When SQ_INSTS_VALU_MFMA_MOPS_*_F64 instructions are used, then 47.8 TF/s is considered the theoretical maximum FLOPS/s. If only SQ_INSTS_VALU_<ADD,MUL,TRANS> are found, then 23.9 TF/s is the theoretical maximum FLOPS/s. Then, we divide the number of FLOPS by the elapsed time of the kernel to find FLOPS per second. This is found from subtracting the rocprof metrics EndNs by BeginNs, provided by --timestamp on, then converting from nanoseconds to seconds by dividing by 1,000,000,000 (power(10,9)).",4.297039046414089
"Will ls -a show the .snapshot subdirectory?
","The backups are accessed via the .snapshot subdirectory. Note that ls alone (or even ls -a) will not show the .snapshot subdirectory exists, though ls .snapshot will show its contents. The .snapshot feature is available in any subdirectory of your project home directory and will show the online backups available for that subdirectory.

To retrieve a backup, simply copy it into your desired destination with the cp command.",4.463330301875716
"Will ls -a show the .snapshot subdirectory?
","The backups are accessed via the .snapshot subdirectory. Note that ls alone (or even ls -a) will not show the .snapshot subdirectory exists, though ls .snapshot will show its contents. The .snapshot feature is available in any subdirectory of your home directory and will show the online backups available for that subdirectory.

To retrieve a backup, simply copy it into your desired destination with the cp command.",4.46265947560597
"Will ls -a show the .snapshot subdirectory?
","$ bsub mlflow_demo.lsf

Congratulations! Once the job completes, you will be able to check the standard output files to find the tracking and artifact directories.",3.9513175338787434
"What is the purpose of the NVIDIA Nsight Systems tool?
","The profiler will print several sections including information about the CUDA API calls made by the application, as well as any GPU kernels that were launched. Nsight Systems can be used for CUDA C++, CUDA Fortran, OpenACC, OpenMP offload, and other programming models that target NVIDIA GPUs, because under the hood they all ultimately take the same path for generating the binary code that runs on the GPU.",4.431808209607554
"What is the purpose of the NVIDIA Nsight Systems tool?
",| Nsight Compute | Felix Schmitt (NVIDIA) | NVIDIA Profiling Tools - Nsight Compute https://www.olcf.ornl.gov/calendar/nvidia-profiling-tools-nsight-compute/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/OLCF-Webinar-Nsight-Compute.pdf https://vimeo.com/398929189 | | 2020-03-09 | Nsight Systems | Holly Wilper (NVIDIA) | NVIDIA Profiling Tools - Nsight Systems https://www.olcf.ornl.gov/calendar/nvidia-profiling-tools-nsight-systems/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/Summit-Nsight-Systems-Introduction.pdf,4.374893484174908
"What is the purpose of the NVIDIA Nsight Systems tool?
","Prior to Nsight Systems and Nsight Compute, the NVIDIA command line profiling tool was nvprof, which provides both tracing and kernel profiling capabilities. Like with Nsight Systems and Nsight Compute, the profiler data output can be saved and imported into the NVIDIA Visual Profiler for additional graphical analysis. nvprof is in maintenance mode now: it still works on Summit and significant bugs will be fixed, but no new feature development is occurring on this tool.

To use nvprof, the cuda module must be loaded.

summit> module load cuda",4.328795410591874
"What is the current workaround for the ""Scalable Render Request Failed (VisItException)"" error message?
","Some users have encountered their compute engine exiting abnormally on Andes after VisIt reaches 100% when drawing a plot, resulting in a ""Scalable Render Request Failed (VisItException)"" error message. This message has also been reported when users try to save plots, if VisIt was successfully able to draw. The error seems to more commonly occur for users that are trying to visualize large datasets.",4.359319814890391
"What is the current workaround for the ""Scalable Render Request Failed (VisItException)"" error message?
","VisIt developers have been notified, and at this time the current workaround is to disable Scalable Rendering from being used. To do this, go to Options→Rendering→Advanced and set the ""Use Scalable Rendering"" option to ""Never"".

However, this workaround has been reported to affect VisIt's ability to save images, as scalable rendering is utilized to save plots as image files (which can result in another compute engine crash). To avoid this, screen capture must be enabled. Go to File→""Set save options"" and check the box labeled ""Screen capture"".",4.330082536544358
"What is the current workaround for the ""Scalable Render Request Failed (VisItException)"" error message?
","Using VisIt on Summit is also an option, as the scalable rendering problem is currently not an issue on Summit (as of Sept. 2021).

As of February 2022, this issue on Andes has been fixed (must use VisIt 3.2.2 or higher).",4.184917092559279
"Can I modify or delete information or programs on OLCF systems without authorization?
","Users are prohibited from taking unauthorized actions to intentionally modify or delete information or programs.

The OLCF reserves the right to remove any data at any time and/or transfer data to other users working on the same or similar project once a user account is deleted or a person no longer has a business association with the OLCF. After a sensitive project has ended or has been terminated, all data related to the project must be purged from all OLCF computing resources within 30 days.",4.529962467312679
"Can I modify or delete information or programs on OLCF systems without authorization?
","Users are not allowed to reconstruct information or software for which they are not authorized. This includes but is not limited to any reverse engineering of copyrighted software or firmware present on OLCF computing resources.

Users are accountable for their actions and may be held accountable to applicable administrative or legal sanctions.",4.39149403280659
"Can I modify or delete information or programs on OLCF systems without authorization?
","The OLCF computer systems are operated as research systems and only contain data related to scientific research and do not contain personally identifiable information (data that falls under the Privacy Act of 1974 5U.S.C. 552a). Use of OLCF resources to store, manipulate, or remotely access any national security information is strictly prohibited. This includes, but is not limited to: classified information, unclassified controlled nuclear information (UCNI), naval nuclear propulsion information (NNPI), the design or development of nuclear, biological, or chemical weapons or any weapons of",4.364215375776352
"What command is run inside the container?
","Secondly, the  Pod needs something to do when it starts. For an nginx server this would be running nginx, for a flask app this would be running the app.py file etc. For illustrative purposes this  Pod is going to be starting a shell with the /bin/sh command, echoing a ""Hello World!"" prompt then running a cat command as a means to keep the pod running. Without the addition of the cat at the end the echo command would end causing the /bin/sh to end causing the  Pod to go from a status of Running to Completed.  To make these changes add the following lines below the image line:",4.1787143258166175
"What command is run inside the container?
","command: [""/bin/sh"",""-c""]

args: [""echo 'Hello World!'; cat""]

Finally, we need a tty. This will give us the ability to open a shell in our  Pod and get a better understanding of what is happing. To do this, add the following two lines under the command line that you just added:

tty: true

stdin: true

Your page should now look as follows:



You can now click the 'Create' button in the lower left which will take you to the screen where the   Pod is created.",4.1211183069927895
"What command is run inside the container?
","If we have a pod that is crash looping then it is exiting too quickly to spawn a shell inside the container. We can use oc debug to start a pod with that same image but instead of the image entrypoint we use /bin/sh instead.

$ oc debug misbehaving-pod-1
Defaulting container name to bad.
Use 'oc describe pod/misbehaving-pod-1' to see all of the containers in this pod.

Debugging with pod/misbehaving-pod-1, original command: <image entrypoint>
Waiting for pod to start ...
If you don't see a command prompt, try pressing enter.
/ $

What if we want to get a shell inside of the container to debug?",4.106901923331784
"Are there any specific software or tools that I need to use when working with the SPI resource?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads. Because of this, many processes used within the SPI are very similar to those used for standard non-SPI. This page lists the differences you may see when using OLCF resources to execute SPI resources.",4.266614773970741
"Are there any specific software or tools that I need to use when working with the SPI resource?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.  This page lists the differences you may see when using OLCF resources to execute SPI workflows.  The page will also point to sections within the site where more information on standard non-SPI use can be found.",4.2549135092875
"Are there any specific software or tools that I need to use when working with the SPI resource?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.

The SPI provides access to the OLCF's Summit resource for compute.  To safely separate SPI and non-SPI workflows, SPI workflows must use a separate login node named https://docs.olcf.ornl.gov/systems/citadel_user_guide.html#Citadel<spi-compute-citadel>.  Citadel provides a login node specifically for SPI workflows.",4.211182396664187
"How can I check the status of a job running on the Andes cluster?
",of how to run a Python script using PvBatch on Andes and Summit.,4.206518222336647
"How can I check the status of a job running on the Andes cluster?
","Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-user-guide.

For a full list of software available at the OLCF, please see the Software section (coming soon).",4.188123029325724
"How can I check the status of a job running on the Andes cluster?
","Job Accounting on Andes

Jobs on Andes are scheduled in full node increments; a node's cores cannot be allocated to multiple jobs. Because the OLCF charges based on what a job makes unavailable to other users, a job is charged for an entire node even if it uses only one core on a node. To simplify the process, users are given a multiples of entire nodes through Slurm.

Allocations on Andes are separate from those on Summit and other OLCF resources.

Node-Hour Calculation

The node-hour charge for each batch job will be calculated as follows:",4.160517980436354
"How can I force a job to stop using bkill?
","You can send signals to jobs with the bkill command. While the command name suggests its only purpose is to terminate jobs, this is not the case. Similar to the kill command found in Unix-like operating systems, this command can be used to send various signals (not just SIGTERM and SIGKILL) to jobs. The command can accept both numbers and names for signals. For a list of accepted signal names, run bkill -l. Common ways to invoke the command include:",4.335102834137478
"How can I force a job to stop using bkill?
","Like bstop and bresume, bkill command also supports identifying the job(s) to be signaled by criteria other than the job id. These include some/all jobs with a given name, in a particular queue, etc. See man bkill for more information.",4.303334949960168
"How can I force a job to stop using bkill?
","| Command | Description | | --- | --- | | bkill 12345 | Force a job to stop by sending SIGINT, SIGTERM, and SIGKILL. These signals are sent in that order, so users can write applications such that they will trap SIGINT and/or SIGTERM and exit in a controlled manner. | | bkill -s USR1 12345 | Send SIGUSR1 to job 12345 NOTE: When specifying a signal by name, omit SIG from the name. Thus, you specify USR1 and not SIGUSR1 on the bkill command line. | | bkill -s 9 12345 | Send signal 9 to job 12345 |",4.290662327951081
"How can I retrieve the measurement outcome of a qubit in Pyquil?
",and view the results of your past jobs. More information about using these IBM quantum resources can be found on the IBM's Documentation or our https://docs.olcf.ornl.gov/quantum/quantum_systems/ibm_quantum.html.,4.105849824261752
"How can I retrieve the measurement outcome of a qubit in Pyquil?
","# Set up your Quantum Quil Program (in this case, a ""Bell State"")
program = Program(
    Declare(""ro"", ""BIT"", 2),
    H(0),
    CNOT(0, 1),
    MEASURE(0, (""ro"", 0)),
    MEASURE(1, (""ro"", 1)),
).wrap_in_numshots_loop(10)

# Set up your QVM
qc = get_qc(""2q-qvm"") # Ask for a QVM with two qubits and generic topology

# Compile and Run (pings your Quilc and QVM servers)
print(qc.run(qc.compile(program)).readout_data.get(""ro""))

After running the above script, you should see something similar to this:

[[1 1]
 [0 0]
 [1 1]
 [0 0]
 [1 1]
 [0 0]
 [1 1]
 [1 1]
 [1 1]
 [0 0]]",4.083037293645207
"How can I retrieve the measurement outcome of a qubit in Pyquil?
","the quantum resources via Jupyter notebooks is available in the UI via the “Examples” tab. Quantinuum’s systems feature mid-circuit measurement and qubit reuse, and are compatible with a variety of software frameworks.",4.048788898001555
"How do I unload a module on Frontier?
","You will need to unload the darshan-runtime module. In some instances you may need to unload the xalt and xl modules.  $ module unload darshan-runtime

Serial

..  C

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load scorep
        $ module load gcc
        $ scorep gcc -c test.c
        $ scorep gcc -o test test.o

..  C++

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load scorep
        $ module load gcc
        $ scorep g++ -c test.cpp main.cpp
        $ scorep g++ -o test test.o main.o

..  Fortran",4.147697663827974
"How do I unload a module on Frontier?
","<p style=""font-size:20px""><b>Frontier: Darshan Runtime 3.4.0 (May 10, 2023)</b></p>

The Darshan Runtime modulefile darshan-runtime/3.4.0 on Frontier is now loaded by default. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>.

Unloading darshan-runtime modulefile is recommended for users profiling their applications with other profilers to prevent conflicts.",4.098078234904177
"How do I unload a module on Frontier?
","below), you can choose between the rendering options by loading its corresponding module on the system you're connected to. For example, to see these modules on Summit:",4.08637852907015
"What is the difference between Summit and Frontier in terms of data storage?
","Notable differences between Summit and Frontier:  Orion scratch filesystem  Frontier mounts Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace. Frontier will not mount Alpine and Summit will not mount Orion. Data will not be automatically transferred from Alpine to Orion, so we recommend that users move only needed data between the file systems with Globus.  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-data-storage section or this recording for more information.  Cray Programming Environment  Frontier utilizes the",4.371013870205457
"What is the difference between Summit and Frontier in terms of data storage?
","Frontier is connected to Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Data will not be automatically transferred from Alpine to Orion. Frontier also has access to the center-wide NFS-based filesystem (which provides user and project home areas). Each compute node has two 1.92TB Non-Volatile Memory storage devices. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-data-storage for more",4.30768961759769
"What is the difference between Summit and Frontier in terms of data storage?
","Summit

Andes

Frontier

Scientific simulations generate large amounts of data on Summit (about 100 Terabytes per day for some applications). Because of how large some datafiles may be, it is important that writing and reading these files is done as fast as possible. Less time spent doing input/output (I/O) leaves more time for advancing a simulation or analyzing data.",4.2859637267374415
"How can I prepend the node-local path to LD_LIBRARY_PATH to pick up the SBCAST libraries?
","# At minimum: prepend the node-local path to LD_LIBRARY_PATH to pick up the SBCAST libraries
# It is also recommended that you **remove** any paths that you don't need, like those that contain the libraries that you just SBCAST'd
# Failure to remove may result in unnecessary calls to stat shared file systems
export LD_LIBRARY_PATH=""/mnt/bb/$USER/${exe}_libs:${LD_LIBRARY_PATH}""",4.568543905065935
"How can I prepend the node-local path to LD_LIBRARY_PATH to pick up the SBCAST libraries?
","# At minimum: prepend the node-local path to LD_LIBRARY_PATH to pick up the SBCAST libraries
# It is also recommended that you **remove** any paths that you don't need, like those that contain the libraries that you just SBCAST'd
# Failure to remove may result in unnecessary calls to stat shared file systems
export LD_LIBRARY_PATH=""/mnt/bb/$USER/${exe}_libs:${LD_LIBRARY_PATH}""",4.568543905065935
"How can I prepend the node-local path to LD_LIBRARY_PATH to pick up the SBCAST libraries?
","# If you SBCAST **all** your libraries (ie, `--exclude=NONE`), you may use the following line:
#export LD_LIBRARY_PATH=""/mnt/bb/$USER/${exe}_libs:$(pkg-config --variable=libdir libfabric)""
# Use with caution -- certain libraries may use ``dlopen`` at runtime, and that is NOT covered by sbcast
# If you use this option, we recommend you contact OLCF Help Desk for the latest list of additional steps required",4.23679430889948
"Can the allocation of hours be transferred or shared among multiple researchers or projects in the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  Title: Project Reporting Policy Version: 12.10",4.3704898955382685
"Can the allocation of hours be transferred or shared among multiple researchers or projects in the OLCF Policy?
","Allocation Overuse Policy

Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.3403504894541385
"Can the allocation of hours be transferred or shared among multiple researchers or projects in the OLCF Policy?
","System reservation (a dedicated set of nodes at a specific date/time)

Increased disk quota

Purge exemption for User/Group/World Work areas

Special requests are reviewed weekly by the OLCF Resource Utilization Council. Please contact help@olcf.ornl.gov for more information.



This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title: Computing Policy Version: 12.10",4.315866264383724
"How can I optimize my HIP program for performance on an AMD GPU?
","The HIP API is very similar to CUDA, so if you are already familiar with using CUDA, the transition to using HIP should be fairly straightforward. Whether you are already familiar with CUDA or not, the best place to start learning about HIP is this Introduction to HIP webinar that was recently given by AMD:

Introduction to AMD GPU Programming with HIP: (slides | recording)

More useful resources, provided by AMD, can be found here:

HIP Programming Guide

HIP API Documentation

HIP Porting Guide

The OLCF is currently adding some simple HIP tutorials here as well:",4.360897381063323
"How can I optimize my HIP program for performance on an AMD GPU?
","The Heterogeneous Interface for Portability (HIP) is AMD’s dedicated GPU programming environment for designing high performance kernels on GPU hardware. HIP is a C++ runtime API and programming language that allows developers to create portable applications on different platforms, including the AMD MI250X. This means that developers can write their GPU applications and with very minimal changes be able to run their code in any environment.  The API is very similar to CUDA, so if you're already familiar with CUDA there is almost no additional work to learn HIP. See here for a series of",4.34566362836915
"How can I optimize my HIP program for performance on an AMD GPU?
","| | 2019-06-07 | Introduction to AMD GPU Programming with HIP | Damon McDougall, Chip Freitag, Joe Greathouse, Nicholas Malaya, Noah Wolfe, Noel Chalmers, Scott Moe, Rene van Oostrum, Nick Curtis (AMD) | Introduction to AMD GPU Programming with HIP https://www.olcf.ornl.gov/calendar/introduction-to-amd-gpu-programming-with-hip/ | (slides | recording) https://exascaleproject.org/wp-content/uploads/2017/05/ORNL_HIP_webinar_20190606_final.pdf https://www.youtube.com/watch?v=3ZXbRJVvgJs&feature=youtu.be | | 2019-05-20 | Job Scheduler/Launcher | Chris Fuson (OLCF) | Introduction to Summit",4.319411915720907
"Can I use the guide for parallel h5py on Frontier for other OLCF systems?
","This guide has been adapted for Frontier only for a conda workflow. Using the default cray-python module on Frontier does not work with parallel h5py (because Python 3.9 is incompatible). Thus, this guide assumes that you are using a personal https://docs.olcf.ornl.gov/software/python/miniconda.html.

For venv users only interested in installing mpi4py, the pip command in this guide is still accurate.

This guide has been adapted from a challenge in OLCF's Hands-On with Summit GitHub repository (Python: Parallel HDF5).",4.432559001376093
"Can I use the guide for parallel h5py on Frontier for other OLCF systems?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.

This guide teaches you how to build a personal, parallel-enabled version of h5py and how to write an HDF5 file in parallel using mpi4py and h5py.

In this guide, you will:

Learn how to install mpi4py

Learn how to install parallel h5py

Test your build with Python scripts

OLCF Systems this guide applies to:

Summit

Andes

Frontier",4.366366848020304
"Can I use the guide for parallel h5py on Frontier for other OLCF systems?
",User Guide<summit-user-guide> and https://docs.olcf.ornl.gov/systems/index.html#Frontier User Guide<frontier-user-guide> can be used when building workflows for the non-SPI as well as the Citadel framework.,4.35890385952288
"How do I specify the username and password for my Paraview session?
","After installing, you must give ParaView the relevant server information to be able to connect to OLCF systems (comparable to VisIt's system of host profiles). The following provides an example of doing so. Although several methods may be used, the one described should work in most cases.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.

For Windows clients, it is necessary to install PuTTY to create an ssh connection in step 2.

Step 1: Save the following servers.pvsc file to your local computer",4.213671710368192
"How do I specify the username and password for my Paraview session?
","<String default=""YOURUSERNAME""/>
        </Option>
        <Switch name=""PV_CLIENT_PLATFORM"">
          <Case value=""Apple"">
            <Set name=""TERM_PATH"" value=""/opt/X11/bin/xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />
            <Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Linux"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.187937415133317
"How do I specify the username and password for my Paraview session?
","<String default=""YOURUSERNAME""/>
        </Option>
        <Switch name=""PV_CLIENT_PLATFORM"">
          <Case value=""Apple"">
            <Set name=""TERM_PATH"" value=""/opt/X11/bin/xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />
            <Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Linux"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.187728375511977
"How can I submit an interactive job on Frontier?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.366633315723098
"How can I submit an interactive job on Frontier?
","To access Summit and Frontier's compute resources for SPI workflows, you must first log into a https://docs.olcf.ornl.gov/systems/index.html#Citadel login node<citadel-login-nodes> and then submit a batch job to one of the SPI specific batch queues.",4.20562420673095
"How can I submit an interactive job on Frontier?
","Users may have only 100 jobs queued in the batch queue at any time (this includes jobs in all states). Additional jobs will be rejected at submit time.

The debug quality of service (QOS) class can be used to access Frontier's compute resources for short non-production debug tasks. The QOS provides a higher priority compare to jobs of the same job size bin in production queues. Production work and job chaining using the debug QOS is prohibited. Each user is limited to one job in any state at any one point. Attempts to submit multiple jobs to this QOS will be rejected upon job submission.",4.18220604729426
"Can I use Slate to create YAML files for my Kubernetes resources?
","Before we dive in there are some terms that need to be understood. This will be a basic set of terms and a copy and paste from our https://docs.olcf.ornl.gov/systems/guided_tutorial.html#slate_glossary, so we recommend reading that document and even keeping it handy until you are familiar with all of the definitions there. On that note, another good piece of reference documentation the https://docs.olcf.ornl.gov/systems/guided_tutorial.html#slate_examples document. There you can find basic YAML definitions for the most common objects in Kubernetes.",4.259442138236876
"Can I use Slate to create YAML files for my Kubernetes resources?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",4.251425491941536
"Can I use Slate to create YAML files for my Kubernetes resources?
","directory of YAML or JSON files

kustomize applications

helm charts

This section will focus on the deployment of Kubernetes resources using kustomize. If the use of helm is preferred, refer to the Continuous Delivery with Helm and ArgoCD blog post as well as the App of Apps Pattern discussed on the ArgoCD Cluster Bootstrapping page.

References to ksonnet for deployment of Kubernetes resources may be mentioned in some documentation. However, the use if ksonnet is no longer supported by ArgoCD.

In order to deploy resources, one should have the following to start with:",4.194897215518283
"Is there a limit to the number of nodes a batch job can request on Summit?
","The batch-hm queue (and the batch-hm-spi queue for Moderate Enhanced security enclave projects) is used to access Summit's high-memory nodes.  Jobs may use all 54 nodes. It enforces the following policies:

Limit of (4) eligible-to-run jobs per user.

Jobs in excess of the per user limit above will be placed into a held state, but will change to eligible-to-run at the appropriate time.

Users may have only (25) jobs queued in the batch-hm queue at any state at any time. Additional jobs will be rejected at submit time.

batch-hm job limits:",4.416003389509397
"Is there a limit to the number of nodes a batch job can request on Summit?
","| Bin | Min Nodes | Max Nodes | Max Walltime (Hours) | Aging Boost (Days) | | --- | --- | --- | --- | --- | | 1 | 2,765 | 4,608 | 24.0 | 15 | | 2 | 922 | 2,764 | 24.0 | 10 | | 3 | 92 | 921 | 12.0 | 0 | | 4 | 46 | 91 | 6.0 | 0 | | 5 | 1 | 45 | 2.0 | 0 |

The batch queue (and the batch-spi queue for Moderate Enhanced security enclave projects) is the default queue for production work on Summit.  Most work on Summit is handled through this queue. It enforces the following policies:

Limit of (4) eligible-to-run jobs per user.",4.322658657732229
"Is there a limit to the number of nodes a batch job can request on Summit?
","Jobs on Summit are scheduled in full node increments; a node's cores cannot be allocated to multiple jobs. Because the OLCF charges based on what a job makes unavailable to other users, a job is charged for an entire node even if it uses only one core on a node. To simplify the process, users request and are allocated multiples of entire nodes through LSF.

Allocations on Summit are separate from those on Andes and other OLCF resources.

The node-hour charge for each batch job will be calculated as follows:

node-hours = nodes requested * ( batch job endtime - batch job starttime )",4.319936116049928
"How do I monitor batch submissions and job history?
","Most users will find batch jobs an easy way to use the system, as they allow you to ""hand off"" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.",4.267979753220584
"How do I monitor batch submissions and job history?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.253248372154655
"How do I monitor batch submissions and job history?
","The most straightforward monitoring is with the bjobs command. This command will show the current queue, including both pending and running jobs. Running bjobs -l will provide much more detail about a job (or group of jobs). For detailed output of a single job, specify the job id after the -l. For example, for detailed output of job 12345, you can run bjobs -l 12345 . Other options to bjobs are shown below. In general, if the command is specified with -u all it will show information for all users/all jobs. Without that option, it only shows your jobs. Note that this is not an exhaustive list.",4.249462178787746
"How many ranks are running on the node with hostname h41n04?
","Rank:    6; NumRanks: 12; RankCore:   0; Hostname: h41n03; GPU: 0
Rank:    7; NumRanks: 12; RankCore:   4; Hostname: h41n03; GPU: 1
Rank:    8; NumRanks: 12; RankCore:   8; Hostname: h41n03; GPU: 2
Rank:    9; NumRanks: 12; RankCore:  88; Hostname: h41n03; GPU: 3
Rank:   10; NumRanks: 12; RankCore:  92; Hostname: h41n03; GPU: 4
Rank:   11; NumRanks: 12; RankCore:  96; Hostname: h41n03; GPU: 5",4.359762936806406
"How many ranks are running on the node with hostname h41n04?
","Rank:   12; NumRanks: 24; RankCore:   0; Hostname: a33n05; GPU: 0, 1, 2
Rank:   13; NumRanks: 24; RankCore:   4; Hostname: a33n05; GPU: 0, 1, 2
Rank:   14; NumRanks: 24; RankCore:   8; Hostname: a33n05; GPU: 0, 1, 2
Rank:   15; NumRanks: 24; RankCore:  12; Hostname: a33n05; GPU: 0, 1, 2
Rank:   16; NumRanks: 24; RankCore:  16; Hostname: a33n05; GPU: 0, 1, 2
Rank:   17; NumRanks: 24; RankCore:  20; Hostname: a33n05; GPU: 0, 1, 2",4.277316522699826
"How many ranks are running on the node with hostname h41n04?
","h41n08
h41n08

Here, Jsrun starts 2 separate Singularity container runtimes since we pass the -n2 flag to start two processes. Each Singularity container runtime then loads the container image simple.sif and executes the hostname command from that container. If we had requested 2 nodes in the batch script and had run jsrun -n2 -r1 singularity exec ./simple.sif hostname, Jsrun would've started a Singularity runtime on each node and the output would look something like

h41n08
h41n09



Creating Singularity containers that run MPI programs require a few additional steps.",4.271123499261153
"How can I see the output of the job?
","The most straightforward monitoring is with the bjobs command. This command will show the current queue, including both pending and running jobs. Running bjobs -l will provide much more detail about a job (or group of jobs). For detailed output of a single job, specify the job id after the -l. For example, for detailed output of job 12345, you can run bjobs -l 12345 . Other options to bjobs are shown below. In general, if the command is specified with -u all it will show information for all users/all jobs. Without that option, it only shows your jobs. Note that this is not an exhaustive list.",4.205883486268848
"How can I see the output of the job?
","$ bsub mlflow_demo.lsf

Congratulations! Once the job completes, you will be able to check the standard output files to find the tracking and artifact directories.",4.146391987967702
"How can I see the output of the job?
","Provides additional details of given job.

The sview tool provide a graphical queue monitoring tool. To use, you will need an X server running on your local system. You will also need to tunnel X traffic through your ssh connection:

local-system> ssh -Y username@andes.ccs.ornl.gov
andes-login> sview",4.144414135016952
"How can you prevent this error from occurring in the future?
",It is the user’s responsibility to insure the appropriate level of backup and integrity checks on critical data and programs.,3.9862607370553382
"How can you prevent this error from occurring in the future?
","During handling of the above exception, another exception occurred:",3.97646830199439
"How can you prevent this error from occurring in the future?
",Users are prohibited from taking unauthorized actions to intentionally modify or delete information or programs.,3.960273226158322
"What is the name of the library that provides the lber_ functions?
","OLCF systems provide many software packages and scientific libraries pre-installed at the system-level for users to take advantage of. In order to link these libraries into an application, users must direct the compiler to their location. The module show command can be used to determine the location of a particular library. For example",4.022852599228001
"What is the name of the library that provides the lber_ functions?
","libgssapi_krb5.so.2 => /usr/lib64/libgssapi_krb5.so.2 (0x00007ffef6ff8000)
    libldap_r-2.4.so.2 => /usr/lib64/libldap_r-2.4.so.2 (0x00007ffef6da4000)
    liblber-2.4.so.2 => /usr/lib64/liblber-2.4.so.2 (0x00007ffef6b95000)
    libzstd.so.1 => /usr/lib64/libzstd.so.1 (0x00007ffef6865000)
    libbrotlidec.so.1 => /usr/lib64/libbrotlidec.so.1 (0x00007ffef6659000)
    libunistring.so.2 => /usr/lib64/libunistring.so.2 (0x00007ffef62d6000)
    libjitterentropy.so.3 => /usr/lib64/libjitterentropy.so.3 (0x00007ffef60cf000)
    libkrb5.so.3 => /usr/lib64/libkrb5.so.3 (0x00007ffef5df6000)",3.9698793814368183
"What is the name of the library that provides the lber_ functions?
","libgssapi_krb5.so.2 => /usr/lib64/libgssapi_krb5.so.2 (0x00007ffef6ff8000)
    libldap_r-2.4.so.2 => /usr/lib64/libldap_r-2.4.so.2 (0x00007ffef6da4000)
    liblber-2.4.so.2 => /usr/lib64/liblber-2.4.so.2 (0x00007ffef6b95000)
    libzstd.so.1 => /usr/lib64/libzstd.so.1 (0x00007ffef6865000)
    libbrotlidec.so.1 => /usr/lib64/libbrotlidec.so.1 (0x00007ffef6659000)
    libunistring.so.2 => /usr/lib64/libunistring.so.2 (0x00007ffef62d6000)
    libjitterentropy.so.3 => /usr/lib64/libjitterentropy.so.3 (0x00007ffef60cf000)
    libkrb5.so.3 => /usr/lib64/libkrb5.so.3 (0x00007ffef5df6000)",3.9698793814368183
"How do I switch between environments in Slate?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",4.039040518699434
"How do I switch between environments in Slate?
","Due to the specific nature of conda on Summit, you must use source activate and source deactivate instead of conda activate and conda deactivate. Let's activate the new environment:

$ source activate /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/py3711-summit

The path to the environment should now be displayed in ""( )"" at the beginning of your terminal lines, which indicate that you are currently using that specific conda environment. And if you check with conda env list again, you should see that the * marker has moved to your newly activated environment:

$ conda env list",4.02826865966972
"How do I switch between environments in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.020036511564754
"What is the purpose of the -n flag in the jsrun command?
","In addition, to debug slow startup JSM provides the option to create a progress file. The file will show information that can be helpful to pinpoint if a specific node is hanging or slowing down the job step launch. To enable it, you can use: jsrun --progress ./my_progress_file_output.txt.

When using file-based specification of resource sets with jsrun -U, the -a flag (number of tasks per resource set) is ignored. This has been reported to IBM and they are investigating. It is generally recommended to use jsrun explicit resource files (ERF) with --erf_input and --erf_output instead of -U.",4.266360970447492
"What is the purpose of the -n flag in the jsrun command?
","As on any system, it is useful to keep in mind the hardware underneath every execution. This is particularly true when laying out resource sets.

jsrun    [ -n #resource sets ]   [tasks, threads, and GPUs within each resource set]   program [ program args ]

Below are common jsrun options. More flags and details can be found in the jsrun man page. The defaults listed in the table below are the OLCF defaults and take precedence over those mentioned in the man page.",4.248113977164154
"What is the purpose of the -n flag in the jsrun command?
","This section describes tools that users might find helpful to better understand the jsrun job launcher.

hello_jsrun is a ""Hello World""-type program that users can run on Summit nodes to better understand how MPI ranks and OpenMP threads are mapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A screencast showing how to use Hello_jsrun is also available: https://vimeo.com/261038849

Job Step Viewer provides a graphical view of an application's runtime layout on Summit. It allows users to preview and quickly iterate with multiple jsrun options to understand and optimize job launch.",4.215023560256206
"Can I use a different MPI implementation instead of Spectrum MPI?
","Spectrum MPI relies on CUDA Inter-process Communication (CUDA IPC) to provide fast on-node between GPUs. At present this capability cannot function with more than one resource set per node.

Set the environment variable PAMI_DISABLE_IPC=1 to force Spectrum MPI to not use fast GPU Peer-to-peer communication. This option will allow your code to run with more than one resource set per host, but you may see slower GPU to GPU communication.

Run in a single resource set per host, i.e. with jsrun --gpu_per_rs 6",4.243949216300814
"Can I use a different MPI implementation instead of Spectrum MPI?
","If on-node MPI communication between GPUs is critical to your application performance, option B is recommended but you’ll need to set the GPU affinity manually. This could be done with an API call in your code (e.g. cudaSetDevice), or by using a wrapper script.

We have seen occasional errors from batch jobs with multiple simultaneous backgrounded jsrun commands. Jobs may see pmix errors during the noted failures.



The following issue was resolved with the software default changes from March 12, 2019 that set Spectrum MPI 10.2.0.11 (20190201) as default and moved ROMIO to version 3.2.1:",4.179520364633767
"Can I use a different MPI implementation instead of Spectrum MPI?
","The reason this occurs is that the PAMI messaging backend, used by Spectrum MPI by default, has a ""CUDA hook"" that records GPU memory allocations. This record is used later during CUDA-aware MPI calls to efficiently detect whether a given message is sent from the CPU or the GPU. This is done by design in the IBM implementation and is unlikely to be changed.

There are two main ways to work around this problem. If CUDA-aware MPI is not a relevant factor for your work (which is naturally true for serial applications) then you can simply disable the CUDA hook with:",4.170636963890361
"What is the purpose of the `module load` command in the provided code?
",".. note::
   If you are using a `https://docs.olcf.ornl.gov/software/python/miniconda.html`, the above ``module load cray-python`` should not be loaded.

Andes

.. code-block:: bash

   $ module load gcc/9.3.0 # works with older GCC versions if using cuda/10.2.89
   $ module load cuda/11.0.2
   $ module load python

Loading a python module puts you in a ""base"" environment, but you need to create a new environment using the conda create command (Summit and Andes) or the venv command (Frontier):

Summit

.. code-block:: bash",4.082532082358114
"What is the purpose of the `module load` command in the provided code?
","First, load the gnu compiler module (most Python packages assume GCC), relevant GPU module (necessary for CuPy), and the python module (allows you to create a new environment):

Summit

.. code-block:: bash

   $ module load gcc/7.5.0 # might work with other GCC versions
   $ module load cuda/11.0.2
   $ module load python

Frontier

.. code-block:: bash

   $ module load PrgEnv-gnu
   $ module load amd-mixed/5.3.0
   $ module load craype-accel-amd-gfx90a
   $ module load cray-python # only if not using Miniconda on Frontier",4.0768376679103415
"What is the purpose of the `module load` command in the provided code?
","First, load the gnu compiler module (most Python packages assume GCC), hdf5 module (necessary for h5py), and the python module (allows you to create a new environment):

Summit

.. code-block:: bash

   $ module load gcc
   $ module load hdf5
   $ module load python

Andes

.. code-block:: bash

   $ module load gcc
   $ module load hdf5
   $ module load python

Frontier

.. code-block:: bash

   $ module load PrgEnv-gnu
   $ module load hdf5

   # Make sure your personal miniconda installation is in your path
   $ export PATH=""/path/to/your/miniconda/bin:$PATH""",4.074555309066411
"How do I submit a batch script to the batch scheduler on Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.44867424762542
"How do I submit a batch script to the batch scheduler on Andes?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.3653029772456335
"How do I submit a batch script to the batch scheduler on Andes?
","The most common way to interact with the batch system is via batch scripts. A batch script is simply a shell script with added directives to request various resoruces from or provide certain information to the scheduling system.  Aside from these directives, the batch script is simply the series of commands needed to set up and run your job.

To submit a batch script, use the command sbatch myjob.sl

Consider the following batch script:

#!/bin/bash
#SBATCH -A ABC123
#SBATCH -J RunSim123
#SBATCH -o %x-%j.out
#SBATCH -t 1:00:00
#SBATCH -p batch
#SBATCH -N 1024",4.364480896887756
"How should the school affiliation be entered for a student?
","Fill in your Employment/Institution Information. If you are student please use your school affiliation for both ""Employer"" and ""Funding Source"". If you are a student and you do not see your school listed, choose ""other"" for both ""Employer"" and ""Funding Source"" and then manually enter your school affiliation in the adjacent fields.  Click “Next” when you are done.

On the Project information screen fill the ""Proposed Contribution to Project"" with ""Participating in OLCF training."" Leave all the questions about the project set to ""no"" and click ""Next"".",4.172163824474085
"How should the school affiliation be entered for a student?
","If you will be contributing to multiple projects, your user account will need to be associated with each. For instructions on joining additional projects with an existing account, see the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#Get access to additional projects<get-additional-projects> section below.

First-time users should apply for an account using the Account Request Form.",3.934257943547092
"How should the school affiliation be entered for a student?
","ORNL Personnel Access System (PAS): All PI’s are required to be entered into the ORNL PAS system. An OLCF Accounts Manager will send the PI a PAS invitation to submit all the pertinent information. Please note that processing a PAS request may take 15 or more days.

User Agreement/Appendix A or Subcontract: A User Agreement/Appendix A or Subcontract must be executed between UT-Battelle and the PI’s institution. If our records indicate this requirement has not been met, all necessary documents will be provided to the applicant by an OLCF Accounts Manager.",3.9084298917609206
"How will the Alpine II filesystem improve performance?
","Please note, Summit will mount a new filesystem once returned to service.  Data stored on Alpine at the time of its decommission on January 01 will not be available.  Users will be responsible for transferring data onto Summit's new filesystem



Alpine II will be available early 2024.

The previous center-wide GPFS scratch filesystem, Alpine, will be decommissioned in January 2024. A new scratch filesystem will be made available for projects with 2024 Summit allocations in early 2024. Users will be responsible for transferring any needed data onto the new scratch filesystem once available.",4.297452446447522
"How will the Alpine II filesystem improve performance?
","On Summit, there is no concept of striping from the user point of view, the user uses the Alpine storage without the need to declare the striping for files/directories. The GPFS will handle the workload, the file system was tuned during the installation.",4.248303237498247
"How will the Alpine II filesystem improve performance?
","Summit mounts a POSIX-based IBM Spectrum Scale parallel filesystem called Alpine. Alpine's maximum capacity is 250 PB. It is consisted of 77 IBM Elastic Storage Server (ESS) GL4 nodes running IBM Spectrum Scale 5.x which are called Network Shared Disk (NSD) servers. Each IBM ESS GL4 node, is a scalable storage unit (SSU), constituted by two dual-socket IBM POWER9 storage servers, and a 4X EDR InfiniBand network for up to 100Gbit/sec of networking bandwidth.  The maximum performance of the final production system will be about 2.5 TB/s for sequential I/O and 2.2 TB/s for random I/O under FPP",4.210749235532959
"How can I run a job on a specific node on Summit?
","to service nodes on that system). All commands within your job script (or the commands you run in an interactive job) will run on a launch node. Like login nodes, these are shared resources so you should not run multiprocessor/threaded programs on Launch nodes. It is appropriate to launch the jsrun command from launch nodes. | | Compute | Most of the nodes on Summit are compute nodes. These are where your parallel job executes. They're accessed via the jsrun command. |",4.379352044806272
"How can I run a job on a specific node on Summit?
","Recall from the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#system-overview section that Summit has three types of nodes: login, launch, and compute. When you log into the system, you are placed on a login node. When your https://docs.olcf.ornl.gov/systems/summit_user_guide.html#batch-scripts or https://docs.olcf.ornl.gov/systems/summit_user_guide.html#interactive-jobs run, the resulting shell will run on a launch node. Compute nodes are accessed via the jsrun command. The jsrun command should only be issued from within an LSF job (either batch or interactive) on a launch node.",4.367566706333884
"How can I run a job on a specific node on Summit?
","Jobs on Summit are scheduled in full node increments; a node's cores cannot be allocated to multiple jobs. Because the OLCF charges based on what a job makes unavailable to other users, a job is charged for an entire node even if it uses only one core on a node. To simplify the process, users request and are allocated multiples of entire nodes through LSF.

Allocations on Summit are separate from those on Andes and other OLCF resources.

The node-hour charge for each batch job will be calculated as follows:

node-hours = nodes requested * ( batch job endtime - batch job starttime )",4.331091224954259
"What is the difference between batch scripts and interactive batch jobs in Frontier?
","Most users will find batch jobs to be the easiest way to interact with the system, since they permit you to hand off a job to the scheduler and then work on other tasks; however, it is sometimes preferable to run interactively on the system. This is especially true when developing, modifying, or debugging a code.",4.352731586414123
"What is the difference between batch scripts and interactive batch jobs in Frontier?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.322432333120508
"What is the difference between batch scripts and interactive batch jobs in Frontier?
","Most users will find batch jobs an easy way to use the system, as they allow you to ""hand off"" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.",4.312021427644543
"What is the current status of the Argocd application controller pod?
","$ oc get all
NAME                                      READY   STATUS    RESTARTS   AGE
pod/argocd-application-controller-0       1/1     Running   0          2m52s
pod/argocd-redis-6b9cd5d47-7dpwh          1/1     Running   0          2m52s
pod/argocd-repo-server-5c4dbb5556-sm2bt   1/1     Running   0          2m52s
pod/argocd-server-5bc4646756-2zkr5        1/1     Running   0          2m52s",4.321409266857133
"What is the current status of the Argocd application controller pod?
","NAME                                            DESIRED   CURRENT   READY   AGE
replicaset.apps/argocd-redis-6b9cd5d47          1         1         1       2m52s
replicaset.apps/argocd-repo-server-5c4dbb5556   1         1         1       2m52s
replicaset.apps/argocd-server-5bc4646756        1         1         1       2m52s

NAME                                             READY   AGE
statefulset.apps/argocd-application-controller   1/1     2m52s",4.283458959363401
"What is the current status of the Argocd application controller pod?
","ArgoCD has successfully accessed the namespace, realized that the state of the resources in the namespace are OutOfSync with the resource requirements of the code repository, and started the Syncing process to create and/or modify resources in the namespace to match the desired configuration. Clicking on the application tile will reveal more detailed information on the process:

Image of ArgoCD application tile detailed information.",4.266585010924716
"How can I obtain the best performance when using VisIt?
","VisIt uses a client-server architecture. You will obtain the best performance by running the VisIt client on your local computer and running the server on OLCF resources. VisIt for your local computer can be obtained here: VisIt Installation.

Recommended VisIt versions on our systems:

Summit: VisIt 3.1.4

Andes: VisIt 3.3.3

Frontier: VisIt 3.3.3

Using a different version than what is listed above is not guaranteed to work properly.",4.356846578714681
"How can I obtain the best performance when using VisIt?
","VisIt is now available on Frontier through the UMS022 module.

VisIt 3.3.3 is now available on Andes.

VisIt is an interactive, parallel analysis and visualization tool for scientific data. VisIt contains a rich set of visualization features so you can view your data in a variety of ways. It can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes. Further information regarding VisIt can be found at the links provided in the https://docs.olcf.ornl.gov/systems/visit.html#visit-resources section.",4.211429027077013
"How can I obtain the best performance when using VisIt?
","Using VisIt on Summit is also an option, as the scalable rendering problem is currently not an issue on Summit (as of Sept. 2021).

As of February 2022, this issue on Andes has been fixed (must use VisIt 3.2.2 or higher).",4.1818099821247685
"What is the difference in denormal handling between the V_DOT2 instruction and matrix instructions for FP16 and BF16 on the MI250X GPU?
","Users leveraging BF16 and FP16 datatypes for applications such as ML/AI training and low-precision matrix multiplication should be aware that the AMD MI250X GPU has different denormal handling than the V100 GPUs on Summit. On the MI250X, the V_DOT2 and the matrix instructions for FP16 and BF16 flush input and output denormal values to zero. FP32 and FP64 MFMA instructions do not flush input and output denormal values to zero.",4.645114392453909
"What is the difference in denormal handling between the V_DOT2 instruction and matrix instructions for FP16 and BF16 on the MI250X GPU?
","Users leveraging BF16 and FP16 datatypes for applications such as ML/AI training and low-precision matrix multiplication should be aware that the AMD MI250X GPU has different denormal handling than the V100 GPUs on Summit. On the MI250X, the V_DOT2 and the matrix instructions for FP16 and BF16 flush input and output denormal values to zero. FP32 and FP64 MFMA instructions do not flush input and output denormal values to zero.",4.645114392453909
"What is the difference in denormal handling between the V_DOT2 instruction and matrix instructions for FP16 and BF16 on the MI250X GPU?
","The MI250X has different denormal handling for FP16 and BF16 datatypes, which is relevant for ML training. Prefer using the BF16 over the FP16 datatype for ML models as you are more likely to encounter denormal values with FP16 (which get flushed to zero, causing failure in convergence for some ML models). See more in https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#using-reduced-precision.",4.463616031783037
"How can I change the GPU's compute mode to DEFAULT on Summit?
","Summit's V100 GPUs are configured to have a default compute mode of EXCLUSIVE_PROCESS. In this mode, the GPU is assigned to only a single process at a time, and can accept work from multiple process threads concurrently.

It may be desirable to change the GPU's compute mode to DEFAULT, which enables multiple processes and their threads to share and submit work to it simultaneously. To change the compute mode to DEFAULT, use the -alloc_flags gpudefault option.",4.440155300800852
"How can I change the GPU's compute mode to DEFAULT on Summit?
","NVIDIA recommends using the EXCLUSIVE_PROCESS compute mode (the default on Summit) when using the Multi-Process Service, but both MPS and the compute mode can be changed by providing both values: -alloc_flags ""gpumps gpudefault"".",4.250645545816875
"How can I change the GPU's compute mode to DEFAULT on Summit?
",http://vimeo.com/306890606 | | 2018-12-05 | Targeting GPUs Using GPU Directives on Summit with GenASiS: A Simple and Effective Fortran Experience | Reuben Budiardja (OLCF) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_budiardja.pdf https://vimeo.com/306890448 | | 2018-12-05 | Experiences Using the Volta Tensor Cores | Wayne Joubert (OLCF) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (recording),4.1320490939504015
"What is the deadline for applying to be a mentor?
","If you would like more information about how you can volunteer to mentor a team at an upcoming Open/GPU hackathon, please visit our Become a Mentor page.

<p style=""font-size:20px""><b>Who can I contact with questions?</b></p>

If you have any questions about the OLCF GPU Hackathon Series, please contact OLCF Help at (help@olcf.ornl.gov).",4.157078680897895
"What is the deadline for applying to be a mentor?
","First, you must decide which event you'd like to attend (use link below to find a hackathon whose dates make sense for your team), and then submit a short proposal form describing your application and team. The organizing committee will then review all proposals after the call for that event closes and select the teams they believe are best suited for the event.",4.111624095124788
"What is the deadline for applying to be a mentor?
","Please visit openhackathons.org/s/events-overview to see the current list of events (new ones added throughout the year) and their proposal deadlines. To submit a proposal, click on the event you'd like to attend and submit the form.

The OLCF-supported events are a subset of a larger number of Open hackathons organized around the world. Look for hackathons with the OLCF logo to find events supported by OLCF.

<p style=""font-size:20px""><b>Want to be a mentor?</b></p>",4.072647039597053
"What is the purpose of the initContainer?
","Users will be building and running containers on Summit without root permissions i.e. containers on Summit are rootless.  This means users can get the benefits of containers without needing additional privileges. This is necessary for a shared system like Summit. And this is part of the reason why Docker doesn't work on Summit. Podman and Singularity provides rootless support but to different extents hence why users need to use a combination of both.

Users will need to set up a file in their home directory /ccs/home/<username>/.config/containers/storage.conf with the following content:",3.940021933840971
"What is the purpose of the initContainer?
","If we have a pod that is crash looping then it is exiting too quickly to spawn a shell inside the container. We can use oc debug to start a pod with that same image but instead of the image entrypoint we use /bin/sh instead.

$ oc debug misbehaving-pod-1
Defaulting container name to bad.
Use 'oc describe pod/misbehaving-pod-1' to see all of the containers in this pod.

Debugging with pod/misbehaving-pod-1, original command: <image entrypoint>
Waiting for pod to start ...
If you don't see a command prompt, try pressing enter.
/ $

What if we want to get a shell inside of the container to debug?",3.927730887867853
"What is the purpose of the initContainer?
","Source to image is a tool to build docker formatted container images. This is accomplished by injecting source code into a container image and then building a new image. The new image will contain the source code ready to run, via the $ docker run command inside the newly built image.

Using the S2I model of building images, it is possible to have your source hosted on a git repository and then pulled and built by Openshift.

Click on the newly created project and then on the blue Add to Project button on the center of the page.",3.924134049637797
"How can I test whether processes and threads are running where intended?
","There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.",4.279881956265699
"How can I test whether processes and threads are running where intended?
","bjobs and jobstat help to identify what’s currently running and scheduled to run, but sometimes it’s beneficial to know how much of the system is not currently in use or scheduled for use.",4.054732465409457
"How can I test whether processes and threads are running where intended?
","This section describes how to map processes (e.g., MPI ranks) and process threads (e.g., OpenMP threads) to the CPUs and GPUs on Spock. The https://docs.olcf.ornl.gov/systems/spock_quick_start_guide.html#spock-compute-nodes diagram will be helpful when reading this section to understand which hardware threads your processes and threads run on.",4.039362820242271
"How do I manage my environment on Andes?
",For Andes:,4.285005363362283
"How do I manage my environment on Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.208251233405292
"How do I manage my environment on Andes?
","Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-user-guide.

For a full list of software available at the OLCF, please see the Software section (coming soon).",4.149943242394109
"What is the issue with the Abstract-Device Interface for I/O (ADIO) driver on Summit?
","Although there are newer CUDA modules on Summit, cuda/11.0.3 is the latest version that is officially supported by the version of IBM's software stack installed on Summit. When loading the newer CUDA modules, a message is printed to the screen stating that the module is for “testing purposes only”. These newer unsupported CUDA versions might work with some users’ applications, but importantly, they are known not to work with GPU-aware MPI.",4.033628399104738
"What is the issue with the Abstract-Device Interface for I/O (ADIO) driver on Summit?
","Summit node architecture diagram

The basic building block of Summit is the IBM Power System AC922 node. Each of the approximately 4,600 compute nodes on Summit contains two IBM POWER9 processors and six NVIDIA Tesla V100 accelerators and provides a theoretical double-precision capability of approximately 40 TF. Each POWER9 processor is connected via dual NVLINK bricks, each capable of a 25GB/s transfer rate in each direction.",4.007074091020031
"What is the issue with the Abstract-Device Interface for I/O (ADIO) driver on Summit?
","Using the hip-cuda/5.1.0 module on Summit requires CUDA v11.4.0 or later. However, only CUDA v11.0.3 and earlier currently support GPU-aware MPI, so GPU-aware MPI is currently not available when using HIP on Summit.

Any codes compiled with post 11.0.3 CUDA (cuda/11.0.3) will not work with MPS enabled (-alloc_flags ""gpumps"") on Summit. The code will hang indefinitely. CUDA v11.0.3 is the latest version that is officially supported by IBM's software stack installed on Summit. We are continuing to look into this issue.",4.000103623740859
"Can I use the spider sub-command to search for modules that are not compatible with the current environment?
","Searching for modules

Modules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the spider sub-command can be used as summarized in the following table.",4.459496790311151
"Can I use the spider sub-command to search for modules that are not compatible with the current environment?
","Modules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the spider sub-command can be used as summarized in the following table.",4.407847506841048
"Can I use the spider sub-command to search for modules that are not compatible with the current environment?
","Modules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the spider sub-command can be used as summarized in the following table.",4.40762852965324
"Is Ascent a user resource for the OLCF?
",Ascent is a training system that is not intended to be used as an OLCF user resource. Access to the system is only obtained through OLCF training events.,4.527753702951073
"Is Ascent a user resource for the OLCF?
","System Overview



Ascent is a stand-alone 18-node system with the same architecture and design as Summit. It's most often utilized as a resource for OLCF training events, workshops, and conferences. Ascent exists in the NCCS Open Security Enclave, which is subject to fewer restrictions than the Moderate Security Enclave that houses systems such as Summit. This means that participants in training events can go through a streamlined version of the approval process before being granted access.",4.476687163388808
"Is Ascent a user resource for the OLCF?
","Ascent is an 18-node stand-alone system with the same architecture as Summit (see https://docs.olcf.ornl.gov/systems/summit_user_guide.html#summit-nodes section above), so most of this Summit User Guide can be referenced for Ascent as well. However, aside from the number of compute nodes, there are other differences between the two systems. Most notably, Ascent sits in the NCCS Open Security Enclave, which is subject to fewer restrictions than the Moderate Security Enclave that systems such as Summit belong to. This means that participants in OLCF training events can go through a streamlined",4.4046084988472485
"How do I ensure that my application is using the correct number of CPU cores and GPUs on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.294348281956771
"How do I ensure that my application is using the correct number of CPU cores and GPUs on Frontier?
","This section describes how to map processes (e.g., MPI ranks) and process threads (e.g., OpenMP threads) to the CPUs, GPUs, and NICs on Frontier.

Users are highly encouraged to use the CPU- and GPU-mapping programs used in the following sections to check their understanding of the job steps (i.e., srun commands) they intend to use in their actual jobs.

For the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-cpu-map and https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-multi-map sections:",4.247559383528576
"How do I ensure that my application is using the correct number of CPU cores and GPUs on Frontier?
","Because a Frontier compute node has two hardware threads available (2 logical cores per physical core), this enables the possibility of multithreading your application (e.g., with OpenMP threads). Although the additional hardware threads can be assigned to additional MPI tasks, this is not recommended. It is highly recommended to only use 1 MPI task per physical core and to use OpenMP threads instead on any additional logical cores gained when using both hardware threads.",4.243581205931843
"What is the difference between `parsl.dataflow.dflow` and `parsl.dataflow.dfail`?
","2021-06-28 16:10:46 parsl.dataflow.dflow:431 [INFO]  Task 0 completed (launched -> exec_done)
Hello from uname_result(system='Linux', node='a01n14', release='4.14.0-115.21.2.el7a.ppc64le', version='#1 SMP Thu May 7 22:22:31 UTC 2020', machine='ppc64le', processor='ppc64le')

Congratulations! You have now run a Parsl job on Summit.",3.976970326007872
"What is the difference between `parsl.dataflow.dflow` and `parsl.dataflow.dfail`?
",Below is a comparison table between srun and jsrun.,3.943847517798818
"What is the difference between `parsl.dataflow.dflow` and `parsl.dataflow.dfail`?
","Parsl is a flexible and scalable parallel programming library for Python which is being developed at the University of Chicago. It augments Python with simple constructs for encoding parallelism. For more information about Parsl, please refer to its documentation.

Parsl can be installed with Conda for use on Summit by running the following from a login node:

$ module load workflows
$ module load parsl/1.1.0

The following instructions illustrate how to run a ""Hello world"" program with Parsl on Summit.",3.922634033068622
"What is the advantage of compiling with -xnack flag?
","Two versions of each kernel will be generated, one that runs with XNACK disabled and one that runs if XNACK is enabled. This is different from ""xnack any"" in that two versions of each kernel are compiled and HIP picks the appropriate one at runtime, rather than there being a single version compatible with both. A ""fat binary"" compiled in this way will have the same performance of ""xnack+"" with HSA_XNACK=1 and as ""xnack-"" with HSA_XNACK=0, but the final executable will be larger since it contains two copies of every kernel.",4.273674823339967
"What is the advantage of compiling with -xnack flag?
","Two versions of each kernel will be generated, one that runs with XNACK disabled and one that runs if XNACK is enabled. This is different from ""xnack any"" in that two versions of each kernel are compiled and HIP picks the appropriate one at runtime, rather than there being a single version compatible with both. A ""fat binary"" compiled in this way will have the same performance of ""xnack+"" with HSA_XNACK=1 and as ""xnack-"" with HSA_XNACK=0, but the final executable will be larger since it contains two copies of every kernel.",4.273674823339967
"What is the advantage of compiling with -xnack flag?
","If no XNACK flag is specificed at compilation the default is ""xnack any"", and objects in `roc-obj-ls` with not have an XNACK mode specified.

.. code::
    $ hipcc --amdgpu-target=gfx90a square.hipref.cpp -o xnack_any.exe
    $ roc-obj-ls -v xnack_any.exe
    Bundle# Entry ID:                                                              URI:
    1       host-x86_64-unknown-linux                                           file://xnack_any.exe#offset=8192&size=0
    1       hipv4-amdgcn-amd-amdhsa--gfx90a                                     file://xnack_any.exe#offset=8192&size=9752",4.270668224579736
"How do programmers manually migrate memory regions on Crusher?
","Most applications that use ""managed"" or ""unified"" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Crusher. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.",4.31623369452309
"How do programmers manually migrate memory regions on Crusher?
","The AMD MI250X and operating system on Crusher supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Crusher. In the following section we'll discuss how the heterogenous memory space on a Crusher node is surfaced within your application.",4.250589351148805
"How do programmers manually migrate memory regions on Crusher?
","The page migration behavior summarized by the following tables represents the current, observable behavior. Said behavior will likely change in the near future.  CPU accesses to migratable memory may behave differently than other platforms you're used to. On Crusher, pages will not migrate from GPU HBM to CPU DDR4 based on access patterns alone. Once a page has migrated to GPU HBM it will remain there even if the CPU accesses it, and all accesses which do not resolve in the CPU cache will occur over the Infinity Fabric between the AMD ""Optimized 3rd Gen EPYC"" CPU and AMD MI250X GPU. Pages",4.186565023647587
"How can I use HIP on Summit with GPU-aware MPI?
","Using the hip-cuda/5.1.0 module on Summit requires CUDA v11.4.0 or later. However, only CUDA v11.0.3 and earlier currently support GPU-aware MPI, so GPU-aware MPI is currently not available when using HIP on Summit.

Any codes compiled with post 11.0.3 CUDA (cuda/11.0.3) will not work with MPS enabled (-alloc_flags ""gpumps"") on Summit. The code will hang indefinitely. CUDA v11.0.3 is the latest version that is officially supported by IBM's software stack installed on Summit. We are continuing to look into this issue.",4.522561653035227
"How can I use HIP on Summit with GPU-aware MPI?
","As mentioned above, HIP can be used on systems running on either the ROCm or CUDA platform, so OLCF users can start preparing their applications for Frontier today on Summit. To use HIP on Summit, you must load the HIP module:

$ module load hip-cuda

CUDA 11.4.0 or later must be loaded in order to load the hip-cuda module. hipBLAS, hipFFT, hipSOLVER, hipSPARSE, and hipRAND have also been added to hip-cuda.",4.39066035582204
"How can I use HIP on Summit with GPU-aware MPI?
","If you want to do GPU computing on Summit with R, we would love to collaborate with you (see contact details at the bottom of this document).

For more information about using R and pbdR effectively in an HPC environment such as Summit, please see the R and pbdR articles. These are long-form articles that introduce HPC concepts like MPI programming in much more detail than we do here.

Also, if you want to use R and/or pbdR on Summit, please feel free to contact us directly:

Mike Matheson - mathesonma AT ornl DOT gov

George Ostrouchov - ostrouchovg AT ornl DOT gov",4.3516779375707095
"What are the main security concerns outlined in the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to or use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title:Security Policy Version: 12.10",4.448749160698305
"What are the main security concerns outlined in the OLCF Policy?
","The requirements outlined in this document apply to all individuals who have an OLCF account. It is your responsibility to ensure that all individuals have the proper need-to-know before allowing them access to the information on OLCF computing resources. This document will outline the main security concerns.

OLCF computing resources are for business use only. Installation or use of software for personal use is not allowed. Incidents of abuse will result in account termination. Inappropriate uses include, but are not limited to:

Sexually oriented information",4.4387936865915485
"What are the main security concerns outlined in the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to or use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title: Data Management Policy Version: 20.02",4.320243528880374
"Can I use symbolic links to access the Member Work directories on Slate?
","Footnotes

1

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

2

Retention is not applicable as files will follow purge cycle.",4.237930290193125
"Can I use symbolic links to access the Member Work directories on Slate?
","Project members get an individual Member Work directory for each associated project; these reside in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. Member Work areas are not shared with other users of the system and are intended for project data that the user does not want to make available to other users. Member Work directories are provided commonly across all systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular",4.224969336309475
"Can I use symbolic links to access the Member Work directories on Slate?
",The difference between the three lies in the accessibility of the data to project members and to researchers outside of the project. Member Work directories are accessible only by an individual project member by default. Project Work directories are accessible by all project members. World Work directories are readable by any user on the system.,4.221206924775453
"What is the purpose of the ""Enumerated list start value not ordinal-1"" message?
","Engage the Community

<string>:3: (INFO/1) Enumerated list start value not ordinal-1: ""3"" (ordinal 3)",4.109033352124212
"What is the purpose of the ""Enumerated list start value not ordinal-1"" message?
","Evaluate Technology

<string>:3: (INFO/1) Enumerated list start value not ordinal-1: ""2"" (ordinal 2)",4.047755518005368
"What is the purpose of the ""Enumerated list start value not ordinal-1"" message?
","<string>:5: (INFO/1) Enumerated list start value not ordinal-1: ""3"" (ordinal 3)



On the user account page, selected ""yes"" or ""no"" for the questions asking about any pre-existing account names. If this is your first account with us, leave those questions set to ""no"". Also enter your preferred shell. If you do not know which shell to use, select ""/bin/bash"". We can change this later if needed. Click ""Next"".",4.02584579675187
"How can teams participate in OLCF GPU Hackathons?
","If you would like more information about how you can volunteer to mentor a team at an upcoming Open/GPU hackathon, please visit our Become a Mentor page.

<p style=""font-size:20px""><b>Who can I contact with questions?</b></p>

If you have any questions about the OLCF GPU Hackathon Series, please contact OLCF Help at (help@olcf.ornl.gov).",4.53591656360716
"How can teams participate in OLCF GPU Hackathons?
","If you want/need to get your code running (or optimized) on a GPU-accelerated system, these hackathons offer a unique opportunity to set aside the time, surround yourself with experts in the field, and push toward your development goals. By the end of the event, each team should have part of their code running (or more optimized) on GPUs, or at least have a clear roadmap of how to get there.

<p style=""font-size:20px""><b>Target audience</b></p>",4.40855886283107
"How can teams participate in OLCF GPU Hackathons?
","A GPU hackathon is a multi-day coding event in which teams of developers port their own applications to run on GPUs, or optimize their applications that already run on GPUs. Each team consists of 3 or more developers who are intimately familiar with (some part of) their application, and they work alongside 1 or more mentors with GPU programming expertise. Our mentors come from universities, national laboratories, supercomputing centers, and industry partners.",4.364155146583137
"How can I copy a file in Slate?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",4.089008967625016
"How can I copy a file in Slate?
","When the installation has finished, click on the Globus icon and select Web: Transfer Files as below



Globus will ask you to login. If your institution does not have an organizational login, you may choose to either Sign in with Google or Sign in with ORCiD iD.



In the main Globus web page, select the two-panel view, then set the source and destination endpoints. (Left/Right order does not matter)



Next, navigate to the appropriate source and destination paths to select the files you want to transfer. Click the ""Start"" button to begin the transfer.",4.012232325581209
"How can I copy a file in Slate?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.010052109853383
"How can I optimize my job submission script for Summit?
","As is the case on other OLCF systems, computational work on Summit is performed within jobs. A typical job consists of several components:

A submission script

An executable

Input files needed by the executable

Output files created by the executable

In general, the process for running a job is to:

Prepare executables and input files

Write the batch script

Submit the batch script

Monitor the job's progress before and during execution",4.329448345671694
"How can I optimize my job submission script for Summit?
","Summit has a node hierarchy that can be very confusing for the uninitiated. The Summit User Guide explains this in depth. This has some consequences that may be unusual for R programmers. A few important ones to note are:

You must have a script that you can run in batch, e.g. with Rscript or R CMD BATCH

All data that needs to be visible to the R process (including the script and your packages) must be on gpfs (not your home directory!).

You must launch your script from the launch nodes with jsrun.

We'll start with something very simple. The script is:

""hello world""",4.220425984431212
"How can I optimize my job submission script for Summit?
","This method on Summit requires the user to be present until the job completes. For users who have long scripts or are unable to monitor the job, you can submit the above lines in a batch script. However, you will wait in the queue twice, so this is not recommended. Alternatively, one can use Andes.

For Andes/Frontier (Slurm Script):

Andes

.. code-block:: bash


   #!/bin/bash
   #SBATCH -A XXXYYY
   #SBATCH -J visit_test
   #SBATCH -N 1
   #SBATCH -p gpu
   #SBATCH -t 0:05:00

   cd $SLURM_SUBMIT_DIR
   date

   module load visit",4.217800365142608
"How can I ensure that my application is deployed efficiently on Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.267856561088817
"How can I ensure that my application is deployed efficiently on Slate?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",4.219292039736668
"How can I ensure that my application is deployed efficiently on Slate?
","The following items need to be established before deploying applications on Slate systems (Marble or Onyx OpenShift clusters).

Follow https://docs.olcf.ornl.gov/systems/helm_prerequisite.html#slate_getting_started if you do not already have a project/namespace established on Onyx or Marble.

It is recommended to use Helm version 3.

Helm enables application deployment via Helm Charts. The high level Helm Architecture docs are also a great reference for understanding Helm.

Like oc, Helm is a single binary executable.

This can be installed on macOS with Homebrew :

$ brew install helm",4.213377212126164
"Can I customize the ArgoCD instance creation form?
","Next Click the ""Create ArgoCD"" button. You will be presented with a form view similar to:

Image of the form view for ArgoCD instance creation.

Starting with the form view of the process, make the following changes allowing for access to the ArgoCD web UI via a route:

Server -> Insecure -> true

Server -> Route -> Enabled -> true

Server -> Route -> Tls -> Termination -> edge

Image of the form view for ArgoCD instance creation with the route settings configured.",4.342927117544464
"Can I customize the ArgoCD instance creation form?
",Image of ArgoCD new application general settings.,4.313833626109
"Can I customize the ArgoCD instance creation form?
",allow for better control of resources allocated to ArgoCD.,4.302000967565735
"What is the advantage of using environment management tools on Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.1885235140247925
"What is the advantage of using environment management tools on Summit?
",For Summit:,4.186760000152821
"What is the advantage of using environment management tools on Summit?
","Summit is connected to an IBM Spectrum Scale™ filesystem providing 250PB of storage capacity with a peak write speed of 2.5 TB/s. Summit also has access to the center-wide NFS-based filesystem (which provides user and project home areas) and has access to the center’s High Performance Storage System (HPSS) for user and project archival storage.

Summit is running Red Hat Enterprise Linux (RHEL) version 8.2.",4.17029431004099
"How can I prepare executables and input files for a job on Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.34363022197896
"How can I prepare executables and input files for a job on Andes?
","Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-user-guide.

For a full list of software available at the OLCF, please see the Software section (coming soon).",4.210459648076068
"How can I prepare executables and input files for a job on Andes?
","Prepare executables and input files.

Write a batch script.

Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on commodity clusters.



Login vs Compute Nodes on Commodity Clusters

Login Nodes

<string>:403: (INFO/1) Duplicate implicit target name: ""login nodes"".",4.196706284859615
"What is the difference between the 'minio.server.name' variable and the 'minio.server.domain' variable in the values.yaml file?
","What do you need to consider?

What should I name my host value? (This will be the URL in which you access your MinIO instance)

What should I name my application? (This is the name value and should be unique to you or your project)

Do I want MinIO to run on an OLCF filesytem? (It can run on NFS or GPFS project spaces. If you do not run it on an OLCF filesystem it uses an isolated volume dedicated to the MinIO server)

What do you need to configure?

host (Set the URL of your application)

name (Set the name of your application)",4.154315770137512
"What is the difference between the 'minio.server.name' variable and the 'minio.server.domain' variable in the values.yaml file?
","Where you cloned the slate_helm_examples repository, in the 'slate_helm_examples/charts/minio-standalone` directory, you will see a values.yaml file. This file containes variables for the Helm chart deployment.

This is how we configure your instance of the MinIO application. All of these changes will be to your local copy of values.yaml.

Here is what it looks like:",4.126788816287863
"What is the difference between the 'minio.server.name' variable and the 'minio.server.domain' variable in the values.yaml file?
","MinIO running on a dedicated volume, allocated automatically from the NetApp storage server, isolated to the MinIO server.

It is important to note that we are also launching MinIO in standalone mode, which is a single MinIO server instance. MinIO also supports distributed mode for more robust implementations, but we are not setting that up in this example.

<string>:5: (INFO/1) Duplicate explicit target name: ""user assistance"".",4.014173050069611
"Can you provide an example of a job script that sends all libraries along with the binary using --exclude=NONE?
","Notice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable. Once libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.

As mentioned above, you can use --exclude=NONE on sbcast to send all libraries along with the binary. Using --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time. A job script for the previous example, modified for sending all libraries is shown below.",4.406252706022556
"Can you provide an example of a job script that sends all libraries along with the binary using --exclude=NONE?
","Notice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable. Once libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.

As mentioned above, you can alternatively use --exclude=NONE on sbcast to send all libraries along with the binary. Using --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time. A job script for the previous example, modified for sending all libraries is shown below.",4.400886844173435
"Can you provide an example of a job script that sends all libraries along with the binary using --exclude=NONE?
","# If you SBCAST **all** your libraries (ie, `--exclude=NONE`), you may use the following line:
#export LD_LIBRARY_PATH=""/mnt/bb/$USER/${exe}_libs:$(pkg-config --variable=libdir libfabric)""
# Use with caution -- certain libraries may use ``dlopen`` at runtime, and that is NOT covered by sbcast
# If you use this option, we recommend you contact OLCF Help Desk for the latest list of additional steps required",4.124649232510515
"What is the process for requesting an allocation (project) on the OLCF's SPI?
","Similar to standard OLCF workflows, to run SPI workflows on OLCF resources, you will first need an approved project.  The project will be awarded an allocation(s) that will allow resource access and use.  The first step to requesting an allocation is to complete the project request form.  The form will initiate the process that includes peer review, export control review, agreements, and others.  Once submitted, members of the OLCF accounts team will help walk you through the process.

Please see the OLCF Accounts and Projects section of this site to request a new project.",4.67652123471105
"What is the process for requesting an allocation (project) on the OLCF's SPI?
","https://docs.olcf.ornl.gov/systems/index.html#Request an allocation (project)<spi-allocations-projects>.  All access and resource use occurs within an approved allocation.

https://docs.olcf.ornl.gov/systems/index.html#Request a user account<spi-user-accounts>.  Once an allocation (project) has been approved, each member of the project must request an account to use the project's allocated resources.",4.608140200120462
"What is the process for requesting an allocation (project) on the OLCF's SPI?
","CITADEL, having undergone comprehensive technical-, legal-, and policy-oriented reviews and received third-party accreditation, has presented new possibilities for research projects that previously could not access utilize OLCF systems due to the nature of their data.

If you are new to the OLCF or the OLCF's SPI, this page can help get you started.  Below are some of the high-level steps needed to begin use of the OLCF's SPI:",4.392342647758392
"What is the cause of segmentation faults in job steps with uneven number of resource sets per node in Summit?
","Executing a parallel binary on the login node or a batch node without using the job step launcher jsrun will result in a segfault.

This also can be encountered when importing parallel Python libraries like mpi4py and h5py directly on these nodes.

The issue has been reported to IBM. The current workaround is to run the binary inside an interactive or batch job via jsrun.

When profiling an MPI application using NVIDIA Nsight Compute, like the following, you may see an error message in Spectrum MPI that aborts the program:

jsrun -n 1 -a 1 -g 1 nv-nsight-cu-cli ./a.out",4.215290364366361
"What is the cause of segmentation faults in job steps with uneven number of resource sets per node in Summit?
","summit>

The following example will create 4 resource sets each with 6 tasks and 3 GPUs. Each set of 6 MPI tasks will have access to 3 GPUs. Ranks 0 - 5 will have access to GPUs 0 - 2 on the first socket of the first node ( red resource set). Ranks 6 - 11 will have access to GPUs 3 - 5 on the second socket of the first node ( green resource set). This pattern will continue until 4 resource sets have been created. The following jsrun command will request 4 resource sets (-n4). Each resource set will contain 6 MPI tasks (-a6), 3 GPUs (-g3), and 6 cores (-c6).",4.163203485772076
"What is the cause of segmentation faults in job steps with uneven number of resource sets per node in Summit?
","Jobs on Summit are scheduled in full node increments; a node's cores cannot be allocated to multiple jobs. Because the OLCF charges based on what a job makes unavailable to other users, a job is charged for an entire node even if it uses only one core on a node. To simplify the process, users request and are allocated multiples of entire nodes through LSF.

Allocations on Summit are separate from those on Andes and other OLCF resources.

The node-hour charge for each batch job will be calculated as follows:

node-hours = nodes requested * ( batch job endtime - batch job starttime )",4.14717110713375
"Can I use ParaView on Summit without using the bsub -Is command?
","For Summit (module):

$ module load visit
$ visit -nowin -cli -v 3.1.4 -l bsub/jsrun -p batch -b XXXYYY -t 00:05 -np 42 -nn 1 -s visit_example.py

Due to the nature of the custom VisIt launcher for Summit, users are unable to solely specify -l jsrun for VisIt to work properly. Instead of manually creating a batch script, as seen in the Andes method outlined below, VisIt submits its own through -l bsub/jsrun. The -t flag sets the time limit, -b specifies the project to be charged, and -p designates the queue the job will submit to.",4.258423395335908
"Can I use ParaView on Summit without using the bsub -Is command?
","All of the above can also be achieved in an interactive batch job through the use of the salloc command on Andes or the bsub -Is command on Summit. Recall that login nodes should not be used for memory- or compute-intensive tasks, including VisIt.",4.23774081513136
"Can I use ParaView on Summit without using the bsub -Is command?
","Although in a single machine setup both the ParaView client and server run on the same host, this need not be the case. It is possible to run a local ParaView client to display and interact with your data while the ParaView server runs in an Andes or Summit batch job, allowing interactive analysis of very large data sets.",4.232516158658241
"Can I use the same tools and software for SPI workflows as I would for standard non-SPI workflows?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.  This page lists the differences you may see when using OLCF resources to execute SPI workflows.  The page will also point to sections within the site where more information on standard non-SPI use can be found.",4.313444079798103
"Can I use the same tools and software for SPI workflows as I would for standard non-SPI workflows?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.

The SPI provides access to the OLCF's Summit resource for compute.  To safely separate SPI and non-SPI workflows, SPI workflows must use a separate login node named https://docs.olcf.ornl.gov/systems/citadel_user_guide.html#Citadel<spi-compute-citadel>.  Citadel provides a login node specifically for SPI workflows.",4.217371652519355
"Can I use the same tools and software for SPI workflows as I would for standard non-SPI workflows?
","The Citadel framework allows use of the OLCF's existing HPC resources Summit and Frontier for SPI workflows.  Citadel adds measures to ensure separation of SPI and non-SPI workflows and data. This section provides differences when using OLCF resources for SPI and non-SPI workflows.  Because the Citadel framework just adds another security layer to existing HPC resources, many system use methods are the same between SPI and non-SPI workflows.  For example, compiling, batch scheduling, and job layout are the same between the two security enclaves.  Because of this, the existing resource user",4.216763731444176
"How do I enable AMP for an MXNet model in Summit?
","The code below will enable AMP for MXNet:

amp.init()
amp.init_trainer(trainer)
with amp.scale_loss(loss, trainer) as scaled_loss:
  autograd.backward(scaled_loss)",4.339175745738027
"How do I enable AMP for an MXNet model in Summit?
","With TensorFlow AMP can be enabled using one of the following techniques.

os.environ['TF_ENABLE_AUTO_MIXED_PRECISION'] = '1'

OR

export TF_ENABLE_AUTO_MIXED_PRECISION=1

Explicit optimizer wrapper available in NVIDIA Container 19.07+, TF 1.14+, TF 2.0:

opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)

Adding the following to a PyTorch model will enable AMP:

model, optimizer = amp.initialize(model, optimizer, opt_level=""O1"")
with amp.scale_loss(loss, optimizer) as scaled_loss:
  scaled_loss.backward()

The code below will enable AMP for MXNet:",4.331317123443667
"How do I enable AMP for an MXNet model in Summit?
","NVIDIA has also integrated a technology called Automatic Mixed Precision (AMP) into several common frameworks, TensorFlow, PyTorch, and MXNet at time of writing. In most cases AMP can be enabled via a small code change or via setting and environment variable. AMP does not strictly replace all matrix multiplication operations with half precision, but uses graph optimization techniques to determine whether a given layer is best run in full or half precision.

Examples are provided for using AMP, but the following sections summarize the usage in the three supported frameworks.",4.212446921130587
"What is the impact of disabling the XNACK feature on the performance of the GPU?
","Disabling XNACK will not necessarily result in an application failure, as most types of memory can still be accessed by the AMD ""Optimized 3rd Gen EPYC"" CPU and AMD MI250X GPU. In most cases, however, the access will occur in a zero-copy fashion over the Infinity Fabric. The exception is memory allocated through standard system allocators such as malloc, which cannot be accessed directly from GPU kernels without previously being registered via a HIP runtime call such as hipHostRegister. Access to malloc'ed and unregistered memory from GPU kernels will result in fatal unhandled page faults.",4.355875099879022
"What is the impact of disabling the XNACK feature on the performance of the GPU?
","Disabling XNACK will not necessarily result in an application failure, as most types of memory can still be accessed by the AMD ""Optimized 3rd Gen EPYC"" CPU and AMD MI250X GPU. In most cases, however, the access will occur in a zero-copy fashion over the Infinity Fabric. The exception is memory allocated through standard system allocators such as malloc, which cannot be accessed directly from GPU kernels without previously being registered via a HIP runtime call such as hipHostRegister. Access to malloc'ed and unregistered memory from GPU kernels will result in fatal unhandled page faults.",4.355875099879022
"What is the impact of disabling the XNACK feature on the performance of the GPU?
","Although XNACK is a capability of the MI250X GPU, it does require that kernels be able to recover from page faults. Both the ROCm and CCE HIP compilers will default to generating code that runs correctly with both XNACK enabled and disabled. Some applications may benefit from using the following compilation options to target specific XNACK modes.

hipcc --amdgpu-target=gfx90a or CC --offload-arch=gfx90a -x hip

Kernels are compiled to a single ""xnack any"" binary, which will run correctly with both XNACK enabled and XNACK disabled.",4.352799326312194
"How can I ensure that MPI tasks are distributed across sockets in a round-robin manner on Frontier?
","There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.

Assigning MPI ranks in a ""round-robin"" (cyclic) manner across L3 cache regions (sockets) is the default behavior on Frontier. This mode will assign consecutive MPI tasks to different sockets before it tries to ""fill up"" a socket.",4.483565671200528
"How can I ensure that MPI tasks are distributed across sockets in a round-robin manner on Frontier?
","Instead, you can assign MPI ranks so that the L3 regions are filled in a ""packed"" (block) manner.  This mode will assign consecutive MPI tasks to the same L3 region (socket) until it is ""filled up"" or ""packed"" before assigning a task to a different socket.

Recall that the -m flag behaves like: -m <node distribution>:<socket distribution>.  Hence, the key setting to achieving the round-robin nature is the -m block:block flag, specifically the block setting provided for the ""socket distribution"". This ensures that the MPI tasks will be distributed in a packed manner.",4.3655767261715495
"How can I ensure that MPI tasks are distributed across sockets in a round-robin manner on Frontier?
","The intent with both of the following examples is to launch 8 MPI ranks across the node where each rank is assigned its own logical (and, in this case, physical) core.  Using the -m distribution flag, we will cover two common approaches to assign the MPI ranks -- in a ""round-robin"" (cyclic) configuration and in a ""packed"" (block) configuration. Slurm's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive method was used to request an allocation of 1 compute node for these examples: salloc -A <project_id> -t 30 -p <parition> -N 1",4.330176646210264
"What is the name of the pod in the Slate example?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.228838003638425
"What is the name of the pod in the Slate example?
","Before we dive in there are some terms that need to be understood. This will be a basic set of terms and a copy and paste from our https://docs.olcf.ornl.gov/systems/guided_tutorial.html#slate_glossary, so we recommend reading that document and even keeping it handy until you are familiar with all of the definitions there. On that note, another good piece of reference documentation the https://docs.olcf.ornl.gov/systems/guided_tutorial.html#slate_examples document. There you can find basic YAML definitions for the most common objects in Kubernetes.",4.099758062589759
"What is the name of the pod in the Slate example?
","From here, in the left hand hamburger menu click on the 'Workloads' tab and then the 'pods' tab:



Here you will be able to view all of the Pods in your Project. Since this is a new Project there will be no Pods in it. To create a  Pod click the 'Create Pod' button.

This will bring you to a screen of pre populated YAML that you can edit in the browser. This YAML is the basis of a podspec that will be sent to the API server once you click the 'Create' button in the lower left to create a  Pod in your Project. Here we will make a few slight modifications to the podspec.",4.09959541043805
"How many files are allowed in the User Archive directories?
","footnotes

1

This entry is for legacy User Archive directories which contained user data on January 14, 2020. There is also a quota/limit of 2,000 files on this directory.

2

User Archive directories that were created (or had no user data) after January 14, 2020. Settings other than permissions are not applicable because directories are root-owned and contain no user files.

Moderate Enhanced projects do not have HPSS storage",4.447565498727593
"How many files are allowed in the User Archive directories?
","2

User Archive directories that were created (or had no user data) after January 14, 2020. Settings other than permissions are not applicable because directories are root-owned and contain no user files.

3",4.41309197878377
"How many files are allowed in the User Archive directories?
","Use of User Archive areas for data storage is deprecated as of January 14, 2020. The user archive area for any user account created after that date (or for any user archive directory that is empty of user files after that date) will contain only symlinks to the top-level directories for each of the user's projects on HPSS. Users with existing data in a User Archive directory are encouraged to move that data to an appropriate project-based directory as soon as possible.  The information below is simply for reference for those users with existing data in User Archive directories.",4.374135268054859
"How do I monitor the usage of the BurstBuffers on Summit?
","Each compute node on Summit has a 1.6TB Non-Volatile Memory (NVMe) storage device (high-memory nodes have a 6.4TB NVMe storage device), colloquially known as a ""Burst Buffer"" with theoretical performance peak of 2.1 GB/s for writing and 5.5 GB/s for reading. The NVMes could be used to reduce the time that applications wait for I/O.  More information can be found later in the Burst Buffer section.



<string>:208: (INFO/1) Duplicate implicit target name: ""software"".",4.149290411477688
"How do I monitor the usage of the BurstBuffers on Summit?
","The following example illustrates how to use the burst buffers (NVMes) by default on Summit. This example uses a submission script, check_nvme.lsf. It is assumed that the files are saved in the user's GPFS scratch area, /gpfs/alpine/scratch/$USER/projid, and that the user is operating from there as well. Do not forget that for all the commands on NVMe, it is required to use jsrun. This will submit a job to run on one node.

Job submssion script: check_nvme.lsf.

#!/bin/bash
#BSUB -P project123
#BSUB -J name_test
#BSUB -o nvme_test.o%J
#BSUB -W 2
#BSUB -nnodes 1
#BSUB -alloc_flags NVME",4.112074838795268
"How do I monitor the usage of the BurstBuffers on Summit?
",NVMes will need to be copied back to the parallel filesystem before the job ends. This largely manual mode of usage will not be the recommended way to use the burst buffer for most applications because tools are actively being developed to automate and improve the NVMe transfer and data management process. Here are the basic steps for using the BurstBuffers in their current limited mode of usage:,4.0622264149980944
"What is the purpose of the --gpu-bind=closest option in Crusher?
","--gpu-bind=closest binds each task to the GPU which is closest.

To further clarify, --gpus-per-task does not actually bind GPUs to MPI ranks. It allocates GPUs to the job step. The --gpu-bind=closest is what actually maps a specific GPU to each rank; namely, the ""closest"" one, which is the GPU on the same NUMA domain as the CPU core the MPI rank is running on (see the https://docs.olcf.ornl.gov/systems/spock_quick_start_guide.html#spock-compute-nodes section).

Without these additional flags, all MPI ranks would have access to all GPUs (which is the default behavior).",4.385900844356906
"What is the purpose of the --gpu-bind=closest option in Crusher?
","| Slurm Option | Description | | --- | --- | | --gpus-per-task | Specify the number of GPUs required for the job on each task. This option requires an explicit task count, e.g. -n | | --gpu-bind=closest | Bind  each  task  to  the GPU(s) which are closest. Here, closest refers to the GPU connected to the L3 where the MPI rank is mapped to. |

Example 1: 8 MPI ranks - each with 7 CPU cores and 1 GPU (single-node)",4.203346491077483
"What is the purpose of the --gpu-bind=closest option in Crusher?
","The output shows the round-robin (cyclic) distribution of MPI ranks to GPUs. In fact, it is a round-robin distribution of MPI ranks to L3 cache regions (the default distribution). The GPU mapping is a consequence of where the MPI ranks are distributed; --gpu-bind=closest simply maps the GPU in an L3 cache region to the MPI ranks in the same L3 region.

Example 6: 32 MPI ranks - where 2 ranks share a GPU (round-robin, multi-node)

This example is an extension of Example 5 to run on 2 nodes.",4.154180073207392
"How can I ensure that each resource set has a unique GPU?
","Based on how your code expects to interact with the system, you can create resource sets containing the needed GPU and core resources. If a code expects to utilize one GPU per task, a resource set would contain one core and one GPU. If a code expects to pass work to a single GPU from two tasks, a resource set would contain two cores and one GPU.

Decide on the number of resource sets needed

Once you understand tasks, threads, and GPUs in a resource set, you simply need to decide the number of resource sets needed.",4.36630567353922
"How can I ensure that each resource set has a unique GPU?
","The first step to creating resource sets is understanding how a code would like the node to appear. For example, the number of tasks/threads per GPU. Once this is understood, the next step is to simply calculate the number of resource sets that can fit on a node. From here, the number of needed nodes can be calculated and passed to the batch job request.

The basic steps to creating resource sets:

Understand how your code expects to interact with the system.

How many tasks/threads per GPU?",4.255597855859634
"How can I ensure that each resource set has a unique GPU?
","The following example will create 12 resource sets each with 1 task, 4 threads, and 1 GPU. Each MPI task will start 4 threads and have access to 1 GPU. Rank 0 will have access to GPU 0 and start 4 threads on the first socket of the first node ( red resource set). Rank 2 will have access to GPU 1 and start 4 threads on the second socket of the first node ( green resource set). This pattern will continue until 12 resource sets have been created. The following jsrun command will create 12 resource sets (-n12). Each resource set will contain 1 MPI task (-a1), 1 GPU (-g1), and 4 cores (-c4).",4.245285117018555
"How can I submit a batch job for five minutes on Summit, Andes, or Frontier?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.318557424045101
"How can I submit a batch job for five minutes on Summit, Andes, or Frontier?
","This method on Summit requires the user to be present until the job completes. For users who have long scripts or are unable to monitor the job, you can submit the above lines in a batch script. However, you will wait in the queue twice, so this is not recommended. Alternatively, one can use Andes.

For Andes/Frontier (Slurm Script):

Andes

.. code-block:: bash


   #!/bin/bash
   #SBATCH -A XXXYYY
   #SBATCH -J visit_test
   #SBATCH -N 1
   #SBATCH -p gpu
   #SBATCH -t 0:05:00

   cd $SLURM_SUBMIT_DIR
   date

   module load visit",4.298243590525493
"How can I submit a batch job for five minutes on Summit, Andes, or Frontier?
",of how to run a Python script using PvBatch on Andes and Summit.,4.2817694463303
"Can I run both serial and parallel jobs on Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.24195133243088
"Can I run both serial and parallel jobs on Andes?
","Please see the https://docs.olcf.ornl.gov/systems/andes_user_guide.html#batch-queues-on-andes section to learn about the queuing policies for these two partitions. Both compute partitions are accessible through the same batch queue from Andes’s https://docs.olcf.ornl.gov/systems/andes_user_guide.html#andes-login-nodes.

Andes features a S8500 Series HDR Infiniband interconnect, with a maximum theoretical transfer rate of 200 Gb/s.",4.197814399361363
"Can I run both serial and parallel jobs on Andes?
","Although in a single machine setup both the ParaView client and server run on the same host, this need not be the case. It is possible to run a local ParaView client to display and interact with your data while the ParaView server runs in an Andes or Summit batch job, allowing interactive analysis of very large data sets.",4.180745260170397
"How can I optimize ParaView performance?
","You will obtain the best performance by running the ParaView client on your local computer and running the server on OLCF resources with the same version of ParaView. It is highly recommended to check the available ParaView versions using module avail paraview on the system you plan to connect ParaView to. Precompiled ParaView binaries for Windows, macOS, and Linux can be downloaded from Kitware.

Recommended ParaView versions on our systems:

Summit: ParaView 5.9.1, 5.10.0, 5.11.0

Andes: ParaView 5.9.1, 5.10.0, 5.11.0",4.33389389823149
"How can I optimize ParaView performance?
","ParaView was developed to analyze extremely large datasets using distributed memory computing resources. The OLCF provides ParaView server installs on Andes and Summit to facilitate large scale distributed visualizations. The ParaView server running on Andes and Summit may be used in a headless batch processing mode or be used to drive a ParaView GUI client running on your local machine.

For a tutorial of how to get started with ParaView on Andes, see our ParaView at OLCF Tutorial.",4.242996942927972
"How can I optimize ParaView performance?
","ParaView is an open-source, multi-platform data analysis and visualization application. ParaView users can quickly build visualizations to analyze their data using qualitative and quantitative techniques. The data exploration can be done interactively in 3D or programmatically using ParaView’s batch processing capabilities. Further information regarding ParaView can be found at the links provided in the https://docs.olcf.ornl.gov/systems/paraview.html#paraview-resources section.",4.239729387441068
"What is the image used by the container?
","Source to image is a tool to build docker formatted container images. This is accomplished by injecting source code into a container image and then building a new image. The new image will contain the source code ready to run, via the $ docker run command inside the newly built image.

Using the S2I model of building images, it is possible to have your source hosted on a git repository and then pulled and built by Openshift.

Click on the newly created project and then on the blue Add to Project button on the center of the page.",4.18025678693988
"What is the image used by the container?
","The -t flag names the container image and the -f flag indicates the file to use for building the image.

Run podman image ls to see the list of images. localhost/simple should be among them. Any container created without an explicit url to a container registry in its name will automatically have the localhost prefix.

$ podman image ls
REPOSITORY             TAG      IMAGE ID      CREATED      SIZE
localhost/simple       latest   e47dbfde3e99  3 hours ago  687 MB
quay.io/centos/centos  stream8  ad6f8b5e7f64  8 days ago   497 MB",4.137365782959698
"What is the image used by the container?
","Building an image in Openshift is the act of transferring a set of input parameters into an object. That object is typically an image. Openshift contains all of the necessary components to build a Docker image or a piece of source code an image that will run in a container.

A Docker build is achieved by linking to a source repository that contains a docker image and all of the necessary Docker artifacts. A build is then triggered through the standard $ docker build command. More specific documentation on docker builds can be found here.",4.133742412551048
"What is the output format of Score-P and TAU when generating trace files for Vampir?
",performance data. Score-P and TAU generate OTF2 trace files for Vampir to visualize.,4.659210547617291
"What is the output format of Score-P and TAU when generating trace files for Vampir?
",performance data. Score-P and TAU generate OTF2 trace files for Vampir to visualize.,4.659210547617291
"What is the output format of Score-P and TAU when generating trace files for Vampir?
","$ export SCOREP_FILTERING_FILE=scorep.filter

Now you are ready to submit your instrumented code to run with tracing enabled. This measurement will generate files of the form traces.otf. The .otf2 file format can be analyzed by a tool called Vampir .

<string>:16: (INFO/1) Duplicate explicit target name: ""vampir"".

Vampir provides a visual GUI to analyze the .otf2 trace file generated with Score-P.",4.392076062966761
"What is the purpose of the SQ_INSTS_VALU_TRANS_F32 instruction in Frontier?
","&+ SQ\_INSTS\_VALU\_MUL\_F64       \\\\
                     &+ SQ\_INSTS\_VALU\_TRANS\_F64     \\\\
                     &+ 2 * SQ\_INSTS\_VALU\_FMA\_F64)  \\\\
              + 1024 &*(SQ\_INSTS\_VALU\_MFMA\_MOPS\_F16) \\\\
              + 1024 &*(SQ\_INSTS\_VALU\_MFMA\_MOPS\_BF16) \\\\
              + 256 *&(SQ\_INSTS\_VALU\_MFMA\_MOPS\_F32) \\\\
              + 256 *&(SQ\_INSTS\_VALU\_MFMA\_MOPS\_F64) \\\\",4.1549707282775055
"What is the purpose of the SQ_INSTS_VALU_TRANS_F32 instruction in Frontier?
","&+ SQ\_INSTS\_VALU\_MUL\_F64       \\\\
                     &+ SQ\_INSTS\_VALU\_TRANS\_F64     \\\\
                     &+ 2 * SQ\_INSTS\_VALU\_FMA\_F64)  \\\\
              + 1024 &*(SQ\_INSTS\_VALU\_MFMA\_MOPS\_F16) \\\\
              + 1024 &*(SQ\_INSTS\_VALU\_MFMA\_MOPS\_BF16) \\\\
              + 256 *&(SQ\_INSTS\_VALU\_MFMA\_MOPS\_F32) \\\\
              + 256 *&(SQ\_INSTS\_VALU\_MFMA\_MOPS\_F64) \\\\",4.1549707282775055
"What is the purpose of the SQ_INSTS_VALU_TRANS_F32 instruction in Frontier?
","TOTAL\_FLOPS =   64  *&(SQ\_INSTS\_VALU\_ADD\_F16         \\\\
                     &+ SQ\_INSTS\_VALU\_MUL\_F16       \\\\
                     &+ SQ\_INSTS\_VALU\_TRANS\_F16     \\\\
                     &+ 2 * SQ\_INSTS\_VALU\_FMA\_F16)  \\\\
              + 64  *&(SQ\_INSTS\_VALU\_ADD\_F32         \\\\
                     &+ SQ\_INSTS\_VALU\_MUL\_F32       \\\\
                     &+ SQ\_INSTS\_VALU\_TRANS\_F32     \\\\
                     &+ 2 * SQ\_INSTS\_VALU\_FMA\_F32)  \\\\
              + 64  *&(SQ\_INSTS\_VALU\_ADD\_F64         \\\\",4.099604887870115
"How can I access the User Assistance Center for help with OLCF systems?
","We're here to provide support at every step. We also provide a collection of Tutorials for applied technical demonstrations, https://docs.olcf.ornl.gov/systems/frequently_asked_questions.html#system-user-guides, Training Events, and the User Assistance Center to answer questions and resolve technical issues as they arise.",4.488169044055852
"How can I access the User Assistance Center for help with OLCF systems?
","When all of the above steps are completed, your user account will be created and you will be notified by email. Now that you have a user account and it has been associated with a project, you're ready to get to work. This website provides extensive documentation for OLCF systems, and can help you efficiently use your project's allocation. We recommend reading the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#System User Guides<system-user-guides> for the machines you will be using often.",4.428143913039047
"How can I access the User Assistance Center for help with OLCF systems?
","Please see the https://docs.olcf.ornl.gov/systems/index.html#OLCF Applying for a User Account<applying-for-a-user-account> section of this site to request a new account and join an existing project.  Once submitted, the OLCF Accounts team will help walk you through the process.",4.4184600426982685
"What is the purpose of the `FROM` command in the Dockerfile?
","$ oc start-build example --from-file=./Dockerfile
  Uploading file ""Dockerfile"" as binary input for the build ...
  build ""example-1"" started

In the above example example was the name of the build config.

Additionally, if there are artifacts that need to be included in your build, a directory containing those artifacts can be used by passing the --from-dir flag to the start-build command like so:

$ oc start-build example --from-dir=./sampledir
  Uploading directory ""sampledir"" as binary input for the build ...
  build ""django-5"" started",4.022770304598905
"What is the purpose of the `FROM` command in the Dockerfile?
","Source to image is a tool to build docker formatted container images. This is accomplished by injecting source code into a container image and then building a new image. The new image will contain the source code ready to run, via the $ docker run command inside the newly built image.

Using the S2I model of building images, it is possible to have your source hosted on a git repository and then pulled and built by Openshift.

Click on the newly created project and then on the blue Add to Project button on the center of the page.",3.999369225941247
"What is the purpose of the `FROM` command in the Dockerfile?
","The -t flag names the container image and the -f flag indicates the file to use for building the image.

Run podman image ls to see the list of images. localhost/simple should be among them. Any container created without an explicit url to a container registry in its name will automatically have the localhost prefix.

$ podman image ls
REPOSITORY             TAG      IMAGE ID      CREATED      SIZE
localhost/simple       latest   e47dbfde3e99  3 hours ago  687 MB
quay.io/centos/centos  stream8  ad6f8b5e7f64  8 days ago   497 MB",3.994765070470012
"Can I assign users to a subproject in OLCF?
","primary project users must be associated with a subproject(s). If you have any questions, or would like to request a subproject, please contact the OLCF Accounts Team at accounts@ccs.ornl.gov.",4.545837834832131
"Can I assign users to a subproject in OLCF?
","Collaborators involved with an approved and activated OLCF project can apply for a user account associated with it. There are several steps in receiving a user account, and we're here to help you through them.

Project PIs do not receive a user account with project creation, and must also apply.",4.418235590416326
"Can I assign users to a subproject in OLCF?
","If you already have a user account at the OLCF, your existing credentials can be leveraged across multiple projects.

If your user account has an associated RSA SecurID (i.e. you have an ""OLCF Moderate"" account), you gain access to another project by logging in to the myOLCF self-service portal and filling out the application under My Account > Join Another Project. For more information, see the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#myOLCF self-service portal documentation<myolcf-overview>.",4.362042687523058
"How does CITADEL enforce security measures for handling vast datasets?
","Although the facility already adheres to the National Institute of Standards and Technology’s security and privacy controls for moderate Official Use Only data, CITADEL was crafted to enforce security measures for handling vast datasets that encompass types of data necessitating heightened privacy safeguards on systems overseen by the Oak Ridge Leadership Computing Facility (OLCF). Extra precautions have been taken to manage private data such that it cannot be accessed by other researchers or used by other projects. For example, HIPAA-protected data for a project sponsored by the VA will be",4.39741700065732
"How does CITADEL enforce security measures for handling vast datasets?
","With the CITADEL framework, researchers can use the OLCF’s large HPC resources including Frontier and Summit to compute data containing protected health information (PHI), personally identifiable information (PII), data protected under International Traffic in Arms Regulations, and other types of data that require privacy.",4.327604170576359
"How does CITADEL enforce security measures for handling vast datasets?
","The National Center for Computational Science (NCCS) and the Oak Ridge Leadership Computing Facility (OLCF) have implemented the CITADEL security framework as part of their Scalable Protected Infrastructure (SPI). This infrastructure provides resources and protocols that enable researchers to process protected data at scale. With the CITADEL framework, researchers can use the OLCF’s large HPC resources to compute data containing protected health information (PHI), personally identifiable information (PII), data protected under International Traffic in Arms Regulations, and other types of data",4.2926414347344
"How can I run the hostname command using Singularity container on Summit?
","h41n08
h41n08

Here, Jsrun starts 2 separate Singularity container runtimes since we pass the -n2 flag to start two processes. Each Singularity container runtime then loads the container image simple.sif and executes the hostname command from that container. If we had requested 2 nodes in the batch script and had run jsrun -n2 -r1 singularity exec ./simple.sif hostname, Jsrun would've started a Singularity runtime on each node and the output would look something like

h41n08
h41n09



Creating Singularity containers that run MPI programs require a few additional steps.",4.291013574858527
"How can I run the hostname command using Singularity container on Summit?
","The reason we include the --disable-cache flag is because Singularity's caching can fill up your home directory without you realizing it. And if the home directory is full, Singularity builds will fail. If you wish to make use of the cache, you can set the environment variable SINGULARITY_CACHEDIR=/tmp/containers/<user>/singularitycache or something like that so that the NVMe storage is used as the cache.

As a simple example, we will run hostname with the Singularity container.

Create a file submit.lsf with the contents below.",4.254611437746899
"How can I run the hostname command using Singularity container on Summit?
","Users will be building and running containers on Summit without root permissions i.e. containers on Summit are rootless.  This means users can get the benefits of containers without needing additional privileges. This is necessary for a shared system like Summit. And this is part of the reason why Docker doesn't work on Summit. Podman and Singularity provides rootless support but to different extents hence why users need to use a combination of both.

Users will need to set up a file in their home directory /ccs/home/<username>/.config/containers/storage.conf with the following content:",4.232069983843528
"Can I use the secret-token.yaml file in my MinIO deployment?
","These are the root credentials referenced here.

To establish these credentials in our Marble project, allowing our MinIO deployment to use them, we need to create a secret-token.yaml file and apply it to our project.

Create this example secret-token.yaml file locally:",4.34927343942411
"Can I use the secret-token.yaml file in my MinIO deployment?
","These values are picked up as environment variables from the templates/minio-standalone-deployment.yaml file.

It is recommended to keep the secret-token.yaml file safe, locally, and not in a repository if unencrypted.

At this point we are ready to install our minio-standalone chart in our Marble project namespace.

To list your available project spaces run this command:

$ oc projects

Check list:

You have the OC CLI Tool

You have Helm version 3

You are logged into Marble, with the OC CLI Tool, and in the correct Marble project.

You have configured your values.yaml file.",4.292633927104215
"Can I use the secret-token.yaml file in my MinIO deployment?
","Replace <name-of-your-app> with the name value you put in your values.yaml file.

Replace <your-choice> with strings of your choice (the access-key length should be at least 3, and the secret-key must be at least 8 characters). These will be the SECRET_TOKEN values.

Once your secret-token.yaml file is set, you can apply it to your Marble project/namespace with this command (assumes you are logged into Marble's CLI):

$ oc apply -f secret-token.yaml

You should get output similar to this:

secret ""rprout-test-minio-access-key"" created
secret ""rprout-test-minio-secret-key"" created",4.186617398325138
"What is the advantage of using Crusher's system allocator for memory allocation?
","Most applications that use ""managed"" or ""unified"" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Crusher. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.",4.279287380627009
"What is the advantage of using Crusher's system allocator for memory allocation?
","The Crusher system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.202631300712401
"What is the advantage of using Crusher's system allocator for memory allocation?
","The AMD MI250X and operating system on Crusher supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Crusher. In the following section we'll discuss how the heterogenous memory space on a Crusher node is surfaced within your application.",4.1966356293228415
"What are the broad aims of the Quantum Computing User Program at OLCF?
","There are several broad aims of the Quantum Computing User Program at OLCF, which are as follows:

Enable Research

The QCUP aims to provide a broad spectrum of user access to the best available quantum computing systems. Once a user’s intended research has been reviewed for merit and user agreements have been established, we seek to provide users with the opportunity to become familiar with the unique aspects and challenges of quantum computing, as well as to implement and test quantum algorithms on the available systems.

Evaluate Technology",4.762911764815495
"What are the broad aims of the Quantum Computing User Program at OLCF?
","The QCUP aims to engage the quantum computing community and support the growth of the quantum information science ecosystems. Our quantum computing users range in quantum computing experience from novice to expert; users are from US national labs, universities, government, and industry.  User groups utilize quantum computing expertise to investigate diverse application interests, using multiple programming languages, quantum-classical programming, and multiple software environments. Most projects focus on proof-of-principle demonstrations and/or new method development. Some projects focus on",4.425730711866283
"What are the broad aims of the Quantum Computing User Program at OLCF?
","Welcome! The information below introduces how we structure projects and user accounts for access to the Quantum resources within the QCUP program. In general, OLCF QCUP resources are granted to projects, which are in turn made available to the approved users associated with each project.",4.409897328450766
"How do users install the QCS CLI?
","To submit a reservation via the QCS CLI, and installation instructions: https://docs.rigetti.com/qcs/guides/using-the-qcs-cli

Accessing the Rigetti QPU systems can only be done during a user's reservation window.  To submit a job, users must have JupyterHub access to the system.  QVM jobs can be run without network access in local environments.  Jobs are compiled via Quilc and submitted via pyQuil (see https://docs.olcf.ornl.gov/systems/rigetti.html#Software Section <rigetti-soft> below) in a python environment or Jupyter notebook.",4.24682893809069
"How do users install the QCS CLI?
","https://docs.rigetti.com/qcs/getting-started/installing-locally

The bare-bones installation only contains the executable binaries and manual pages, and doesn’t contain any of the requisite dynamic libraries. As such, installation doesn’t require administrative or sudo privileges. This method of installation requires one, through whatever means, to install shared libraries for BLAS, LAPACK, libffi, and libzmq3. Some download methods are listed here:

Lapack (with BLAS) download: http://www.netlib.org/lapack/

libffi download:

Older versions: https://sourceware.org/ftp/libffi/",4.205374251946271
"How do users install the QCS CLI?
","Users are able to install Rigetti software locally for the purpose of development using a provided Quantum Virtual Machine, or QVM, an implementation of a quantum computer simulator that can run Rigetti's Quil programs.  This can be done via two methods:

Installing manually: https://docs.rigetti.com/qcs/getting-started/installing-locally

Docker: https://hub.docker.com/r/rigetti/forest",4.190178497659581
"Can I use mpirun or aprun instead of jsrun?
","While jsrun performs similar job launching functions as aprun and mpirun, its syntax is very different. A large reason for syntax differences is the introduction of the resource set concept. Through resource sets, jsrun can control how a node appears to each job. Users can, through jsrun command line flags, control which resources on a node are visible to a job. Resource sets also allow the ability to run multiple jsruns simultaneously within a node. Under the covers, a resource set is a cgroup.

At a high level, a resource set allows users to configure what a node look like to their job.",4.340135598840151
"Can I use mpirun or aprun instead of jsrun?
","This section describes tools that users might find helpful to better understand the jsrun job launcher.

hello_jsrun is a ""Hello World""-type program that users can run on Summit nodes to better understand how MPI ranks and OpenMP threads are mapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A screencast showing how to use Hello_jsrun is also available: https://vimeo.com/261038849

Job Step Viewer provides a graphical view of an application's runtime layout on Summit. It allows users to preview and quickly iterate with multiple jsrun options to understand and optimize job launch.",4.263814937348624
"Can I use mpirun or aprun instead of jsrun?
",Below is a comparison table between srun and jsrun.,4.256904814143916
"Where can I find more information about IBM Quantum Insider?
","Simulator backends currently available: https://quantum-computing.ibm.com/services?services=simulators

IBM's Documentation

IBM Quantum Insider",4.400320309403376
"Where can I find more information about IBM Quantum Insider?
",and view the results of your past jobs. More information about using these IBM quantum resources can be found on the IBM's Documentation or our https://docs.olcf.ornl.gov/quantum/quantum_systems/ibm_quantum.html.,4.378992444261916
"Where can I find more information about IBM Quantum Insider?
","Users can access information about IBM Quantum's systems, view queue information, and submit jobs on their cloud dashboard at https://quantum-computing.ibm.com. The cloud dashboard allows access to IBM Quantum's graphical circuit builder, Quantum Composer, IBM's Quantum Lab, and associated program examples.  A Jupyterlab server is provisioned with IBM Quantum's Qiskit programming framework for job submission.",4.289672749130795
"What is the value of the ""altGreeting"" key in the ConfigMap?
","$ kustomize build base/
apiVersion: v1
data:
  altGreeting: Good Morning!
  enableRisky: ""false""
kind: ConfigMap
metadata:
  labels:
    app: hello
  name: the-map
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: hello
  name: the-service
spec:
  ports:
  - port: 8666
    protocol: TCP
    targetPort: 8080
  selector:
    app: hello
    deployment: hello
  type: LoadBalancer
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: hello
  name: the-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: hello
      deployment: hello
  template:",4.1694983821854
"What is the value of the ""altGreeting"" key in the ConfigMap?
","deployment: hello
  template:
    metadata:
      labels:
        app: hello
        deployment: hello
    spec:
      containers:
      - command:
        - /hello
        - --port=8080
        - --enableRiskyFeature=$(ENABLE_RISKY)
        env:
        - name: ALT_GREETING
          valueFrom:
            configMapKeyRef:
              key: altGreeting
              name: the-map
        - name: ENABLE_RISKY
          valueFrom:
            configMapKeyRef:
              key: enableRisky
              name: the-map
        image: monopole/hello:1
        name: the-container",4.104105340120628
"What is the value of the ""altGreeting"" key in the ConfigMap?
","there are a few meta information blocks present: namePrefix, commonLabels, and commonAnnotations. Additionally, we see that there is a patch specified with the patchesStrategicMerge block where a patch file to be merged is specified:

$ cat overlays/staging/map.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: the-map
data:
  altGreeting: ""Have a pineapple!""
  enableRisky: ""true""",3.955686831355859
"How do I include a launch node in the reservation?
","The requesting project’s allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Frontier would be equivalent to using 16,000 Frontier node-hours of a project’s allocation.

Reservations should not be confused with priority requests. If quick turnaround is needed for a few jobs or for a period of time, a priority boost should be requested. A reservation should only be requested if users need to guarantee availability of a set of nodes at a given time, such as for a live demonstration at a conference.",4.21159740296683
"How do I include a launch node in the reservation?
","Projects may request to reserve a set of nodes for a period of time by contacting help@olcf.ornl.gov. If the reservation is granted, the reserved nodes will be blocked from general use for a given period of time. Only users that have been authorized to use the reservation can utilize those resources. To access the reservation, please add -U {reservation name} to bsub or job script. Since no other users can access the reserved resources, it is crucial that groups given reservations take care to ensure the utilization on those resources remains high. To prevent reserved resources from remaining",4.184100125999378
"How do I include a launch node in the reservation?
","A dialog box follows, in which you must enter in your username and project allocation, the number of nodes to reserve and a duration to reserve them for. This is also where you can choose between the OSMesa and EGL rendering options (via the ""Server headless API"" box).



When you click OK, a windows command prompt or xterm pops up. In this window enter your credentials at the OLCF login prompt.",4.177152134029799
"How do I specify the allocation of resources in a batch script?
","Once resources have been allocated through the batch system, users have the option of running commands on the allocated resources' primary compute node (a serial job) and/or running an MPI/OpenMP executable across all the resources in the allocated resource pool simultaneously (a parallel job).

The executable portion of batch scripts is interpreted by the shell specified on the first line of the script. If a shell is not specified, the submitting user’s default shell will be used.",4.372437219242455
"How do I specify the allocation of resources in a batch script?
","Job Execution

Once resources have been allocated through the batch system, users have the option of running commands on the allocated resources' primary compute node (a serial job) and/or running an MPI/OpenMP executable across all the resources in the allocated resource pool simultaneously (a parallel job).

Serial Job Execution

The executable portion of batch scripts is interpreted by the shell specified on the first line of the script. If a shell is not specified, the submitting user’s default shell will be used.",4.296948251089889
"How do I specify the allocation of resources in a batch script?
","to request an interactive batch job with the same resources that the batch script above requests, you would use salloc -A ABC123 -J RunSim123 -t 1:00:00 -p batch -N 1024. Note there is no option for an output file...you are running interactively, so standard output and standard error will be displayed to the terminal.",4.283556966489383
"Can I use OLCF resources for projects that involve export-controlled information?
","Export Control: The project request will be reviewed by ORNL Export Control to determine whether sensitive or proprietary data will be generated or used. The results of this review will be forwarded to the PI. If the project request is deemed sensitive and/or proprietary, the OLCF Security Team will schedule a conference call with the PI to discuss the data protection needs.",4.420062879558115
"Can I use OLCF resources for projects that involve export-controlled information?
","The OLCF computer systems are operated as research systems and only contain data related to scientific research and do not contain personally identifiable information (data that falls under the Privacy Act of 1974 5U.S.C. 552a). Use of OLCF resources to store, manipulate, or remotely access any national security information is strictly prohibited. This includes, but is not limited to: classified information, unclassified controlled nuclear information (UCNI), naval nuclear propulsion information (NNPI), the design or development of nuclear, biological, or chemical weapons or any weapons of",4.395476283002254
"Can I use OLCF resources for projects that involve export-controlled information?
","The OLCF computer systems are operated as research systems and only contain data related to scientific research and do not contain personally identifiable information (data that falls under the Privacy Act of 1974 5U.S.C. 552a). Use of OLCF resources to store, manipulate, or remotely access any national security information is strictly prohibited. This includes, but is not limited to: classified information, unclassified controlled nuclear information (UCNI), naval nuclear propulsion information (NNPI), the design or development of nuclear, biological, or chemical weapons or any weapons of",4.395476283002254
"How can I set the SSH_PATH environment variable for the ORNL Summit server in Paraview?
","After installing, you must give ParaView the relevant server information to be able to connect to OLCF systems (comparable to VisIt's system of host profiles). The following provides an example of doing so. Although several methods may be used, the one described should work in most cases.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.

For Windows clients, it is necessary to install PuTTY to create an ssh connection in step 2.

Step 1: Save the following servers.pvsc file to your local computer",4.202228357057823
"How can I set the SSH_PATH environment variable for the ORNL Summit server in Paraview?
","After setting up and installing ParaView, you can connect to OLCF systems remotely to visualize your data interactively through ParaView's GUI. To do so, go to File→Connect and select either ORNL Andes or ORNL Summit (provided they were successfully imported -- as outlined in https://docs.olcf.ornl.gov/systems/paraview.html#paraview-install-setup). Next, click on Connect and change the values in the Connection Options box.",4.2015067365185335
"How can I set the SSH_PATH environment variable for the ORNL Summit server in Paraview?
","<Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Windows"">
            <Set name=""TERM_PATH"" value=""cmd"" />
            <Set name=""TERM_ARG1"" value=""/C"" />
            <Set name=""TERM_ARG2"" value=""start"" />
            <Set name=""TERM_ARG3"" value="""" />
            <Set name=""SSH_PATH"" value=""plink.exe"" />
          </Case>
          <Case value=""Unix"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.145318480003882
"How do I leverage features of Deployments in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.369073954957024
"How do I leverage features of Deployments in Slate?
","Jenkins

OpenShift Pipelines

GitLab Runners

Deploying a GitLab Runner into a Slate project may be found in the https://docs.olcf.ornl.gov/systems/overview.html#slate_gitlab_runners document which leverages a localized Slate Helm Chart. The GitLab Pipelines documentation as well as GitLab CI/CD Examples provide more details on GitLab Runner capabilities and usage.

Jenkins

For Jenkins deployment, a Slate Helm Chart is planned to be released. Documentation concerning Jenkins Pipelines as well as Jenkins Pipeline Tutorials are available.

OpenShift Pipelines",4.180224756771389
"How do I leverage features of Deployments in Slate?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",4.17113712564615
"How do I run a ParaView batch job from the command line?
","Using ParaView via the command line should always be done through a batch job, and should always be executed on a compute node -- never the login or launch nodes.

ParaView can be controlled through Python without opening the ParaView GUI. To do this on OLCF systems, one must use a batch script in combination with PvBatch (one of the Python interfaces available in ParaView). PvBatch accepts commands from Python scripts and will run in parallel using MPI. Example batch scripts, along with a working Python example, are provided below.

Andes

.. code-block:: bash",4.506060362384704
"How do I run a ParaView batch job from the command line?
",of how to run a Python script using PvBatch on Andes and Summit.,4.354536192023485
"How do I run a ParaView batch job from the command line?
","# Render scene and save resulting image
Render()
SaveScreenshot('pvbatch-test.png',ImageResolution=[1080, 1080])

For older versions of ParaView (e.g., 5.9.1), line 23 should be 'AnyLocation' (no space).



If everything is working properly, the above image should be generated after the batch job is complete.

All of the above can also be achieved in an interactive batch job through the use of the salloc command on Andes or the bsub -Is command on Summit. Recall that login nodes should not be used for memory- or compute-intensive tasks, including ParaView.",4.298730469179173
"What is the difference between a BuildConfig and an ImageStream?
","OpenShift has an integrated container image build service that users interact with through BuildConfig objects. BuildConfig's are very powerful, builds can be triggered by git repo or image tag pushes and connected into a pipeline to do automated deployments of newly built images. While powerful, these mechanisms can be cumbersome when starting out so we will be using a BuildConfig in a slightly simpler setup.

We will create a BuildConfig that will take a Binary (Local) source which will stream the contents of our local filesystem to the builder.",4.315165619686744
"What is the difference between a BuildConfig and an ImageStream?
","First, we will log into the cluster using the oc CLI tool

oc login https://api.<cluster>.ccs.ornl.gov

Next we will create the ImageStream that the BuildConfig will push the completed image to. The ImageStream is a direct mapping to the image stored in the OpenShift integrated registry.

oc create imagestream local-image

Next, we will create the BuildConfig object",4.188006836031391
"What is the difference between a BuildConfig and an ImageStream?
","* A Docker build using binary input will be created
      * The resulting image will be pushed to image stream ""example:latest""
      * A binary build was created, use 'start-build --from-dir' to trigger a new build

--> Creating resources with label build=example ...
    imagestream ""example"" created
    buildconfig ""example"" created
--> Success

That will create a new build config, from that build config you can then use your app by running the start-build command with the name of the newly created build config.",4.166401128875113
"What is the difference between the hipcc and hip-runtime commands?
","Key features include:

HIP is a thin layer and has little or no performance impact over coding directly in CUDA.

HIP allows coding in a single-source C++ programming language including features such as templates, C++11 lambdas, classes, namespaces, and more.

The “hipify” tools automatically convert source from CUDA to HIP.

Developers can specialize for the platform (CUDA or HIP) to tune for performance or handle tricky cases.",4.071140065181351
"What is the difference between the hipcc and hip-runtime commands?
","| Vendor | Module | Language | Compiler | OpenMP flag (GPU) | | --- | --- | --- | --- | --- | | Cray | cce | C C++ |  | -fopenmp | | Fortran | ftn |  | | AMD | rocm |  |  |  |

This section shows how to compile HIP codes using the Cray compiler wrappers and hipcc compiler driver.

Make sure the craype-accel-amd-gfx908 module is loaded when using HIP.

| Compiler | Compile/Link Flags, Header Files, and Libraries | | --- | --- | | CC |  | | hipcc |  |",4.050744753848938
"What is the difference between the hipcc and hip-runtime commands?
","HIP (Heterogeneous-Compute Interface for Portability) is a C++ runtime API that allows developers to write portable code to run on AMD and NVIDIA GPUs. It is an interface that uses the underlying Radeon Open Compute (ROCm) or CUDA platform that is installed on a system. The API is similar to CUDA so porting existing codes from CUDA to HIP should be fairly straightforward in most cases. In addition, HIP provides porting tools which can be used to help port CUDA codes to the HIP layer, with no overhead compared to the original CUDA application. HIP is not intended to be a drop-in replacement",4.04857476233172
"What is the peak performance of the NVIDIA Tesla V100 accelerator on Summit?
","The NVIDIA Tesla V100 GPUs in Summit are capable of over 7TF/s of double-precision and 15 TF/s of single-precision floating point performance. Additionally, the V100 is capable of over 120 TF/s of half-precision floating point performance when using its Tensor Core feature. The Tensor Cores are purpose-built accelerators for half-precision matrix multiplication operations. While they were designed especially to accelerate machine learning workflows, they are exposed through several other APIs that are useful to other HPC applications. This section provides information for using the V100",4.527297872919261
"What is the peak performance of the NVIDIA Tesla V100 accelerator on Summit?
","The NVIDIA Tesla V100 accelerator has a peak performance of 7.8 TFLOP/s (double-precision) and contributes to a majority of the computational work performed on Summit. Each V100 contains 80 streaming multiprocessors (SMs), 16 GB (32 GB on high-memory nodes) of high-bandwidth memory (HBM2), and a 6 MB L2 cache that is available to the SMs. The GigaThread Engine is responsible for distributing work among the SMs and (8) 512-bit memory controllers control access to the 16 GB (32 GB on high-memory nodes) of HBM2 memory. The V100 uses NVIDIA's NVLink interconnect to pass data between GPUs as well",4.527196441506163
"What is the peak performance of the NVIDIA Tesla V100 accelerator on Summit?
","Each Summit Compute node has 6 NVIDIA V100 GPUs.  The NVIDIA Tesla V100 accelerator has a peak performance of 7.8 TFLOP/s (double-precision) and contributes to a majority of the computational work performed on Summit. Each V100 contains 80 streaming multiprocessors (SMs), 16 GB (32 GB on high-memory nodes) of high-bandwidth memory (HBM2), and a 6 MB L2 cache that is available to the SMs. The GigaThread Engine is responsible for distributing work among the SMs and (8) 512-bit memory controllers control access to the 16 GB (32 GB on high-memory nodes) of HBM2 memory. The V100 uses NVIDIA's",4.509969012989472
"Can I use a workaround to fix the issue?
","VisIt developers have been notified, and at this time the current workaround is to disable Scalable Rendering from being used. To do this, go to Options→Rendering→Advanced and set the ""Use Scalable Rendering"" option to ""Never"".

However, this workaround has been reported to affect VisIt's ability to save images, as scalable rendering is utilized to save plots as image files (which can result in another compute engine crash). To avoid this, screen capture must be enabled. Go to File→""Set save options"" and check the box labeled ""Screen capture"".",3.903110992124798
"Can I use a workaround to fix the issue?
","Users that encounter this issue, can use the following workaround. copy the entirety of ${OLCF_CUDA_ROOT}/include/thrust to a private location, make the above edits to thrust/complex.h and thrust/detail/complex/complex.inl, and then add that to your include path:

$ nvcc -ccbin=g++ --expt-relaxed-constexpr assignment.cu -I./

A permanent fix of this issue is expected in the version of Thrust packed with CUDA 10.1 update 1",3.8737392845942207
"Can I use a workaround to fix the issue?
",to use this script (especially after system upgrades) for testing purposes.,3.858440934495552
"Can I build a container image with a different name than the default?
","The -t flag names the container image and the -f flag indicates the file to use for building the image.

Run podman image ls to see the list of images. localhost/simple should be among them. Any container created without an explicit url to a container registry in its name will automatically have the localhost prefix.

$ podman image ls
REPOSITORY             TAG      IMAGE ID      CREATED      SIZE
localhost/simple       latest   e47dbfde3e99  3 hours ago  687 MB
quay.io/centos/centos  stream8  ad6f8b5e7f64  8 days ago   497 MB",4.153120336459416
"Can I build a container image with a different name than the default?
","We will use this Dockerfile to generate a BuildConfig and then build a new image in our project that has the correct permissions.

cat Dockerfile | oc new-build --dockerfile=- --to=my-image:tag

The build should start automatically, monitor it with oc logs bc/my-image -f.

Now that we have a new image with our /opt/application-data directory owned by the right user we can either update an existing deployment or create a new one with the image.",4.13818704879915
"Can I build a container image with a different name than the default?
","We will use OpenShift to build a new image based on the upstream one and change owner of the directories that need to be writable during container execution. Here is an example Dockerfile which derives from an upstream image and changes ownership of directories to the user id that the container will run as in the cluster.

For example, if we are using the UID 63114 for our NCCS project user and we need to write to /opt/application-data during the runtime of the container image we could do this:

FROM upstream-image:tag
USER 0
RUN chown -R 63114 /opt/application-data
USER 63114",4.13537488974821
"How can I set the SMT level to 2 in Summit?
",The default SMT level is 4.,4.372264435610972
"How can I set the SMT level to 2 in Summit?
","SMT level to 2, add the line #BSUB –alloc_flags smt2 to your batch script or add the option -alloc_flags smt2 to you bsub command line.",4.257459074774255
"How can I set the SMT level to 2 in Summit?
",For Summit:,4.147898899799476
"What is the role of the Principal Investigator in the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  Title: Project Reporting Policy Version: 12.10",4.491309787602527
"What is the role of the Principal Investigator in the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to or use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title:Security Policy Version: 12.10",4.433409608854169
"What is the role of the Principal Investigator in the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to or use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title: Data Management Policy Version: 20.02",4.415297027447605
"What is the purpose of the `ls -lh` commands in the given scenario?
","The backups are accessed via the .snapshot subdirectory. Note that ls alone (or even ls -a) will not show the .snapshot subdirectory exists, though ls .snapshot will show its contents. The .snapshot feature is available in any subdirectory of your home directory and will show the online backups available for that subdirectory.

To retrieve a backup, simply copy it into your desired destination with the cp command.",3.996781280816821
"What is the purpose of the `ls -lh` commands in the given scenario?
","To check your project’s current usage, run df -h /ccs/proj/abc123 (where abc123 is your project ID). Quotas are enforced on project home directories. The current limit is shown in the table above.

The default permissions for project home directories are 0770 (full access to the user and group). The directory is owned by root and the group includes the project’s group members. All members of a project should also be members of that group-specific project. For example, all members of project “ABC123” should be members of the “abc123” UNIX group.",3.980254545233509
"What is the purpose of the `ls -lh` commands in the given scenario?
","The backups are accessed via the .snapshot subdirectory. Note that ls alone (or even ls -a) will not show the .snapshot subdirectory exists, though ls .snapshot will show its contents. The .snapshot feature is available in any subdirectory of your project home directory and will show the online backups available for that subdirectory.

To retrieve a backup, simply copy it into your desired destination with the cp command.",3.979948302720345
"Can I clone a persistent volume that is already cloned?
","Cloning a persistent volume is just as easy as implementing a snapshot. First, find a Persistent Volume Claim in the same namespace that you would like to clone for your new persistent volume. Then it's as simple as adding the trident.netapp.io/cloneFromPVC annotation with a value of the name of the Persistent Volume Claim you would like to clone.

In the below example, we clone a persistent volume named source-clone-pvc into a new volume called destination-clone-pvc",4.442191873633088
"Can I clone a persistent volume that is already cloned?
","Now, if the data in the PersistentVolume becomes corrupted or lost in some way, we can create a new PersistentVolume that is identical to the original PersistentVolume at the time the VolumeSnapshot was created.

To do this, create a PersistentVolumeClaim that contains a dataSource field in the Pod's spec. An example follows:",4.30723075993983
"Can I clone a persistent volume that is already cloned?
","apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    trident.netapp.io/cloneFromPVC: ""source-clone-pvc""
    volume.beta.kubernetes.io/storage-class: ""basic""
    trident.netapp.io/splitOnClone: ""true""
  name: destination-clone-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

Cloning has applications outside of backups such as testing changes on a new Persistent Volume.",4.239018207759202
"How does Crusher handle page faults in GPU kernels?
","On Pascal-generation GPUs and later, this automatic migration is enhanced with hardware support. A page migration engine enables GPU page faulting, which allows the desired pages to be migrated to the GPU ""on demand"" instead of the entire ""managed"" allocation. In addition, 49-bit virtual addressing allows programs using unified memory to access the full system memory size. The combination of GPU page faulting and larger virtual addressing allows programs to oversubscribe the system memory, so very large data sets can be processed. In addition, new CUDA API functions introduced in CUDA8 allow",4.298277778896245
"How does Crusher handle page faults in GPU kernels?
","The accessibility of memory from GPU kernels and whether pages may migrate depends three factors: how the memory was allocated; the XNACK operating mode of the GPU; whether the kernel was compiled to support page migration. The latter two factors are intrinsically linked, as the MI250X GPU operating mode restricts the types of kernels which may run.",4.240774064398295
"How does Crusher handle page faults in GPU kernels?
","The accessibility of memory from GPU kernels and whether pages may migrate depends three factors: how the memory was allocated; the XNACK operating mode of the GPU; whether the kernel was compiled to support page migration. The latter two factors are intrinsically linked, as the MI250X GPU operating mode restricts the types of kernels which may run.",4.240774064398295
"How do I specify the number of tasks per node for my job on Frontier?
","Unlike Summit and Titan, there are no launch/batch nodes on Frontier. This means your batch script runs on a node allocated to you rather than a shared node. You still must use the job launcher (srun) to run parallel jobs across all of your nodes, but serial tasks need not be launched with srun.



To easily visualize job examples (see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-mapping further below), the compute node diagram has been simplified to the picture shown below.

Simplified Frontier node architecture diagram",4.278097122041428
"How do I specify the number of tasks per node for my job on Frontier?
","To override this default layout (not recommended), set -S 0 at job allocation.



Frontier uses SchedMD's Slurm Workload Manager for scheduling and managing jobs. Slurm maintains similar functionality to other schedulers such as IBM's LSF, but provides unique control of Frontier's resources through custom commands and options specific to Slurm. A few important commands can be found in the conversion table below, but please visit SchedMD's Rosetta Stone of Workload Managers for a more complete conversion reference.",4.240382769016675
"How do I specify the number of tasks per node for my job on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.238934172191369
"How can I ensure that my jobs are running efficiently on Crusher?
","This section describes how to run programs on the Crusher compute nodes, including a brief overview of Slurm and also how to map processes and threads to CPU cores and GPUs.

Slurm is the workload manager used to interact with the compute nodes on Crusher. In the following subsections, the most commonly used Slurm commands for submitting, running, and monitoring jobs will be covered, but users are encouraged to visit the official documentation and man pages for more information.",4.247537852638496
"How can I ensure that my jobs are running efficiently on Crusher?
","Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.",4.175858107261599
"How can I ensure that my jobs are running efficiently on Crusher?
","Crusher users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.16394971352691
"What is the purpose of the hipcc compiler option --amdgpu-target=gfx90a?
",hipcc --amdgpu-target=gfx90a:xnack- --amdgpu-target=gfx90a:xnack+ -x hip or CC --offload-arch=gfx90a:xnack- --offload-arch=gfx90a:xnack+ -x hip,4.4721928064317655
"What is the purpose of the hipcc compiler option --amdgpu-target=gfx90a?
",hipcc --amdgpu-target=gfx90a:xnack- --amdgpu-target=gfx90a:xnack+ -x hip or CC --offload-arch=gfx90a:xnack- --offload-arch=gfx90a:xnack+ -x hip,4.4721928064317655
"What is the purpose of the hipcc compiler option --amdgpu-target=gfx90a?
","hipcc --amdgpu-target=gfx90a:xnack+ or CC --offload-arch=gfx90a:xnack+ -x hip

Kernels are compiled in ""xnack plus"" mode and will only be able to run on GPUs with HSA_XNACK=1 to enable XNACK. Performance may be better than ""xnack any"", but attempts to run with XNACK disabled will fail.

hipcc --amdgpu-target=gfx90a:xnack- or CC --offload-arch=gfx90a:xnack- -x hip

Kernels are compiled in ""xnack minus"" mode and will only be able to run on GPUs with HSA_XNACK=0 and XNACK disabled. Performance may be better than ""xnack any"", but attempts to run with XNACK enabled will fail.",4.359455737788637
"What is the purpose of the ""gendata.sh"" script?
","In order to demonstrate the data generation, we have a script that downloads image data from the NOAA website periodically. The image is a geographical image showing current cloud cover over south-east US. The code gendata.sh looks like so:

#!/bin/bash
set -eu

function cleanup() {
  \rm -f ./data/earth*.jpg
}

while true
do
  uid=$(uuidgen | awk -F- '{print $1}')
  wget -q https://cdn.star.nesdis.noaa.gov/GOES16/ABI/SECTOR/se/GEOCOLOR/1200x1200.jpg -O ./data/earth${uid}.jpg
  sleep 5
  trap cleanup EXIT
done",4.146415287098723
"What is the purpose of the ""gendata.sh"" script?
","Next, we have the data processing script called processdata.sh that looks as follows:

#!/bin/bash
set -eu

TASK=convert
DATA=$1
echo ""\nProcessing ${DATA}\n""
${TASK} ${DATA} -fuzz 10% -fill white -opaque white -fill black +opaque white -format ""%[fx:100*mean]"" info:
sleep 5

The above script computes the cloud cover percentage by looking at the amount of white pixels in the image. Note that it uses ImageMagick's convert utility.",4.139429633069807
"What is the purpose of the ""gendata.sh"" script?
","The suggested directory structure is to have a outer directory say swift-work that has the swift source and shell scripts. Inside of swift-work create a new directory called data.

Additionally, we will need two terminals open. In the first terminal window, navigate to the swift-work directory and invoke the data generation script like so:

$ ./gendata.sh

In the second terminal, we will run the swift workflow as follows (make sure to change the project name per your allocation):",4.108545232908184
"How can I export an environment from Conda at OLCF?
","This guide introduces a user to the basic workflow of using conda environments, as well as providing an example of how to create a conda environment that uses a different version of Python than the base environment uses on Summit. Although Summit is being used in this guide, all of the concepts still apply to other OLCF systems. If using Frontier, this assumes you already have installed your own version of conda (e.g., by https://docs.olcf.ornl.gov/software/python/miniconda.html).

OLCF Systems this guide applies to:

Summit

Andes

Frontier (if using conda)",4.395827151810799
"How can I export an environment from Conda at OLCF?
","This page describes how a user can successfully install specific quantum software tools to run on select OLCF HPC systems. This allows a user to utilize our systems to run a simulator, or even as a ""frontend"" to a vendor's quantum machine (""backend"").

For most of these instructions, we use conda environments. For more detailed information on how to use Python modules and conda environments on OLCF systems, see our :doc:`Python on OLCF Systems page</software/python/index>`, and our https://docs.olcf.ornl.gov/software/python/conda_basics.html.",4.32103972475703
"How can I export an environment from Conda at OLCF?
","Exporting (sharing) an environment:

You may want to share your environment with someone else. One way to do this is by creating your environment in a shared location where other users can access it. A different way (the method described below) is to export a list of all the packages and versions of your environment (an environment.yml file). If a different user provides conda the list you made, conda will install all the same package versions and recreate your environment for them -- essentially ""sharing"" your environment. To export your environment list:",4.307395408620328
"How can I get the properties of the colorbar in Paraview?
","# Set Colorbar Properties
display.SetScalarBarVisibility(curr_view,True) # Show bar
scalarBar = GetScalarBar(cmap, curr_view)      # Get bar's properties
scalarBar.WindowLocation = 'Any Location'       # Allows free movement
scalarBar.Orientation = 'Horizontal'           # Switch from Vertical to Horizontal
scalarBar.Position = [0.15,0.80]               # Bar Position in [x,y]
scalarBar.LabelFormat = '%.0f'                 # Format of tick labels
scalarBar.RangeLabelFormat = '%.0f'            # Format of min/max tick labels
scalarBar.ScalarBarLength = 0.7                # Set length of bar",4.215668836815668
"How can I get the properties of the colorbar in Paraview?
","display = Show(p)                              # Show data
curr_view = GetActiveView()                    # Retrieve current view

# Generate a colormap for Proc Id's
cmap = GetColorTransferFunction(""ProcessId"")   # Generate a function based on Proc ID
cmap.ApplyPreset('Viridis (matplotlib)')       # Apply the Viridis preset colors
#print(GetLookupTableNames())                  # Print a list of preset color schemes",3.979482039770288
"How can I get the properties of the colorbar in Paraview?
","# Create a pseudocolor plot
AddPlot(""Pseudocolor"", ""temperature"") # Plot type, variable name

# Pseudocolor attributes settings
PseudocolorAtts = PseudocolorAttributes()
PseudocolorAtts.centering = PseudocolorAtts.Nodal  # Natural, Nodal, Zonal -- Nodal for smoothing
PseudocolorAtts.colorTableName = ""viridis_light""   # Set colormap
PseudocolorAtts.invertColorTable = 1               # Invert colors
SetPlotOptions(PseudocolorAtts)",3.944227145403234
"What are the benefits of using hardware-based FP atomics on Frontier?
","The fast hardware-based Floating point (FP) atomic operations available on MI250X are assumed to be working on coarse grained memory regions; when these instructions are applied to a fine-grained memory region, they will silently produce a no-op. To avoid returning incorrect results, the compiler never emits hardware-based FP atomics instructions by default, even when applied to coarse grained memory regions. Currently, users can use the -munsafe-fp-atomics flag to force the compiler to emit hardware-based FP atomics. Using hardware-based FP atomics translates in a substantial performance",4.30183505872424
"What are the benefits of using hardware-based FP atomics on Frontier?
","The fast hardware-based Floating point (FP) atomic operations available on MI250X are assumed to be working on coarse grained memory regions; when these instructions are applied to a fine-grained memory region, they will silently produce a no-op. To avoid returning incorrect results, the compiler never emits hardware-based FP atomics instructions by default, even when applied to coarse grained memory regions. Currently, users can use the -munsafe-fp-atomics flag to force the compiler to emit hardware-based FP atomics. Using hardware-based FP atomics translates in a substantial performance",4.30183505872424
"What are the benefits of using hardware-based FP atomics on Frontier?
",atomics translates in a substantial performance improvement over the default choice.,4.259825837432964
"How can a user stay informed about changes to the OLCF Policy?
","that any of the accounts used to access OLCF have been compromised. Users should inform the OLCF promptly of any changes in their contact information (E-mail, phone, affiliation, etc.) Updates should be sent to accounts@ccs.ornl.gov.",4.29418238128452
"How can a user stay informed about changes to the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to or use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title:Security Policy Version: 12.10",4.290530773526224
"How can a user stay informed about changes to the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  Title: Project Reporting Policy Version: 12.10",4.289439748635966
"How do I optimize the performance of my parallel h5py application?
","Both HDF5 and h5py can be compiled with MPI support, which allows you to optimize your HDF5 I/O in parallel. MPI support in Python is accomplished through the mpi4py package, which provides complete Python bindings for MPI. Building h5py against mpi4py allows you to write to an HDF5 file using multiple parallel processes, which can be helpful for users handling large datasets in Python. h5Py is available after loading the default Python module on either Summit or Andes, but it has not been built with parallel support.",4.34047925417652
"How do I optimize the performance of my parallel h5py application?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.

This guide teaches you how to build a personal, parallel-enabled version of h5py and how to write an HDF5 file in parallel using mpi4py and h5py.

In this guide, you will:

Learn how to install mpi4py

Learn how to install parallel h5py

Test your build with Python scripts

OLCF Systems this guide applies to:

Summit

Andes

Frontier",4.269961122973696
"How do I optimize the performance of my parallel h5py application?
","The HDF5_MPI flag is the key to telling pip to build h5py with parallel support, while the CC flag makes sure that you are using the correct C wrapper for MPI. This installation will take much longer than both the mpi4py and NumPy installations (5+ minutes if the system is slow). When the installation finishes, you will see a ""Successfully installed h5py"" message.

Test your build by trying to write an HDF5 file in parallel using 42 MPI tasks.

First, change directories to your GPFS scratch area:

$ cd $MEMBERWORK/<YOUR_PROJECT_ID>
$ mkdir h5py_test
$ cd h5py_test",4.197534196921765
"How do I integrate Valgrind into my Andes development workflow?
","module load valgrind

Additional information about Valgrind usage and OLCF-provided builds can be found on the Valgrind Software Page.



Visualization tools

ParaView

Information regarding ParaView, and how to run it on both Andes and Summit, has moved to the Software Section. Click https://docs.olcf.ornl.gov/software/viz_tools/paraview.html to go to the new page.

VisIt

Information regarding VisIt, and how to run it on both Andes and Summit, has moved to the Software Section. Click https://docs.olcf.ornl.gov/software/viz_tools/visit.html to go to the new page.",4.368787836984503
"How do I integrate Valgrind into my Andes development workflow?
","module load valgrind

Additional information about Valgrind usage and OLCF-provided builds can be found on the Valgrind Software Page.



Information regarding ParaView, and how to run it on both Andes and Summit, has moved to the Software Section. Click https://docs.olcf.ornl.gov/software/viz_tools/paraview.html to go to the new page.

Information regarding VisIt, and how to run it on both Andes and Summit, has moved to the Software Section. Click https://docs.olcf.ornl.gov/software/viz_tools/visit.html to go to the new page.",4.347854759066952
"How do I integrate Valgrind into my Andes development workflow?
","The Valgrind tool suite provides a number of debugging and profiling tools. The most popular is Memcheck, a memory checking tool which can detect many common memory errors such as:

Touching memory you shouldn’t (eg. overrunning heap block boundaries, or reading/writing freed memory).

Using values before they have been initialized.

Incorrect freeing of memory, such as double-freeing heap blocks.

Memory leaks.

Valgrind is available on Andes via the valgrind module:

module load valgrind",4.30583512201671
"How do I navigate to the oc portion of the document?
","After authenticating, you are redirected to the project pages area of myOLCF.

Every individual page in the project pages area should be interpreted within the context of a single, current project, which is displayed at the top of the left navigation menu (e.g. ""ABC123""):

project pages left navigation menu

The top navigation bar has a dropdown menu that can be used to switch the current project context to any of the projects of which you are a member.

switch projects dropdown menu",4.179430145417576
"How do I navigate to the oc portion of the document?
","At any time, you can view account pages by clicking on the ""My Account"" link in the top navigation menu:

link to my account page

There is only (1) account context in myOLCF: ""you"" as the currently-authenticated user. This account context is linked to the OLCF Moderate account that you used to authenticate to myOLCF.

account page left navigation menu

The left navigation menu also includes an expandable item with links to account-centric pages.",4.125963666075104
"How do I navigate to the oc portion of the document?
","OLCF Tutorials – Simple HIP Examples

The links below point to event pages from previous Frontier training events. Under the ""Presentations"" tab on each event page, you will find the presentations given during the event.

Frontier Application Readiness Kick-Off Workshop (October 2019)

Please check back to this section regularly as we will continue to add new content for our users.",4.105456631532554
"What command can be used to create a new build config in Slate?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.1565539075392
"What command can be used to create a new build config in Slate?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",4.122068973844461
"What command can be used to create a new build config in Slate?
","Openshift will automatically pull and build your source. If you make a change and need an updated build simply navigate to the build option on the menu to the left and select your project and click Start Build in the upper right.

The first step to creating the the build is to create a build config. This is done in one of three ways depending on how your code is structured. If you have a git repository already configured and it is public then the command

oc new-build .

will create your build config from that repository.",4.110590176766095
"Can I use ArgoCD to deploy resources to a specific namespace?
","From the ArgoCD Applications screen, click the Create Application button. In the General application settings, give the application deployment a name for ArgoCD to refer to in the display. For Project usually the ArgoCD default project created during the ArgoCD instance installation is sufficient. However, your workload may benefit from a different logical grouping by using multiple ArgoCD projects. If it is desired to have ArgoCD automatically deploy resources to an OpenShift namespace, change the Sync Policy to Automatic. Check the Prune Resources to automatically remove objects when they",4.381636059693595
"Can I use ArgoCD to deploy resources to a specific namespace?
",kubernetes cluster to deploy. This will likely be https://kubernetes.default.svc- the same cluster the ArgoCD instance is installed. The Namespace setting should be the OpenShift namespace that ArgoCD will deploy resources. This may or may not be the same namespace that ArgoCD is installed (see prior discussion on multiple namespace management in this document).,4.380685330058852
"Can I use ArgoCD to deploy resources to a specific namespace?
","ArgoCD has successfully accessed the namespace, realized that the state of the resources in the namespace are OutOfSync with the resource requirements of the code repository, and started the Syncing process to create and/or modify resources in the namespace to match the desired configuration. Clicking on the application tile will reveal more detailed information on the process:

Image of ArgoCD application tile detailed information.",4.370977344305018
"How can I see the entire possible graph of modules in Andes?
","VisIt is now available on Frontier through the UMS022 module.

VisIt 3.3.3 is now available on Andes.

VisIt is an interactive, parallel analysis and visualization tool for scientific data. VisIt contains a rich set of visualization features so you can view your data in a variety of ways. It can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes. Further information regarding VisIt can be found at the links provided in the https://docs.olcf.ornl.gov/systems/visit.html#visit-resources section.",4.17898372086642
"How can I see the entire possible graph of modules in Andes?
","Alternatively, the ParaView installations in /sw/andes/paraview (i.e., the paraview/5.9.1-egl and paraview/5.9.1-osmesa modules) can also be loaded to avoid this issue.



The ParaView at OLCF Tutorial highlights how to get started on Andes with example datasets.

The Official ParaView User's Guide and the Python API Documentation contain all information regarding the GUI and Python interfaces.

A full list of ParaView Documentation can be found on ParaView's website.

The ParaView Wiki contains extensive information about all things ParaView.",4.156156598720711
"How can I see the entire possible graph of modules in Andes?
","Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-user-guide.

For a full list of software available at the OLCF, please see the Software section (coming soon).",4.140905118115899
"How can I disable GPU hooks in Dask scheduler?
","#Wait for the dask-scheduler to start
sleep 10

jsrun --rs_per_host 6 --tasks_per_rs 1 --cpu_per_rs 2 --gpu_per_rs 1 --smpiargs=""-disable_gpu_hooks"" \
      dask-cuda-worker --nthreads 1 --memory-limit 82GB --device-memory-limit 16GB --rmm-pool-size=15GB \
                       --interface ib0 --scheduler-file $SCHEDULER_FILE --local-directory $WORKER_DIR \
                       --no-dashboard &

#Wait for WORKERS
sleep 10

WORKERS=12

python -u $CONDA_PREFIX/examples/dask-cuda/verify_dask_cuda_cluster.py $SCHEDULER_FILE $WORKERS

wait

#clean DASK files
rm -fr $SCHEDULER_DIR

echo ""Done!""",4.035348861551621
"How can I disable GPU hooks in Dask scheduler?
","As mentioned earlier, the RAPIDS code should perform four main tasks as shown in the following script. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

import sys
from dask.distributed import Client

def disconnect(client, workers_list):
    client.retire_workers(workers_list, close_workers=True)
    client.shutdown()

if __name__ == '__main__':

    sched_file = str(sys.argv[1]) #scheduler file
    num_workers = int(sys.argv[2]) # number of workers to wait for",4.032587829477383
"How can I disable GPU hooks in Dask scheduler?
","#Wait for the dask-scheduler to start
sleep 10

jsrun --rs_per_host 6 --tasks_per_rs 1 --cpu_per_rs 2 --gpu_per_rs 1 --smpiargs=""-disable_gpu_hooks"" \
      dask-cuda-worker --nthreads 1 --memory-limit 82GB --device-memory-limit 16GB --rmm-pool-size=15GB \
                       --death-timeout 60  --interface ib0 --scheduler-file $SCHEDULER_FILE --local-directory $WORKER_DIR \
                       --no-dashboard &

#Wait for WORKERS
sleep 10

export BSQL_BLAZING_LOGGING_DIRECTORY=$BSQL_LOG_DIR
export BSQL_BLAZING_LOCAL_LOGGING_DIRECTORY=$BSQL_LOG_DIR",4.026348456278706
"How does the -munsafe-fp-atomics flag impact the behavior of atomic operations on memory regions allocated via regular hipMalloc()?
","Users applying floating point atomic operations (e.g., atomicAdd) on memory regions allocated via regular hipMalloc() can safely apply the -munsafe-fp-atomics flags to their codes to get the best possible performance and leverage hardware supported floating point atomics. Atomic operations supported in hardware on non-FP datatypes  (e.g., INT32) will work correctly regardless of the nature of the memory region used.",4.684421755764732
"How does the -munsafe-fp-atomics flag impact the behavior of atomic operations on memory regions allocated via regular hipMalloc()?
","Users applying floating point atomic operations (e.g., atomicAdd) on memory regions allocated via regular hipMalloc() can safely apply the -munsafe-fp-atomics flags to their codes to get the best possible performance and leverage hardware supported floating point atomics. Atomic operations supported in hardware on non-FP datatypes  (e.g., INT32) will work correctly regardless of the nature of the memory region used.",4.684421755764732
"How does the -munsafe-fp-atomics flag impact the behavior of atomic operations on memory regions allocated via regular hipMalloc()?
","In ROCm-5.1 and earlier versions, the flag -munsafe-fp-atomics is interpreted as a suggestion by the compiler, whereas from ROCm-5.2 the flag will always enforce the use of fast hardware-based FP atomics.

The following tables summarize the result granularity of various combinations of allocators, flags and arguments.

For hipHostMalloc(), the following table shows the nature of the memory returned based on the flag passed as argument.",4.546343078062965
"How do I get started with quantum computing on OLCF's Quantum Computing User Portal?
","After submitting the OLCF quantum account application and receiving approval, proceed to https://quantum-computing.ibm.com/ and click on ""Create an IBMid account"". Your IBM Quantum Hub account email will be the email associated with your OLCF account. If sign-in fails, contact help@olcf.ornl.gov. Once logged in, users will have access to the IBM Quantum Hub, IBM’s online platform for QPU access, forums for quantum computing discussion, etc. From the IBM Quantum Hub Dashboard, users can manage system reservations, view system (backend) statuses, and view the results of your past jobs. More",4.542196006081204
"How do I get started with quantum computing on OLCF's Quantum Computing User Portal?
","After submitting the OLCF quantum account application and receiving approval, you will receive an email from Quantinuum inviting you to create your quantum account. Once logged in, users will have access to Quantinuum's User Interface, https://um.qapi.quantinuum.com, their online platform for managing jobs and accessing the available quantum systems, including the System Model H1, via the cloud. From the UI, users can view system status and upcoming system availability, as well as monitor batch submissions and job history. Information on using the quantum resources via Jupyter notebooks is",4.50183611895048
"How do I get started with quantum computing on OLCF's Quantum Computing User Portal?
","with the project will need to apply for a https://docs.olcf.ornl.gov/systems/quantum_access.html#User Account <quantum-user>. After your user account is approved, you can then move on to accessing the quantum resources offered by our vendors (see https://docs.olcf.ornl.gov/systems/quantum_access.html#quantum-vendors).",4.489762924207596
"How do I move files from my User Home area to my Project Home area?
","The storage area to use in any given situation depends upon the activity you wish to carry out. Each user has a User Home area on a Network File System (NFS) and a User Archive area on the archival High Performance Storage System (HPSS). These user storage areas are intended to house user-specific files. Each project has a Project Home area on NFS, multiple Work areas on Spectrum Scale, and multiple Archive areas on HPSS. These project storage areas are intended to house project-centric files. We have defined several areas as listed below by function:",4.193406484046625
"How do I move files from my User Home area to my Project Home area?
","Open and Moderate Projects are provided with a Project Home storage area in the NFS-mounted filesystem. This area is intended for storage of data, code, and other files that are of interest to all members of a project. Since Project Home is an NFS-mounted filesystem, its performance will not be as high as other filesystems.

Moderate Enhanced projects are not provided with Project Home spaces, just Project Work spaces.

Project Home area is accessible at /ccs/proj/abc123 (where abc123 is your project ID).",4.163484921898119
"How do I move files from my User Home area to my Project Home area?
","When the installation has finished, click on the Globus icon and select Web: Transfer Files as below



Globus will ask you to login. If your institution does not have an organizational login, you may choose to either Sign in with Google or Sign in with ORCiD iD.



In the main Globus web page, select the two-panel view, then set the source and destination endpoints. (Left/Right order does not matter)



Next, navigate to the appropriate source and destination paths to select the files you want to transfer. Click the ""Start"" button to begin the transfer.",4.145096482798505
"What is the relationship between ArgoCD and kustomize?
","Before going into how ArgoCD will use a kustomize configuration setup, a word about organizing the code repository. Prior to starting work with kustomize, take some time to consider what makes sense for setting up the directory of repository. Looking at the GitHub repository for kustomize, there is a kustomize Hello World document illustrating the basic layout to start with:",4.391879607642041
"What is the relationship between ArgoCD and kustomize?
","With a git repository defined in the ArgoCD Repositories settings that has kustomize environments ready for use, one can start using ArgoCD to deploy and manage kubernetes resources.",4.319918069654862
"What is the relationship between ArgoCD and kustomize?
","Image of the ArgoCD applications tab.

Prior to using ArgoCD, a git repository containing Kubernetes custom resources needs to be added for use.

The git repository containing Kubernetes custom resources is typically not the same git repository used to build the application.

There are three ways for ArgoCD to connect to a git repository:

connect using ssh

connect using https

connect using GitHub App",4.202246791478995
"How can I delete a service that I created using Slate?
","cluster is the Slate cluster (marble, onyx)

The port number should be the one listed from the service command listed above. It may differ from the example, so be sure to update accordingly.

Change the password to the randomly generated one you created during set up.

Once we have finished, we should remove the resources we created.

We have to remove the PVC that was created by the StatefulSet

oc delete service mongo
oc delete statefulset mongo
oc delete persistentvolumeclaim mongo-store-mongo-0
oc delete deployment mongoku",4.037284137721756
"How can I delete a service that I created using Slate?
","For example, let's look at service that was created in the https://docs.olcf.ornl.gov/systems/nodeport.html#slate_services document. This document assumes that it was deployed to the my-project project.

If you run oc get services, you should see your service in the list.

$ oc get services
NAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)     AGE
my-service            ClusterIP   172.25.170.246   <none>        8080/TCP    8s

Then, get some information about the service with oc describe service my-service.",4.02521737840417
"How can I delete a service that I created using Slate?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.011431545546626
"Can you explain the difference between local read/write and migrate to GPU HBM on touch in Crusher's memory management?
","| System Allocator (malloc,new,allocate, etc) | Determined by first touch | Zero copy read/write                       | Migrate to GPU HBM on touch, then local read/write |
+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+
| hipMallocManaged                            | GPU HBM                   | Zero copy read/write                       | Populate in  GPU HBM, then local read/write        |",4.292071227700156
"Can you explain the difference between local read/write and migrate to GPU HBM on touch in Crusher's memory management?
","| System Allocator (malloc,new,allocate, etc) | Determined by first touch | Zero copy read/write                       | Migrate to GPU HBM on touch, then local read/write |
+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+
| hipMallocManaged                            | GPU HBM                   | Zero copy read/write                       | Populate in  GPU HBM, then local read/write        |",4.292071227700156
"Can you explain the difference between local read/write and migrate to GPU HBM on touch in Crusher's memory management?
","The AMD MI250X and operating system on Crusher supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Crusher. In the following section we'll discuss how the heterogenous memory space on a Crusher node is surfaced within your application.",4.24640326856217
"Can I search for specific modules using the spider sub-command?
","Searching for modules

Modules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the spider sub-command can be used as summarized in the following table.",4.463755442894255
"Can I search for specific modules using the spider sub-command?
","Modules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the spider sub-command can be used as summarized in the following table.",4.394088600121054
"Can I search for specific modules using the spider sub-command?
","Modules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the spider sub-command can be used as summarized in the following table.",4.3937795348738025
"How can I create a Deployment for the StatefulSet?
","We will be deploying MongoDB with a StatefulSet. This is a special kind of deployment controller that is different from a normal Deployment in a few distinct ways and is primarily meant for for applications that rely on well known names for each pod.

We will create a Service with the StatefulSet because the StatefulSet controller requires a headless service in order to provide well-known DNS identifiers.",4.374906247100425
"How can I create a Deployment for the StatefulSet?
","In the Deployment make sure to change the YOUR_NAMESPACE string.

Create the Deployment object:

oc create -f deployment.yaml

View the deployment:

oc get deployment nginx-hello-world
NAME                DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-hello-world   3         3         3            3           9s

You should see Desired: 3 and Current: 3

After the deployment has been created it will spin up a pod running NGINX but we need to get traffic from outside the cluster to the pod so that we can display the hello world.",4.205129566221649
"How can I create a Deployment for the StatefulSet?
","As with other objects in OpenShift, you can create this Deployment with oc create -f {FILE_NAME}.

Once created, you can check the status of the ongoing deployment process with the command

oc rollout status deploy/{NAME}

To get basic information about the available revisions, if you had done any updates to the deployment since, run

oc rollout history deploy/{NAME}

You can view a specific revision with

oc rollout history deploy/{NAME} --revision=1

For more detailed information about a Deployment, use

oc describe deploy {NAME}

To roll back a deployment, run",4.200753854802871
"What is the name of the GCC programming environment used by Frontier?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers. Please see the following https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more detailed inforation on how to compile using these modules.",4.356795587920529
"What is the name of the GCC programming environment used by Frontier?
",Programming Environment  Frontier utilizes the Cray Programming Environment.  Many aspects including LMOD are similar to Summit's environment.  But Cray's compiler wrappers and the Cray and AMD compilers are worth noting.  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more information.  AMD GPUs  Each frontier node has 4 AMD MI250X accelerators with two Graphic Compute Dies (GCDs) in each accelerator. The system identifies each GCD as an independent device (so for simplicity we use the term GPU when we talk about a GCD) for a total of 8,4.3547924463297845
"What is the name of the GCC programming environment used by Frontier?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers.

It is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) whenever possible. See the next section for more details.",4.318572213793807
"What is the purpose of the -Is option in an interactive batch job on Summit?
","All of the above can also be achieved in an interactive batch job through the use of the salloc command on Andes or the bsub -Is command on Summit. Recall that login nodes should not be used for memory- or compute-intensive tasks, including VisIt.",4.346103497490769
"What is the purpose of the -Is option in an interactive batch job on Summit?
","On Summit and Andes, batch-interactive jobs using bash (i.e. those submitted with bsub -Is or salloc) run as interactive, non-login shells (and therefore source ~/.bashrc, if it exists). Regular batch jobs using bash on those systems are non-interactive, non-login shells and source the file defined by the variable $BASH_ENV in the shell from which you submitted the job. This variable is not set by default, so this means that none of these files will be sourced for a regular batch job unless you explicitly set that variable.",4.290416837277052
"What is the purpose of the -Is option in an interactive batch job on Summit?
","by a shell name. For example, to request an interactive batch job (with bash as the shell) equivalent to the sample batch script above, you would use the command: bsub -W 3:00 -nnodes 2048 -P ABC123 -Is /bin/bash",4.271451059108436
"How do I connect to Home?
","home.ccs.ornl.gov (Home) is a general purpose system that can be used to log into other OLCF systems that are not directly accessible from outside the OLCF network. For example, running the screen or tmux utility is one common use of Home. Compiling, data transfer, or executing long-running or memory-intensive tasks should never be performed on Home.



Home access is automatically granted to all enabled OLCF users.

To connect to Home, SSH to home.ccs.ornl.gov. For example:

ssh username@home.ccs.ornl.gov",4.09627007119938
"How do I connect to Home?
","ssh username@home.ccs.ornl.gov

For more information on connecting to OLCF resources, see https://docs.olcf.ornl.gov/systems/home_user_guide.html#connecting-to-olcf.

The Home system should only be used to access systems within the OLCF network. The following are examples of appropriate uses of Home:

RSA SecurID Token setup

SSH

Vi and other non-GUI editors

Screen or other terminal multiplexers

The following are examples of inappropriate uses of Home:

Compiling

Data Transfers

Long-running or memory-intensive processes",4.075365268179407
"How do I connect to Home?
","Launch the Vampir GUI on your local machine

Similar to how we have connected Vampir to the VampirServer in the https://docs.olcf.ornl.gov/systems/Vampir.html#previous section, <vampauth> you will follow the same steps except you will use localhost for the server name and your local machine port number you selected. Press 'Connect' and this should open the authentication window where you will enter your UserID and the https://docs.olcf.ornl.gov/systems/Vampir.html#VampirServer password <vampserpw> printed after a successful connection.",3.995938706034853
"What is the purpose of the `--send-libs` option in the SBCAST command?
","Notice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable. Once libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.

As mentioned above, you can alternatively use --exclude=NONE on sbcast to send all libraries along with the binary. Using --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time. A job script for the previous example, modified for sending all libraries is shown below.",4.293029285887332
"What is the purpose of the `--send-libs` option in the SBCAST command?
","Notice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable. Once libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.

As mentioned above, you can use --exclude=NONE on sbcast to send all libraries along with the binary. Using --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time. A job script for the previous example, modified for sending all libraries is shown below.",4.283532991670166
"What is the purpose of the `--send-libs` option in the SBCAST command?
","sbcast --send-libs -pf ${exe} /mnt/bb/$USER/${exe}
if [ ! ""$?"" == ""0"" ]; then
    # CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,
    # your application may pick up partially complete shared library files, which would give you confusing errors.
    echo ""SBCAST failed!""
    exit 1
fi",4.1617403874243575
"How can I stay up-to-date with the latest developments in GitOps practices and the work of the GitOps WG?
","To foster collaboration, discussion, and knowledge sharing, the CNCF GitOps Working Group held GitOpsCon North America 2021 with sessions concerning GitOps in general practice as well as specific tools. Additionally, there are two awesome lists where one may find more information concerning GitOps and tools:

Awesome Argo

Awesome GitOps

The former is curated by one of the committers to the ArgoCD project while the latter is curated by Weaveworks. The remainder of this document is focused solely on the use of the Red Hat OpenShift GitOps operator.",4.2779004668780365
"How can I stay up-to-date with the latest developments in GitOps practices and the work of the GitOps WG?
","With GitOps potentially meaning different ideas to different groups, the mission of the Cloud Native Computing Foundation (CNCF) GitOps Working Group (WG) is to define vendor neutral, principle-led meaning for GitOps practices. With the KubeCon NA conference in October, 2021, the GitOps WG released a set of four core GitOps Principles where the desired state of a GitOps managed system must be:

Declarative: A system managed by GitOps must have its desired state expressed declaratively.",4.264726071734219
"How can I stay up-to-date with the latest developments in GitOps practices and the work of the GitOps WG?
","From the release notes:

Red Hat OpenShift GitOps is a declarative way to implement continuous deployment for cloud native applications. Red Hat OpenShift GitOps ensures consistency in applications when you deploy them to different clusters in different environments, such as: development, staging, and production.",4.132442716374432
"How can we ensure that the MPI ranks are properly assigned to the GPUs?
","While this level of control over mapping MPI ranks to GPUs might be useful for some applications, it is always important to consider the implication of the mapping. For example, if the order of the GPU IDs in the map_gpu option is reversed, the MPI ranks and the GPUs they are mapped to would be in different L3 cache regions, which could potentially lead to poorer performance.

Example 4: 16 MPI ranks - each with 2 OpenMP threads and 1 *specific* GPU (multi-node)",4.442108043967145
"How can we ensure that the MPI ranks are properly assigned to the GPUs?
","As mentioned previously, all GPUs are accessible by all MPI ranks by default, so it is possible to programatically map any combination of MPI ranks to GPUs. However, there is currently no way to use Slurm to map multiple GPUs to a single MPI rank. If this functionality is needed for an application, please submit a ticket by emailing help@olcf.ornl.gov.

There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_jobstep program and test whether or not processes and threads are running where intended.",4.38746005086151
"How can we ensure that the MPI ranks are properly assigned to the GPUs?
","The output shows the round-robin (cyclic) distribution of MPI ranks to GPUs. In fact, it is a round-robin distribution of MPI ranks to NUMA domains (the default distribution). The GPU mapping is a consequence of where the MPI ranks are distributed; --gpu-bind=closest simply maps the GPU in a NUMA domain to the MPI ranks in the same NUMA domain.

Example 6: 16 MPI ranks - where 2 ranks share a GPU (round-robin, multi-node)

This example is an extension of Example 5 to run on 2 nodes.",4.386777152298047
"Can I use Slurm's job scheduling features to run my interactive batch job at a specific time?
","Since all compute resources are managed and scheduled by Slurm, it is not possible to simply log into the system and immediately begin running parallel codes interactively. Rather, you must request the appropriate resources from Slurm and, if necessary, wait for them to become available. This is done through an ""interactive batch"" job. Interactive batch jobs are submitted with the salloc command. Resources are requested via the same options that are passed via #SBATCH in a regular batch script (but without the #SBATCH prefix). For example, to request an interactive batch job with the same",4.414968702374869
"Can I use Slurm's job scheduling features to run my interactive batch job at a specific time?
","sbatch test.slurm

If successfully submitted, a Slurm job ID will be returned. This ID can be used to track the job. It is also helpful in troubleshooting a failed job; make a note of the job ID for each of your jobs in case you must contact the OLCF User Assistance Center for support.



Interactive Batch Jobs on Commodity Clusters

Batch scripts are useful when one has a pre-determined group of commands to execute, the results of which can be viewed at a later time. However, it is often necessary to run tasks on compute resources interactively.",4.356521371369968
"Can I use Slurm's job scheduling features to run my interactive batch job at a specific time?
","Use the sbatch --test-only command to see when a job of a specific size could be scheduled. For example, the snapshot below shows that a (2) node job would start at 10:54.

$ sbatch --test-only -N2 -t1:00:00 batch-script.slurm

  sbatch: Job 1375 to start at 2019-08-06T10:54:01 using 64 processors on nodes andes[499-500] in partition batch

The queue is fluid, the given time is an estimate made from the current queue state and load. Future job submissions and job completions will alter the estimate.



The following table summarizes frequently-used options to Slurm:",4.351611911326071
"How can I avoid improper permissions on my SSH configuration file in Summit?
","To fix this, use a more secure permission setting on the configuration file. An appropriate setting would be read and write permission for the user and no other permissions. You can set this with the command chmod 600 ~/.ssh/config.",4.3025082438281546
"How can I avoid improper permissions on my SSH configuration file in Summit?
","settings on user ~/.ssh/config files. (The batch system uses SSH, and the improper settings cause SSH to fail.) If you notice your jobs alternating between PEND and RUN, you might want to check permissions of your ~/.ssh/config file to make sure it does not have write permission for ""group"" or ""other"". (A setting of read/write for the user and no other permissions, which can be set with chmod 600 ~/.ssh/config, is recommended.)",4.246506464385451
"How can I avoid improper permissions on my SSH configuration file in Summit?
","LSF uses SSH to communicate with nodes allocated to your job, and in this case the improper permissions (i.e. write permission for anyone other than the user) cause SSH to fail, which in turn causes the job launch to fail. Note that SSH only checks the permissions of the configuration file itself. Thus, even if the ~/.ssh/ directory itself grants no group or other permissions, SSH will fail due to permissions on the configuration file.",4.20305833836228
"Can I use VisIt without specifying a specific version?
","VisIt uses a client-server architecture. You will obtain the best performance by running the VisIt client on your local computer and running the server on OLCF resources. VisIt for your local computer can be obtained here: VisIt Installation.

Recommended VisIt versions on our systems:

Summit: VisIt 3.1.4

Andes: VisIt 3.3.3

Frontier: VisIt 3.3.3

Using a different version than what is listed above is not guaranteed to work properly.",4.210736248753388
"Can I use VisIt without specifying a specific version?
","VisIt is now available on Frontier through the UMS022 module.

VisIt 3.3.3 is now available on Andes.

VisIt is an interactive, parallel analysis and visualization tool for scientific data. VisIt contains a rich set of visualization features so you can view your data in a variety of ways. It can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes. Further information regarding VisIt can be found at the links provided in the https://docs.olcf.ornl.gov/systems/visit.html#visit-resources section.",4.200715161272429
"Can I use VisIt without specifying a specific version?
","Past VisIt Tutorials are available on the Visit User's Wiki along with a set of Updated Tutorials available in the VisIt User Manual.

Sample data not pre-packaged with VisIt can be found in the VisIt Data Archive.

Older VisIt Versions with their release notes can be found on the old VisIt website, and Newer Versions can be found on the new VisIt website with release notes found on the VisIt Blog or VisIt Github Releases page.

Non-ORNL related bugs and issues in VisIt can be found and reported on Github.",4.196783604692092
"Can I specify the number of workers in libEnsemble?
","libEnsemble is a complete https://docs.olcf.ornl.gov/systems/libensemble.html#Python<py-index> toolkit for steering dynamic ensembles of calculations. Workflows are highly portable and detect/integrate heterogeneous resources with little effort. For instance, libEnsemble can automatically detect, assign, and reassign allocated processors and GPUs to ensemble members.",4.223936230098357
"Can I specify the number of workers in libEnsemble?
","Upon initialization, libEnsemble will detect available nodes and GPUs from the Slurm environment, and allocate those resources towards application-launches.

Start an interactive session:

$ salloc --nodes=2 -A <project_id> --time=00:10:00

Within the session (multiprocessing comms, all processes on first node):

$ python my_libensemble_script.py --comms local --nworkers 8",4.179852473409714
"Can I specify the number of workers in libEnsemble?
","Users select or supply generator and simulator functions to express their ensembles; the generator typically steers the ensemble based on prior simulator results. Such functions can also launch and monitor external executables at any scale.

Begin by loading the python module:

$ module load cray-python

libEnsemble is available on PyPI, conda-forge, the xSDK, and E4S. Most users install libEnsemble via pip:

$ pip install libensemble",4.1032315640749495
"How can I see the currently queued jobs for project ABC123?
","The most straightforward monitoring is with the bjobs command. This command will show the current queue, including both pending and running jobs. Running bjobs -l will provide much more detail about a job (or group of jobs). For detailed output of a single job, specify the job id after the -l. For example, for detailed output of job 12345, you can run bjobs -l 12345 . Other options to bjobs are shown below. In general, if the command is specified with -u all it will show information for all users/all jobs. Without that option, it only shows your jobs. Note that this is not an exhaustive list.",4.207170641650887
"How can I see the currently queued jobs for project ABC123?
","The squeue command is used to show the batch queue. You can filter the level of detail through several command-line options. For example:

| squeue -l | Show all jobs currently in the queue | | --- | --- | |  | Show all of your jobs currently in the queue |

The sacct command gives detailed information about jobs currently in the queue and recently-completed jobs. You can also use it to see the various steps within a batch jobs.",4.115356536535792
"How can I see the currently queued jobs for project ABC123?
","Most users will find batch jobs an easy way to use the system, as they allow you to ""hand off"" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.",4.094421179815254
"What is the purpose of the echo ""*****ldd ./${exe}*****"" command in the example file?
","# Check to see if file exists
echo ""*****ls -lh /mnt/bb/$USER*****""
ls -lh /mnt/bb/$USER/
echo ""*****ls -lh /mnt/bb/$USER/${exe}_libs*****""
ls -lh /mnt/bb/$USER/${exe}_libs

# SBCAST sends all libraries detected by `ld` (minus any excluded), and stores them in the same directory in each node's node-local storage
# Any libraries opened by `dlopen` are NOT sent, since they are not known by the linker at run-time.",4.129560521024871
"What is the purpose of the echo ""*****ldd ./${exe}*****"" command in the example file?
","# Check to see if file exists
echo ""*****ls -lh /mnt/bb/$USER*****""
ls -lh /mnt/bb/$USER/
echo ""*****ls -lh /mnt/bb/$USER/${exe}_libs*****""
ls -lh /mnt/bb/$USER/${exe}_libs

# SBCAST sends all libraries detected by `ld` (minus any excluded), and stores them in the same directory in each node's node-local storage
# Any libraries opened by `dlopen` are NOT sent, since they are not known by the linker at run-time.",4.129560521024871
"What is the purpose of the echo ""*****ldd ./${exe}*****"" command in the example file?
","# You may notice that some libraries are still linked from /sw/frontier, even after SBCASTing.
# This is because the Spack-build modules use RPATH to find their dependencies.
echo ""*****ldd /mnt/bb/$USER/${exe}*****""
ldd /mnt/bb/$USER/${exe}
echo ""*************************************""

and here is the output from that script:",4.122226397786333
"How can I submit a ticket for assistance with GPU mapping?
","However, there is currently no way to use Slurm to map multiple GPUs to a single MPI rank. If this functionality is needed for an application, please submit a ticket by emailing help@olcf.ornl.gov.

There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_jobstep program and test whether or not processes and threads are running where intended.",4.207557355544775
"How can I submit a ticket for assistance with GPU mapping?
","As mentioned previously, all GPUs are accessible by all MPI ranks by default, so it is possible to programatically map any combination of MPI ranks to GPUs. However, there is currently no way to use Slurm to map multiple GPUs to a single MPI rank. If this functionality is needed for an application, please submit a ticket by emailing help@olcf.ornl.gov.

There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_jobstep program and test whether or not processes and threads are running where intended.",4.153116594165564
"How can I submit a ticket for assistance with GPU mapping?
","In this sub-section, an MPI+OpenMP+HIP ""Hello, World"" program (hello_jobstep) will be used to show how to make only specific GPUs available to processes - which we will refer to as ""GPU mapping"". Again, Slurm's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive method was used to request an allocation of 2 compute nodes for these examples: salloc -A <project_id> -t 30 -p <parition> -N 2. The CPU mapping part of this example is very similar to the example used above in the Multithreading sub-section, so the focus here will be on the GPU mapping part.",4.10689852286733
"Who is the target audience for OLCF GPU Hackathons?
","If you want/need to get your code running (or optimized) on a GPU-accelerated system, these hackathons offer a unique opportunity to set aside the time, surround yourself with experts in the field, and push toward your development goals. By the end of the event, each team should have part of their code running (or more optimized) on GPUs, or at least have a clear roadmap of how to get there.

<p style=""font-size:20px""><b>Target audience</b></p>",4.4826735519474505
"Who is the target audience for OLCF GPU Hackathons?
","If you would like more information about how you can volunteer to mentor a team at an upcoming Open/GPU hackathon, please visit our Become a Mentor page.

<p style=""font-size:20px""><b>Who can I contact with questions?</b></p>

If you have any questions about the OLCF GPU Hackathon Series, please contact OLCF Help at (help@olcf.ornl.gov).",4.365879259194355
"Who is the target audience for OLCF GPU Hackathons?
","I used html for the section headings to avoid individual entries in the associated menu (TP)



Each year, the Oak Ridge Leadership Computing Facility (OLCF) works with our vendor partners to organize a series of GPU hackathons at a number of host locations around the world.

<p style=""font-size:20px""><b>What is a GPU hackathon?</b></p>",4.312728789219168
"How does FireWorks handle workflow failures?
","FireWorks is a free, open-source tool for defining, managing, and executing workflows. Complex workflows can be defined using Python, JSON, or YAML, are stored using MongoDB, and can be monitored through a built-in web interface. Workflow execution can be automated over arbitrary computing resources, including those that have a queueing system. FireWorks has been used to run millions of workflows encompassing tens of millions of CPU-hours across diverse application areas and in long-term production projects over the span of multiple years.",4.280818795921301
"How does FireWorks handle workflow failures?
","To learn more about FireWorks, please refer to its extensive online documentation.

Before using FireWorks itself, you will need a MongoDB service running on https://docs.olcf.ornl.gov/systems/fireworks.html#Slate<slate>. A tutorial to deploy MongoDB is available in the Slate documentation.

You will need to know the connection information for both MongoDB so that FireWorks can be configured to connect to it.

Then, to use FireWorks on Summit, load the module as shown below:

$ module load workflows
$ module load fireworks/2.0.2",4.056221004865939
"How does FireWorks handle workflow failures?
","module load workflows
module load fireworks/2.0.2

# Edit the following line to match your own MongoDB connection string.
export MONGODB_URI=""mongodb://admin:password@apps.marble.ccs.ornl.gov:32767/test""

jsrun -n 1 python3 demo.py

Finally, submit the batch job to LSF by executing the following command from a Summit login node:

$ bsub fireworks_demo.lsf

Congratulations! Once the batch job completes, you will find new directories beginning with launcher_ and containing FW.json files that detail exactly what happened.",4.046020496484327
"How are threads in a block scheduled in Frontier?
","Because a Frontier compute node has two hardware threads available (2 logical cores per physical core), this enables the possibility of multithreading your application (e.g., with OpenMP threads). Although the additional hardware threads can be assigned to additional MPI tasks, this is not recommended. It is highly recommended to only use 1 MPI task per physical core and to use OpenMP threads instead on any additional logical cores gained when using both hardware threads.",4.130875862748983
"How are threads in a block scheduled in Frontier?
","The 2 GCDs on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The OLCF Director's Discretionary (DD) program allocates approximately 10% of the available Frontier hours in a calendar year. Frontier is allocated in *node* hours, and a typical DD project is awarded between 15,000 - 20,000 *node* hours. For more information about Frontier, please visit the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#frontier-user-guide.",4.109879436147062
"How are threads in a block scheduled in Frontier?
","Each block (or workgroup) of threads is assigned to a single Compute Unit i.e. a single block won’t be split across multiple CUs. The threads in a block are scheduled in units of 64 threads called wavefronts (similar to warps in CUDA, but warps only have 32 threads instead of 64). When launching a kernel, up to 64KB of block level shared memory called the Local Data Store (LDS) can be statically or dynamically allocated. This shared memory between the threads in a block allows the threads to access block local data with much lower latency compared to using the HBM since the data is in the",4.106239806803233
"How will I be notified about the status of my quantum computing application?
","Once your application is evaluated and approved, you will be notified via email of your account creation, and the quantum resource vendor will be contacted with instructions to grant you access.

You can check the general status of your application at any time using the myOLCF self-service portal's account status page. For more information, see our https://docs.olcf.ornl.gov/services_and_applications/myolcf/overview.html page. If you need to make further inquiries about your application, you may email our Accounts Team at accounts@ccs.ornl.gov.",4.34027889036882
"How will I be notified about the status of my quantum computing application?
","After submitting the OLCF quantum account application and receiving approval, you will receive an email from Quantinuum inviting you to create your quantum account. Once logged in, users will have access to Quantinuum's User Interface, https://um.qapi.quantinuum.com, their online platform for managing jobs and accessing the available quantum systems, including the System Model H1, via the cloud. From the UI, users can view system status and upcoming system availability, as well as monitor batch submissions and job history. Information on using the quantum resources via Jupyter notebooks is",4.3248909889964
"How will I be notified about the status of my quantum computing application?
","After submitting the OLCF quantum account application and receiving approval, you will receive an email from IonQ inviting you to create your quantum account. Once logged in, users will have access to IonQ's User Interface, https://cloud.ionq.com/, their online platform for managing jobs and accessing the available quantum systems, including the Harmony and Aria-1 systems, as well as the simulator, via the cloud. From the UI, users can view system status and upcoming system availability, as well as monitor batch submissions and job history.",4.313689031119445
"How do I build my source code in Slate?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",4.157899871574833
"How do I build my source code in Slate?
","Openshift will automatically pull and build your source. If you make a change and need an updated build simply navigate to the build option on the menu to the left and select your project and click Start Build in the upper right.

The first step to creating the the build is to create a build config. This is done in one of three ways depending on how your code is structured. If you have a git repository already configured and it is public then the command

oc new-build .

will create your build config from that repository.",4.111833728968938
"How do I build my source code in Slate?
","Source to image is a tool to build docker formatted container images. This is accomplished by injecting source code into a container image and then building a new image. The new image will contain the source code ready to run, via the $ docker run command inside the newly built image.

Using the S2I model of building images, it is possible to have your source hosted on a git repository and then pulled and built by Openshift.

Click on the newly created project and then on the blue Add to Project button on the center of the page.",4.101044656651706
"What is the purpose of the `LD_LIBRARY_PATH` environment variable in Crusher?
","Crusher users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.114952667281277
"What is the purpose of the `LD_LIBRARY_PATH` environment variable in Crusher?
","# EXPORT PATHS (can add to .bashrc / .bash_profile if desired)
$ export LD_LIBRARY_PATH=""/ccs/home/YOUR_USERNAME/lapackblas:$LD_LIBRARY_PATH""
$ export LD_LIBRARY_PATH=""/ccs/home/YOUR_USERNAME/ffi/lib64:$LD_LIBRARY_PATH""
$ export LD_LIBRARY_PATH=""/ccs/home/YOUR_USERNAME/zmq/lib:$LD_LIBRARY_PATH""
$ export PATH=""/ccs/home/YOUR_USERNAME/rigetti/forest-sdk_2.23.0-linux-barebones:$PATH""

# VERIFY QUILC / QVM INSTALL

$ quilc —-version
1.23.0 [e6c0939]
$ qvm —-version
1.17.1 [cf3f91f]",4.107865118432937
"What is the purpose of the `LD_LIBRARY_PATH` environment variable in Crusher?
","Environment modules are provided through Lmod, a Lua-based module system for dynamically altering shell environments. By managing changes to the shell’s environment variables (such as PATH, LD_LIBRARY_PATH, and PKG_CONFIG_PATH), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.

The interface to Lmod is provided by the module command:",4.092637538888129
"How will the Slurm core specialization default change on Crusher?
","Slurm's core specialization default will change: Slurm --core-spec or -S value will be set to 8. This will provide a symmetric distribution of cores per GCD to the application and will reserve one core per L3 cache region. After the outage, the default number of cores available to each GCD on a node will be 7. To change from the new default value, you can set --core-spec or -S in your job submission.

The default NIC mapping will be updated to MPICH_OFI_NIC_POLICY=NUMA to address known issues described in OLCFDEV-192 and OLCFDEV-1366.",4.339426939603822
"How will the Slurm core specialization default change on Crusher?
","This section describes how to run programs on the Crusher compute nodes, including a brief overview of Slurm and also how to map processes and threads to CPU cores and GPUs.

Slurm is the workload manager used to interact with the compute nodes on Crusher. In the following subsections, the most commonly used Slurm commands for submitting, running, and monitoring jobs will be covered, but users are encouraged to visit the official documentation and man pages for more information.",4.236735102634536
"How will the Slurm core specialization default change on Crusher?
","Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.",4.236234530571647
"How does Orion handle file striping?
","On Alpine, there was no user-exposed concept of file striping, the process of dividing a file between the storage elements of the filesystem. Orion uses a feature called Progressive File Layout (PFL) that changes the striping of files as they grow. Because of this, we ask users not to manually adjust the file striping. If you feel the default striping behavior of Orion is not meeting your needs, please contact help@olcf.ornl.gov.

As with Alpine, files older than 90 days are purged from Orion.  Please plan your data management and lifecycle at OLCF before generating the data.",4.53494975875054
"How does Orion handle file striping?
","On Summit, there is no concept of striping from the user point of view, the user uses the Alpine storage without the need to declare the striping for files/directories. The GPFS will handle the workload, the file system was tuned during the installation.",4.185502294974642
"How does Orion handle file striping?
","Frontier mounts Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Frontier will not mount Alpine when Orion is in production.",4.093001200294956
"How do I optimize my code for performance when working with abc123_mde?
","There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.",3.96163844328498
"How do I optimize my code for performance when working with abc123_mde?
","Most users will find batch jobs an easy way to use the system, as they allow you to ""hand off"" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.",3.9145844510864567
"How do I optimize my code for performance when working with abc123_mde?
","Most users will find batch jobs to be the easiest way to interact with the system, since they permit you to hand off a job to the scheduler and then work on other tasks; however, it is sometimes preferable to run interactively on the system. This is especially true when developing, modifying, or debugging a code.",3.912993624769119
"Can I use ArgoCD to manage resources in a single project on Slate?
","The Red Hat OpenShift GitOps operator is already installed onto each of the Slate clusters. However, to use ArgoCD an instance will need to be added into a project namespace. If one is to use ArgoCD to manage resources in a single project, then the ArgoCD instance could be located in the same project as the resources being managed. However, if the application being managed is resource (CPU and memory) sensitive or if resources in multiple projects are to be managed, it may be better to have the ArgoCD instance deployed into a separate project to allow for better control of resources allocated",4.350467676022949
"Can I use ArgoCD to manage resources in a single project on Slate?
",allow for better control of resources allocated to ArgoCD.,4.343477704796022
"Can I use ArgoCD to manage resources in a single project on Slate?
","ArgoCD has successfully accessed the namespace, realized that the state of the resources in the namespace are OutOfSync with the resource requirements of the code repository, and started the Syncing process to create and/or modify resources in the namespace to match the desired configuration. Clicking on the application tile will reveal more detailed information on the process:

Image of ArgoCD application tile detailed information.",4.228199249825583
"How can I get access to the OLCF's ParaView server?
","After setting up and installing ParaView, you can connect to OLCF systems remotely to visualize your data interactively through ParaView's GUI. To do so, go to File→Connect and select either ORNL Andes or ORNL Summit (provided they were successfully imported -- as outlined in https://docs.olcf.ornl.gov/systems/paraview.html#paraview-install-setup). Next, click on Connect and change the values in the Connection Options box.",4.528495360832806
"How can I get access to the OLCF's ParaView server?
","After installing, you must give ParaView the relevant server information to be able to connect to OLCF systems (comparable to VisIt's system of host profiles). The following provides an example of doing so. Although several methods may be used, the one described should work in most cases.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.

For Windows clients, it is necessary to install PuTTY to create an ssh connection in step 2.

Step 1: Save the following servers.pvsc file to your local computer",4.495284309397861
"How can I get access to the OLCF's ParaView server?
","ParaView was developed to analyze extremely large datasets using distributed memory computing resources. The OLCF provides ParaView server installs on Andes and Summit to facilitate large scale distributed visualizations. The ParaView server running on Andes and Summit may be used in a headless batch processing mode or be used to drive a ParaView GUI client running on your local machine.

For a tutorial of how to get started with ParaView on Andes, see our ParaView at OLCF Tutorial.",4.428570637442284
"What is the name of the memory model used by Frontier's CDNA2-based architecture MI250X cards?
","The Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.530550839645753
"What is the name of the memory model used by Frontier's CDNA2-based architecture MI250X cards?
","The Crusher system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.35554058563254
"What is the name of the memory model used by Frontier's CDNA2-based architecture MI250X cards?
","The AMD MI250X and operating system on Frontier supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Frontier. In the following section we'll discuss how the heterogenous memory space on a Frontier node is surfaced within your application.",4.282755285928436
"What are some potential drawbacks of using CuPy for data science tasks?
","As noted in AMD+CuPy limitations, data sizes explored here hang. So, this section currently does not apply to Frontier.

Now that you know how to use CuPy, time to see the actual benefits that CuPy provides for large datasets. More specifically, let's see how much faster CuPy can be than NumPy on Summit. You won't need to fix any errors; this is mainly a demonstration on what CuPy is capable of.

There are a few things to consider when running on GPUs, which also apply to using CuPy:

Higher precision means higher cost (time and space)

The structuring of your data is important",4.388650296878824
"What are some potential drawbacks of using CuPy for data science tasks?
","datasets, but represents what CuPy is capable of at this scale.",4.309900929716916
"What are some potential drawbacks of using CuPy for data science tasks?
","CuPy is a library that implements NumPy arrays on NVIDIA GPUs by utilizing CUDA Toolkit libraries like cuBLAS, cuRAND, cuSOLVER, cuSPARSE, cuFFT, cuDNN and NCCL. Although optimized NumPy is a significant step up from Python in terms of speed, performance is still limited by the CPU (especially at larger data sizes) -- this is where CuPy comes in. Because CuPy's interface is nearly a mirror of NumPy, it acts as a replacement to run existing NumPy/SciPy code on NVIDIA CUDA platforms, which helps speed up calculations further. CuPy supports most of the array operations that NumPy provides,",4.308143328406659
"How does the FLOPS/s of a kernel on Crusher compare to the theoretical maximum FLOPS/s?
","When SQ_INSTS_VALU_MFMA_MOPS_*_F64 instructions are used, then 47.8 TF/s is considered the theoretical maximum FLOPS/s. If only SQ_INSTS_VALU_<ADD,MUL,TRANS> are found, then 23.9 TF/s is the theoretical maximum FLOPS/s. Then, we divide the number of FLOPS by the elapsed time of the kernel to find FLOPS per second. This is found from subtracting the rocprof metrics EndNs by BeginNs, provided by --timestamp on, then converting from nanoseconds to seconds by dividing by 1,000,000,000 (power(10,9)).",4.357260088603201
"How does the FLOPS/s of a kernel on Crusher compare to the theoretical maximum FLOPS/s?
","When SQ_INSTS_VALU_MFMA_MOPS_*_F64 instructions are used, then 47.8 TF/s is considered the theoretical maximum FLOPS/s. If only SQ_INSTS_VALU_<ADD,MUL,TRANS> are found, then 23.9 TF/s is the theoretical maximum FLOPS/s. Then, we divide the number of FLOPS by the elapsed time of the kernel to find FLOPS per second. This is found from subtracting the rocprof metrics EndNs by BeginNs, provided by --timestamp on, then converting from nanoseconds to seconds by dividing by 1,000,000,000 (power(10,9)).",4.357260088603201
"How does the FLOPS/s of a kernel on Crusher compare to the theoretical maximum FLOPS/s?
","theoretical peak is determined by the hardware specifications and is not attainable in practice. attaiable peak is the performance as measured by in-situ microbenchmarks designed to best utilize the hardware. achieved performance is what the profiled application actually achieves.

The theoretical roofline can be constructed as:

FLOPS_{peak} = minimum(ArithmeticIntensity * BW_{HBM}, TheoreticalFLOPS)

On Crusher, the memory bandwidth for HBM is 1.6 TB/s, and the theoretical peak floating-point FLOPS/s when using vector registers is calculated by:",4.3291071486624215
"How can I use TAU to profile the serial, MPI, MPI+OpenMP, and MPI+OpenACC versions of the MiniWeather application on Summit?
","Add the following in your submission file:

export TAU_METRICS=TIME
export TAU_PROFILE=1
export TAU_TRACK_MESSAGE=1
export TAU_COMM_MATRIX=1
jsrun -n 6 -r 6 --smpiargs=""-gpu"" -g 1  tau_exec -T mpi,pgi,pdt -openacc ./miniWeather_mpi_openacc

We declare to TAU to profile the MPI with PDT support through the -T parameters, as well as using the pgi tag for the TAU makefile and OpenACC.

CUPTI metrics for OpenACC are not yet supported for TAU.

When the execution of the instrumented application finishes, there is one directory for each TAU_METRICS declaration with the format MULTI__",4.435921042397167
"How can I use TAU to profile the serial, MPI, MPI+OpenMP, and MPI+OpenACC versions of the MiniWeather application on Summit?
","Below, we'll look at using TAU to profile each case.

Edit the cmake_summit_pgi.sh and replace mpic++ with tau_cxx.sh. This applies only for the non-GPU versions.

TAU works with special TAU makefiles to declare what programming models are expected from the application:

The available makefiles are located inside TAU installation:",4.414868690512234
"How can I use TAU to profile the serial, MPI, MPI+OpenMP, and MPI+OpenACC versions of the MiniWeather application on Summit?
","Next, we will look at using the paraprof tool for the MPI version of MiniWeather.

For the MPI version, we should use a makefile with MPI. The compilation could fail if the makefile supports MPI+OpenMP, but the code doesn't include any OpenMP calls. Moreover, with TAU_OPTIONS declared below, we will add options to the linker.

$ module load tau
$ export TAU_MAKEFILE=/sw/summit/tau/tau2/ibm64linux/lib/Makefile.tau-pgi-papi-mpi-cupti-pdt-pgi
$ export TAU_OPTIONS='-optLinking=-lpnetcdf -optVerbose'
$ make mpi",4.414442354444838
"What is the difference between hipMallocManaged and hipHostMalloc in terms of memory allocation in Crusher?
","Finally, the following table summarizes the nature of the memory returned based on the flag passed as argument to hipMallocManaged() and the use of CPU regular malloc() routine with the possible use of hipMemAdvise().

| API | MemAdvice | Result | | --- | --- | --- | | hipMallocManaged() |  | Fine grained | | hipMallocManaged() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained | | malloc() |  | Fine grained | | malloc() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained |",4.284752642206094
"What is the difference between hipMallocManaged and hipHostMalloc in terms of memory allocation in Crusher?
","Finally, the following table summarizes the nature of the memory returned based on the flag passed as argument to hipMallocManaged() and the use of CPU regular malloc() routine with the possible use of hipMemAdvise().

| API | MemAdvice | Result | | --- | --- | --- | | hipMallocManaged() |  | Fine grained | | hipMallocManaged() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained | | malloc() |  | Fine grained | | malloc() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained |",4.284752642206094
"What is the difference between hipMallocManaged and hipHostMalloc in terms of memory allocation in Crusher?
","+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+
| hipHostMalloc                               | CPU DDR4                  | Local read/write                           | Zero copy read/write over Infinity Fabric          |
+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+",4.27178640741735
"What is the peak host-to-device (H2D) and device-to-host (D2H) bandwidth between the CPU and GCD?
","The 50-GB/s peak bandwidth stated above is for data transfers in a single direction. However, this bandwidth can be achieved in both directions simultaneously, giving a peak ""bi-directional"" bandwidth of 100 GB/s between processors.

The figure below shows a schematic of the NVLink connections between the CPU and GPUs on a single socket of a Summit node.",4.258192929538293
"What is the peak host-to-device (H2D) and device-to-host (D2H) bandwidth between the CPU and GCD?
","The 2 GCDs on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The GCDs on different MI250X are connected with Infinity Fabric GPU-GPU in the arrangement shown in the Crusher Node Diagram below, where the peak bandwidth ranges from 50-100 GB/s based on the number of Infinity Fabric connections between individual GCDs.",4.231409816729494
"What is the peak host-to-device (H2D) and device-to-host (D2H) bandwidth between the CPU and GCD?
","Each Frontier compute node consists of [1x] 64-core AMD ""Optimized 3rd Gen EPYC"" CPU (with 2 hardware threads per physical core) with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X, each with 2 Graphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can think of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth memory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU, allowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of 36+36 GB/s. The 2 GCDs on the same MI250X are connected with Infinity",4.194696051131899
"What is the advantage of using Singularity instead of Docker?
","Singularity is a container framework from Sylabs. On Summit, we will use Singularity solely as the runtime. We will convert the tar files of the container images Podman creates into sif files, store those sif files on GPFS, and run them with Jsrun. Singularity also allows building images but ordinary users cannot utilize that on Summit due to additional permissions not allowed for regular users.",4.286997018724572
"What is the advantage of using Singularity instead of Docker?
","Users can build container images with Podman, convert it to a SIF file, and run the container using the Singularity runtime. This page is intended for users with some familiarity with building and running containers.

Users will make use of two applications on Summit - Podman and Singularity - in their container build and run workflow. Both are available without needing to load any modules.",4.260927159565016
"What is the advantage of using Singularity instead of Docker?
","You can now create a Singularity sif file with singularity build --disable-cache --docker-login simple.sif docker://docker.io/<username>/simple.

This will ask you to enter your Docker username and password again for Singularity to download the image from Dockerhub and convert it to a sif file.",4.216796225483886
"How can the Volume Snapshot be accessed?
","Now, if the data in the PersistentVolume becomes corrupted or lost in some way, we can create a new PersistentVolume that is identical to the original PersistentVolume at the time the VolumeSnapshot was created.

To do this, create a PersistentVolumeClaim that contains a dataSource field in the Pod's spec. An example follows:",4.1737572152505855
"How can the Volume Snapshot be accessed?
","Add Storage Menu

You should see a green popup appear in the upper right saying that the storage was added. This should additionally trigger a new deployment. To make sure a new deployment happened look at the Created time of the top most deployment.

A PersistentVolume is backed up by creating a VolumeSnapshot object. The VolumeSnapshot will create a point in time backup of your PersistentVolume and is something that you would likely do before an upgrade.

A Volume Snapshot will bind to a PersistentVolumeClaim. An example PersistentVolumeClaim to bind a VolumeSnapshot to follows:",4.167902277386244
"How can the Volume Snapshot be accessed?
","The backups are accessed via the .snapshot subdirectory. Note that ls alone (or even ls -a) will not show the .snapshot subdirectory exists, though ls .snapshot will show its contents. The .snapshot feature is available in any subdirectory of your project home directory and will show the online backups available for that subdirectory.

To retrieve a backup, simply copy it into your desired destination with the cp command.",4.153274781241414
"How does the OLCF Policy adjust the apparent submit time of a job?
","Allocation Overuse Policy

Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.459256798740085
"How does the OLCF Policy adjust the apparent submit time of a job?
","Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.441462678225975
"How does the OLCF Policy adjust the apparent submit time of a job?
","Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used. For example, consider that job1 is submitted at the same time as job2. The project associated with",4.3684377380245865
"How do I ensure that my code is compatible with Score-P?
","To instrument your code, you need to compile the code using the Score-P instrumentation command (scorep), which is added as a prefix to your compile statement. In most cases the Score-P instrumentor is able to automatically detect the programming paradigm from the set of compile and link options given to the compiler. Some cases will, however, require some additional link options within the compile statement e.g. CUDA instrumentation.

Below are some basic examples of the different instrumentation scenarios:",4.32238726350347
"How do I ensure that my code is compatible with Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi-processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations.

For detailed information about using Score-P on Summit and the builds available, please see the Score-P Software Page.",4.2466072538888495
"How do I ensure that my code is compatible with Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations. It works in combination with Periscope, Scalasca, Vampir, and Tau.

Steps in a typical Score-P workflow to run on https://docs.olcf.ornl.gov/systems/Scorep.html#summit-user-guide:",4.2388818772452055
"How can I resolve the issue with MPI_Finalize hanging when using Spectrum MPI 10.2.0.10 and ROMIO 3.2.1?
","There is a known issue in Spectrum MPI 10.2.0.10 provided by the spectrum-mpi/10.2.0.10-20181214 modulefile that causes a hang in MPI_Finalize when ROMIO 3.2.1 is being used and the darshan-runtime modulefile is loaded. The recommended and default Spectrum MPI version as of March 3, 2019 is Spectrum MPI 10.2.0.11 provided by the spectrum-mpi/10.2.0.11-20190201 modulefile. If you are seeing this issue, please make sure that you are using the latest version of Spectrum MPI. If you need to use a previous version of Spectrum MPI, your options are:

Unload the darshan-runtime modulefile.",4.626443611429965
"How can I resolve the issue with MPI_Finalize hanging when using Spectrum MPI 10.2.0.10 and ROMIO 3.2.1?
","A performance issue has been identified using parallel HDF5 with the default version of ROMIO provided in spectrum-mpi/10.2.0.10-20181214. To fully take advantage of parallel HDF5, users need to switch to the newer version of ROMIO and use ROMIO hints. The following shows recommended variables and hints for a 2 node job. Please note that hints must be tuned for a specific job.

$ module unload darshan-runtime
$ export OMPI_MCA_io=romio321
$ export ROMIO_HINTS=./my_romio_hints
$ cat $ROMIO_HINTS
romio_cb_write enable
romio_ds_write enable
cb_buffer_size 16777216
cb_nodes 2",4.279014811638183
"How can I resolve the issue with MPI_Finalize hanging when using Spectrum MPI 10.2.0.10 and ROMIO 3.2.1?
","If on-node MPI communication between GPUs is critical to your application performance, option B is recommended but you’ll need to set the GPU affinity manually. This could be done with an API call in your code (e.g. cudaSetDevice), or by using a wrapper script.

We have seen occasional errors from batch jobs with multiple simultaneous backgrounded jsrun commands. Jobs may see pmix errors during the noted failures.



The following issue was resolved with the software default changes from March 12, 2019 that set Spectrum MPI 10.2.0.11 (20190201) as default and moved ROMIO to version 3.2.1:",4.236559409216843
"What is the purpose of the `./cmake_summit_pgi.sh` script?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.208905336229773
"What is the purpose of the `./cmake_summit_pgi.sh` script?
","$ module load tau
$ export TAU_MAKEFILE=/sw/summit/tau/tau2/ibm64linux/lib/Makefile.tau-pgi-papi-mpi-cupti-pdt-pgi
$ export TAU_OPTIONS='-optLinking=-lpnetcdf -optVerbose'
$ ./cmake_summit_pgi.sh
$ make serial

If there were no MPI headers, you should select the makefile /sw/summit/tau/tau2//ibm64linux/lib/Makefile.tau-pgi-papi-pdt-pgi or if you don't want PDT support, /sw/summit/tau/tau2//ibm64linux/lib/Makefile.tau-pgi-papi-pgi Add to your submission script the TAU variables that you want to use (or uncomment them below). By default the TAU will apply profiling, and not apply tracing.",4.170824164849635
"What is the purpose of the `./cmake_summit_pgi.sh` script?
","Below, we'll look at using TAU to profile each case.

Edit the cmake_summit_pgi.sh and replace mpic++ with tau_cxx.sh. This applies only for the non-GPU versions.

TAU works with special TAU makefiles to declare what programming models are expected from the application:

The available makefiles are located inside TAU installation:",4.13950627629346
"Can I run a Crusher job step in the background?
","To execute multiple job steps concurrently, standard UNIX process backgrounding can be used by adding a & at the end of the command. This will return control to the job script and execute the next command immediately, allowing multiple job launches to start at the same time. The jsruns will not share core/gpu resources in this configuration. The batch node allocation is equal to the sum of those of each jsrun, and the total walltime must be equal to or greater than that of the longest running jsrun task.",4.160542975389353
"Can I run a Crusher job step in the background?
","This section describes how to run programs on the Crusher compute nodes, including a brief overview of Slurm and also how to map processes and threads to CPU cores and GPUs.

Slurm is the workload manager used to interact with the compute nodes on Crusher. In the following subsections, the most commonly used Slurm commands for submitting, running, and monitoring jobs will be covered, but users are encouraged to visit the official documentation and man pages for more information.",4.146725739875705
"Can I run a Crusher job step in the background?
","Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.",4.131682850105668
"Is it recommended to use multiple MPI tasks per physical core on Frontier?
","Because a Frontier compute node has two hardware threads available (2 logical cores per physical core), this enables the possibility of multithreading your application (e.g., with OpenMP threads). Although the additional hardware threads can be assigned to additional MPI tasks, this is not recommended. It is highly recommended to only use 1 MPI task per physical core and to use OpenMP threads instead on any additional logical cores gained when using both hardware threads.",4.503322418679413
"Is it recommended to use multiple MPI tasks per physical core on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.48041687981456
"Is it recommended to use multiple MPI tasks per physical core on Frontier?
","As has been pointed out previously in the Frontier documentation, notice that GPUs are NOT mapped to MPI ranks in sequential order (e.g., MPI rank 0 is mapped to physical CPU cores 1-7 and GPU 4, MPI rank 1 is mapped to physical CPU cores 9-15 and GPU 5), but this IS expected behavior. It is simply a consequence of the Frontier node architectures as shown in the Frontier Node Diagram and subsequent https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Note on NUMA domains <numa-note>.

Example 2: 1 MPI rank with 7 CPU cores and 1 GPU (single-node)",4.3607551732986805
"What is the minimum value of the ""tensor_precision_fu_utilization"" metric for the kernel used in the example program?
","When using NVIDIA’s nvprof profiler, one should add the -m tensor_precision_fu_utilization option to measure Tensor Core utilization. Below is the output from measuring this metric on one of the example programs.

$ nvprof -m tensor_precision_fu_utilization ./simpleCUBLAS
==43727== NVPROF is profiling process 43727, command: ./simpleCUBLAS
GPU Device 0: ""Tesla V100-SXM2-16GB"" with compute capability 7.0",4.340633871650759
"What is the minimum value of the ""tensor_precision_fu_utilization"" metric for the kernel used in the example program?
","When attempting to use Tensor Cores it is useful to measure and confirm that the Tensor Cores are being used within your code. For implicit use via a library like cuBLAS, the Tensor Cores will only be used above a certain threshold, so Tensor Core use should not be assumed. The NVIDIA Tools provide a performance metric to measure Tensor Core utilization on a scale from 0 (Idle) to 10 (Max) utilization.",4.177558061106571
"What is the minimum value of the ""tensor_precision_fu_utilization"" metric for the kernel used in the example program?
","simpleCUBLAS test running..
simpleCUBLAS test passed.
==43727== Profiling application: ./simpleCUBLAS
==43727== Profiling result:
==43727== Metric result:
Invocations                               Metric Name                           Metric Description         Min         Max         Avg
Device ""Tesla V100-SXM2-16GB (0)""
    Kernel: volta_h884gemm_128x64_ldg8_nn
          1           tensor_precision_fu_utilization   Tensor-Precision Function Unit Utilization     Low (3)     Low (3)     Low (3)",4.11422664480037
"How do I know if my application is suitable for using hardware-based FP atomics instructions on a MI250X processor?
","The fast hardware-based Floating point (FP) atomic operations available on MI250X are assumed to be working on coarse grained memory regions; when these instructions are applied to a fine-grained memory region, they will silently produce a no-op. To avoid returning incorrect results, the compiler never emits hardware-based FP atomics instructions by default, even when applied to coarse grained memory regions. Currently, users can use the -munsafe-fp-atomics flag to force the compiler to emit hardware-based FP atomics. Using hardware-based FP atomics translates in a substantial performance",4.421586837431136
"How do I know if my application is suitable for using hardware-based FP atomics instructions on a MI250X processor?
","The fast hardware-based Floating point (FP) atomic operations available on MI250X are assumed to be working on coarse grained memory regions; when these instructions are applied to a fine-grained memory region, they will silently produce a no-op. To avoid returning incorrect results, the compiler never emits hardware-based FP atomics instructions by default, even when applied to coarse grained memory regions. Currently, users can use the -munsafe-fp-atomics flag to force the compiler to emit hardware-based FP atomics. Using hardware-based FP atomics translates in a substantial performance",4.421586837431136
"How do I know if my application is suitable for using hardware-based FP atomics instructions on a MI250X processor?
","Users applying floating point atomic operations (e.g., atomicAdd) on memory regions allocated via regular hipMalloc() can safely apply the -munsafe-fp-atomics flags to their codes to get the best possible performance and leverage hardware supported floating point atomics. Atomic operations supported in hardware on non-FP datatypes  (e.g., INT32) will work correctly regardless of the nature of the memory region used.",4.22167238168629
"Can I use DDT to debug applications that are using MPI?
","Linaro DDT is an advanced debugging tool used for scalar, multi-threaded, and large-scale parallel applications. In addition to traditional debugging features (setting breakpoints, stepping through code, examining variables), DDT also supports attaching to already-running processes and memory debugging. In-depth details of DDT can be found in the Official DDT User Guide, and instructions for how to use it on OLCF systems can be found on the https://docs.olcf.ornl.gov/software/debugging/index.html page. DDT is the OLCF's recommended debugging software for large parallel applications.",4.32170910149334
"Can I use DDT to debug applications that are using MPI?
","Linaro DDT is an advanced debugging tool used for scalar, multi-threaded, and large-scale parallel applications. In addition to traditional debugging features (setting breakpoints, stepping through code, examining variables), DDT also supports attaching to already-running processes and memory debugging. In-depth details of DDT can be found in the Official DDT User Guide, and instructions for how to use it on OLCF systems can be found on the https://docs.olcf.ornl.gov/software/debugging/index.html page. DDT is the OLCF's recommended debugging software for large parallel applications.",4.32170910149334
"Can I use DDT to debug applications that are using MPI?
","Linaro DDT is an advanced debugging tool used for scalar, multi-threaded, and large-scale parallel applications. In addition to traditional debugging features (setting breakpoints, stepping through code, examining variables), DDT also supports attaching to already-running processes and memory debugging. In-depth details of DDT can be found in the Official DDT User Guide, and instructions for how to use it on OLCF systems can be found on the https://docs.olcf.ornl.gov/software/debugging/index.html page. DDT is the OLCF's recommended debugging software for large parallel applications.",4.32170910149334
"How can I ensure that my srun commands are executed in a specific order?
",The following srun options will be used in the examples below. See man srun for a complete list of options and more information.,4.2971770561255775
"How can I ensure that my srun commands are executed in a specific order?
",The following srun options will be used in the examples below. See man srun for a complete list of options and more information.,4.2971770561255775
"How can I ensure that my srun commands are executed in a specific order?
","Using srun

By default, commands will be executed on the job’s primary compute node, sometimes referred to as the job’s head node. The srun command is used to execute an MPI binary on one or more compute nodes in parallel.

srun accepts the following common options:

| -N | Minimum number of nodes | | --- | --- | | -n | Total number of MPI tasks | | --cpu-bind=no | Allow code to control thread affinity | | -c | Cores per MPI task | | --cpu-bind=cores | Bind to cores |

If you do not specify the number of MPI tasks to srun via -n, the system will default to using only one task per node.",4.293088007456562
"How do I manage my OLCF user account?
","When all of the above steps are completed, your user account will be created and you will be notified by email. Now that you have a user account and it has been associated with a project, you're ready to get to work. This website provides extensive documentation for OLCF systems, and can help you efficiently use your project's allocation. We recommend reading the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#System User Guides<system-user-guides> for the machines you will be using often.",4.4761627270427615
"How do I manage my OLCF user account?
","If you already have a user account at the OLCF, your existing credentials can be leveraged across multiple projects.

If your user account has an associated RSA SecurID (i.e. you have an ""OLCF Moderate"" account), you gain access to another project by logging in to the myOLCF self-service portal and filling out the application under My Account > Join Another Project. For more information, see the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#myOLCF self-service portal documentation<myolcf-overview>.",4.457635238088421
"How do I manage my OLCF user account?
","Please see the https://docs.olcf.ornl.gov/systems/index.html#OLCF Applying for a User Account<applying-for-a-user-account> section of this site to request a new account and join an existing project.  Once submitted, the OLCF Accounts team will help walk you through the process.",4.446796052736549
"How can I optimize my job's I/O performance on Frontier?
","Each compute node on Frontier has [2x] 1.92TB Non-Volatile Memory (NVMe) storage devices (SSDs), colloquially known as a ""Burst Buffer"" with a peak sequential performance of 5500 MB/s (read) and 2000 MB/s (write). The purpose of the Burst Buffer system is to bring improved I/O performance to appropriate workloads. Users are not required to use the NVMes. Data can also be written directly to the parallel filesystem.



The NVMes on Frontier are local to each node.",4.2237086492995175
"How can I optimize my job's I/O performance on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.199100069157335
"How can I optimize my job's I/O performance on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.160033463336951
"How do I troubleshoot issues with HAProxy in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",3.984801020914732
"How do I troubleshoot issues with HAProxy in Slate?
","The following items need to be established before deploying applications on Slate systems (Marble or Onyx OpenShift clusters).

Follow https://docs.olcf.ornl.gov/systems/helm_prerequisite.html#slate_getting_started if you do not already have a project/namespace established on Onyx or Marble.

It is recommended to use Helm version 3.

Helm enables application deployment via Helm Charts. The high level Helm Architecture docs are also a great reference for understanding Helm.

Like oc, Helm is a single binary executable.

This can be installed on macOS with Homebrew :

$ brew install helm",3.979005946493605
"How do I troubleshoot issues with HAProxy in Slate?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",3.9401522567488834
"How can I fix the SSH failure in Summit?
","If VisIt never asks for your passcode and hangs after trying to connect to one of our systems, then this means VisIt is unable to establish a proper SSH connection. Here are a few different approaches to fix this issue:

Double check your host profile, especially the ""remote host name"", ""host name aliases"", and ""tunnel data connections through SSH"" sections.

If you are using a VPN (including GlobalProtect VPN), try turning it off.",4.128535786082538
"How can I fix the SSH failure in Summit?
",For Summit:,4.120998004603419
"How can I fix the SSH failure in Summit?
","settings on user ~/.ssh/config files. (The batch system uses SSH, and the improper settings cause SSH to fail.) If you notice your jobs alternating between PEND and RUN, you might want to check permissions of your ~/.ssh/config file to make sure it does not have write permission for ""group"" or ""other"". (A setting of read/write for the user and no other permissions, which can be set with chmod 600 ~/.ssh/config, is recommended.)",4.070228155845016
"How do I create a custom virtual environment using venv on Frontier?
","Currently, Frontier does NOT have an Anaconda/Conda module.  To use conda, you will have to download and install Miniconda on your own (see our https://docs.olcf.ornl.gov/software/python/miniconda.html). Alternatively, you can use Python's native virtual environments venv feature with the cray-python module (as we will explore in the guides below).  For more details on venv, see Python's Official Documentation.  Contact help@olcf.ornl.gov if conda is required for your workflow, or if you have any issues.",4.310578176392816
"How do I create a custom virtual environment using venv on Frontier?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.

This guide teaches you how to build CuPy from source into a custom virtual environment. On Summit and Andes, this is done using conda, while on Frontier this is done using venv.

In this guide, you will:

Learn how to install CuPy

Learn the basics of CuPy

Compare speeds to NumPy

OLCF Systems this guide applies to:

Summit

Frontier

Andes",4.294762543105501
"How do I create a custom virtual environment using venv on Frontier?
","Currently, Crusher and Frontier do NOT have Anaconda/Conda modules. If your workflow better suits conda environments, you can install your own Miniconda on Frontier.

The install process is rather simple (with a few notable warnings, see https://docs.olcf.ornl.gov/systems/miniconda.html#Cautionary Notes <miniconda-notes> further below):

mkdir miniconda_frontier/
cd miniconda_frontier/
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
chmod u+x Miniconda3-latest-Linux-x86_64.sh
./Miniconda3-latest-Linux-x86_64.sh -u -p ~/miniconda_frontier",4.269713628200763
"How can I check the status of the andes-gpu5.olcf.ornl.gov server?
","You can check the general status of your application at any time using the myOLCF self-service portal's account status page. For more information, see the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#myOLCF self-service portal documentation<myolcf-overview>. If you need to make further inquiries about your application, you may email our Accounts Team at accounts@ccs.ornl.gov.",4.222836540142226
"How can I check the status of the andes-gpu5.olcf.ornl.gov server?
","In a new terminal, open a tunneling connection with andes-gpu5.olcf.ornl.gov and                                                                              port 5901
example:
     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov
     andes: ssh -4L 5901:localhost:5901 andes-gpu5

**************************************************************************",4.218305246761492
"How can I check the status of the andes-gpu5.olcf.ornl.gov server?
","In a new terminal, open a tunneling connection with andes-gpu5.olcf.ornl.gov and                                                                              port 5901
example:
     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov
     andes: ssh -4L 5901:localhost:5901 andes-gpu5

**************************************************************************",4.218305246761492
"How does Nsight Systems compare to other GPU debugging tools?
","The profiler will print several sections including information about the CUDA API calls made by the application, as well as any GPU kernels that were launched. Nsight Systems can be used for CUDA C++, CUDA Fortran, OpenACC, OpenMP offload, and other programming models that target NVIDIA GPUs, because under the hood they all ultimately take the same path for generating the binary code that runs on the GPU.",4.412909831946204
"How does Nsight Systems compare to other GPU debugging tools?
","Prior to Nsight Systems and Nsight Compute, the NVIDIA command line profiling tool was nvprof, which provides both tracing and kernel profiling capabilities. Like with Nsight Systems and Nsight Compute, the profiler data output can be saved and imported into the NVIDIA Visual Profiler for additional graphical analysis. nvprof is in maintenance mode now: it still works on Summit and significant bugs will be fixed, but no new feature development is occurring on this tool.

To use nvprof, the cuda module must be loaded.

summit> module load cuda",4.304605443689782
"How does Nsight Systems compare to other GPU debugging tools?
",| Nsight Compute | Felix Schmitt (NVIDIA) | NVIDIA Profiling Tools - Nsight Compute https://www.olcf.ornl.gov/calendar/nvidia-profiling-tools-nsight-compute/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/OLCF-Webinar-Nsight-Compute.pdf https://vimeo.com/398929189 | | 2020-03-09 | Nsight Systems | Holly Wilper (NVIDIA) | NVIDIA Profiling Tools - Nsight Systems https://www.olcf.ornl.gov/calendar/nvidia-profiling-tools-nsight-systems/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/Summit-Nsight-Systems-Introduction.pdf,4.292868295586302
"How does Summit's SMT feature work?
",| (recording) http://vimeo.com/306890517 | | 2018-12-05 | IBM Power9 SMT Deep Dive | Brian Thompto (IBM) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_thompto_smt.pdf https://vimeo.com/306890804 | | 2018-12-05 | Network Features & MPI Tuning | Christopher Zimmer (OLCF) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording),4.246752403523667
"How does Summit's SMT feature work?
",For Summit:,4.238834389576629
"How does Summit's SMT feature work?
","Hardware threads are a feature of the POWER9 processor through which individual physical cores can support multiple execution streams, essentially looking like one or more virtual cores (similar to hyperthreading on some Intel      microprocessors). This feature is often called Simultaneous Multithreading or SMT. The POWER9 processor on Summit supports SMT levels of 1, 2, or 4, meaning (respectively) each physical core looks like 1, 2, or 4 virtual cores. The SMT level is controlled by the -alloc_flags option to bsub. For example, to set the SMT level to 2, add the line #BSUB –alloc_flags",4.226451902493944
"What is the name of the compiler wrapper used by Frontier for GCC C++ compilations?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers. Please see the following https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more detailed inforation on how to compile using these modules.",4.3858209421775936
"What is the name of the compiler wrapper used by Frontier for GCC C++ compilations?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers.

It is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) whenever possible. See the next section for more details.",4.3773296360823215
"What is the name of the compiler wrapper used by Frontier for GCC C++ compilations?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.236185007228049
"How can I set the environment variable PERSIST_DIR on Summit?
","This will create a .condarc file in your $HOME directory if you do not have one already, which will now contain this new envs_dirs location. This will now enable you to use the --name env_name flag when using conda commands for environments stored in the summit directory, instead of having to use the -p /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/env_name flag and specifying the full path to the environment. For example, you can do source activate py3711-summit instead of source activate /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/py3711-summit.",4.2296769809364285
"How can I set the environment variable PERSIST_DIR on Summit?
","It is highly recommended to create new environments in the ""Project Home"" directory (on Summit, this is /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>). This space avoids purges, allows for potential collaboration within your project, and works better with the compute nodes. It is also recommended, for convenience, that you use environment names that indicate the hostname, as virtual environments created on one system will not necessarily work on others.",4.185088125838089
"How can I set the environment variable PERSIST_DIR on Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.139684313172338
"How can I allocate an array on a specific device?
","CuPy has a concept of a ""current device"", which is the current activated GPU device that will operate on an array or where future arrays will be allocated. Most of the time, if not explicitly declared or switched, the initial default device will be GPU 0. To find out what device a CuPy array is allocated on, you can call the cupy.ndarray.device attribute:

>>> x_gpu.device
<CUDA Device 0>

To get a total number of devices that you can access, use the getDeviceCount function:

>>> cp.cuda.runtime.getDeviceCount()
4

The current device can be switched using cupy.cuda.Device(<DEVICE_ID>).use():",4.142704872720879
"How can I allocate an array on a specific device?
","| Allocator | Initial Physical Location | CPU Access after GPU First Touch | Default Behavior for GPU Access | | --- | --- | --- | --- | | System Allocator (malloc,new,allocate, etc) | CPU DDR4 | Migrate to CPU DDR4 on touch | Migrate to GPU HBM on touch | | hipMallocManaged | CPU DDR4 | Migrate to CPU DDR4 on touch | Migrate to GPU HBM on touch | | hipHostMalloc | CPU DDR4 | Local read/write | Zero copy read/write over Infinity Fabric | | hipMalloc | GPU HBM | Zero copy read/write over Inifinity Fabric | Local read/write |",4.042227118819366
"How can I allocate an array on a specific device?
","| Allocator | Initial Physical Location | CPU Access after GPU First Touch | Default Behavior for GPU Access | | --- | --- | --- | --- | | System Allocator (malloc,new,allocate, etc) | CPU DDR4 | Migrate to CPU DDR4 on touch | Migrate to GPU HBM on touch | | hipMallocManaged | CPU DDR4 | Migrate to CPU DDR4 on touch | Migrate to GPU HBM on touch | | hipHostMalloc | CPU DDR4 | Local read/write | Zero copy read/write over Infinity Fabric | | hipMalloc | GPU HBM | Zero copy read/write over Inifinity Fabric | Local read/write |",4.042227118819366
"How can I troubleshoot issues with my SSH configuration file in Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.135293441210232
"How can I troubleshoot issues with my SSH configuration file in Summit?
","settings on user ~/.ssh/config files. (The batch system uses SSH, and the improper settings cause SSH to fail.) If you notice your jobs alternating between PEND and RUN, you might want to check permissions of your ~/.ssh/config file to make sure it does not have write permission for ""group"" or ""other"". (A setting of read/write for the user and no other permissions, which can be set with chmod 600 ~/.ssh/config, is recommended.)",4.129641616431693
"How can I troubleshoot issues with my SSH configuration file in Summit?
","Login to https://docs.olcf.ornl.gov/systems/Scorep.html#Summit <connecting-to-olcf>: ssh <user_id>@summit.olcf.ornl.gov

Instrument your code with Score-P

Perform a measurement run with profiling enabled

Perform a profile analysis with CUBE or cube_stat

Use scorep-score to define a filter

Perform a measurement run with tracing enabled and the filter applied

Perform in-depth analysis on the trace data with Vampir",4.105623585440328
"How many processes are used in the broadcast operation in TAU?
","755          8          8          8          0  Message size for all-reduce
       302  2.621E+05          4  1.302E+05  1.311E+05  Message size for broadcast
---------------------------------------------------------------------------------------",4.064528515495174
"How many processes are used in the broadcast operation in TAU?
","TAU is installed with Program Database Toolkit (PDT) on Summit. PDT is a framework for analyzing source code written in several programming languages. Moreover, Performance Application Programming Interface (PAPI) is supported. PAPI counters are used to assess the CPU performance. In this section, some approaches for profiling and tracing will be presented.

In most cases, we need to use wrappers to recompile the application:

For C: replace the compiler with the TAU wrapper tau_cc.sh

For C++: replace the compiler with the TAU wrapper tau_cxx.sh",4.045914662429112
"How many processes are used in the broadcast operation in TAU?
","Below, we'll look at using TAU to profile each case.

Edit the cmake_summit_pgi.sh and replace mpic++ with tau_cxx.sh. This applies only for the non-GPU versions.

TAU works with special TAU makefiles to declare what programming models are expected from the application:

The available makefiles are located inside TAU installation:",4.014940515994748
"How can I access older versions of VisIt?
","Past VisIt Tutorials are available on the Visit User's Wiki along with a set of Updated Tutorials available in the VisIt User Manual.

Sample data not pre-packaged with VisIt can be found in the VisIt Data Archive.

Older VisIt Versions with their release notes can be found on the old VisIt website, and Newer Versions can be found on the new VisIt website with release notes found on the VisIt Blog or VisIt Github Releases page.

Non-ORNL related bugs and issues in VisIt can be found and reported on Github.",4.346793217569202
"How can I access older versions of VisIt?
","VisIt uses a client-server architecture. You will obtain the best performance by running the VisIt client on your local computer and running the server on OLCF resources. VisIt for your local computer can be obtained here: VisIt Installation.

Recommended VisIt versions on our systems:

Summit: VisIt 3.1.4

Andes: VisIt 3.3.3

Frontier: VisIt 3.3.3

Using a different version than what is listed above is not guaranteed to work properly.",4.226619942378013
"How can I access older versions of VisIt?
","VisIt is now available on Frontier through the UMS022 module.

VisIt 3.3.3 is now available on Andes.

VisIt is an interactive, parallel analysis and visualization tool for scientific data. VisIt contains a rich set of visualization features so you can view your data in a variety of ways. It can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes. Further information regarding VisIt can be found at the links provided in the https://docs.olcf.ornl.gov/systems/visit.html#visit-resources section.",4.180248212349777
"How do I request a temporary user account extension?
","First-time users should apply for an account using the Account Request Form. You will need the correct 6 character project ID from your PI.

When our accounts team begins processing your application, you will receive an automated email containing a unique 36-character confirmation code. Make note of it; you can use it to check the status of your application at any time.

The principal investigator (PI) of the project must approve your account and system access. We will make the project PI aware of your request.",4.209933066633569
"How do I request a temporary user account extension?
","When our accounts team begins processing your application, you will receive an automated email containing an unique 36-character confirmation code. Make note of it; you can use it to check the status of your application at any time.

The principal investigator (PI) of the project must approve your account and system access. We will make the project PI aware of your request.",4.149555363959814
"How do I request a temporary user account extension?
","When all of the above steps are completed, your user account will be created and you will be notified by email. Now that you have a user account and it has been associated with a project, you're ready to get to work.",4.122860650539645
"Can I modify or cancel a reservation request on IBM Quantum Services?
","All jobs run on Rigetti's systems are submitted via system reservation.  This can be done either by using Rigetti's QCS dashboard to schedule the reservation, or via interacting with the QCS via the Command Line Interface (CLI).  Scheduled reservations can be viewed and/or cancelled via either method, either in the dashboard or from the CLI.

To submit a reservation via the QCS dashboard: https://docs.rigetti.com/qcs/guides/reserving-time-on-a-qpu#using-the-qcs-dashboard",4.261696618467744
"Can I modify or cancel a reservation request on IBM Quantum Services?
","There is a limited number of minutes per month that can be reserved on each device. Reservations are supported on these devices with these monthly allocations:

ibmq_kolkata, 2400 minutes per month

ibmq_jakarta, 480 minutes per month

In order to make the most efficient use of reservation allocations:

Reservations requests must be submitted to the project Principle Investigator (PI) to help@olcf.ornl.gov

Requests for reservations must include technical justification.

Once submitted, requests will be sent to the Quantum Resource Utilization Council (QRUC) for consideration.",4.251810786136436
"Can I modify or cancel a reservation request on IBM Quantum Services?
","Access to the IBM Quantum Computing queues, reservations, and simulators can be obtained via multiple methods -- either through the https://docs.olcf.ornl.gov/systems/ibm_quantum.html#cloud <ibm-cloud> or https://docs.olcf.ornl.gov/systems/ibm_quantum.html#locally <ibm-local>.",4.194553214682861
"Are there any restrictions on using expert mode in BSUB for deep learning jobs on Summit?
","Most users will want to use easy mode. However, if you need precise control over your job’s resources, such as placement on (or avoidance of) specific nodes, you will need to use expert mode. To use expert mode, add #BSUB -csm y to your batch script (or -csm y to your bsub command line).",4.263207602973655
"Are there any restrictions on using expert mode in BSUB for deep learning jobs on Summit?
","When making node reservations for DDL jobs, it can sometimes improve performance to reserve nodes in a rack-contiguous manner.

In order to instruct BSUB to reserve nodes in the same rack, expert mode must be used (-csm y), and the user needs to explicitly specify the reservation string. For more information on Expert mode see: https://docs.olcf.ornl.gov/systems/ibm-wml-ce.html#easy_mode_v_expert_mode

The following BSUB arguments and reservation string instruct bsub to reserve 2 compute nodes within the same rack:",4.137491033065093
"Are there any restrictions on using expert mode in BSUB for deep learning jobs on Summit?
","There are special queue names when submitting jobs to citadel.ccs.ornl.gov (the Moderate Enhanced version of Summit). These queues are: batch-spi, batch-hm-spi, and debug-spi.  For example, to submit a job to the batch-spi queue on Citadel, you would need -q batch-spi when using the bsub command or #BSUB -q batch-spi when using a job script.

Except for the enhanced security policies for jobs in these queues, all other queue properties are the same as the respective Summit queues described above, such as maximum walltime and number of eligible running jobs.",4.113717021741213
"How do I sync a directory from my local system to the OLCF using rsync?
","Sending a file to OLCF

scp yourfile $USER@dtn.ccs.ornl.gov:/path/

Retrieving a file from OLCF

scp $USER@dtn.ccs.ornl.gov:/path/yourfile .

Sending a directory to OLCF

scp -r yourdirectory $USER@dtn.ccs.ornl.gov:/path/

rsync - a fast, versatile, remote (and local) file-copying tool

Sync a directory named mydir from your local system to the OLCF

rsync -avz mydir/ $USER@dtn.ccs.ornl.gov:/path/

where:

a is for archive mode

v is for verbose mode

z is for compressed mode

Sync a directory from the OLCF to a local directory

rsync -avz  $USER@dtn.ccs.ornl.gov:/path/dir/ mydir/",4.364978075630248
"How do I sync a directory from my local system to the OLCF using rsync?
","This page has been deprecated, and relevant information is now available on https://docs.olcf.ornl.gov/systems/transferring.html#data-storage-and-transfers. Please update any bookmarks to use that page.

In general, when transferring data into or out of the OLCF from the command line, it's best to initiate the transfer from outside the OLCF. If moving many small files, it can be beneficial to compress them into a single archive file, then transfer just the one archive file.

scp and rsync are available for remote transfers.

scp - secure copy (remote file copy program)

Sending a file to OLCF",4.257065173107003
"How do I sync a directory from my local system to the OLCF using rsync?
","When prompted, authenticate into the OLCF DTN endpoint using your OLCF username and PIN followed by your RSA passcode.

Click in the left side “Path” box in the File Manager and enter the path to your data on Alpine. For example, /gpfs/alpine/stf007/proj-shared/my_alpine_data. You should see a list of your files and folders under the left “Path” Box.

Click on all files or folders that you want to transfer in the list. This will highlight them.

Click on the right side “Collection” box in the File Manager and type “OLCF DTN”",4.232262391221405
"How can I specify the number of GPUs to use for a job on Summit?
","summit>

The following example will create 4 resource sets each with 6 tasks and 3 GPUs. Each set of 6 MPI tasks will have access to 3 GPUs. Ranks 0 - 5 will have access to GPUs 0 - 2 on the first socket of the first node ( red resource set). Ranks 6 - 11 will have access to GPUs 3 - 5 on the second socket of the first node ( green resource set). This pattern will continue until 4 resource sets have been created. The following jsrun command will request 4 resource sets (-n4). Each resource set will contain 6 MPI tasks (-a6), 3 GPUs (-g3), and 6 cores (-c6).",4.294422119269605
"How can I specify the number of GPUs to use for a job on Summit?
","Summit's V100 GPUs are configured to have a default compute mode of EXCLUSIVE_PROCESS. In this mode, the GPU is assigned to only a single process at a time, and can accept work from multiple process threads concurrently.

It may be desirable to change the GPU's compute mode to DEFAULT, which enables multiple processes and their threads to share and submit work to it simultaneously. To change the compute mode to DEFAULT, use the -alloc_flags gpudefault option.",4.291627245499229
"How can I specify the number of GPUs to use for a job on Summit?
","If you plan on using the EGL version of the ParaView module (e.g., paraview/5.11.0-egl), then you must be connected to the GPUs. On Andes, this is done by using the gpu partition via #SBATCH -p gpu, while on Summit the -g flag in the jsrun command must be greater than zero.",4.2881885133229805
"Can the DTN mounts be used when transferring data with rsync and cp tools?
","Standard tools such as rsync and scp can also be used through the DTN, but may be slower and require more manual intervention than Globus

Copying data directly from Alpine (GPFS) to Orion (Lustre)

Globus is the suggested tool to transfer needed data from Alpine to Orion.

Globus should be used when transfer large amounts of data.

Standard tools such as rsync and cp can also be used. The DTN mounts both filesystems and should be used when transferring with rsync and cp tools. These methods should not be used to transfer large amounts of data.

Copying data to the HPSS archive system",4.380694006182006
"Can the DTN mounts be used when transferring data with rsync and cp tools?
","To assist you with moving your data off of Alpine, the DTN's mount the new Orion filesystem and all projects with access to Alpine have now been granted access to the Orion filesystem.

Please do not wait to migrate needed data, begin migrating all needed data now.",4.181619087841908
"Can the DTN mounts be used when transferring data with rsync and cp tools?
","rsync -avz --dry-run mydir/ $USER@dtn.ccs.ornl.gov:/path/

See the manual pages for more information:

$ man scp
$ man rsync

Differences:

scp cannot continue if it is interrupted. rsync can.

rsync is optimized for performance.

By default, rsync checks if the transfer of the data was successful.

Standard file transfer protocol (FTP) and remote copy (RCP) should not be used to transfer files to the NCCS high-performance computing (HPC) systems due to security concerns.",4.151455616518132
"What is the ideal scenario for optimal I/O performance on Summit?
","Summit is connected to an IBM Spectrum Scale™ filesystem providing 250PB of storage capacity with a peak write speed of 2.5 TB/s. Summit also has access to the center-wide NFS-based filesystem (which provides user and project home areas) and has access to the center’s High Performance Storage System (HPSS) for user and project archival storage.

Summit is running Red Hat Enterprise Linux (RHEL) version 8.2.",4.325270458269191
"What is the ideal scenario for optimal I/O performance on Summit?
","When a user occupies more than one compute node, then they are using more NVMes and the I/O can scale linearly. For example in the following plot you can observe the scalability of the IOR benchmark on 2048 compute nodes on Summit where the write performance achieves 4TB/s and the read 11.3 TB/s",4.322424389768181
"What is the ideal scenario for optimal I/O performance on Summit?
","Summit

Andes

Frontier

Scientific simulations generate large amounts of data on Summit (about 100 Terabytes per day for some applications). Because of how large some datafiles may be, it is important that writing and reading these files is done as fast as possible. Less time spent doing input/output (I/O) leaves more time for advancing a simulation or analyzing data.",4.321153622365393
"How do users schedule a reservation via the QCS dashboard?
","All jobs run on Rigetti's systems are submitted via system reservation.  This can be done either by using Rigetti's QCS dashboard to schedule the reservation, or via interacting with the QCS via the Command Line Interface (CLI).  Scheduled reservations can be viewed and/or cancelled via either method, either in the dashboard or from the CLI.

To submit a reservation via the QCS dashboard: https://docs.rigetti.com/qcs/guides/reserving-time-on-a-qpu#using-the-qcs-dashboard",4.484677629223597
"How do users schedule a reservation via the QCS dashboard?
","To submit a reservation via the QCS CLI, and installation instructions: https://docs.rigetti.com/qcs/guides/using-the-qcs-cli

Accessing the Rigetti QPU systems can only be done during a user's reservation window.  To submit a job, users must have JupyterHub access to the system.  QVM jobs can be run without network access in local environments.  Jobs are compiled via Quilc and submitted via pyQuil (see https://docs.olcf.ornl.gov/systems/rigetti.html#Software Section <rigetti-soft> below) in a python environment or Jupyter notebook.",4.253011570151183
"How do users schedule a reservation via the QCS dashboard?
","Because of the queuing method described above, users have no set allocation. Job throughput is only limited via the dynamic queue.

There is a time limit on program-wide usage of reservable systems (see below).

In addition to the fair-share queue, users may request a backend reservation for a certain period of time by contacting help@olcf.ornl.gov. If the reservation is granted, the reserved backend will be blocked from general use for a specified period of time, and the user will have sole use of the backend for that period.",4.172528107857968
"How can I list all the jobs that are currently suspended on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.221177372473858
"How can I list all the jobs that are currently suspended on Summit?
","LSF supports user-level suspension and resumption of jobs. Jobs are suspended with the bstop command and resumed with the bresume command. The simplest way to invoke these commands is to list the job id to be suspended/resumed:

bstop 12345
bresume 12345

Instead of specifying a job id, you can specify other criteria that will allow you to suspend some/all jobs that meet other criteria such as a job name, a queue name, etc. These are described in the manpages for bstop and bresume.",4.1810427416178575
"How can I list all the jobs that are currently suspended on Summit?
","-s | Show suspended jobs, including the reason(s) they're suspended | | bjobs -r | Show running jobs | | bjobs -p | Show pending jobs | | bjobs -w | Use ""wide"" formatting for output |",4.1513146029754
"What are the benefits of attending an OLCF GPU Hackathon?
","If you want/need to get your code running (or optimized) on a GPU-accelerated system, these hackathons offer a unique opportunity to set aside the time, surround yourself with experts in the field, and push toward your development goals. By the end of the event, each team should have part of their code running (or more optimized) on GPUs, or at least have a clear roadmap of how to get there.

<p style=""font-size:20px""><b>Target audience</b></p>",4.479622916596118
"What are the benefits of attending an OLCF GPU Hackathon?
","If you would like more information about how you can volunteer to mentor a team at an upcoming Open/GPU hackathon, please visit our Become a Mentor page.

<p style=""font-size:20px""><b>Who can I contact with questions?</b></p>

If you have any questions about the OLCF GPU Hackathon Series, please contact OLCF Help at (help@olcf.ornl.gov).",4.4068837924214
"What are the benefits of attending an OLCF GPU Hackathon?
","A GPU hackathon is a multi-day coding event in which teams of developers port their own applications to run on GPUs, or optimize their applications that already run on GPUs. Each team consists of 3 or more developers who are intimately familiar with (some part of) their application, and they work alongside 1 or more mentors with GPU programming expertise. Our mentors come from universities, national laboratories, supercomputing centers, and industry partners.",4.380160397302837
"How can I run a job on Summit with a specific number of nodes and CPUs?
","Jobs on Summit are scheduled in full node increments; a node's cores cannot be allocated to multiple jobs. Because the OLCF charges based on what a job makes unavailable to other users, a job is charged for an entire node even if it uses only one core on a node. To simplify the process, users request and are allocated multiples of entire nodes through LSF.

Allocations on Summit are separate from those on Andes and other OLCF resources.

The node-hour charge for each batch job will be calculated as follows:

node-hours = nodes requested * ( batch job endtime - batch job starttime )",4.309939897942023
"How can I run a job on Summit with a specific number of nodes and CPUs?
","to service nodes on that system). All commands within your job script (or the commands you run in an interactive job) will run on a launch node. Like login nodes, these are shared resources so you should not run multiprocessor/threaded programs on Launch nodes. It is appropriate to launch the jsrun command from launch nodes. | | Compute | Most of the nodes on Summit are compute nodes. These are where your parallel job executes. They're accessed via the jsrun command. |",4.3032164314649854
"How can I run a job on Summit with a specific number of nodes and CPUs?
","summit>



Jsrun provides the ability to launch multiple jsrun job launches within a single batch job allocation. This can be done within a single node, or across multiple nodes.

By default, multiple invocations of jsrun in a job script will execute serially in order. In this configuration, jobs will launch one at a time and the next one will not start until the previous is complete. The batch node allocation is equal to the largest jsrun submitted, and the total walltime must be equal to or greater then the sum of all jsruns issued.",4.28419435282978
"How can I analyze the report file generated by the profiler on Summit?
","Small trace files can be viewed locally on your machine if you have the Vampir client downloaded, otherwise they can be viewed locally on Summit. For large trace files, it is strongly recommended to run vampirserver reverse-connected to a local copy of the Vampir client. See the https://docs.olcf.ornl.gov/systems/Scorep.html#vamptunnel section for more details.

In addition to automatically profiling and tracing functions, there is also a way to manually instrument a specific region in the source code. To do this, you will need to add the --user flag to the scorep command when compiling:",4.187432361698287
"How can I analyze the report file generated by the profiler on Summit?
","Login to https://docs.olcf.ornl.gov/systems/Scorep.html#Summit <connecting-to-olcf>: ssh <user_id>@summit.olcf.ornl.gov

Instrument your code with Score-P

Perform a measurement run with profiling enabled

Perform a profile analysis with CUBE or cube_stat

Use scorep-score to define a filter

Perform a measurement run with tracing enabled and the filter applied

Perform in-depth analysis on the trace data with Vampir",4.172609356892931
"How can I analyze the report file generated by the profiler on Summit?
","If you add the -o option, as above, the report will be saved to file with the extension .qdrep. That report file can later be analyzed in the Nsight Systems UI by selecting File > Open and locating the vectorAdd.qdrep file on your filesystem. Nsight Systems does not currently have a Power9 version of the UI, so you will need to download the UI for your local system, which is supported on Windows, Mac, and Linux (x86). Then use scp or some other file transfer utility for copying the report file from Summit to your local machine.",4.170636522013595
"What are some potential challenges in implementing a job scheduling policy that prioritizes leadership-class jobs?
","would allow smaller, shorter jobs to use those otherwise idle resources,

and with the proper algorithm, the start time of the large job would not

be delayed. While this does make more effective use of the system, it

indirectly encourages the submission of smaller jobs.



The DOE Leadership-Class Job Mandate

------------------------------------



As a DOE Leadership Computing Facility, the OLCF has a mandate that a

large portion of Titan's usage come from large, *leadership-class* (aka

*capability*) jobs. To ensure the OLCF complies with DOE directives, we",4.272127084721106
"What are some potential challenges in implementing a job scheduling policy that prioritizes leadership-class jobs?
","agreed to by the following persons as a condition of access to or use of

OLCF computational resources:



-  Principal Investigators (Non-Profit)

-  Principal Investigators (Industry)

-  All Users



**Title:** Titan Scheduling Policy **Version:** 13.02



In a simple batch queue system, jobs run in a first-in, first-out (FIFO)

order. This often does not make effective use of the system. A large job

may be next in line to run. If the system is using a strict FIFO queue,

many processors sit idle while the large job waits to run. *Backfilling*",4.160031469397846
"What are some potential challenges in implementing a job scheduling policy that prioritizes leadership-class jobs?
","strongly encourage users to run jobs on Titan that are as large as their

code will warrant. To that end, the OLCF implements queue policies that

enable large jobs to run in a timely fashion.



.. note::

The OLCF implements queue policies that encourage the

submission and timely execution of large, leadership-class jobs on

Titan.



The basic priority-setting mechanism for jobs waiting in the queue is

the time a job has been waiting relative to other jobs in the queue.

However, several factors are applied by the batch system to modify the",4.159554250049205
"What is the purpose of the `LD_LIBRARY_PATH` environment variable?
","Environment modules are provided through Lmod, a Lua-based module system for dynamically altering shell environments. By managing changes to the shell’s environment variables (such as PATH, LD_LIBRARY_PATH, and PKG_CONFIG_PATH), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.

The interface to Lmod is provided by the module command:",4.1764515055990605
"What is the purpose of the `LD_LIBRARY_PATH` environment variable?
","Environment modules are provided through Lmod, a Lua-based module system for dynamically altering shell environments. By managing changes to the shell’s environment variables (such as PATH, LD_LIBRARY_PATH, and PKG_CONFIG_PATH), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.

The interface to Lmod is provided by the module command:",4.1764515055990605
"What is the purpose of the `LD_LIBRARY_PATH` environment variable?
","Environment modules are provided through Lmod, a Lua-based module system for dynamically altering shell environments. By managing changes to the shell’s environment variables (such as PATH, LD_LIBRARY_PATH, and PKG_CONFIG_PATH), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.

The interface to Lmod is provided by the module command:",4.1764515055990605
"How does Crusher handle memory allocation and deallocation?
","Most applications that use ""managed"" or ""unified"" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Crusher. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.",4.236938836853682
"How does Crusher handle memory allocation and deallocation?
","The AMD MI250X and operating system on Crusher supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Crusher. In the following section we'll discuss how the heterogenous memory space on a Crusher node is surfaced within your application.",4.204245704735977
"How does Crusher handle memory allocation and deallocation?
","The Crusher system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.152735593714671
"How do I compile SYCL codes using the DPC++ compiler?
","This section shows how to compile SYCL codes using the DPC++ compiler.

Make sure the ums ums015 dpcpp module is loaded when compiling SYCL with clang or clang++.

| Compiler | Compile/Link Flags, Header Files, and Libraries | | --- | --- | |  |  |

These compilers are built weekly from the latest open-source rather than releases. As such, these compilers will get new features and updates quickly but may break on occasion. If you experience regressions, please load an older version of the module rather than the latest.",4.673077678997302
"How do I compile SYCL codes using the DPC++ compiler?
","This section shows how to compile code with OpenACC. Currently only the Cray compiler supports OpenACC for Fortran. The AMD and GNU programming environments do not support OpenACC at all. C and C++ support for OpenACC is provided by clacc which maintains a fork of the LLVM compiler with added support for OpenACC. It can be obtained by loading the UMS modules ums, ums025, and clacc.",4.124612030454349
"How do I compile SYCL codes using the DPC++ compiler?
","Information about compiling code for different XNACK modes (which control page migration between GPU and CPU memory) can be found in the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#compiling-hip-kernels-for-xnack-modes section.

This section shows how to compile HIP + OpenMP CPU threading hybrid codes.

Make sure the craype-accel-amd-gfx90a module is loaded when compiling HIP with the Cray compiler wrappers.

| Vendor | Compiler | Compile/Link Flags, Header Files, and Libraries | | --- | --- | --- | | AMD/Cray | CC |  | | hipcc |  | | GNU | CC |  |",4.065465225306266
"What is the purpose of using Score-P and Vampir tools together?
","This recording is from the 2018 Score-P / Vampir workshop that took place at ORNL on August 17, 2018. In the video, Ronny Brendel gives an introduction to the Score-P and Vampir tools, which are often used together to collect performance profiles/traces from an application and visualize the results.",4.456686826058927
"What is the purpose of using Score-P and Vampir tools together?
","<p>This recording is from the 2018 Score-P / Vampir workshop that took place at ORNL on August 17, 2018. In the video, Ronny Brendel gives an introduction to the Score-P and Vampir tools, which are often used together to collect performance profiles/traces from an application and visualize the results.</p>",4.408590437702057
"What is the purpose of using Score-P and Vampir tools together?
","The regions ""sum"" and ""my_calculations"" in the above examples would then be included in the profiling and tracing runs and can be analysed with Vampir. For more details, refer to the Advanced Score-P training in the https://docs.olcf.ornl.gov/systems/Scorep.html#training-archive.

Please see the provided video below to watch a brief demo of using Score-P provided by TU-Dresden and presented by Ronny Brendel.",4.322445710247176
"How do I know if AMP is enabled for my model in Summit?
","below), you can choose between the rendering options by loading its corresponding module on the system you're connected to. For example, to see these modules on Summit:",4.159761676438456
"How do I know if AMP is enabled for my model in Summit?
",For Summit:,4.090132881036178
"How do I know if AMP is enabled for my model in Summit?
","NVIDIA has also integrated a technology called Automatic Mixed Precision (AMP) into several common frameworks, TensorFlow, PyTorch, and MXNet at time of writing. In most cases AMP can be enabled via a small code change or via setting and environment variable. AMP does not strictly replace all matrix multiplication operations with half precision, but uses graph optimization techniques to determine whether a given layer is best run in full or half precision.

Examples are provided for using AMP, but the following sections summarize the usage in the three supported frameworks.",4.0344349096400505
"Can I use Score-P to instrument code written in languages other than C, C++, and Fortran?
","To instrument your code, you need to compile the code using the Score-P instrumentation command (scorep), which is added as a prefix to your compile statement. In most cases the Score-P instrumentor is able to automatically detect the programming paradigm from the set of compile and link options given to the compiler. Some cases will, however, require some additional link options within the compile statement e.g. CUDA instrumentation.

Below are some basic examples of the different instrumentation scenarios:",4.428393128725958
"Can I use Score-P to instrument code written in languages other than C, C++, and Fortran?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi-processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations.

For detailed information about using Score-P on Summit and the builds available, please see the Score-P Software Page.",4.342575045996351
"Can I use Score-P to instrument code written in languages other than C, C++, and Fortran?
","$ scorep --user gcc -c test.c
$ scorep --user gcc -o test test.o

Now you can manually instrument Score-P to the source code as seen below:

C,C++

.. code::

   #include <scorep/SCOREP_User.h>

   void foo() {
      SCOREP_USER_REGION_DEFINE(my_region)
      SCOREP_USER_REGION_BEGIN(my_region, ""foo"", SCOREP_USER_REGION_TYPE_COMMON)
      // do something
      SCOREP_USER_REGION_END(my_region)
   }

Fortran

.. code::

   #include <scorep/SCOREP_User.inc>",4.3233897828193095
"How do I ensure that my job uses only the requested GPUs in Frontier?
","Due to the unique architecture of Frontier compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.

Before jumping into the examples, it is helpful to understand the output from the hello_jobstep program:",4.245642158707855
"How do I ensure that my job uses only the requested GPUs in Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.210118701376581
"How do I ensure that my job uses only the requested GPUs in Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.193554862548039
"What is the name of the supercomputing platform that Summit is a part of?
","Summit is an IBM system located at the Oak Ridge Leadership Computing Facility. With a theoretical peak double-precision performance of approximately 200 PF, it is one of the most capable systems in the world for a wide range of traditional computational science applications. It is also one of the ""smartest"" computers in the world for deep learning applications with a mixed-precision capability in excess of 3 EF.



Summit node architecture diagram",4.491351312086394
"What is the name of the supercomputing platform that Summit is a part of?
","Summit node architecture diagram

The basic building block of Summit is the IBM Power System AC922 node. Each of the approximately 4,600 compute nodes on Summit contains two IBM POWER9 processors and six NVIDIA Tesla V100 accelerators and provides a theoretical double-precision capability of approximately 40 TF. Each POWER9 processor is connected via dual NVLINK bricks, each capable of a 25GB/s transfer rate in each direction.",4.413679628908933
"What is the name of the supercomputing platform that Summit is a part of?
","Summit is connected to an IBM Spectrum Scale™ filesystem providing 250PB of storage capacity with a peak write speed of 2.5 TB/s. Summit also has access to the center-wide NFS-based filesystem (which provides user and project home areas) and has access to the center’s High Performance Storage System (HPSS) for user and project archival storage.

Summit is running Red Hat Enterprise Linux (RHEL) version 8.2.",4.3776573490631785
"How can I check the status of my job on Crusher?
","If you have problems or need helping running on Crusher, please submit a ticket by emailing help@olcf.ornl.gov.



JIRA_CONTENT_HERE",4.109935097557607
"How can I check the status of my job on Crusher?
","This section describes how to run programs on the Crusher compute nodes, including a brief overview of Slurm and also how to map processes and threads to CPU cores and GPUs.

Slurm is the workload manager used to interact with the compute nodes on Crusher. In the following subsections, the most commonly used Slurm commands for submitting, running, and monitoring jobs will be covered, but users are encouraged to visit the official documentation and man pages for more information.",4.102099256755948
"How can I check the status of my job on Crusher?
","The most straightforward monitoring is with the bjobs command. This command will show the current queue, including both pending and running jobs. Running bjobs -l will provide much more detail about a job (or group of jobs). For detailed output of a single job, specify the job id after the -l. For example, for detailed output of job 12345, you can run bjobs -l 12345 . Other options to bjobs are shown below. In general, if the command is specified with -u all it will show information for all users/all jobs. Without that option, it only shows your jobs. Note that this is not an exhaustive list.",4.0588697261370505
"How do I submit a job script to the batch system on Summit?
","As is the case on other OLCF systems, computational work on Summit is performed within jobs. A typical job consists of several components:

A submission script

An executable

Input files needed by the executable

Output files created by the executable

In general, the process for running a job is to:

Prepare executables and input files

Write the batch script

Submit the batch script

Monitor the job's progress before and during execution",4.412154333073184
"How do I submit a job script to the batch system on Summit?
","| Task | LSF (Summit) | Slurm | | --- | --- | --- | | View batch queue | jobstat | squeue | | Submit batch script | bsub | sbatch | | Submit interactive batch job | bsub -Is $SHELL | salloc | | Run parallel code within batch job | jsrun | srun |

Writing Batch Scripts

Batch scripts, or job submission scripts, are the mechanism by which a user configures and submits a job for execution. A batch script is simply a shell script that also includes commands to be interpreted by the batch scheduling software (e.g. Slurm).",4.385421884622168
"How do I submit a job script to the batch system on Summit?
","The most common way to interact with the batch system is via batch scripts. A batch script is simply a shell script with added directives to request various resoruces from or provide certain information to the scheduling system.  Aside from these directives, the batch script is simply the series of commands needed to set up and run your job.

To submit a batch script, use the command sbatch myjob.sl

Consider the following batch script:

#!/bin/bash
#SBATCH -A ABC123
#SBATCH -J RunSim123
#SBATCH -o %x-%j.out
#SBATCH -t 1:00:00
#SBATCH -p batch
#SBATCH -N 1024",4.379921980503206
"How can I set the memory limit for each worker when running the `dask-cuda-worker` command?
","echo ""Done!""

Note twelve dask-cuda-workers are executed, one per each available GPU, --memory-limit is set to 82 GB and  --device-memory-limit is set to 16 GB. If using Summit's high-memory nodes --memory-limit can be increased and setting --device-memory-limit to 32 GB  and --rmm-pool-size to 30 GB or so is recommended. Also note it is recommeded to wait some seconds for the dask-scheduler and dask-cuda-workers to start.",4.308194729182684
"How can I set the memory limit for each worker when running the `dask-cuda-worker` command?
","Running RAPIDS multi-gpu/multi-node workloads requires a dask-cuda cluster. Setting up a dask-cuda cluster on Summit requires two components:

dask-scheduler.

dask-cuda-workers.

Once the dask-cluster is running, the RAPIDS script should perform four main tasks. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

Reference of multi-gpu/multi-node operation with cuDF, cuML, cuGraph is available in the next links:

10 Minutes to cuDF and Dask-cuDF.

cuML's Multi-Node, Multi-GPU Algorithms.",4.0440117590366
"How can I set the memory limit for each worker when running the `dask-cuda-worker` command?
","workers_info=client.scheduler_info()['workers']
    connected_workers = len(workers_info)
    print(str(connected_workers) + "" workers connected"")

    # 2. Create a BlazingContext that takes in the dask client
    # you want to set `allocator='existing'` if you are launching the dask-cuda-worker with an rmm memory pool
    bc = BlazingContext(dask_client = client, network_interface='ib0', allocator='existing')

    # 3. Create some tables
    bc.create_table('my_table','/data/file*.parquet')

    # 4. Run queries
    ddf = bc.sql('select count(*) from my_table')
    print(ddf.head())",4.002846087457493
"Can I use Slate to build a Python 3.5 application with a specific set of dependencies?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",4.059468020753657
"Can I use Slate to build a Python 3.5 application with a specific set of dependencies?
","The OS-provided Python will no longer be accessible as python (including variations like /usr/bin/python or /usr/bin/env python); rather, you must specify it as python2 or python3. If you are using python from one of the modulefiles rather than the version in /usr/bin, this change should not affect how you invoke python in your scripts, although we encourage specifying python2 or python3 as a best practice.



<p style=""font-size:20px""><b>Summit: OpenCE 1.5.0 (December 29, 2021)</b></p>",4.04911682586501
"Can I use Slate to build a Python 3.5 application with a specific set of dependencies?
","The following items need to be established before deploying applications on Slate systems (Marble or Onyx OpenShift clusters).

Follow https://docs.olcf.ornl.gov/systems/helm_prerequisite.html#slate_getting_started if you do not already have a project/namespace established on Onyx or Marble.

It is recommended to use Helm version 3.

Helm enables application deployment via Helm Charts. The high level Helm Architecture docs are also a great reference for understanding Helm.

Like oc, Helm is a single binary executable.

This can be installed on macOS with Homebrew :

$ brew install helm",4.034991939849756
"What is the purpose of the --timestamp option in the rocprof command?
","Integer instructions and cache levels are currently not documented here.

To get started, you will need to make an input file for rocprof, to be passed in through rocprof -i <input_file> --timestamp on -o my_output.csv <my_exe>. Below is an example, and contains the information needed to roofline profile GPU 0, as seen by each rank:",4.274588986471847
"What is the purpose of the --timestamp option in the rocprof command?
","Integer instructions and cache levels are currently not documented here.

To get started, you will need to make an input file for rocprof, to be passed in through rocprof -i <input_file> --timestamp on -o my_output.csv <my_exe>. Below is an example, and contains the information needed to roofline profile GPU 0, as seen by each rank:",4.274588986471847
"What is the purpose of the --timestamp option in the rocprof command?
","rocprof gathers metrics on kernels run on AMD GPU architectures. The profiler works for HIP kernels, as well as offloaded kernels from OpenMP target offloading, OpenCL, and abstraction layers such as Kokkos. For a simple view of kernels being run, rocprof --stats --timestamp on is a great place to start. With the --stats option enabled, rocprof will generate a file that is named results.stats.csv by default, but named <output>.stats.csv if the -o flag is supplied. This file will list all kernels being run, the number of times they are run, the total duration and the average duration (in",4.210798766852966
"How long does the approval process take?
","On the ""Policies & Agreements"" page click the links to read the polices and then answer ""Yes"" to affirm you have read each. Certify your application by selecting ""Yes"" as well. Then Click ""Submit.""

<string>:5: (INFO/1) Enumerated list start value not ordinal-1: ""8"" (ordinal 8)

After submitting your application, it will need to pass through the approval process. Depending on when you submit, approval might not occur until the next business day.",4.197701537809956
"How long does the approval process take?
","Foreign national participants will be sent an Oak Ridge National Lab (ORNL) Personnel Access System (PAS) request specific for the facility and cyber-only access. After receiving your response, it takes between 15-35 days for approval.

Fully-executed Institutional User Agreements with each institution having participants are required. If our records indicate your institution needs to sign either an Institutional User Agreement and/or Appendix A, the proper form(s), along with instructions, will be sent via email.",4.1573295620656125
"How long does the approval process take?
","When our accounts team begins processing your application, you will receive an automated email containing an unique 36-character confirmation code. Make note of it; you can use it to check the status of your application at any time.

The principal investigator (PI) of the project must approve your account and system access. We will make the project PI aware of your request.",4.126036518475388
"How can I view the YAML representation of a network policy in Slate?
","to view object's YAML.

To create a Network Policy, define one in YAML similar to the output of the previous command and run:

oc create -f FILENAME

For a more complex example of a Network Policy please see the Kubernetes doc.

A full reference of Network Policies can be found here.",4.310368677946022
"How can I view the YAML representation of a network policy in Slate?
","Network policies are specifications of how groups of pods are allowed to communicate with each other and other network endpoints. A pod is selected in a Network Policy based on a label so the Network Policy can define rules to specify traffic to that pod.

When you create a namespace in Slate with the Quota Dashboard some Network Policies are added to your newly created namespace. This means that pods inside your project are isolated from connections not defined in these Network Policies.",4.215200008837488
"How can I view the YAML representation of a network policy in Slate?
","Network Policies do not conflict, they are additive. This means that if two policies match a pod the pod will be restricted to what is allowed by the union of those policies' ingress/egress rules.

To create a Network policy using the GUI click the Networking tab followed by the Network Policy tab:

Creating Network Policies

This will place you in an editor with some boiler plate YAML. From here you can define the network policy that you need for your namespace. Below is an example Network Policy that you would use to allow all external traffic to access a nodePort in your namespace.",4.128894746556338
"How long does it take for the port forward command to take effect?
","Port Forwarding

ssh -L <localport>:<Node ID>:<Remote port>  <USERID>@summit.olcf.ornl.gov

The local port number can be any unused port number on your local machine...try a number between 30000-30030.  To check if the port you picked is open run:    $ netstat -ab | grep ""<selected port number>""  #This can take a minute to return anything. If nothing is returned, your selected port is open

After submitting the port forward command as seen above, it will ask for your login password to access Summit. Leave this terminal window open!

Launch the Vampir GUI on your local machine",4.085147290911593
"How long does it take for the port forward command to take effect?
","Additionally, oc port-forward doesn't have to be given a pod name. This tool is aware of services and deployments as well. If you had a service called nginx-svc and a deployment called nginx, for example, the following commands would achieve the same result:

oc port-forward deployment/nginx 7777:8080
oc port-forward svc/nginx-svc 7777:8080

You will be forwarded to any of the pods matched by the service or deployment.

Furthermore, this doesn't only work for http traffic. You could also access other exposed services such as databases.",4.067361935913512
"How long does it take for the port forward command to take effect?
","This tool will forward a local port on your system to a pod inside the cluster.

For example, if you have an nginx deployment running on port 8080 inside the container, you can view this nginx instance locally by running:

oc port-forward ${pod_name} 7777:8080

The first port is the local port you want forwarded, and the second port is the port exposed by the pod. After running this command, you can go into your browser (or use curl in a second terminal) and connect to http://localhost:7777.",4.0470756922626965
"How can I use CUDA C/C++ support in Summit?
","Using the hip-cuda/5.1.0 module on Summit requires CUDA v11.4.0 or later. However, only CUDA v11.0.3 and earlier currently support GPU-aware MPI, so GPU-aware MPI is currently not available when using HIP on Summit.

Any codes compiled with post 11.0.3 CUDA (cuda/11.0.3) will not work with MPS enabled (-alloc_flags ""gpumps"") on Summit. The code will hang indefinitely. CUDA v11.0.3 is the latest version that is officially supported by IBM's software stack installed on Summit. We are continuing to look into this issue.",4.340434288010642
"How can I use CUDA C/C++ support in Summit?
","Although there are newer CUDA modules on Summit, cuda/11.0.3 is the latest version that is officially supported by the version of IBM's software stack installed on Summit. When loading the newer CUDA modules, a message is printed to the screen stating that the module is for “testing purposes only”. These newer unsupported CUDA versions might work with some users’ applications, but importantly, they are known not to work with GPU-aware MPI.",4.319341545795753
"How can I use CUDA C/C++ support in Summit?
","Last Updated: 07 February 2023

Using the hip-cuda/5.1.0 module on Summit, applications cannot build using a CMakeLists.txt that requires HIP language support or references the hip::host and hip::device identifiers. There is no known workaround for this issue. Applications wishing to compile HIP code with CMake need to avoid using HIP language support or hip::host and hip::device identifiers.",4.317604136155945
"What is the name of the project associated with the scratch directory?
","Each project is granted a Project Work directory; these reside in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. Project Work directories can be accessed by all members of a project and are intended for sharing data within a project. Project Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but",4.277097453479273
"What is the name of the project associated with the scratch directory?
","Each project has a World Work directory that resides in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. World Work areas can be accessed by all users of the system and are intended for sharing of data between projects. World Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but rather should be",4.203250125497439
"What is the name of the project associated with the scratch directory?
","Project members get an individual Member Work directory for each associated project; these reside in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. Member Work areas are not shared with other users of the system and are intended for project data that the user does not want to make available to other users. Member Work directories are provided commonly across all systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular",4.179061172685332
"How can I compile a Fortran program with CUDA Fortran support and link it to a pre-installed library on Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.290128363925139
"How can I compile a Fortran program with CUDA Fortran support and link it to a pre-installed library on Summit?
","First, load the gnu compiler module (most Python packages assume GCC), relevant GPU module (necessary for CuPy), and the python module (allows you to create a new environment):

Summit

.. code-block:: bash

   $ module load gcc/7.5.0 # might work with other GCC versions
   $ module load cuda/11.0.2
   $ module load python

Frontier

.. code-block:: bash

   $ module load PrgEnv-gnu
   $ module load amd-mixed/5.3.0
   $ module load craype-accel-amd-gfx90a
   $ module load cray-python # only if not using Miniconda on Frontier",4.271401145423511
"How can I compile a Fortran program with CUDA Fortran support and link it to a pre-installed library on Summit?
",http://vimeo.com/306890606 | | 2018-12-05 | Targeting GPUs Using GPU Directives on Summit with GenASiS: A Simple and Effective Fortran Experience | Reuben Budiardja (OLCF) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_budiardja.pdf https://vimeo.com/306890448 | | 2018-12-05 | Experiences Using the Volta Tensor Cores | Wayne Joubert (OLCF) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (recording),4.26010946808866
"Are there any specific instructions for compiling and batch scheduling SPI workflows on OLCF resources?
","To access Summit and Frontier's compute resources for SPI workflows, you must first log into a https://docs.olcf.ornl.gov/systems/index.html#Citadel login node<citadel-login-nodes> and then submit a batch job to one of the SPI specific batch queues.",4.393105559891673
"Are there any specific instructions for compiling and batch scheduling SPI workflows on OLCF resources?
","Similar to standard OLCF workflows, to run SPI workflows on OLCF resources, you will first need an approved project.  The project will be awarded an allocation(s) that will allow resource access and use.  The first step to requesting an allocation is to complete the project request form.  The form will initiate the process that includes peer review, export control review, agreements, and others.  Once submitted, members of the OLCF accounts team will help walk you through the process.

Please see the OLCF Accounts and Projects section of this site to request a new project.",4.361089715789969
"Are there any specific instructions for compiling and batch scheduling SPI workflows on OLCF resources?
","The Citadel framework allows use of the OLCF's existing HPC resources Summit and Frontier for SPI workflows.  Citadel adds measures to ensure separation of SPI and non-SPI workflows and data. This section provides differences when using OLCF resources for SPI and non-SPI workflows.  Because the Citadel framework just adds another security layer to existing HPC resources, many system use methods are the same between SPI and non-SPI workflows.  For example, compiling, batch scheduling, and job layout are the same between the two security enclaves.  Because of this, the existing resource user",4.331408043064283
"What is the difference between a login node and a compute node on a high-performance computing cluster?
","Login nodes should be used for basic tasks such as file editing, code compilation, data backup, and job submission. Login nodes should not be used for memory- or compute-intensive tasks. Users should also limit the number of simultaneous tasks performed on the login resources. For example, a user should not run (10) simultaneous tar processes on a login node.

Compute-intensive, memory-intensive, or otherwise disruptive processes running on login nodes may be killed without warning.",4.489902578987894
"What is the difference between a login node and a compute node on a high-performance computing cluster?
","On Frontier, there are two major types of nodes you will encounter: Login and Compute. While these are similar in terms of hardware (see: https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes), they differ considerably in their intended use.",4.477872995221144
"What is the difference between a login node and a compute node on a high-performance computing cluster?
","On Summit, there are three major types of nodes you will encounter: Login, Launch, and Compute. While all of these are similar in terms of hardware (see: https://docs.olcf.ornl.gov/systems/summit_user_guide.html#summit-nodes), they differ considerably in their intended use.",4.438935424832952
"How can we run 32 MPI ranks on two compute nodes?
","This example is an extension of Example 6 to use 2 compute nodes. With the appropriate changes put in place in Example 6, it is a straightforward exercise to change to using 2 nodes (-N2) and 32 MPI ranks (-n32).

$ OMP_NUM_THREADS=1 srun -N2 -n32 -c4 --ntasks-per-gpu=2 --gpu-bind=closest --distribution=*:block ./hello_jobstep | sort

Example 8: 56 MPI ranks - where 7 ranks share a GPU (packed, single-node)",4.432534304596124
"How can we run 32 MPI ranks on two compute nodes?
","The intent with both of the following examples is to launch 8 MPI ranks across the node where each rank is assigned its own logical (and, in this case, physical) core.  Using the -m distribution flag, we will cover two common approaches to assign the MPI ranks -- in a ""round-robin"" (cyclic) configuration and in a ""packed"" (block) configuration. Slurm's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive method was used to request an allocation of 1 compute node for these examples: salloc -A <project_id> -t 30 -p <parition> -N 1",4.394696260002508
"How can we run 32 MPI ranks on two compute nodes?
","Extending Examples 2 and 3 to run on 2 nodes is also a straightforward exercise by changing the number of nodes to 2 (-N2) and the number of MPI ranks to 8 (-n8).

$ export OMP_NUM_THREADS=2
$ srun -N2 -n8 -c2 --gpus-per-task=1 --gpu-bind=map_gpu:0,1,2,3 ./hello_jobstep | sort

In the following examples, 2 MPI ranks will be mapped to 1 GPU. For the sake of brevity, OMP_NUM_THREADS will be set to 1, so -c1 will be used unless otherwise specified.

On AMD's MI100 GPUs, multi-process service (MPS) is not needed since multiple MPI ranks per GPU is supported natively.",4.393802719450275
"How can I check if a Parsl task has completed successfully?
","2021-06-28 16:10:46 parsl.dataflow.dflow:431 [INFO]  Task 0 completed (launched -> exec_done)
Hello from uname_result(system='Linux', node='a01n14', release='4.14.0-115.21.2.el7a.ppc64le', version='#1 SMP Thu May 7 22:22:31 UTC 2020', machine='ppc64le', processor='ppc64le')

Congratulations! You have now run a Parsl job on Summit.",4.089716466506696
"How can I check if a Parsl task has completed successfully?
","Parsl is a flexible and scalable parallel programming library for Python which is being developed at the University of Chicago. It augments Python with simple constructs for encoding parallelism. For more information about Parsl, please refer to its documentation.

Parsl can be installed with Conda for use on Summit by running the following from a login node:

$ module load workflows
$ module load parsl/1.1.0

The following instructions illustrate how to run a ""Hello world"" program with Parsl on Summit.",4.009479310249576
"How can I check if a Parsl task has completed successfully?
","Parsl needs to be able to write to the working directory from compute nodes, so we will work from within the member work directory and assume a project ID ABC123:

$ mkdir -p ${MEMBERWORK}/abc123/parsl-demo/
$ cd ${MEMBERWORK}/abc123/parsl-demo/

To run an example ""Hello world"" program with Parsl on Summit, create a file called hello-parsl.py with the following contents, but with your own project ID in the line specified:",3.9781962784698015
"How do I optimize the performance of the h5py program?
","Both HDF5 and h5py can be compiled with MPI support, which allows you to optimize your HDF5 I/O in parallel. MPI support in Python is accomplished through the mpi4py package, which provides complete Python bindings for MPI. Building h5py against mpi4py allows you to write to an HDF5 file using multiple parallel processes, which can be helpful for users handling large datasets in Python. h5Py is available after loading the default Python module on either Summit or Andes, but it has not been built with parallel support.",4.338085501948479
"How do I optimize the performance of the h5py program?
","There are various tools that allow users to interact with HDF5 data, but we will be focusing on h5py -- a Python interface to the HDF5 library. h5py provides a simple interface to exploring and manipulating HDF5 data as if they were Python dictionaries or NumPy arrays. For example, you can extract specific variables through slicing, manipulate the shapes of datasets, and even write completely new datasets from external NumPy arrays.",4.30971562287692
"How do I optimize the performance of the h5py program?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.

This guide teaches you how to build a personal, parallel-enabled version of h5py and how to write an HDF5 file in parallel using mpi4py and h5py.

In this guide, you will:

Learn how to install mpi4py

Learn how to install parallel h5py

Test your build with Python scripts

OLCF Systems this guide applies to:

Summit

Andes

Frontier",4.216257599206188
"How can I set the project name for Paraview?
","Select the host in the left side of the window.

Select the ""Launch Profiles"" tab in the right side of the window. This will display the known launch profiles for this host.

Select a ""Launch Profile"" and the settings are displayed in the tabs below.

You can set your Project ID in the ""Default Bank/Account"" field in the ""Parallel"" tab.

You can change the queue used by modifying the ""Partition/Pool/Queue"" field in the ""Parallel"" tab.

Once you have made your changes, press the ""Apply"" button, and then save the settings (Options/Save Settings).",4.141619471929304
"How can I set the project name for Paraview?
","<String default=""YOURUSERNAME""/>
        </Option>
        <Switch name=""PV_CLIENT_PLATFORM"">
          <Case value=""Apple"">
            <Set name=""TERM_PATH"" value=""/opt/X11/bin/xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />
            <Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Linux"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.090091119543707
"How can I set the project name for Paraview?
","<String default=""YOURUSERNAME""/>
        </Option>
        <Switch name=""PV_CLIENT_PLATFORM"">
          <Case value=""Apple"">
            <Set name=""TERM_PATH"" value=""/opt/X11/bin/xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />
            <Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Linux"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.088948407547861
"Run the script.py file using the python3 command.
","The OS-provided Python will no longer be accessible as python (including variations like /usr/bin/python or /usr/bin/env python); rather, you must specify it as python2 or python3. If you are using python from one of the modulefiles rather than the version in /usr/bin, this change should not affect how you invoke python in your scripts, although we encourage specifying python2 or python3 as a best practice.",4.121389145140498
"Run the script.py file using the python3 command.
",of how to run a Python script using PvBatch on Andes and Summit.,4.108006795711258
"Run the script.py file using the python3 command.
","You can find the version of Python that exists in this base environment by executing:

$ python --version

Python 3.8.3

For this guide, you are going to install a different version of Python.

To do so, create a new environment using the conda create command:

$ conda create -p /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/py3711-summit python=3.7.11",4.100334915193617
"What is the purpose of the TMPDIR environment variable on Summit?
","Setting the TMPDIR environment variable causes jobs to fail with JSM (jsrun) errors and can also cause jobs to bounce back and forth between eligible and running states until a retry limit has been reached and the job is placed in a blocked state (NOTE: This ""bouncing"" of job state can be caused for multiple reasons. Please see the known issue Jobs suspended due to retry limit / Queued job flip-flops between queued/running states if you are not setting TMPDIR). A bug has been filed with IBM to address this issue.",4.196717783704033
"What is the purpose of the TMPDIR environment variable on Summit?
","It is highly recommended to create new environments in the ""Project Home"" directory (on Summit, this is /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>). This space avoids purges, allows for potential collaboration within your project, and works better with the compute nodes. It is also recommended, for convenience, that you use environment names that indicate the hostname, as virtual environments created on one system will not necessarily work on others.",4.165121676036304
"What is the purpose of the TMPDIR environment variable on Summit?
","On Summit, Rhea and the DTNs, additional paths to the various project-centric work areas are available via the following symbolic links and/or environment variables:

Member Work Directory:  /gpfs/alpine/scratch/[userid]/[projid] or $MEMBERWORK/[projid]

Project Work Directory: /gpfs/alpine/proj-shared/[projid] or $PROJWORK/[projid]

World Work Directory: /gpfs/alpine/world-shared/[projid] or $WORLDWORK/[projid]",4.1539386541649295
"How long will my job wait in the queue?
","The basic priority-setting mechanism for jobs waiting in the queue is the time a job has been waiting relative to other jobs in the queue.

If your jobs require resources outside these queue policies such as higher priority or longer walltimes, please contact help@olcf.ornl.gov.

Jobs are aged according to the job's requested processor count (older age equals higher queue priority). Each job's requested processor count places it into a specific bin. Each bin has a different aging parameter, which all jobs in the bin receive.",4.232080071864026
"How long will my job wait in the queue?
","*apparent* time a job has been waiting. These factors include:



-  The number of nodes requested by the job.

-  The queue to which the job is submitted.

-  The 8-week history of usage for the project associated with the job.

-  The 8-week history of usage for the user associated with the job.



If your jobs require resources outside these queue policies, please complete the

relevant request form on the `Special Requests

<https://www.olcf.ornl.gov/support/getting-started/special-request-form/>`__

page. If you have any questions or comments on the queue policies below, please",4.219539434146537
"How long will my job wait in the queue?
","The basic priority mechanism for jobs waiting in the queue is the time the job has been waiting in the queue. If your jobs require resources outside these policies such as higher priority or longer walltimes, please contact help@olcf.ornl.gov

Jobs are aged according to the job's requested node count (older age equals higher queue priority). Each job's requested node count places it into a specific bin. Each bin has a different aging parameter, which all jobs in the bin receive.",4.197169013549429
"How do I check if my service is running correctly?
","For example, let's look at service that was created in the https://docs.olcf.ornl.gov/systems/nodeport.html#slate_services document. This document assumes that it was deployed to the my-project project.

If you run oc get services, you should see your service in the list.

$ oc get services
NAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)     AGE
my-service            ClusterIP   172.25.170.246   <none>        8080/TCP    8s

Then, get some information about the service with oc describe service my-service.",4.095499424779329
"How do I check if my service is running correctly?
","You should now be on the 'Pod' screen with the 'Overview' tab selected From here you can get a quick idea of the amount of resources (memory, CPU etc) that your  Pod is using.

Click on the 'Logs' tab to get the logs from your pod. This will display ""Hello World!"" in our example because of our echo command. There will be a dropdown here that for our example will contain only one item named 'hello-openshift'. This is the name of the container that you are viewing the logs for inside your pod.",4.087211588970315
"How do I check if my service is running correctly?
","On the command line, services can be created with the command oc create. Assuming our YAML file from above is in the file my-service.yaml, you can create the service with

$ oc create -f my-service.yaml

Then, you can run oc describe service my-service to see some information about it.",4.083048604353809
"How can I specify the mount path for a PVC in a Deployment?
","Below is a example of a Deployment that mounts a PVC:

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: test-mount-pvc
  name: test-mount-pvc
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test-mount-pvc
  template:
    metadata:
      labels:
        app: test-mount-pvc
    spec:
      containers:
      - image: busybox
        name: busybox
        volumeMounts:
        - mountPath: /data
          name: storage
      volumes:
      - name: storage
        persistentVolumeClaim:
          claimName: storage-1",4.293452969405329
"How can I specify the mount path for a PVC in a Deployment?
","To add the PVC to a pod using the web GUI first select Workloads and then Deployments in the hamburger menu on the left had side.

Application Deployments

Next, select the deployment that contains the pod you wish to add the storage to.

Select Actions in the upper left and then and then Add Storage.

Edit YAML

Fill out your Mount point and other options if you need them to be non-default values. Otherwise, hit the Add button at the bottom.

Add Storage Menu",4.251306318375083
"How can I specify the mount path for a PVC in a Deployment?
","PV Settings

This will redirect you to the status page of your new PVC. You should see Bound in the Status field.

PV Status

Once the claim has been granted to the project a pod within that project can access that storage. To do this, edit the YAML of the pod and under the volume portion of the file add a name and the name of the claim. It will look similar to the following.

volumes:
  name: pvol
  persistentVolumeClaim:
    claimName: storage-1

Then all that is left to do is mount the storage into a container:

volumeMounts:
  - mountPath: /tmp/
    name: pvol",4.2237064581608985
"How can I check which packages are available in the Anaconda Distribution Repository?
","$ pip uninstall numpy

The traditional, and more basic, approach to installing/uninstalling packages into a conda environment is to use the commands conda install and conda remove. Installing packages with this method checks the Anaconda Distribution Repository for pre-built binary packages to install. Let's do this to install NumPy:

$ conda install numpy

Because NumPy depends on other packages for optimization, this will also install all of its dependencies. You have just installed an optimized version of NumPy, now let's test it.",4.144107623450831
"How can I check which packages are available in the Anaconda Distribution Repository?
","$ conda env list

# conda environments:
#
                      *  /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/py3711-summit
base                     /sw/summit/python/3.8/anaconda3/2020.07-rhel8

Next, let's install a package (NumPy). There are a few different approaches.",4.136911696908122
"How can I check which packages are available in the Anaconda Distribution Repository?
","One way to install packages into your conda environment is to build packages from source using pip. This approach is useful if a specific package or package version is not available in the conda repository, or if the pre-compiled binaries don't work on the HPC resources (which is common). However, building from source means you need to take care of some of the dependencies yourself, especially for optimization. Pip is available to use after installing Python into your conda environment, which you have already done.",4.1059484341247545
"What are the permissions for the Member Work area on Lustre Orion?
","3

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

4

Retention is not applicable as files will follow purge cycle.",4.290945183217983
"What are the permissions for the Member Work area on Lustre Orion?
","3

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

4

Retention is not applicable as files will follow purge cycle.",4.290945183217983
"What are the permissions for the Member Work area on Lustre Orion?
",| Area | Path | Type | Permissions | Quota | Backups | Purged | Retention | On Compute Nodes | | --- | --- | --- | --- | --- | --- | --- | --- | --- | | Member Work | /lustre/orion/[projid]/scratch/[userid] | Lustre HPE ClusterStor | 700 | 50 TB | No | 90 days | N/A | Yes | | Project Work | /lustre/orion/[[projid]/proj-shared | Lustre HPE ClusterStor | 770 | 50 TB | No | 90 days | N/A | Yes | | World Work | /lustre/orion/[[projid]/world-shared | Lustre HPE ClusterStor | 775 | 50 TB | No | 90 days | N/A | Yes |,4.273542772265773
"How do I load the TAU module?
","$ module show tau
---------------------------------------------------------------
   /sw/summit/modulefiles/core/tau/2.28.1:
---------------------------------------------------------------
whatis(""TAU 2.28.1 github "")
setenv(""TAU_DIR"",""/sw/summit/tau/tau2/ibm64linux"")
prepend_path(""PATH"",""/sw/summit/tau/tau2/ibm64linux/bin"")
help([[https://www.olcf.ornl.gov/software_package/tau
]])

The available Makefiles are named per-compiler and are located in:",4.2554270870077495
"How do I load the TAU module?
","TAU is a portable profiling and tracing toolkit that supports many programming languages. The instrumentation can be done by inserting in the source code using an automatic tool based on the Program Database Toolkit (PDT), on the compiler instrumentation, or manually using the instrumentation API.

Webpage: https://www.cs.uoregon.edu/research/tau/home.php",4.1769243273538725
"How do I load the TAU module?
","TAU is installed with Program Database Toolkit (PDT) on Summit. PDT is a framework for analyzing source code written in several programming languages. Moreover, Performance Application Programming Interface (PAPI) is supported. PAPI counters are used to assess the CPU performance. In this section, some approaches for profiling and tracing will be presented.

In most cases, we need to use wrappers to recompile the application:

For C: replace the compiler with the TAU wrapper tau_cc.sh

For C++: replace the compiler with the TAU wrapper tau_cxx.sh",4.162734429270524
"Is it necessary to map GPUs using Slurm if I'm already mapping them programmatically within my code?
","In general, GPU mapping can be accomplished in different ways. For example, an application might map GPUs to MPI ranks programmatically within the code using, say, hipSetDevice. In this case, there might not be a need to map GPUs using Slurm (since it can be done in the code itself). However, many applications expect only 1 GPU to be available to each rank. It is this latter case that the following examples refer to.",4.488583255049093
"Is it necessary to map GPUs using Slurm if I'm already mapping them programmatically within my code?
","In general, GPU mapping can be accomplished in different ways. For example, an application might map MPI ranks to GPUs programmatically within the code using, say, hipSetDevice. In this case, since all GPUs on a node are available to all MPI ranks on that node by default, there might not be a need to map to GPUs using Slurm (just do it in the code). However, in another application, there might be a reason to make only a subset of GPUs available to the MPI ranks on a node. It is this latter case that the following examples refer to.",4.470934997957436
"Is it necessary to map GPUs using Slurm if I'm already mapping them programmatically within my code?
","In general, GPU mapping can be accomplished in different ways. For example, an application might map MPI ranks to GPUs programmatically within the code using, say, hipSetDevice. In this case, since all GPUs on a node are available to all MPI ranks on that node by default, there might not be a need to map to GPUs using Slurm (just do it in the code). However, in another application, there might be a reason to make only a subset of GPUs available to the MPI ranks on a node. It is this latter case that the following examples refer to.",4.470934997957436
"What is the advantage of using a block volume instead of the default NFS volume for persistent storage?
","By design pods are ephemeral. They are meant to be something that can be killed with relatively little impact to the application. Since containers are ephemeral, we need a way to consume storage that is persistent and can hold data for our applications. One option that Kubernetes has for storing data is with Persistent Volumes. PersistentVolume objects are created by the cluster administrator and reference storage that is on a resilient storage platform such as NetApp. A user can request storage by creating a PersistentVolumeClaim which requests a desired size for a PersistentVolume. The",4.135666771991285
"What is the advantage of using a block volume instead of the default NFS volume for persistent storage?
","Due to Podman's lack of support for storage on GPFS and NFS, container images will be built on the login nodes using the node-local NVMe on the login node. This NVMe is mounted in /tmp/containers. Users should treat this storage as temporary. Any data (container image layers or otherwise) in this storage will be purged if the node is ever rebooted or when it gets full.  So any images created with Podman need to be converted to tar files using podman save and stored elsewhere if you wish to preserve your image.",4.077620529911697
"What is the advantage of using a block volume instead of the default NFS volume for persistent storage?
",a desired size for a PersistentVolume. The cluster administrator or some automated mechanism will provision the storage on the backend and make it available to the cluster via the PersistentVolumeClaim.,4.063026429482041
"Is it allowed to use OLCF resources for sexually oriented information?
","The requirements outlined in this document apply to all individuals who have an OLCF account. It is your responsibility to ensure that all individuals have the proper need-to-know before allowing them access to the information on OLCF computing resources. This document will outline the main security concerns.

OLCF computing resources are for business use only. Installation or use of software for personal use is not allowed. Incidents of abuse will result in account termination. Inappropriate uses include, but are not limited to:

Sexually oriented information",4.347209212749795
"Is it allowed to use OLCF resources for sexually oriented information?
","Computers, software, and communications systems provided by the OLCF are to be used for work associated with and within the scope of the approved project. The use of OLCF resources for personal or non-work-related activities is prohibited. All computers, networks, E-mail, and storage systems are property of the United States Government. Any misuse or unauthorized access is prohibited, and is subject to criminal and civil penalties. OLCF systems are provided to our users without any warranty. OLCF will not be held liable in the event of any system failure or data loss or corruption for any",4.293821615079802
"Is it allowed to use OLCF resources for sexually oriented information?
","The OLCF computer systems are operated as research systems and only contain data related to scientific research and do not contain personally identifiable information (data that falls under the Privacy Act of 1974 5U.S.C. 552a). Use of OLCF resources to store, manipulate, or remotely access any national security information is strictly prohibited. This includes, but is not limited to: classified information, unclassified controlled nuclear information (UCNI), naval nuclear propulsion information (NNPI), the design or development of nuclear, biological, or chemical weapons or any weapons of",4.288106739896643
"How can you ensure compliance with the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  Title: Project Reporting Policy Version: 12.10",4.335961680341906
"How can you ensure compliance with the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to or use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title:Security Policy Version: 12.10",4.321987165442805
"How can you ensure compliance with the OLCF Policy?
","The requirements outlined in this document apply to all individuals who have an OLCF account. It is your responsibility to ensure that all individuals have the proper need-to-know before allowing them access to the information on OLCF computing resources. This document will outline the main security concerns.

OLCF computing resources are for business use only. Installation or use of software for personal use is not allowed. Incidents of abuse will result in account termination. Inappropriate uses include, but are not limited to:

Sexually oriented information",4.318813247706544
"Can Valgrind help me optimize my Andes program's memory usage?
","The Valgrind tool suite provides a number of debugging and profiling tools. The most popular is Memcheck, a memory checking tool which can detect many common memory errors such as:

Touching memory you shouldn’t (eg. overrunning heap block boundaries, or reading/writing freed memory).

Using values before they have been initialized.

Incorrect freeing of memory, such as double-freeing heap blocks.

Memory leaks.

Valgrind is available on Andes via the valgrind module:

module load valgrind",4.404501025959419
"Can Valgrind help me optimize my Andes program's memory usage?
","The Valgrind tool suite provides a number of debugging and profiling tools. The most popular is Memcheck, a memory checking tool which can detect many common memory errors such as:

Touching memory you shouldn’t (eg. overrunning heap block boundaries, or reading/writing freed memory).

Using values before they have been initialized.

Incorrect freeing of memory, such as double-freeing heap blocks.

Memory leaks.

Valgrind is available on Andes via the valgrind module:

module load valgrind",4.404501025959419
"Can Valgrind help me optimize my Andes program's memory usage?
","Valgrind

Valgrind is an instrumentation framework for building dynamic analysis tools. There are Valgrind tools that can automatically detect many memory management and threading bugs, and profile your programs in detail. You can also use Valgrind to build new tools.

The Valgrind distribution currently includes five production-quality tools: a memory error detector, a thread error detector, a cache and branch-prediction profiler, a call-graph generating cache profiler, and a heap profiler. It also includes two experimental tools: a data race detector, and an instant memory leak detector.",4.2980634158596445
"What are the rendering options available in ParaView?
","We offer two rendering modes of the ParaView API on our systems: OSMesa and EGL.  OSMesa is intended for use on regular compute nodes, whereas EGL is intended for use on GPU enabled nodes. When running interactively, you do not need to download or install anything special to use the EGL or OSMesa versions, as you'll be able to choose between those options when connecting to the system (see https://docs.olcf.ornl.gov/systems/paraview.html#paraview-gui below). If instead you're running in batch mode on the command line (see https://docs.olcf.ornl.gov/systems/paraview.html#paraview-command-line",4.269191688759891
"What are the rendering options available in ParaView?
","ParaView is an open-source, multi-platform data analysis and visualization application. ParaView users can quickly build visualizations to analyze their data using qualitative and quantitative techniques. The data exploration can be done interactively in 3D or programmatically using ParaView’s batch processing capabilities. Further information regarding ParaView can be found at the links provided in the https://docs.olcf.ornl.gov/systems/paraview.html#paraview-resources section.",4.196621291736048
"What are the rendering options available in ParaView?
","below), you can choose between the rendering options by loading its corresponding module on the system you're connected to. For example, to see these modules on Summit:",4.134438192000646
"What is the emphasis of the ALCC program?
","ALCC – The ASCR Leadership Computing Challenge (ALCC) is open to scientists from the research community in national laboratories, academia and industry. The ALCC program allocates computational resources at the OLCF for special situations of interest to the Department with an emphasis on high-risk, high-payoff simulations in areas directly related to the Department’s energy mission in areas such as advancing the clean energy agenda and understanding the Earth’s climate, for national emergencies, or for broadening the community of researchers capable of using leadership computing resources.",4.327845407909418
"What is the emphasis of the ALCC program?
","OLCF training events can go through a streamlined version of the approval process before gaining access to the system. The remainder of this section of the user guide describes ""Ascent-specific"" information intended for participants of OLCF training events.",4.096547762524531
"What is the emphasis of the ALCC program?
","SummitPLUS is one of the new allocation programs that will be used to allocate a significant portion of the system for 2024. The program is open to researchers from academia, government laboratories, federal agencies, and industry. We welcome proposals for computationally ready projects from investigators who are new to Summit, as well as from previous INCITE, ALCC, DD, ECP awardees and projects. We encourage proposals on emerging paradigms for computational campaigns including data-intensive science and AI/ML.",4.08780380945901
"Can I use the Globus data transfer service to move data between Frontier and HPSS?
",Please note that the HPSS is not mounted directly onto Frontier nodes. There are two main methods for accessing and moving data to/from the HPSS. The first is to use the command line utilities hsi and htar. The second is to use the Globus data transfer service. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-hpss for more information on both of these methods.,4.572543156288137
"Can I use the Globus data transfer service to move data between Frontier and HPSS?
","Due to available bandwidth, transferring data through the HPSS will be a slower route than using Globus to transfer directly between Alpine and Orion.

Transferring data through the HPSS is a multi-step process and will be slower than direct transfers using Globus.

Globus is the suggested tool to migrate data off of Alpine.  Please do not use HPSS as a data migration method.



On January 01, data remaining on the GPFS filesystem, Alpine, will no longer be accessible and will be permanently deleted . Following this date, the OLCF will no longer be able to retrieve data remaining on Alpine.",4.442701399156875
"Can I use the Globus data transfer service to move data between Frontier and HPSS?
",Data will not be automatically transferred from Alpine to Orion. Users should consider the data needed from Alpine and transfer it. Globus is the preferred method and there is access to both Orion and Alpine through the Globus OLCF DTN endpoint. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-transferring-data-globus. Use of HPSS to specifically stage data for the Alpine to Orion transfer is discouraged.,4.436755131873338
"Can I run a job on a specific set of nodes on Frontier?
","Unlike Summit and Titan, there are no launch/batch nodes on Frontier. This means your batch script runs on a node allocated to you rather than a shared node. You still must use the job launcher (srun) to run parallel jobs across all of your nodes, but serial tasks need not be launched with srun.



To easily visualize job examples (see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-mapping further below), the compute node diagram has been simplified to the picture shown below.

Simplified Frontier node architecture diagram",4.333930807053015
"Can I run a job on a specific set of nodes on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.286855862762005
"Can I run a job on a specific set of nodes on Frontier?
","Recall from the System Overview that Frontier contains two node types: Login and Compute. When you connect to the system, you are placed on a login node. Login nodes are used for tasks such as code editing, compiling, etc. They are shared among all users of the system, so it is not appropriate to run tasks that are long/computationally intensive on login nodes. Users should also limit the number of simultaneous tasks on login nodes (e.g. concurrent tar commands, parallel make",4.252947744995132
"Is there a way to view the allocation and usage of projects on the OLCF?
","Each user may view usage for projects on which they are members from the command line tool showusage and the myOLCF site.

On the Command Line via showusage

The showusage utility can be used to view your usage from January 01 through midnight of the previous day. For example:

$ showusage
  Usage:
                           Project Totals
  Project             Allocation      Usage      Remaining     Usage
  _________________|______________|___________|____________|______________
  abc123           |  20000       |   126.3   |  19873.7   |   1560.80",4.42034477127671
"Is there a way to view the allocation and usage of projects on the OLCF?
","Each user may view usage for projects on which they are members from the command line tool showusage and the myOLCF site.

The showusage utility can be used to view your usage from January 01 through midnight of the previous day. For example:

$ showusage
  Usage:
                           Project Totals
  Project             Allocation      Usage      Remaining     Usage
  _________________|______________|___________|____________|______________
  abc123           |  20000       |   126.3   |  19873.7   |   1560.80

The -h option will list more usage details.",4.398018676539779
"Is there a way to view the allocation and usage of projects on the OLCF?
","Each user may view usage for projects on which they are members from the command line tool showusage and the myOLCF site.

The showusage utility can be used to view your usage from January 01 through midnight of the previous day. For example:

$ showusage
  Usage:
                           Project Totals
  Project             Allocation      Usage      Remaining     Usage
  _________________|______________|___________|____________|______________
  abc123           |  20000       |   126.3   |  19873.7   |   1560.80

The -h option will list more usage details.",4.398018676539779
"How can I view the YAML configuration for a route in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.114980836166529
"How can I view the YAML configuration for a route in Slate?
","Once your project has been approved, you only need to give your route a label to tell the OpenShift router to expose this service externally. You can do this in the CLI or in the web interface.

<string>:249: (INFO/1) Duplicate implicit target name: ""cli"".

On the CLI, run oc label route {ROUTE_NAME} ccs.ornl.gov/externalRoute=true.

In the web interface, from the side menu, select Networking, then Routes.

Routes Menu

This will show a list of your routes. Click the route you want to expose, and click the YAML tab.",4.114668240077645
"How can I view the YAML configuration for a route in Slate?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.08421443352813
"How does Lmod handle dependencies between modules?
","Lmod is a recursive environment module system, meaning it is aware of module compatibility and actively alters the environment to protect against conflicts. Messages to stderr are issued upon Lmod implicitly altering the environment. Environment modules are structured hierarchically by compiler family such that packages built with a given compiler will only be accessible if the compiler family is first present in the environment.

Lmod can interpret both Lua modulefiles and legacy Tcl modulefiles. However, long and logic-heavy Tcl modulefiles may require porting to Lua.",4.340149523636558
"How does Lmod handle dependencies between modules?
","Modules are changed recursively. Some commands, such as module swap, are available to maintain compatibility with scripts using Tcl Environment Modules, but are not necessary since Lmod recursively processes loaded modules and automatically resolves conflicts.",4.336862798680711
"How does Lmod handle dependencies between modules?
","Modules are changed recursively. Some commands, such as module swap, are available to maintain compatibility with scripts using Tcl Environment Modules, but are not necessary since Lmod recursively processes loaded modules and automatically resolves conflicts.",4.336862798680711
"Can I import parallel Python libraries like mpi4py and h5py directly on the nodes on Summit?
","Both HDF5 and h5py can be compiled with MPI support, which allows you to optimize your HDF5 I/O in parallel. MPI support in Python is accomplished through the mpi4py package, which provides complete Python bindings for MPI. Building h5py against mpi4py allows you to write to an HDF5 file using multiple parallel processes, which can be helpful for users handling large datasets in Python. h5Py is available after loading the default Python module on either Summit or Andes, but it has not been built with parallel support.",4.292407605223617
"Can I import parallel Python libraries like mpi4py and h5py directly on the nodes on Summit?
","Summit

.. code-block:: bash

   $ HDF5_MPI=""ON"" CC=mpicc pip install --no-cache-dir --no-binary=h5py h5py

Andes

.. code-block:: bash

   $ HDF5_MPI=""ON"" CC=mpicc pip install --no-cache-dir --no-binary=h5py h5py

Frontier

.. code-block:: bash

   $ HDF5_MPI=""ON"" CC=cc HDF5_DIR=${OLCF_HDF5_ROOT} pip install --no-cache-dir --no-binary=h5py h5py",4.279757440516718
"Can I import parallel Python libraries like mpi4py and h5py directly on the nodes on Summit?
","Large workloads.

Long runtimes on Summit's high memory nodes.

Your Python script has support for multi-gpu/multi-node execution via dask-cuda.

Your Python script is single GPU but requires simultaneous job steps.

RAPIDS is provided in Jupyter following  these instructions.

Note that Python scripts prepared on Jupyter can be deployed on Summit if they use the same RAPIDS version. Use !jupyter nbconvert --to script my_notebook.ipynb to convert notebook files to Python scripts.

RAPIDS is provided on Summit through the module load command:",4.267759340537503
"How can I specify the base image and installed packages in my Dockerfile?
","Here you will have the option to select from a number of container images. Select the one that matches the source code in the git repository that you will be using. In this example we'll be using Python.

Using the drop down menu select the version of the language that your source is written in and click Select.

Then input the repo that contains the repository you wish to use.",4.1587153935426455
"How can I specify the base image and installed packages in my Dockerfile?
","# using the base image
        image: ""image-registry.openshift-image-registry.svc:5000/openshift/ccs-rhel7-base-amd64""
        # Generic command that will not return
        command: [""cat""]
        # Need a tty if we are to SSH. Need stdin for tty
        tty: true
        stdin: true",4.095419011178484
"How can I specify the base image and installed packages in my Dockerfile?
","# using the base image
        image: ""image-registry.openshift-image-registry.svc:5000/openshift/ccs-rhel7-base-amd64""
        # Generic command that will not return
        command: [""cat""]
        # Need a tty if we are to SSH. Need stdin for tty
        tty: true
        stdin: true",4.095419011178484
"Can I use my non-SPI account to access SPI resources?
","Similar to the non-SPI resources, SPI resources require two-factor authentication.  If you are new to the center, you will receive a SecurID fob during the account approval/creation process.  If you are an existing user of non-SPI resources, you can use the same SecurID fob and PIN used on your non-SPI account.

Also similar to non-SPI resources, you will connect directly to the SPI resources through ssh.

ORNL's KDI users are an exception and cannot, by policy, log directly into SPI resources.  KDI users, please follow the KDI documented procedures:",4.346821151269187
"Can I use my non-SPI account to access SPI resources?
","Once a project has been approved and created, the next step will be to request user accounts.  A user account will allow an individual to access the project's allocated resources.    This process to request an account is very similar to the process used for non-SPI projects.  One notable difference between SPI and non-SPI accounts: SPI usernames are unique to a project.  SPI usernames use the format: <userid>_<proj>_mde.  If you have access to three SPI projects, you will have three userIDs with three separate home areas.",4.3053645484460965
"Can I use my non-SPI account to access SPI resources?
","For users with accounts on non-SPI resources, you will use the same SecurID fob and PIN, but you must specify your unique SPI userID when you connect.  The ID will be used to place you in the proper UNIX groups allowing access to the project specific data, directories, and allocation.",4.273509538385147
"What kind of data can I find in the VisIt Data Archive?
","For sample data and additional examples, explore the VisIt Data Archive and various VisIt Tutorials. Supplementary test data can be found in your local installation in the data directory:

Linux: /path/to/visit/data

macOS: /path/to/VisIt.app/Contents/Resources/data

Windows: C:\path\to\LLNL\VisIt x.y.z\data

Additionally, check out our beginner friendly OLCF VisIt Tutorial which uses Andes to visualize example datasets.",4.404663025883609
"What kind of data can I find in the VisIt Data Archive?
","Past VisIt Tutorials are available on the Visit User's Wiki along with a set of Updated Tutorials available in the VisIt User Manual.

Sample data not pre-packaged with VisIt can be found in the VisIt Data Archive.

Older VisIt Versions with their release notes can be found on the old VisIt website, and Newer Versions can be found on the new VisIt website with release notes found on the VisIt Blog or VisIt Github Releases page.

Non-ORNL related bugs and issues in VisIt can be found and reported on Github.",4.320158768391838
"What kind of data can I find in the VisIt Data Archive?
","VisIt is now available on Frontier through the UMS022 module.

VisIt 3.3.3 is now available on Andes.

VisIt is an interactive, parallel analysis and visualization tool for scientific data. VisIt contains a rich set of visualization features so you can view your data in a variety of ways. It can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes. Further information regarding VisIt can be found at the links provided in the https://docs.olcf.ornl.gov/systems/visit.html#visit-resources section.",4.212383759286857
"How do I set up a Spack environment for application development using the OLCF provided files as a template?
","This not intended as a guide for a new Spack user.  Please see the Spack 101 tutorial if you need assistance starting out with Spack.

The provided Spack environment files are intended to assist OLCF users in setup their development environment at the OLCF.  The base environment file includes the compilers and packages that are installed at the system level.",4.562204724861847
"How do I set up a Spack environment for application development using the OLCF provided files as a template?
","This guide meant as an example for a user to setup a Spack environment for application development using the OLCF provided files as a template.

The OLCF uses an internal mirror of the Spack repo that is customized for use on OLCF systems.  This results in the hash values generated by another version of Spack to not match.  It is recommended to use the existing module as external packages instead of chaining at this time.

The provided spack.yaml files are templates for a user to use as an example.",4.517645604565708
"How do I set up a Spack environment for application development using the OLCF provided files as a template?
","Traditionally, the user environment is modified by module files.  For example, a user would add use  module load cmake/3.18.2 to load CMake version 3.18.2 into their environment.  Using a Spack environment, a user can add an OLCF provided package and build against it using Spack without having to load the module file separately.

The information presented here is a subset of what can be found at the Spack documentation site.",4.424157628860609
"What is the maximum number of CPUs that can access a single GPU in Summit?
","Each Summit Compute node has 6 NVIDIA V100 GPUs.  The NVIDIA Tesla V100 accelerator has a peak performance of 7.8 TFLOP/s (double-precision) and contributes to a majority of the computational work performed on Summit. Each V100 contains 80 streaming multiprocessors (SMs), 16 GB (32 GB on high-memory nodes) of high-bandwidth memory (HBM2), and a 6 MB L2 cache that is available to the SMs. The GigaThread Engine is responsible for distributing work among the SMs and (8) 512-bit memory controllers control access to the 16 GB (32 GB on high-memory nodes) of HBM2 memory. The V100 uses NVIDIA's",4.373938927425795
"What is the maximum number of CPUs that can access a single GPU in Summit?
","Summit node architecture diagram

The basic building block of Summit is the IBM Power System AC922 node. Each of the approximately 4,600 compute nodes on Summit contains two IBM POWER9 processors and six NVIDIA Tesla V100 accelerators and provides a theoretical double-precision capability of approximately 40 TF. Each POWER9 processor is connected via dual NVLINK bricks, each capable of a 25GB/s transfer rate in each direction.",4.289593562535529
"What is the maximum number of CPUs that can access a single GPU in Summit?
","Summit's V100 GPUs are configured to have a default compute mode of EXCLUSIVE_PROCESS. In this mode, the GPU is assigned to only a single process at a time, and can accept work from multiple process threads concurrently.

It may be desirable to change the GPU's compute mode to DEFAULT, which enables multiple processes and their threads to share and submit work to it simultaneously. To change the compute mode to DEFAULT, use the -alloc_flags gpudefault option.",4.289381291083554
"How can I use bpeek to interact with a batch job?
","If you want to check the STDOUT/STDERR of a currently running job, you can do so with the bpeek command. The command supports several options:

| Command | Description | | --- | --- | | bpeek -J jobname | Show STDOUT/STDERR for the job you've most recently submitted with the name jobname | | bpeek 12345 | Show STDOUT/STDERR for job 12345 | | bpeek -f ... | Used with other options. Makes bpeek use tail -f and exit once the job completes. |",4.337550185778567
"How can I use bpeek to interact with a batch job?
","Most users will find batch jobs an easy way to use the system, as they allow you to ""hand off"" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.",4.252086273011651
"How can I use bpeek to interact with a batch job?
","Most users will find batch jobs to be the easiest way to interact with the system, since they permit you to hand off a job to the scheduler and then work on other tasks; however, it is sometimes preferable to run interactively on the system. This is especially true when developing, modifying, or debugging a code.",4.241285506819959
"What is a common issue that may arise when running containers as non-root on Slate?
","Running containers as non-root can be challenging when consuming container images from an upstream source such as the Docker Hub since many images are build with the intention of running as root. You may encounter this issue if your container enters a CrashLoopBackoff state, check the logs for the container and if there are Permission Denied issues then you may need to fix directory permissions in the image.",4.349883393943009
"What is a common issue that may arise when running containers as non-root on Slate?
","Users will be building and running containers on Summit without root permissions i.e. containers on Summit are rootless.  This means users can get the benefits of containers without needing additional privileges. This is necessary for a shared system like Summit. And this is part of the reason why Docker doesn't work on Summit. Podman and Singularity provides rootless support but to different extents hence why users need to use a combination of both.

Users will need to set up a file in their home directory /ccs/home/<username>/.config/containers/storage.conf with the following content:",4.188997306393657
"What is a common issue that may arise when running containers as non-root on Slate?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",4.108285052620696
"How can I check the status of a job?
","The most straightforward monitoring is with the bjobs command. This command will show the current queue, including both pending and running jobs. Running bjobs -l will provide much more detail about a job (or group of jobs). For detailed output of a single job, specify the job id after the -l. For example, for detailed output of job 12345, you can run bjobs -l 12345 . Other options to bjobs are shown below. In general, if the command is specified with -u all it will show information for all users/all jobs. Without that option, it only shows your jobs. Note that this is not an exhaustive list.",4.275793422207766
"How can I check the status of a job?
","bjobs and jobstat help to identify what’s currently running and scheduled to run, but sometimes it’s beneficial to know how much of the system is not currently in use or scheduled for use.",4.199693236742985
"How can I check the status of a job?
","To view the status of multiple jobs launched sequentially or concurrently within a batch script, you can use jslist to see which are completed, running, or still queued. If you are using it outside of an interactive batch job, use the -c option to specify the CSM allocation ID number. The following example shows how to obtain the CSM allocation number for a non interactive job and then check its status.

$ bsub test.lsf
Job <26238> is submitted to default queue <batch>.

$ bjobs -l 26238 | grep CSM_ALLOCATION_ID
Sun Feb 16 19:01:18: CSM_ALLOCATION_ID=34435",4.174611712406701
"How much storage space is available for each user in the Member Archive area?
","Use of User Archive areas for data storage is deprecated as of January 14, 2020. The user archive area for any user account created after that date (or for any user archive directory that is empty of user files after that date) will contain only symlinks to the top-level directories for each of the user's projects on HPSS. Users with existing data in a User Archive directory are encouraged to move that data to an appropriate project-based directory as soon as possible.  The information below is simply for reference for those users with existing data in User Archive directories.",4.254876411821923
"How much storage space is available for each user in the Member Archive area?
",| 50 TB | No | 90 days | N/A 4 | Yes | | Member Archive | /hpss/prod/[projid]/users/$USER | HPSS | 700 | 100 TB | No | No | 90 days | No | | Project Archive | /hpss/prod/[projid]/proj-shared | HPSS | 770 | 100 TB | No | No | 90 days | No | | World Archive | /hpss/prod/[projid]/world-shared | HPSS | 775 | 100 TB | No | No | 90 days | No |,4.2311619373062
"How much storage space is available for each user in the Member Archive area?
","Project members get an individual Member Archive directory for each associated project; these reside on the High Performance Storage System (HPSS), OLCF's tape-archive storage system. Member Archive areas are not shared with other users of the system and are intended for project data that the user does not want to make available to other users.  HPSS is intended for data that do not require day-to-day access. Users should not store data unrelated to OLCF projects on HPSS. Users should periodically review files and remove unneeded ones. See the section",4.224723577676512
"How do I set the atomics mode in Frontier's rocBLAS library?
","results from these libraries, it is recommended to turn the atomic operations off by setting the mode via the rocBLAS or hipBLAS handle:",4.392276833616072
"How do I set the atomics mode in Frontier's rocBLAS library?
","...
rocblas_create_handle(handle);
rocblas_set_atomics_mode(handle, rocblas_atomics_not_allowed);

hipblasCreate(&handle);
hipblasSetAtomicsMode(handle, HIPBLAS_ATOMICS_NOT_ALLOWED);



On Tuesday, September 19, 2023, Frontier's system software was upgraded. The following changes took place:

The system was upgraded to Slingshot Host Software 2.1.0.

ROCm 5.6.0 and 5.7.0 are now available via the rocm/5.6.0 and rocm/5.7.0 modulefiles, respectively.

HPE/Cray Programming Environments (PE) 23.09 is now available via the cpe/23.09 modulefile.

ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.",4.27995846348767
"How do I set the atomics mode in Frontier's rocBLAS library?
","Some functionality provided by the rocBLAS and hipBLAS libraries use atomic operations to improve performance by default. This can cause results to not be bit-wise reproducible. Level 2 functions that may use atomic operations include: gemv, hemv, and symv, which introduced atomic operations in ROCm 5.5. All of the Level 3 functions, along with Level 2 trsv, may use atomic operations where dependent on gemm. Atomic operations are used for problem sizes where they are shown to improve performance. If it is necessary to have bit-wise reproducible results from these libraries, it is recommended",4.155555049861376
"How do I change the working directory in a batch script?
","6: This line is left blank, so it will be ignored.

7: This command will change the current directory to the directory from where the script was submitted.

8: This command will run the date command.

9: This command will run (8) MPI instances of the executable a.out on the compute nodes allocated by the batch system.

Batch scripts can be submitted for execution using the sbatch command. For example, the following will submit the batch script named test.slurm:

sbatch test.slurm",4.170351923553756
"How do I change the working directory in a batch script?
","Shell Commands

<string>:526: (INFO/1) Duplicate implicit target name: ""shell commands"".

6: This line is left blank, so it will be ignored.

7: This command will change the current directory to the directory from where the script was submitted.

8: This command will run the date command.

9: This command will run (8) MPI instances of the executable a.out on the compute nodes allocated by the batch system.

Batch scripts can be submitted for execution using the sbatch command. For example, the following will submit the batch script named test.slurm:

sbatch test.slurm",4.107052488419548
"How do I change the working directory in a batch script?
","by a shell name. For example, to request an interactive batch job (with bash as the shell) equivalent to the sample batch script above, you would use the command: bsub -W 3:00 -nnodes 2048 -P ABC123 -Is /bin/bash",4.078636569056479
"What is the purpose of the OLCF QCUP?
","There are several broad aims of the Quantum Computing User Program at OLCF, which are as follows:

Enable Research

The QCUP aims to provide a broad spectrum of user access to the best available quantum computing systems. Once a user’s intended research has been reviewed for merit and user agreements have been established, we seek to provide users with the opportunity to become familiar with the unique aspects and challenges of quantum computing, as well as to implement and test quantum algorithms on the available systems.

Evaluate Technology",4.418715167947309
"What is the purpose of the OLCF QCUP?
","Welcome! The information below introduces how we structure projects and user accounts for access to the Quantum resources within the QCUP program. In general, OLCF QCUP resources are granted to projects, which are in turn made available to the approved users associated with each project.",4.38914843775825
"What is the purpose of the OLCF QCUP?
","The OLCF will then establish a QCUP project and notify the PI of its creation along with the 6-character OLCF QCUP Project ID and resources allocation details. At this time project participants may proceed with applying for their individual user accounts.

QCUP Projects have a finite duration; when starting, projects get however many months are left in that allocation period and then must be renewed for subsequent 6 month intervals. Projects can be renewed by filling out a renewal form (:download:`Accounts Renewal Form <Quantum-Renewal-Form.docx>`) and emailing it to accounts@ccs.ornl.gov.",4.325601470184725
"How do I compile a C++ program using Score-P?
","To instrument your code, you need to compile the code using the Score-P instrumentation command (scorep), which is added as a prefix to your compile statement. In most cases the Score-P instrumentor is able to automatically detect the programming paradigm from the set of compile and link options given to the compiler. Some cases will, however, require some additional link options within the compile statement e.g. CUDA instrumentation.

Below are some basic examples of the different instrumentation scenarios:",4.396262036413534
"How do I compile a C++ program using Score-P?
","$ scorep --user gcc -c test.c
$ scorep --user gcc -o test test.o

Now you can manually instrument Score-P to the source code as seen below:

C,C++

.. code::

   #include <scorep/SCOREP_User.h>

   void foo() {
      SCOREP_USER_REGION_DEFINE(my_region)
      SCOREP_USER_REGION_BEGIN(my_region, ""foo"", SCOREP_USER_REGION_TYPE_COMMON)
      // do something
      SCOREP_USER_REGION_END(my_region)
   }

Fortran

.. code::

   #include <scorep/SCOREP_User.inc>",4.327817356499319
"How do I compile a C++ program using Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi-processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations.

For detailed information about using Score-P on Summit and the builds available, please see the Score-P Software Page.",4.261933642443999
"Can you provide an example of a build location provided by the partner project?
",project on Summit/Frontier.  The partner project would provide login access to the non-SPI Summit/Frontier login nodes and a build location that is writable from the non-SPI Summit/Frontier and read-only from within the Citadel framework.  For example the partner project would provide the ability to build on Summit/Frontier in /sw/summit/mde/abc123_mde where abc123_mde is replaced by your Citadel project. This location is writable from Summit/Frontier but only readable from within the Citadel framework.,4.143137968027116
"Can you provide an example of a build location provided by the partner project?
",Project-Centric Storage Areas,4.01021965959001
"Can you provide an example of a build location provided by the partner project?
","Project PI must ensure that installations are tested to ensure basic functionality before being released to users. These are expected to be at minimum basic function/unit tests to ensure that the build/install was successful.

The resources provided by the OLCF for UMS shall not be used for software development or for routine testing purposes beyond the installation testing as described above.

Products may be removed from UMS at the request of the Project PI by notifying the OLCF (help@olcf.ornl.gov) of their intent and cleaning up their directory space.",3.987676853586793
"How do I enter my Docker username and password for Singularity to download the image from Dockerhub?
","You can now create a Singularity sif file with singularity build --disable-cache --docker-login simple.sif docker://docker.io/<username>/simple.

This will ask you to enter your Docker username and password again for Singularity to download the image from Dockerhub and convert it to a sif file.",4.414033372754759
"How do I enter my Docker username and password for Singularity to download the image from Dockerhub?
","If you are familiar with using a container registry like DockerHub, you can use that to save your Podman container images and use Singularity to pull from the registry and build the sif file. Below, we will use DockerHub as the example but there are many other container registries that you can use.

Using the simple example from the previous section, build the container image with podman build -t docker.io/<username>/simple -f simple.dockerfile . where <username> is your user on DockerHub.

podman push uses the URL in the container image's name to push to the appropriate registry.",4.245739756002876
"How do I enter my Docker username and password for Singularity to download the image from Dockerhub?
","Check if your image is created

$ podman image ls
REPOSITORY                         TAG      IMAGE ID      CREATED      SIZE
docker.io/subilabrahamornl/simple  latest   e47dbfde3e99  3 hours ago  687 MB
localhost/simple                   latest   e47dbfde3e99  3 hours ago  687 MB
quay.io/centos/centos              stream8  ad6f8b5e7f64  8 days ago   497 MB

Run podman login docker.io and enter your account's username and password so that Podman is logged in to the container registry before pushing.

Push the container image to the registry with podman push docker.io/<username>/simple.",4.12834354539607
"How can I create a Service for the MongoDB deployment in Slate?
","MongoDB is a common ""NoSQL"" database. We will be creating a Deployment to run the MongoDB service and expose it external to the cluster after setting up authentication. We will also be deploying a management Web UI for viewing queries.

Access to an allocation in Slate, the NCCS Kubernetes service

oc client installed

CLI client is logged into the cluster (oc login https://api.<cluster>.ccs.ornl.gov)

<string>:18: (INFO/1) Duplicate implicit target name: ""deploy mongodb"".",4.328293927868686
"How can I create a Service for the MongoDB deployment in Slate?
","To learn more about FireWorks, please refer to its extensive online documentation.

Before using FireWorks itself, you will need a MongoDB service running on https://docs.olcf.ornl.gov/systems/fireworks.html#Slate<slate>. A tutorial to deploy MongoDB is available in the Slate documentation.

You will need to know the connection information for both MongoDB so that FireWorks can be configured to connect to it.

Then, to use FireWorks on Summit, load the module as shown below:

$ module load workflows
$ module load fireworks/2.0.2",4.2350040048053055
"How can I create a Service for the MongoDB deployment in Slate?
","We will be deploying MongoDB with a StatefulSet. This is a special kind of deployment controller that is different from a normal Deployment in a few distinct ways and is primarily meant for for applications that rely on well known names for each pod.

We will create a Service with the StatefulSet because the StatefulSet controller requires a headless service in order to provide well-known DNS identifiers.",4.175011683714475
"What is the difference between SMT1, SMT2, and SMT4 on Summit?
",The default SMT level is 4.,4.32193763482889
"What is the difference between SMT1, SMT2, and SMT4 on Summit?
",For Summit:,4.146978714126995
"What is the difference between SMT1, SMT2, and SMT4 on Summit?
","Hardware threads are a feature of the POWER9 processor through which individual physical cores can support multiple execution streams, essentially looking like one or more virtual cores (similar to hyperthreading on some Intel      microprocessors). This feature is often called Simultaneous Multithreading or SMT. The POWER9 processor on Summit supports SMT levels of 1, 2, or 4, meaning (respectively) each physical core looks like 1, 2, or 4 virtual cores. The SMT level is controlled by the -alloc_flags option to bsub. For example, to set the SMT level to 2, add the line #BSUB –alloc_flags",4.138148467633122
"Can I use Nvidia Rapids to connect to a dask-cuda-cluster on a remote machine?
","Running RAPIDS multi-gpu/multi-node workloads requires a dask-cuda cluster. Setting up a dask-cuda cluster on Summit requires two components:

dask-scheduler.

dask-cuda-workers.

Once the dask-cluster is running, the RAPIDS script should perform four main tasks. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

Reference of multi-gpu/multi-node operation with cuDF, cuML, cuGraph is available in the next links:

10 Minutes to cuDF and Dask-cuDF.

cuML's Multi-Node, Multi-GPU Algorithms.",4.331636360599019
"Can I use Nvidia Rapids to connect to a dask-cuda-cluster on a remote machine?
","As mentioned earlier, the RAPIDS code should perform four main tasks as shown in the following script. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

import sys
from dask.distributed import Client

def disconnect(client, workers_list):
    client.retire_workers(workers_list, close_workers=True)
    client.shutdown()

if __name__ == '__main__':

    sched_file = str(sys.argv[1]) #scheduler file
    num_workers = int(sys.argv[2]) # number of workers to wait for",4.175435244547864
"Can I use Nvidia Rapids to connect to a dask-cuda-cluster on a remote machine?
","# 3. Do computation
    # ...
    # ...

    # 4. Shutting down the dask-cuda-cluster
    print(""Shutting down the cluster"")
    workers_list = list(workers_info)
    disconnect (client, workers_list)

The RAPIDS environment is read-only. Therefore, users cannot install any additional packages that may be needed. If users need any additional conda or pip packages, they can clone the RAPIDS environment into their preferred directory and then add any packages they need.

Cloning the RAPIDS environment can be done with the next commands:",4.161278092769347
"What is the name of the device that is mentioned in the error message?
","The information below the dashes which we omitted can be occasionally helpful for debugging, say if there is some kind of hardware problem..",4.015716696228672
"What is the name of the device that is mentioned in the error message?
","The above log messages indicate the type of image required by each device, given its current mode (amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack-) and the images found in the binary (hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+).",3.985012595554945
"What is the name of the device that is mentioned in the error message?
","The above log messages indicate the type of image required by each device, given its current mode (amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack-) and the images found in the binary (hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+).",3.9832712395181273
"What is the purpose of the -nn option in visit?
","the -s flag will launch the CLI in the form of a Python shell, which can be useful for interactive batch jobs.  The -np and -nn flags represent the number of processors and nodes VisIt will use to execute the Python script, while the -l flag specifies the specific parallel method to do so (required). Execute visit -fullhelp to get a list of all command line options.",4.2677066729292505
"What is the purpose of the -nn option in visit?
","module load visit

   visit -nowin -cli -v 3.3.3 -l srun -np 28 -nn 1 -s visit_example.py

Frontier

.. code-block:: bash


   #!/bin/bash
   #SBATCH -A XXXYYY
   #SBATCH -J visit_test
   #SBATCH -N 1
   #SBATCH -p batch
   #SBATCH -t 0:05:00

   cd $SLURM_SUBMIT_DIR
   date

   module load ums
   module load ums022

   visit -nowin -cli -v 3.3.3 -l srun -np 28 -nn 1 -s visit_example.py",4.021394884967342
"What is the purpose of the -nn option in visit?
","For Summit (module):

$ module load visit
$ visit -nowin -cli -v 3.1.4 -l bsub/jsrun -p batch -b XXXYYY -t 00:05 -np 42 -nn 1 -s visit_example.py

Due to the nature of the custom VisIt launcher for Summit, users are unable to solely specify -l jsrun for VisIt to work properly. Instead of manually creating a batch script, as seen in the Andes method outlined below, VisIt submits its own through -l bsub/jsrun. The -t flag sets the time limit, -b specifies the project to be charged, and -p designates the queue the job will submit to.",4.014511990160706
"How can I avoid getting my account disabled on OLCF computing resources?
","authorized individual or investigative agency is in effect during the period of your access to information on a DOE computer and for a period of three years thereafter. OLCF personnel and users are required to address, safeguard against, and report misuse, abuse and criminal activities. Misuse of OLCF resources can lead to temporary or permanent disabling of accounts, loss of DOE allocations, and administrative or legal actions. Users who have not accessed a OLCF computing resource in at least 6 months will be disabled. They will need to reapply to regain access to their account. All users",4.383872648814607
"How can I avoid getting my account disabled on OLCF computing resources?
","The requirements outlined in this document apply to all individuals who have an OLCF account. It is your responsibility to ensure that all individuals have the proper need-to-know before allowing them access to the information on OLCF computing resources. This document will outline the main security concerns.

OLCF computing resources are for business use only. Installation or use of software for personal use is not allowed. Incidents of abuse will result in account termination. Inappropriate uses include, but are not limited to:

Sexually oriented information",4.380028662898553
"How can I avoid getting my account disabled on OLCF computing resources?
",The Oak Ridge Leadership Computing Facility (OLCF) computing resources are provided to users for research purposes. All users must agree to abide by all security measures described in this document. Failure to comply with security procedures will result in termination of access to OLCF computing resources and possible legal actions.,4.34468693661899
"How can I see the output of a job that has a specific ID and is eligible to run?
","The most straightforward monitoring is with the bjobs command. This command will show the current queue, including both pending and running jobs. Running bjobs -l will provide much more detail about a job (or group of jobs). For detailed output of a single job, specify the job id after the -l. For example, for detailed output of job 12345, you can run bjobs -l 12345 . Other options to bjobs are shown below. In general, if the command is specified with -u all it will show information for all users/all jobs. Without that option, it only shows your jobs. Note that this is not an exhaustive list.",4.225418997827381
"How can I see the output of a job that has a specific ID and is eligible to run?
","bjobs and jobstat help to identify what’s currently running and scheduled to run, but sometimes it’s beneficial to know how much of the system is not currently in use or scheduled for use.",4.110968670307676
"How can I see the output of a job that has a specific ID and is eligible to run?
","Limit of (4) eligible-to-run jobs per user.

Jobs in excess of the per user limit above will be placed into a held state, but will change to eligible-to-run at the appropriate time.

Users may have only (100) jobs queued in the batch queue at any state at any time. Additional jobs will be rejected at submit time.

The eligible-to-run state is not the running state. Eligible-to-run jobs have not started and are waiting for resources. Running jobs are actually executing.",4.098369358800294
"Is there a recommended way to manage dependencies for applications deployed with Helm?
","Helm is the OLCF preferred package manager for Kubernetes. There are a variety of upstream applications that can be installed with helm, such as databases, web frontends, and monitoring tools. Helm has ""packages"" called ""charts"", which can essentially be thought of as kubernetes object templates. You can pass in values which helm uses to fill in these templates, and create the objects (pods, deployments, services, etc)

Follow https://docs.olcf.ornl.gov/systems/helm_example.html#helm_prerequisite for installing Helm.",4.306714976195647
"Is there a recommended way to manage dependencies for applications deployed with Helm?
","It is also possible to write your own charts for helm, if you have an application that can be deployed to many namespaces or that could benefit from templating objects. How to write charts is outside the scope of this documentation, but the upstream docs are a great place to start.",4.263660945547305
"Is there a recommended way to manage dependencies for applications deployed with Helm?
","The following items need to be established before deploying applications on Slate systems (Marble or Onyx OpenShift clusters).

Follow https://docs.olcf.ornl.gov/systems/helm_prerequisite.html#slate_getting_started if you do not already have a project/namespace established on Onyx or Marble.

It is recommended to use Helm version 3.

Helm enables application deployment via Helm Charts. The high level Helm Architecture docs are also a great reference for understanding Helm.

Like oc, Helm is a single binary executable.

This can be installed on macOS with Homebrew :

$ brew install helm",4.223114710795151
"How do I log in to the container registry using Podman?
","If you are familiar with using a container registry like DockerHub, you can use that to save your Podman container images and use Singularity to pull from the registry and build the sif file. Below, we will use DockerHub as the example but there are many other container registries that you can use.

Using the simple example from the previous section, build the container image with podman build -t docker.io/<username>/simple -f simple.dockerfile . where <username> is your user on DockerHub.

podman push uses the URL in the container image's name to push to the appropriate registry.",4.309289591007984
"How do I log in to the container registry using Podman?
","Check if your image is created

$ podman image ls
REPOSITORY                         TAG      IMAGE ID      CREATED      SIZE
docker.io/subilabrahamornl/simple  latest   e47dbfde3e99  3 hours ago  687 MB
localhost/simple                   latest   e47dbfde3e99  3 hours ago  687 MB
quay.io/centos/centos              stream8  ad6f8b5e7f64  8 days ago   497 MB

Run podman login docker.io and enter your account's username and password so that Podman is logged in to the container registry before pushing.

Push the container image to the registry with podman push docker.io/<username>/simple.",4.240617879831551
"How do I log in to the container registry using Podman?
","docker push registry.apps.<cluster>.ccs.ornl.gov/<namespace>/<image>:<tag>

OpenShift has an integrated container registry that can be accessed from outside the cluster to push and pull images as well as run containers.

This assumes that you have Docker installed locally. Installing Docker is outside of the scope of this documentation.

First you have to log into OpenShift

oc login https://api.<cluster>.ccs.ornl.gov

Next you can use your token to log into the integrated registry.

docker login -u user -p $(oc whoami -t) registry.apps.<cluster>.ccs.ornl.gov",4.19572395097908
"How do I specify a job dependency for my job?
","Oftentimes, a job will need data from some other job in the queue, but it's nonetheless convenient to submit the second job before the first finishes. Slurm allows you to submit a job with constraints that will keep it from running until these dependencies are met. These are specified with the -d option to Slurm. Common dependency flags are summarized below. In each of these examples, only a single jobid is shown but you can specify multiple job IDs as a colon-delimited list (i.e. #SBATCH -d afterok:12345:12346:12346). For the after dependency, you can optionally specify a +time value for",4.1985646343445024
"How do I specify a job dependency for my job?
","Dependency expressions can be combined with logical operators. For example, if you want a job held until job 12345 is DONE and job 12346 has started, you can use #BSUB -w ""done(12345) && started(12346)""



The default job launcher for Summit is jsrun. jsrun was developed by IBM for the Oak Ridge and Livermore Power systems. The tool will execute a given program on resources allocated through the LSF batch scheduler; similar to mpirun and aprun functionality.

The following compute node image will be used to discuss jsrun resource sets and layout.



1 node

2 sockets (grey)",4.071168651716791
"How do I specify a job dependency for my job?
","cannot start until job 12345 exits with an exit code of 0. See the Job Dependency section for more information | | -C | #SBATCH -C nvme | Request the burst buffer/NVMe on each node be made available for your job. See the Burst Buffers section for more information on using them. | | -J | #SBATCH -J MyJob123 | Specify the job name (this will show up in queue listings) | | -o | #SBATCH -o jobout.%j | File where job STDOUT will be directed (%j will be replaced with the job ID). If no -e option is specified, job STDERR will be placed in this file, too. | | -e | #SBATCH -e joberr.%j | File where",4.06935761514539
"How many eligible-to-run jobs can a user have in the batch-hm queue at any given time?
","Limit of (4) eligible-to-run jobs per user.

Jobs in excess of the per user limit above will be placed into a held state, but will change to eligible-to-run at the appropriate time.

Users may have only (100) jobs queued in the batch queue at any state at any time. Additional jobs will be rejected at submit time.

The eligible-to-run state is not the running state. Eligible-to-run jobs have not started and are waiting for resources. Running jobs are actually executing.",4.622872349999999
"How many eligible-to-run jobs can a user have in the batch-hm queue at any given time?
","The batch-hm queue (and the batch-hm-spi queue for Moderate Enhanced security enclave projects) is used to access Summit's high-memory nodes.  Jobs may use all 54 nodes. It enforces the following policies:

Limit of (4) eligible-to-run jobs per user.

Jobs in excess of the per user limit above will be placed into a held state, but will change to eligible-to-run at the appropriate time.

Users may have only (25) jobs queued in the batch-hm queue at any state at any time. Additional jobs will be rejected at submit time.

batch-hm job limits:",4.547799372025707
"How many eligible-to-run jobs can a user have in the batch-hm queue at any given time?
","same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. Additionally, projects that are at 125% of their allocated time will be limited to only 3 running jobs at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.286885609847093
"How do I know which version of VisIt is right for me?
","VisIt uses a client-server architecture. You will obtain the best performance by running the VisIt client on your local computer and running the server on OLCF resources. VisIt for your local computer can be obtained here: VisIt Installation.

Recommended VisIt versions on our systems:

Summit: VisIt 3.1.4

Andes: VisIt 3.3.3

Frontier: VisIt 3.3.3

Using a different version than what is listed above is not guaranteed to work properly.",4.219725596366856
"How do I know which version of VisIt is right for me?
","Past VisIt Tutorials are available on the Visit User's Wiki along with a set of Updated Tutorials available in the VisIt User Manual.

Sample data not pre-packaged with VisIt can be found in the VisIt Data Archive.

Older VisIt Versions with their release notes can be found on the old VisIt website, and Newer Versions can be found on the new VisIt website with release notes found on the VisIt Blog or VisIt Github Releases page.

Non-ORNL related bugs and issues in VisIt can be found and reported on Github.",4.136071170907839
"How do I know which version of VisIt is right for me?
","VisIt is now available on Frontier through the UMS022 module.

VisIt 3.3.3 is now available on Andes.

VisIt is an interactive, parallel analysis and visualization tool for scientific data. VisIt contains a rich set of visualization features so you can view your data in a variety of ways. It can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes. Further information regarding VisIt can be found at the links provided in the https://docs.olcf.ornl.gov/systems/visit.html#visit-resources section.",4.128243309013006
"What is the include path that implies #include <hip/hip_runtime.h> is included in the source file?
","-I${ROCM_PATH}/include
-L${ROCM_PATH}/lib -lamdhip64

where the include path implies that #include <hip/hip_runtime.h> is included in the source file.

To use hipcc with GPU-aware Cray MPICH, use the following environment variables to setup the needed header files and libraries.

-I${MPICH_DIR}/include
-L${MPICH_DIR}/lib -lmpi \
  ${CRAY_XPMEM_POST_LINK_OPTS} -lxpmem \
  ${PE_MPICH_GTL_DIR_amd_gfx90a} ${PE_MPICH_GTL_LIBS_amd_gfx90a}

HIPFLAGS = --amdgpu-target=gfx90a",4.316722226703416
"What is the include path that implies #include <hip/hip_runtime.h> is included in the source file?
","## These must be set before running
export MPIR_CVAR_GPU_EAGER_DEVICE_MEM=0
export MPICH_GPU_SUPPORT_ENABLED=1
export MPICH_SMP_SINGLE_COPY_MODE=CMA

In addition, the following header files and libraries must be included:

-I${ROCM_PATH}/include
-L${ROCM_PATH}/lib -lamdhip64 -lhsa-runtime64

where the include path implies that #include <hip/hip_runtime.h> is included in the source file.",4.224167157047319
"What is the include path that implies #include <hip/hip_runtime.h> is included in the source file?
","Key features include:

HIP is a thin layer and has little or no performance impact over coding directly in CUDA.

HIP allows coding in a single-source C++ programming language including features such as templates, C++11 lambdas, classes, namespaces, and more.

The “hipify” tools automatically convert source from CUDA to HIP.

Developers can specialize for the platform (CUDA or HIP) to tune for performance or handle tricky cases.",4.149499220910746
"What is the purpose of the square.hipref.cpp file?
","Key features include:

HIP is a thin layer and has little or no performance impact over coding directly in CUDA.

HIP allows coding in a single-source C++ programming language including features such as templates, C++11 lambdas, classes, namespaces, and more.

The “hipify” tools automatically convert source from CUDA to HIP.

Developers can specialize for the platform (CUDA or HIP) to tune for performance or handle tricky cases.",4.005871515008418
"What is the purpose of the square.hipref.cpp file?
",work to learn HIP. See here for a series of tutorials on programming with HIP and also converting existing CUDA code to HIP with the hipify tools .,3.9269009712267966
"What is the purpose of the square.hipref.cpp file?
","Last Updated: 07 February 2023

Using the hip-cuda/5.1.0 module on Summit, applications cannot build using a CMakeLists.txt that requires HIP language support or references the hip::host and hip::device identifiers. There is no known workaround for this issue. Applications wishing to compile HIP code with CMake need to avoid using HIP language support or hip::host and hip::device identifiers.",3.9176540148636896
"May there be a delay in job startup due to the reboot process?
",Use of the SPI queue will trigger configuration changes to the compute nodes to allow enhanced data protection. Compute nodes will be booted before and after each SPI batch job. Compute nodes will be booted into an image that mounts only the Arx filesystem. The image will also restrict connections. Please note: the reboot process may cause a slight delay in job startup.,4.186812178861922
"May there be a delay in job startup due to the reboot process?
","In addition, to debug slow startup JSM provides the option to create a progress file. The file will show information that can be helpful to pinpoint if a specific node is hanging or slowing down the job step launch. To enable it, you can use: jsrun --progress ./my_progress_file_output.txt.

When using file-based specification of resource sets with jsrun -U, the -a flag (number of tasks per resource set) is ignored. This has been reported to IBM and they are investigating. It is generally recommended to use jsrun explicit resource files (ERF) with --erf_input and --erf_output instead of -U.",4.045077216898488
"May there be a delay in job startup due to the reboot process?
","system, incorrect SSH key setup, attempting to load unavailable/broken modules. or system problems with individual nodes. When jobs are observed to flip-flop between running and queued, and/or become ineligible without explanation, then deeper investigation is required and the user should write to help@olcf.ornl.gov.",4.037304031869235
"How can I get started with using GitLab Runners on Slate?
","Jenkins

OpenShift Pipelines

GitLab Runners

Deploying a GitLab Runner into a Slate project may be found in the https://docs.olcf.ornl.gov/systems/overview.html#slate_gitlab_runners document which leverages a localized Slate Helm Chart. The GitLab Pipelines documentation as well as GitLab CI/CD Examples provide more details on GitLab Runner capabilities and usage.

Jenkins

For Jenkins deployment, a Slate Helm Chart is planned to be released. Documentation concerning Jenkins Pipelines as well as Jenkins Pipeline Tutorials are available.

OpenShift Pipelines",4.46741418974748
"How can I get started with using GitLab Runners on Slate?
","GitLab CI/CD jobs may be accomplished using the GitLab Runner application. An open-source application written in Go, a GitLab Runner may be installed with a variety of executors. This documentation focuses on the installation and configuration for use of the GitLab Runner Kubernetes Executor on Slate.

https://docs.gitlab.com/runner/

https://docs.gitlab.com/runner/executors/kubernetes.html

https://docs.gitlab.com/runner/install/kubernetes.html",4.4237965886337065
"How can I get started with using GitLab Runners on Slate?
","To deploy a GitLab Runner from the Developer Perspective, navigate to ""Topology"" -> ""Helm Chart"" and select ""GitLab Runner vX.X.X Provided by Slate Helm Charts"". From the new window, select ""Install Helm Chart"".

On the resulting screen, one can customize the installation of the GitLab Runner helm chart. The chart has been forked from upstream to allow for customized deployment to Slate. From the Form View, expand the ""GitLab Runner"" fields. Using the registration token retrieved in the prior section, enter token for adding the runner to the GitLab server in the empty field.",4.422447399727273
"Can users cancel a reservation using the QCS CLI?
","All jobs run on Rigetti's systems are submitted via system reservation.  This can be done either by using Rigetti's QCS dashboard to schedule the reservation, or via interacting with the QCS via the Command Line Interface (CLI).  Scheduled reservations can be viewed and/or cancelled via either method, either in the dashboard or from the CLI.

To submit a reservation via the QCS dashboard: https://docs.rigetti.com/qcs/guides/reserving-time-on-a-qpu#using-the-qcs-dashboard",4.344834543965002
"Can users cancel a reservation using the QCS CLI?
","To submit a reservation via the QCS CLI, and installation instructions: https://docs.rigetti.com/qcs/guides/using-the-qcs-cli

Accessing the Rigetti QPU systems can only be done during a user's reservation window.  To submit a job, users must have JupyterHub access to the system.  QVM jobs can be run without network access in local environments.  Jobs are compiled via Quilc and submitted via pyQuil (see https://docs.olcf.ornl.gov/systems/rigetti.html#Software Section <rigetti-soft> below) in a python environment or Jupyter notebook.",4.241101680677911
"Can users cancel a reservation using the QCS CLI?
","There is a limited number of minutes per month that can be reserved on each device. Reservations are supported on these devices with these monthly allocations:

ibmq_kolkata, 2400 minutes per month

ibmq_jakarta, 480 minutes per month

In order to make the most efficient use of reservation allocations:

Reservations requests must be submitted to the project Principle Investigator (PI) to help@olcf.ornl.gov

Requests for reservations must include technical justification.

Once submitted, requests will be sent to the Quantum Resource Utilization Council (QRUC) for consideration.",4.100072717090048
"How do I connect to a debugging session on Frontier from a client running on my workstation?
",One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Frontier from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Frontier. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.,4.41958646644525
"How do I connect to a debugging session on Frontier from a client running on my workstation?
",One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Frontier from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Frontier. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.,4.41958646644525
"How do I connect to a debugging session on Frontier from a client running on my workstation?
",One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Andes from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Andes. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.,4.163786809760632
"How many nodes can be used for the batch job?
","A batch job's usage is calculated solely on requested nodes and the batch job's start and end time. The number of cores actually used within any particular node within the batch job is not used in the calculation. For example, if a job requests (6) nodes through the batch script, runs for (1) hour, uses only (2) CPU cores per node, the job will still be charged for 6 nodes * 1 hour = 6 node-hours.

Viewing Usage",4.384940154649029
"How many nodes can be used for the batch job?
","Where batch job starttime is the time the job moves into a running state, and batch job endtime is the time the job exits a running state.

A batch job's usage is calculated solely on requested nodes and the batch job's start and end time. The number of cores actually used within any particular node within the batch job is not used in the calculation. For example, if a job requests (6) nodes through the batch script, runs for (1) hour, uses only (2) CPU cores per node, the job will still be charged for 6 nodes * 1 hour = 6 node-hours.",4.329216949678441
"How many nodes can be used for the batch job?
","Where batch job starttime is the time the job moves into a running state, and batch job endtime is the time the job exits a running state.

A batch job's usage is calculated solely on requested nodes and the batch job's start and end time. The number of cores actually used within any particular node within the batch job is not used in the calculation. For example, if a job requests (6) nodes through the batch script, runs for (1) hour, uses only (2) CPU cores per node, the job will still be charged for 6 nodes * 1 hour = 6 node-hours.",4.329216949678441
"How do I avoid interfering with other users' login node tasks on Summit?
","Login nodes should be used for basic tasks such as file editing, code compilation, data backup, and job submission. Login nodes should not be used for memory- or compute-intensive tasks. Users should also limit the number of simultaneous tasks performed on the login resources. For example, a user should not run (10) simultaneous tar processes on a login node.

Compute-intensive, memory-intensive, or otherwise disruptive processes running on login nodes may be killed without warning.",4.316508993474054
"How do I avoid interfering with other users' login node tasks on Summit?
","On Summit, there are three major types of nodes you will encounter: Login, Launch, and Compute. While all of these are similar in terms of hardware (see: https://docs.olcf.ornl.gov/systems/summit_user_guide.html#summit-nodes), they differ considerably in their intended use.",4.244877530748531
"How do I avoid interfering with other users' login node tasks on Summit?
","The login nodes listed above mirrors the Summit and Frontier login nodes in hardware and software.  The login node also provides access to the same compute resources as are accessible from Summit and Frontier's non-SPI workflows.

The Citadel login nodes cannot access the external network and are only accessible from whitelisted IP addresses.",4.244392197432302
"What is the difference between Onyx and Marble authentication in Slate?
","This section will describe the project allocation process for Slate. This is applicable to both the Onyx and Marble OLCF OpenShift clusters.

In addition, we will go over the process to log into Onyx and Marble, and how namespaces work within the OpenShift context.

Please fill out the Slate Request Form in order to use Slate resources. This must be done in order to proceed.

If you have any question please contact User Assistance, via a Help Ticket Submission or by emailing help@olcf.ornl.gov.

Required information:

- Existing OLCF Project ID

- Project PI",4.11803254712329
"What is the difference between Onyx and Marble authentication in Slate?
","The following items need to be established before deploying applications on Slate systems (Marble or Onyx OpenShift clusters).

Follow https://docs.olcf.ornl.gov/systems/helm_prerequisite.html#slate_getting_started if you do not already have a project/namespace established on Onyx or Marble.

It is recommended to use Helm version 3.

Helm enables application deployment via Helm Charts. The high level Helm Architecture docs are also a great reference for understanding Helm.

Like oc, Helm is a single binary executable.

This can be installed on macOS with Homebrew :

$ brew install helm",4.043257575808499
"What is the difference between Onyx and Marble authentication in Slate?
","| Cluster | URL | | --- | --- | | Marble (Moderate Production cluster with access to Summit/Alpine) | Marble Web Console - https://marble.ccs.ornl.gov https://marble.ccs.ornl.gov/ | | Onyx   (Open Production Cluster with access to Wolf) | Onyx Web Console - https://onyx.ccs.ornl.gov https://onyx.ccs.ornl.gov/ |

Slate Namespaces map directly to OLCF Project ID's.",4.03771207217987
"How do I set the SSH path in Paraview?
","<Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Windows"">
            <Set name=""TERM_PATH"" value=""cmd"" />
            <Set name=""TERM_ARG1"" value=""/C"" />
            <Set name=""TERM_ARG2"" value=""start"" />
            <Set name=""TERM_ARG3"" value="""" />
            <Set name=""SSH_PATH"" value=""plink.exe"" />
          </Case>
          <Case value=""Unix"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.192242101905348
"How do I set the SSH path in Paraview?
","<Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Windows"">
            <Set name=""TERM_PATH"" value=""cmd"" />
            <Set name=""TERM_ARG1"" value=""/C"" />
            <Set name=""TERM_ARG2"" value=""start"" />
            <Set name=""TERM_ARG3"" value="""" />
            <Set name=""SSH_PATH"" value=""plink.exe"" />
          </Case>
          <Case value=""Unix"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.192242101905348
"How do I set the SSH path in Paraview?
","After installing, you must give ParaView the relevant server information to be able to connect to OLCF systems (comparable to VisIt's system of host profiles). The following provides an example of doing so. Although several methods may be used, the one described should work in most cases.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.

For Windows clients, it is necessary to install PuTTY to create an ssh connection in step 2.

Step 1: Save the following servers.pvsc file to your local computer",4.191505393179877
"What is the purpose of the image provided in the information?
",for more information.,4.085562074665929
"What is the purpose of the image provided in the information?
","The above log messages indicate the type of image required by each device, given its current mode (amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack-) and the images found in the binary (hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+).",3.989625251218005
"What is the purpose of the image provided in the information?
","The above log messages indicate the type of image required by each device, given its current mode (amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack-) and the images found in the binary (hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+).",3.986516343697865
"Can a user request a specific allocation percentage for their job?
",| % Of Allocation Used | Priority Reduction | number eligible-to-run | number running | | --- | --- | --- | --- | | < 100% | 0 days | 4 jobs | unlimited jobs | | 100% to 125% | 30 days | 4 jobs | unlimited jobs | | > 125% | 365 days | 4 jobs | 1 job |,4.257893629367591
"Can a user request a specific allocation percentage for their job?
","towards their fair share of the system's utilization: in this case, 5%

of the system's utilization per user and 10% of the system's utilization

per project. To do this, the job scheduler adds (30) minutes priority

aging per user and (1) hour of priority aging per project for every (1)

percent the user or project is under its fair share value for the prior

(8) weeks. Similarly, the job scheduler subtracts priority in the same

way for users or projects that are over their fair share. For instance,

a user who has personally used 0.0% of the system's utilization over the",4.235226082677891
"Can a user request a specific allocation percentage for their job?
","Because of the queuing method described above, users have no set allocation. Job throughput is only limited via the dynamic queue.

There is a time limit on program-wide usage of reservable systems (see below).

In addition to the fair-share queue, users may request a backend reservation for a certain period of time by contacting help@olcf.ornl.gov. If the reservation is granted, the reserved backend will be blocked from general use for a specified period of time, and the user will have sole use of the backend for that period.",4.232691428070585
"What is the benefit of using `cupy.asarray()` to transfer an array?
","most of the array operations that NumPy provides, including array indexing, math, and transformations. Most operations provide an immediate speed-up out of the box, and some operations are sped up by over a factor of 100 (see CuPy benchmark timings below, from the Single-GPU CuPy Speedups article).",4.142494161437512
"What is the benefit of using `cupy.asarray()` to transfer an array?
","As is the standard with NumPy being imported as ""np"", CuPy is often imported in a similar fashion:

>>> import numpy as np
>>> import cupy as cp

Similar to NumPy arrays, CuPy arrays can be declared with the cupy.ndarray class. NumPy arrays will be created on the CPU (the ""host""), while CuPy arrays will be created on the GPU (the ""device""):

>>> x_cpu = np.array([1,2,3])
>>> x_gpu = cp.array([1,2,3])

Manipulating a CuPy array can also be done in the same way as manipulating NumPy arrays:",4.128522288637463
"What is the benefit of using `cupy.asarray()` to transfer an array?
",">>> with cp.cuda.Device(1):
...    cp.asarray(x_gpu_0) * 2  # fixes the error, moves x_gpu_0 to GPU 1
...
array([ 2,  4,  6,  8, 10])

A NumPy array on the CPU can also be transferred to a GPU using the same cupy.asarray() function:

>>> x_cpu = np.array([1, 1, 1]) # create an array on the CPU
>>> x_gpu = cp.asarray(x_cpu)  # move the CPU array to the current device
>>> x_gpu
array([1, 1, 1])

To transfer from a GPU back to the CPU, you use the cupy.asnumpy() function instead:",4.070598759475825
"What is the purpose of the `--no-show` option in Dask scheduler?
","Use the sbatch --test-only command to see when a job of a specific size could be scheduled. For example, the snapshot below shows that a (2) node job would start at 10:54.

$ sbatch --test-only -N2 -t1:00:00 batch-script.slurm

  sbatch: Job 1375 to start at 2019-08-06T10:54:01 using 64 processors on nodes andes[499-500] in partition batch

The queue is fluid, the given time is an estimate made from the current queue state and load. Future job submissions and job completions will alter the estimate.



The following table summarizes frequently-used options to Slurm:",3.924420721432915
"What is the purpose of the `--no-show` option in Dask scheduler?
","Use the sbatch --test-only command to see when a job of a specific size could be scheduled. For example, the snapshot below shows that a (2) node job would start at 10:54.

$ sbatch --test-only -N2 -t1:00:00 batch-script.slurm

  sbatch: Job 1375 to start at 2019-08-06T10:54:01 using 64 processors on nodes andes[499-500] in partition batch

The queue is fluid, the given time is an estimate made from the current queue state and load. Future job submissions and job completions will alter the estimate.



Common Batch Options to Slurm",3.9077692823308894
"What is the purpose of the `--no-show` option in Dask scheduler?
","#Wait for the dask-scheduler to start
sleep 10

jsrun --rs_per_host 6 --tasks_per_rs 1 --cpu_per_rs 2 --gpu_per_rs 1 --smpiargs=""-disable_gpu_hooks"" \
      dask-cuda-worker --nthreads 1 --memory-limit 82GB --device-memory-limit 16GB --rmm-pool-size=15GB \
                       --death-timeout 60  --interface ib0 --scheduler-file $SCHEDULER_FILE --local-directory $WORKER_DIR \
                       --no-dashboard &

#Wait for WORKERS
sleep 10

export BSQL_BLAZING_LOGGING_DIRECTORY=$BSQL_LOG_DIR
export BSQL_BLAZING_LOCAL_LOGGING_DIRECTORY=$BSQL_LOG_DIR",3.895742631145296
"How can I set a password for VNC sessions on the Andes cluster?
","Install a vncviewer (turbovnc, tigervnc, etc.) on your local machine.  When running vncviewer for the first time, it will ask to set a password for this and future vnc sessions.

<string>:1336: (INFO/1) Duplicate implicit target name: ""step 2 (terminal 1)"".

From an Andes connection launch a batch job and execute the below vmd-vgl.sh script to start the vncserver and run vmd within:

localsytem: ssh -X username@andes.olcf.ornl.gov

andes: salloc -A abc123 -N 1 -t 1:00:00 -p gpu --x11=batch

andes: ./vmd-vgl.sh

$ ./vmd-vgl.sh

Starting X",4.386109181405967
"How can I set a password for VNC sessions on the Andes cluster?
","Step 1 (local system)

<string>:1329: (INFO/1) Duplicate implicit target name: ""step 1 (local system)"".

Install a vncviewer (turbovnc, tigervnc, etc.) on your local machine.  When running vncviewer for the first time, it will ask to set a password for this and future vnc sessions.

Step 2 (terminal 1)

<string>:1336: (INFO/1) Duplicate implicit target name: ""step 2 (terminal 1)"".

From an Andes connection launch a batch job and execute the below vmd-vgl.sh script to start the vncserver and run vmd within:

localsytem: ssh -X username@andes.olcf.ornl.gov",4.283031335019747
"How can I set a password for VNC sessions on the Andes cluster?
","In addition to the instructions below, Benjamin Hernandez of the OLCF Advanced Technologies Section presented a related talk, GPU Rendering in Rhea and Titan, during the 2016 OLCF User Meeting.

Install a vncviewer (turbovnc, tigervnc, etc.) on your local machine.  When running vncviewer for the first time, it will ask to set a password for this and future vnc sessions.

From an Andes connection launch a batch job and execute the below matlab-vnc.sh script to start the vncserver and run matlab within:

localsytem: ssh -X username@andes.olcf.ornl.gov",4.253965811651261
"How do I configure my deployment to use NFS or GPFS with MinIO?
","Depending on you how configured your deployment, this could be your NFS or GPFS project space or an isolated volume dedicated/isolated to this MinIO server.

Within the GUI you can create buckets and upload/download data. If you are running this on NFS or GPFS the bucket will map to a directory.",4.451119920032521
"How do I configure my deployment to use NFS or GPFS with MinIO?
","We hope this provides a starting point for more robust implementations of MinIO, if your workload/project benefits from that. In addition, it gives insight into some of the core building blocks for establishing your own, different, applications on Slate.

This example deployment of MinIO enables two possible configurations (which we configure in the following sections):

MinIO running on a NCCS filesystem (GPFS or NFS - exposing filesystem project space through the MinIO GUI).",4.44335597272788
"How do I configure my deployment to use NFS or GPFS with MinIO?
","What do you need to consider?

What should I name my host value? (This will be the URL in which you access your MinIO instance)

What should I name my application? (This is the name value and should be unique to you or your project)

Do I want MinIO to run on an OLCF filesytem? (It can run on NFS or GPFS project spaces. If you do not run it on an OLCF filesystem it uses an isolated volume dedicated to the MinIO server)

What do you need to configure?

host (Set the URL of your application)

name (Set the name of your application)",4.23905720939675
"Can I use Lmod to install new software on Summit?
","The E4S software list is installed along side the existing software on Summit and can be access via lmod modulefiles.

To access the installed software, load the desired compiler via:

module load < compiler/version >
.. ie ..
module load gcc/9.1.0
.. or ..
module load gcc/7.5.0

Then use module avail to see the installed list of packages.

List of installed packages on Summit for E4S release 21.08:",4.288483917424653
"Can I use Lmod to install new software on Summit?
","In order to avoid conflicts between user-defined collections on multiple compute systems that share a home file system (e.g. /ccs/home/[username]), Lmod appends the hostname of each system to the files saved in in your ~/.lmod.d directory (using the environment variable lmod_system_name). This ensures that only collections appended with the name of the current system are visible.

The following screencast shows an example of setting up user-defined module collections on Summit. https://vimeo.com/293582400



Installed Software",4.192247086261515
"Can I use Lmod to install new software on Summit?
","Environment management with lmod

The modules software package allows you to dynamically modify your user environment by using pre-written modulefiles. Environment modules are provided through Lmod, a Lua-based module system for dynamically altering shell environments.  By managing changes to the shell’s environment variables (such as path, ld_library_path, and pkg_config_path), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.",4.1796826541183405
"How can I use the roc-obj-ls command to diagnose issues with the xnack_any.exe file?
","The AMD tool `roc-obj-ls` will let you see what code objects are in a binary.

.. code::
    $ hipcc --amdgpu-target=gfx90a:xnack+ square.hipref.cpp -o xnack_plus.exe
    $ roc-obj-ls -v xnack_plus.exe
    Bundle# Entry ID:                                                              URI:
    1       host-x86_64-unknown-linux                                           file://xnack_plus.exe#offset=8192&size=0
    1       hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+                              file://xnack_plus.exe#offset=8192&size=9752",4.136863161193556
"How can I use the roc-obj-ls command to diagnose issues with the xnack_any.exe file?
","The AMD tool `roc-obj-ls` will let you see what code objects are in a binary.

.. code::
    $ hipcc --amdgpu-target=gfx90a:xnack+ square.hipref.cpp -o xnack_plus.exe
    $ roc-obj-ls -v xnack_plus.exe
    Bundle# Entry ID:                                                              URI:
    1       host-x86_64-unknown-linux                                           file://xnack_plus.exe#offset=8192&size=0
    1       hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+                              file://xnack_plus.exe#offset=8192&size=9752",4.136863161193556
"How can I use the roc-obj-ls command to diagnose issues with the xnack_any.exe file?
","If no XNACK flag is specificed at compilation the default is ""xnack any"", and objects in `roc-obj-ls` with not have an XNACK mode specified.

.. code::
    $ hipcc --amdgpu-target=gfx90a square.hipref.cpp -o xnack_any.exe
    $ roc-obj-ls -v xnack_any.exe
    Bundle# Entry ID:                                                              URI:
    1       host-x86_64-unknown-linux                                           file://xnack_any.exe#offset=8192&size=0
    1       hipv4-amdgcn-amd-amdhsa--gfx90a                                     file://xnack_any.exe#offset=8192&size=9752",4.123227420693397
"How can I cancel a running batch job in Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.268997260540272
"How can I cancel a running batch job in Andes?
","Batch Queues on Andes

The compute nodes on Andes are separated into two partitions the ""batch partition"" and the ""GPU partition"" as described in the https://docs.olcf.ornl.gov/systems/your_file.html#andes-compute-nodes section. The scheduling policies for the individual partitions are as follows:

Batch Partition Policy (default)

Jobs that do not specify a partition will run in the 704 node batch partition:",4.207137344534136
"How can I cancel a running batch job in Andes?
","| % Of Allocation Used | Priority Reduction | number eligible-to-run | number running | | --- | --- | --- | --- | | < 100% | 0 days | 4 jobs | unlimited jobs | | 100% to 125% | 30 days | 4 jobs | unlimited jobs | | > 125% | 365 days | 4 jobs | 1 job |



Job Accounting on Andes",4.151377378575801
"What is the purpose of the `metadata.name` field in Slate?
","The meta information in the kustomization.yaml illustrates what is referred to as a cross-cutting field. In this case, the commonLabels block adds a label app: hello which will be included in all of the resources specified in the resource files. Cross-cutting fields could also be used to set the namespace (namespace) for the resources to be created in, add a prefix (namePrefix) or suffix (nameSuffix) to all resource names, or add a set of annotations (commonAnnotations).

To see the results of the commonLabels field, the kustomize build command will display the output for inspection:",3.9848179534181978
"What is the purpose of the `metadata.name` field in Slate?
","Before we dive in there are some terms that need to be understood. This will be a basic set of terms and a copy and paste from our https://docs.olcf.ornl.gov/systems/guided_tutorial.html#slate_glossary, so we recommend reading that document and even keeping it handy until you are familiar with all of the definitions there. On that note, another good piece of reference documentation the https://docs.olcf.ornl.gov/systems/guided_tutorial.html#slate_examples document. There you can find basic YAML definitions for the most common objects in Kubernetes.",3.905448083426342
"What is the purpose of the `metadata.name` field in Slate?
","This section will describe the project allocation process for Slate. This is applicable to both the Onyx and Marble OLCF OpenShift clusters.

In addition, we will go over the process to log into Onyx and Marble, and how namespaces work within the OpenShift context.

Please fill out the Slate Request Form in order to use Slate resources. This must be done in order to proceed.

If you have any question please contact User Assistance, via a Help Ticket Submission or by emailing help@olcf.ornl.gov.

Required information:

- Existing OLCF Project ID

- Project PI",3.900161377066977
"How can I distribute MPI tasks in a block layout across nodes with the srun command?
","The below srun command will achieve the intended 7 MPI ""packed"" layout:

$ export OMP_NUM_THREADS=1
$ srun -N1 -n7 -c1 --cpu-bind=threads --threads-per-core=1 -m block:block ./hello_mpi_omp | sort



Breaking down the srun command, the only difference than the previous example is:

-m block:block: distribute the tasks in a block layout across nodes (default), and in a block (packed) socket layout",4.462258967824337
"How can I distribute MPI tasks in a block layout across nodes with the srun command?
","Using srun

By default, commands will be executed on the job’s primary compute node, sometimes referred to as the job’s head node. The srun command is used to execute an MPI binary on one or more compute nodes in parallel.

srun accepts the following common options:

| -N | Minimum number of nodes | | --- | --- | | -n | Total number of MPI tasks | | --cpu-bind=no | Allow code to control thread affinity | | -c | Cores per MPI task | | --cpu-bind=cores | Bind to cores |

If you do not specify the number of MPI tasks to srun via -n, the system will default to using only one task per node.",4.414541994197261
"How can I distribute MPI tasks in a block layout across nodes with the srun command?
","The intent with both of the following examples is to launch 8 MPI ranks across the node where each rank is assigned its own logical (and, in this case, physical) core.  Using the -m distribution flag, we will cover two common approaches to assign the MPI ranks -- in a ""round-robin"" (cyclic) configuration and in a ""packed"" (block) configuration. Slurm's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive method was used to request an allocation of 1 compute node for these examples: salloc -A <project_id> -t 30 -p <parition> -N 1",4.400714401843089
"How many nodes are available for scheduling on the cluster?
","The requesting project's allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Summit would be equivalent to using 16,000 Summit node-hours of a project's allocation.



As is the case with many other queuing systems, it is possible to place dependencies on jobs to prevent them from running until other jobs have started/completed/etc. Several possible dependency settings are described in the table below:",4.196068892592721
"How many nodes are available for scheduling on the cluster?
","1 node

2 sockets (grey)

42 physical cores* (dark blue)

168 hardware cores (light blue)

6 GPUs (orange)

2 Memory blocks (yellow)

*Core Isolation: 1 core on each socket has been set aside for overhead and is not available for allocation through jsrun. The core has been omitted and is not shown in the above image.",4.184737694681743
"How many nodes are available for scheduling on the cluster?
","The requesting project’s allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Frontier would be equivalent to using 16,000 Frontier node-hours of a project’s allocation.

Reservations should not be confused with priority requests. If quick turnaround is needed for a few jobs or for a period of time, a priority boost should be requested. A reservation should only be requested if users need to guarantee availability of a set of nodes at a given time, such as for a live demonstration at a conference.",4.178289459838379
"What should I do if SBCAST fails?
","# SBCAST file from Orion to NVMe -- NOTE: ``-C nvme`` is required to use the NVMe drive
sbcast -pf test.txt /mnt/bb/$USER/test.txt
if [ ! ""$?"" == ""0"" ]; then
    # CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,
    # your application may pick up partially complete shared library files, which would give you confusing errors.
    echo ""SBCAST failed!""
    exit 1
fi",4.075470529014405
"What should I do if SBCAST fails?
","# SBCAST file from Orion to NVMe -- NOTE: ``-C nvme`` is required to use the NVMe drive
sbcast -pf test.txt /mnt/bb/$USER/test.txt
if [ ! ""$?"" == ""0"" ]; then
    # CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,
    # your application may pick up partially complete shared library files, which would give you confusing errors.
    echo ""SBCAST failed!""
    exit 1
fi",4.075470529014405
"What should I do if SBCAST fails?
","# SBCAST executable from Orion to NVMe -- NOTE: ``-C nvme`` is needed in SBATCH headers to use the NVMe drive
# NOTE: dlopen'd files will NOT be picked up by sbcast
sbcast --send-libs --exclude=NONE -pf ${exe} /mnt/bb/$USER/${exe}
if [ ! ""$?"" == ""0"" ]; then
    # CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,
    # your application may pick up partially complete shared library files, which would give you confusing errors.
    echo ""SBCAST failed!""
    exit 1
fi",4.072042894122117
"What is the difference between clang and clang++ on Crusher?
","Cray, AMD, and GCC compilers are provided through modules on Crusher. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers.

It is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) whenever possible. See the next section for more details.",4.204852803385769
"What is the difference between clang and clang++ on Crusher?
","Cray, AMD, and GCC compilers are provided through modules on Spock. The Cray and AMD compilers are both based on LLVM/Clang. There are also system/OS versions of both Clang and GCC available in /usr/bin. The table below lists details about each of the module-provided compilers.

It is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) whenever possible. See the next section for more details.",4.114551612992955
"What is the difference between clang and clang++ on Crusher?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers.

It is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) whenever possible. See the next section for more details.",4.110081930842357
"How can I access external repositories from within the Citadel framework?
","The Citadel framework prevents login and compute resources from accessing the internet.  Because of this, https://docs.olcf.ornl.gov/systems/index.html#Citadel login nodes<citadel-login-nodes> cannot reach repositories external to the system.  If your build workflow attempts to access external repositories, you may need to alter your build workflows to use data stored locally.  For cases where you are unable to modify your workflow to use only local data, please reach out to help@olcf.ornl.gov.  We may be able to help by providing a partner project on Summit/Frontier.  The partner project",4.385598733826857
"How can I access external repositories from within the Citadel framework?
","Login and compute resources in the Citadel framework can not access the internet.  This may impact workflows that attempt to access external repositories.

More information on building codes for Citadel including programming environments, compilers, and available software can be found on https://docs.olcf.ornl.gov/systems/index.html#Summit User Guide<summit-user-guide> and https://docs.olcf.ornl.gov/systems/index.html#Frontier User Guide<frontier-user-guide>.",4.315152465353432
"How can I access external repositories from within the Citadel framework?
","The Citadel framework allows use of the Summit/Frontier compute resources but adds additional layers of security to ensure data protection.  To ensure proper configuration and protection access to the compute resources, the following batch queue(s) must be used from the https://docs.olcf.ornl.gov/systems/index.html#Citadel login nodes<citadel-login-nodes>:

batch-spi",4.146166097090706
"How can I check which versions of R are available on Summit?
","Several versions of R are available on Summit. You can see which by entering the command module spider r. Throughout this example, we will be using R version 3.6.1.

If you have logged in with the default modules, then you need to swap xl for gcc and the load R:

module swap xl gcc/6.4.0
module load r/3.6.1

If we do that and launch R, then we see:",4.434037618510935
"How can I check which versions of R are available on Summit?
","If you want to do GPU computing on Summit with R, we would love to collaborate with you (see contact details at the bottom of this document).

For more information about using R and pbdR effectively in an HPC environment such as Summit, please see the R and pbdR articles. These are long-form articles that introduce HPC concepts like MPI programming in much more detail than we do here.

Also, if you want to use R and/or pbdR on Summit, please feel free to contact us directly:

Mike Matheson - mathesonma AT ornl DOT gov

George Ostrouchov - ostrouchovg AT ornl DOT gov",4.2641167722637805
"How can I check which versions of R are available on Summit?
","Summit has a node hierarchy that can be very confusing for the uninitiated. The Summit User Guide explains this in depth. This has some consequences that may be unusual for R programmers. A few important ones to note are:

You must have a script that you can run in batch, e.g. with Rscript or R CMD BATCH

All data that needs to be visible to the R process (including the script and your packages) must be on gpfs (not your home directory!).

You must launch your script from the launch nodes with jsrun.

We'll start with something very simple. The script is:

""hello world""",4.231960441106991
"What types of codes are approved for running by parties with sensitive data agreements on OLCF computers?
","All software used on OLCF computers must be appropriately acquired and used according to the appropriate software license agreement. Possession, use, or transmission of illegally obtained software is prohibited. Likewise, users shall not copy, store, or transfer copyrighted software, except as permitted by the owner of the copyright. Only export-controlled codes approved by the Export Control Office may be run by parties with sensitive data agreements.

Users must not intentionally introduce or use malicious software such as computer viruses, Trojan horses, or worms.",4.326509723605968
"What types of codes are approved for running by parties with sensitive data agreements on OLCF computers?
","The OLCF computer systems are operated as research systems and only contain data related to scientific research and do not contain personally identifiable information (data that falls under the Privacy Act of 1974 5U.S.C. 552a). Use of OLCF resources to store, manipulate, or remotely access any national security information is strictly prohibited. This includes, but is not limited to: classified information, unclassified controlled nuclear information (UCNI), naval nuclear propulsion information (NNPI), the design or development of nuclear, biological, or chemical weapons or any weapons of",4.246322177812713
"What types of codes are approved for running by parties with sensitive data agreements on OLCF computers?
","The OLCF computer systems are operated as research systems and only contain data related to scientific research and do not contain personally identifiable information (data that falls under the Privacy Act of 1974 5U.S.C. 552a). Use of OLCF resources to store, manipulate, or remotely access any national security information is strictly prohibited. This includes, but is not limited to: classified information, unclassified controlled nuclear information (UCNI), naval nuclear propulsion information (NNPI), the design or development of nuclear, biological, or chemical weapons or any weapons of",4.246322177812713
"How can I update the packages in the active environment using Conda at OLCF?
","This guide introduces a user to the basic workflow of using conda environments, as well as providing an example of how to create a conda environment that uses a different version of Python than the base environment uses on Summit. Although Summit is being used in this guide, all of the concepts still apply to other OLCF systems. If using Frontier, this assumes you already have installed your own version of conda (e.g., by https://docs.olcf.ornl.gov/software/python/miniconda.html).

OLCF Systems this guide applies to:

Summit

Andes

Frontier (if using conda)",4.321998010344461
"How can I update the packages in the active environment using Conda at OLCF?
","This page describes how a user can successfully install specific quantum software tools to run on select OLCF HPC systems. This allows a user to utilize our systems to run a simulator, or even as a ""frontend"" to a vendor's quantum machine (""backend"").

For most of these instructions, we use conda environments. For more detailed information on how to use Python modules and conda environments on OLCF systems, see our :doc:`Python on OLCF Systems page</software/python/index>`, and our https://docs.olcf.ornl.gov/software/python/conda_basics.html.",4.303775045470926
"How can I update the packages in the active environment using Conda at OLCF?
","While default Python modules on OLCF systems are already set to Python 3, we recommend all users follow PEP394 by explicitly invoking either ‘python2’ or ‘python3’ instead of simply ‘python’. Python 2 Conda Environments and user installations of Python 2 will remain as options for using Python 2 on OLCF systems.

Official documentation for porting from Python 2 to Python3 can be found at: https://docs.python.org/3/howto/pyporting.html

General information and a list of open source packages dropping support for Python 2 can be found at: https://python3statement.org/",4.232822770838721
"How does jsrun set up OMP_NUM_THREADS based on the -c and -b options?
","In addition to specifying the SMT level, you can also control the number of threads per MPI task by exporting the OMP_NUM_THREADS environment variable. If you don't export it yourself, Jsrun will automatically set the number of threads based on the number of cores requested (-c) and the binding (-b) option. It is better to be explicit and set the OMP_NUM_THREADS value yourself rather than relying on Jsrun constructing it for you. Especially when you are using Job Step Viewer which relies on the presence of that environment variable to give you visual thread assignment information.",4.49592686764676
"How does jsrun set up OMP_NUM_THREADS based on the -c and -b options?
","Because jsrun sees 8 cores and the -brs flag, it assigns all 8 cores to each of the 2 tasks in the resource set. Jsrun will set up OMP_NUM_THREADS as 32 (8 cores with 4 threads per core) which will apply to all the tasks in the resource set. This means that each task sees that it can have 32 threads (which means 64 threads for the 2 tasks combined) which will oversubscribe the cores and may decrease efficiency as a result.",4.469060220447688
"How does jsrun set up OMP_NUM_THREADS based on the -c and -b options?
","In the below example, you could also do export OMP_NUM_THREADS=16 in your job script instead of passing it as a -E flag to jsrun. The below example starts 1 resource set with 2 tasks and 8 cores, 4 cores bound to each task, 16 threads for each task. We can set 16 threads since there are 4 cores per task and the default is smt4 for each core (4 * 4 = 16 threads).

jsrun -n1 -a2 -c8 -g1 -bpacked:4 -dpacked -EOMP_NUM_THREADS=16 csh -c 'echo $OMP_NUM_THREADS $OMP_PLACES'

16 0:4,4:4,8:4,12:4
16 16:4,20:4,24:4,28:4",4.427662030676119
"How can I check the version of Clang being used by CCE and ROCm on Frontier?
","Both the CCE and ROCm compilers are Clang-based, so please be sure to use consistent (major) Clang versions when using them together. You can check which version of Clang is being used with CCE and ROCm by giving the --version flag to CC and hipcc, respectively.

<string>:653: (INFO/1) Duplicate implicit target name: ""mpi"".

The MPI implementation available on Frontier is Cray's MPICH, which is ""GPU-aware"" so GPU buffers can be passed directly to MPI calls.",4.382739937143255
"How can I check the version of Clang being used by CCE and ROCm on Frontier?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers. Please see the following https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more detailed inforation on how to compile using these modules.",4.204886187397839
"How can I check the version of Clang being used by CCE and ROCm on Frontier?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers.

It is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) whenever possible. See the next section for more details.",4.176611591611788
"How do I configure HAProxy to use a different load balancing strategy in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.203161359934427
"How do I configure HAProxy to use a different load balancing strategy in Slate?
","Edit the route to include the second service with alternateBackends (see the https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes for more information)

   oc edit route my-app
   ...
   metadata:
       name: my-app
       annotations:
           haproxy.router.openshift.io/balance: roundrobin
   spec:
       host: my-app.{PROJECT}.{CLUSTER}.ccs.ornl.gov
       to:
           kind: Service
           name: my-app-a
           weight: 10
       alternateBackends:
         - kind: Service
           name: my-app-b
           weight: 15",4.129110924750305
"How do I configure HAProxy to use a different load balancing strategy in Slate?
","When using alternateBackends, be sure to set .metadata.annotations.haproxy.router.openshift.io/balance to roundrobin, like in the above example. This will ensure that HAProxy will use a round robin load balancing strategy.",4.123430178456229
"How do I delete a launch profile?
","Select the host in the left side of the window.

Select the ""Launch Profiles"" tab in the right side of the window. This will display the known launch profiles for this host.

Select a ""Launch Profile"" and the settings are displayed in the tabs below.

You can set your Project ID in the ""Default Bank/Account"" field in the ""Parallel"" tab.

You can change the queue used by modifying the ""Partition/Pool/Queue"" field in the ""Parallel"" tab.

Once you have made your changes, press the ""Apply"" button, and then save the settings (Options/Save Settings).",4.1743179775528185
"How do I delete a launch profile?
","Another solution is to delete all copies of a host profile (including the original) and remake them. This can be achieved with the ""Delete Host"" button in the Host Profiles window. Make sure to save your settings after deleting the profiles, exit and restart VisIt, and then proceed with remaking your profiles.

If none of the above solutions work for you, the final option would be to delete the duplicate host profile entirely and just modify the settings of the original when needed.",4.079606360175784
"How do I delete a launch profile?
","Under the “Launch Profiles” tab create a launch profile. Most of these values
are arbitrary

- **Profile Name**: ``batch`` (arbitrary)
- **Timeout**: 480 (arbitrary)
- **Number of threads per task**: 0 (arbitrary, but not tested
  with OMP/pthread support)
- **Additional arguments**: blank (arbitrary)

Under the “Parallel” Tab:

- **Launch parallel engine**: Checked (required)
- Launch Tab:",4.070002746231904
"What is the version of the VMD software used in the Andes environment?
","Using VisIt on Summit is also an option, as the scalable rendering problem is currently not an issue on Summit (as of Sept. 2021).

As of February 2022, this issue on Andes has been fixed (must use VisIt 3.2.2 or higher).",4.196369684470326
"What is the version of the VMD software used in the Andes environment?
","Andes: ParaView 5.9.1, 5.10.0, 5.11.0

Using a different version than what is listed above is not guaranteed to work properly.",4.195050872674183
"What is the version of the VMD software used in the Andes environment?
",of how to run a Python script using PvBatch on Andes and Summit.,4.176245228019624
"How can I connect to Rigetti's machines using pyQuil?
","Rigetti provides system access via a cloud-based JupyterLab development environment: https://docs.rigetti.com/qcs/getting-started/jupyterlab-ide.  From there, users can access a JupyterLab server loaded with Rigetti's PyQuil programming framework, Rigetti's Forest Software Development Kit, and associated program examples and tutorials.  This is the method that allows access to Rigetti's QPU's directly, as opposed to simulators.",4.467156700310966
"How can I connect to Rigetti's machines using pyQuil?
","Quil is the Rigetti-developed quantum instruction/assembly language. PyQuil is a Python library for writing and running quantum programs using Quil.

Installing pyQuil requires installing the Forest SDK. To quote Rigetti: ""pyQuil, along with quilc, the QVM, and other libraries, make up what is called the Forest SDK"". Because we don't have Docker functionality and due to normal users not having sudo privileges, this means that you will have to install the SDK via the ""bare-bones"" method. The general info below came from:

https://pyquil-docs.rigetti.com/en/stable/start.html",4.372884670438212
"How can I connect to Rigetti's machines using pyQuil?
","To submit a reservation via the QCS CLI, and installation instructions: https://docs.rigetti.com/qcs/guides/using-the-qcs-cli

Accessing the Rigetti QPU systems can only be done during a user's reservation window.  To submit a job, users must have JupyterHub access to the system.  QVM jobs can be run without network access in local environments.  Jobs are compiled via Quilc and submitted via pyQuil (see https://docs.olcf.ornl.gov/systems/rigetti.html#Software Section <rigetti-soft> below) in a python environment or Jupyter notebook.",4.315927422786807
"What is the maximum number of nodes available for a job in the batch partition?
","The batch-hm queue (and the batch-hm-spi queue for Moderate Enhanced security enclave projects) is used to access Summit's high-memory nodes.  Jobs may use all 54 nodes. It enforces the following policies:

Limit of (4) eligible-to-run jobs per user.

Jobs in excess of the per user limit above will be placed into a held state, but will change to eligible-to-run at the appropriate time.

Users may have only (25) jobs queued in the batch-hm queue at any state at any time. Additional jobs will be rejected at submit time.

batch-hm job limits:",4.30605929329799
"What is the maximum number of nodes available for a job in the batch partition?
","As submission scripts (and interactive sessions) are executed on batch nodes, the number of concurrent job steps is limited by the per-user process limit on a batch node, where a single user is only permitted 4096 simultaneous processes. This limit is per user on each batch node, not per batch job.

Each job step will create 3 processes, and JSM management may create up to ~23 processes. This creates an upper-limit of ~1350 simultaneous job steps.",4.273275442536152
"What is the maximum number of nodes available for a job in the batch partition?
","| Bin | Min Nodes | Max Nodes | Max Walltime (Hours) | Aging Boost (Days) | | --- | --- | --- | --- | --- | | 1 | 5,645 | 9,408 | 12.0 | 8 | | 2 | 1,882 | 5,644 | 12.0 | 4 | | 3 | 184 | 1,881 | 12.0 | 0 | | 4 | 92 | 183 | 6.0 | 0 | | 5 | 1 | 91 | 2.0 | 0 |

The batch queue is the default queue for production work on Frontier. Most work on Frontier is handled through this queue. The following policies are enforced for the batch queue:

Limit of four eligible-to-run jobs per user. (Jobs in excess of this number will be held, but will move to the eligible-to-run state at the appropriate time.)",4.253317363066548
"Can all members of a project access the Project Work directory?
",The difference between the three lies in the accessibility of the data to project members and to researchers outside of the project. Member Work directories are accessible only by an individual project member by default. Project Work directories are accessible by all project members. World Work directories are readable by any user on the system.,4.530327373781138
"Can all members of a project access the Project Work directory?
","3

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

4

Retention is not applicable as files will follow purge cycle.",4.443886198307035
"Can all members of a project access the Project Work directory?
","3

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

4

Retention is not applicable as files will follow purge cycle.",4.443886198307035
"How can I deploy a new version of my application to Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.250022493682481
"How can I deploy a new version of my application to Slate?
","The following items need to be established before deploying applications on Slate systems (Marble or Onyx OpenShift clusters).

Follow https://docs.olcf.ornl.gov/systems/helm_prerequisite.html#slate_getting_started if you do not already have a project/namespace established on Onyx or Marble.

It is recommended to use Helm version 3.

Helm enables application deployment via Helm Charts. The high level Helm Architecture docs are also a great reference for understanding Helm.

Like oc, Helm is a single binary executable.

This can be installed on macOS with Homebrew :

$ brew install helm",4.202320824127957
"How can I deploy a new version of my application to Slate?
","Jenkins

OpenShift Pipelines

GitLab Runners

Deploying a GitLab Runner into a Slate project may be found in the https://docs.olcf.ornl.gov/systems/overview.html#slate_gitlab_runners document which leverages a localized Slate Helm Chart. The GitLab Pipelines documentation as well as GitLab CI/CD Examples provide more details on GitLab Runner capabilities and usage.

Jenkins

For Jenkins deployment, a Slate Helm Chart is planned to be released. Documentation concerning Jenkins Pipelines as well as Jenkins Pipeline Tutorials are available.

OpenShift Pipelines",4.159969226278381
"Can I use srun to run a job that uses a custom MPI implementation?
","Using srun

By default, commands will be executed on the job’s primary compute node, sometimes referred to as the job’s head node. The srun command is used to execute an MPI binary on one or more compute nodes in parallel.

srun accepts the following common options:

| -N | Minimum number of nodes | | --- | --- | | -n | Total number of MPI tasks | | --cpu-bind=no | Allow code to control thread affinity | | -c | Cores per MPI task | | --cpu-bind=cores | Bind to cores |

If you do not specify the number of MPI tasks to srun via -n, the system will default to using only one task per node.",4.445689295746513
"Can I use srun to run a job that uses a custom MPI implementation?
","jsrun --smpiargs=""-gpu"" ...

Not using the --smpiargs=""-gpu"" flag might result in confusing segmentation faults. If you see a segmentation fault when trying to do GPU aware MPI, check to see if you have the flag set correctly.

LSF provides several utilities with which you can monitor jobs. These include monitoring the queue, getting details about a particular job, viewing STDOUT/STDERR of running jobs, and more.",4.289213479665396
"Can I use srun to run a job that uses a custom MPI implementation?
","In this sub-section, a simple MPI+OpenMP ""Hello, World"" program (hello_mpi_omp) will be used to clarify the mappings. Slurm's https://docs.olcf.ornl.gov/systems/spock_quick_start_guide.html#interactive method was used to request an allocation of 1 compute node for these examples: salloc -A <project_id> -t 30 -p <parition> -N 1

The srun options used in this section are (see man srun for more information):

| -c, --cpus-per-task=<ncpus> |  | | --- | --- | | --threads-per-core=<threads> |  | | --cpu-bind=threads |  |",4.280389699325773
"Can I use the Ascent system for work that requires a higher level of security?
","System Overview



Ascent is a stand-alone 18-node system with the same architecture and design as Summit. It's most often utilized as a resource for OLCF training events, workshops, and conferences. Ascent exists in the NCCS Open Security Enclave, which is subject to fewer restrictions than the Moderate Security Enclave that houses systems such as Summit. This means that participants in training events can go through a streamlined version of the approval process before being granted access.",4.401331857305102
"Can I use the Ascent system for work that requires a higher level of security?
",Ascent is a training system that is not intended to be used as an OLCF user resource. Access to the system is only obtained through OLCF training events.,4.349700359746025
"Can I use the Ascent system for work that requires a higher level of security?
","Ascent is an 18-node stand-alone system with the same architecture as Summit (see https://docs.olcf.ornl.gov/systems/summit_user_guide.html#summit-nodes section above), so most of this Summit User Guide can be referenced for Ascent as well. However, aside from the number of compute nodes, there are other differences between the two systems. Most notably, Ascent sits in the NCCS Open Security Enclave, which is subject to fewer restrictions than the Moderate Security Enclave that systems such as Summit belong to. This means that participants in OLCF training events can go through a streamlined",4.331952375258649
"What will happen if I don't report a missing SecurID token?
","If you are processing sensitive or proprietary data, additional paperwork is required and will be sent to you.

If you need an RSA SecurID token from our facility, the token and additional paperwork will be sent to you via email to complete identity proofing.",4.174534064799817
"What will happen if I don't report a missing SecurID token?
","You will also be sent a request to complete identity verification. When your account is approved, your RSA SecurID token will also be enabled. Please refer to our https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#system-user-guides for more information on host access. DO NOT share your PIN or RSA SecurID token with anyone. Sharing of accounts will result in termination. If your SecurID token is stolen or misplaced, contact the OLCF immediately and report the missing token. Upon termination of your account access, return the token to the OLCF in person or via mail.",4.146871854328527
"What will happen if I don't report a missing SecurID token?
","For users with accounts on non-SPI resources, you will use the same SecurID fob and PIN, but you must specify your unique SPI userID when you connect.  The ID will be used to place you in the proper UNIX groups allowing access to the project specific data, directories, and allocation.",3.9327336502696646
"How do I join a team during the hackathon?
","First, you must decide which event you'd like to attend (use link below to find a hackathon whose dates make sense for your team), and then submit a short proposal form describing your application and team. The organizing committee will then review all proposals after the call for that event closes and select the teams they believe are best suited for the event.",4.474554882277685
"How do I join a team during the hackathon?
","Typically, these hackathons are in-person events, where each team (app developers + mentors) sits at their own round table in a single large conference room. This structure allows teams to hack away on their own codes, but also to interact (ask questions, give advice, etc.) with members/mentors from other teams when needed.",4.311250064874538
"How do I join a team during the hackathon?
","If you would like more information about how you can volunteer to mentor a team at an upcoming Open/GPU hackathon, please visit our Become a Mentor page.

<p style=""font-size:20px""><b>Who can I contact with questions?</b></p>

If you have any questions about the OLCF GPU Hackathon Series, please contact OLCF Help at (help@olcf.ornl.gov).",4.188436248632666
"What is the role of the quantum resource vendor in the application approval process?
","Once submitted, you will receive email notification of successful proposal submission.  The proposal is then reviewed by the Quantum Resource Utilization Council (QRUC), as well as independent referees for merit and to ensure the feasibility of project success using the resources available to the QCUP. You will be notified of the QRUC decision via email.

Once a project request is approved by the QRUC, an OLCF Accounts Manager will communicate with the project’s PI to finalize activation and request a signed Principal Investigator’s PI Agreement to be submitted.",4.317143114914646
"What is the role of the quantum resource vendor in the application approval process?
","with the project will need to apply for a https://docs.olcf.ornl.gov/systems/quantum_access.html#User Account <quantum-user>. After your user account is approved, you can then move on to accessing the quantum resources offered by our vendors (see https://docs.olcf.ornl.gov/systems/quantum_access.html#quantum-vendors).",4.290882644536417
"What is the role of the quantum resource vendor in the application approval process?
","To gain access, you must first submit a project proposal to the OLCF QCUP (see https://docs.olcf.ornl.gov/systems/quantum_access.html#quantum-proj) or join an existing QCUP project (see https://docs.olcf.ornl.gov/systems/quantum_access.html#quantum-user). The Quantum Resource Utilization Council (QRUC), as well as independent referees, review and approve all QCUP project proposals.  Applications to QCUP are accepted year-round via the project application form found below. Once a project is approved, then all of the users associated with the project will need to apply for a",4.242921324222052
"What is the default SMT level in Summit?
",The default SMT level is 4.,4.605238546274183
"What is the default SMT level in Summit?
",For Summit:,4.228054594242555
"What is the default SMT level in Summit?
",| (recording) http://vimeo.com/306890517 | | 2018-12-05 | IBM Power9 SMT Deep Dive | Brian Thompto (IBM) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_thompto_smt.pdf https://vimeo.com/306890804 | | 2018-12-05 | Network Features & MPI Tuning | Christopher Zimmer (OLCF) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording),4.1464161472699175
"How can you access the enableRisky setting in the configuration map?
","of the CI/CD settings. In the ""Specific Runners"" area, the registration token should be available for retrieval.",3.897837072291604
"How can you access the enableRisky setting in the configuration map?
","Under metadata, add a label for ccs.ornl.gov/externalRoute: 'true' as shown below and click the Save button at the bottom of the page.

Route After

After saving, your route will be exposed on two routers, default and external. This means your service is now accessible from outside ORNL. Note that if your project has not yet been approved for external routing, this second router will not expose your route.

Route Exposed",3.885402775159978
"How can you access the enableRisky setting in the configuration map?
","If the runner is to be registered to a specific project, first ensure that the project is enabled for pipelines by navigating to the project in GitLab. In the Settings for the project, select General. Expand the ""Visibility, project features, and permissions"" section and locate the ""Pipelines"" option. If it is currently disabled, enable the ""Pipelines"" option and then ""Save Changes"". Once saved, refresh the project General Settings page, and locate the newly available ""CI/CD Settings"" option. Select ""CI/CD"", and expand the ""Runners"" section of the CI/CD settings. In the ""Specific Runners""",3.869075301768965
"Can pages automatically migrate back to CPU DDR4 on Frontier?
","The page migration behavior summarized by the following tables represents the current, observable behavior. Said behavior will likely change in the near future.  CPU accesses to migratable memory may behave differently than other platforms you're used to. On Frontier, pages will not migrate from GPU HBM to CPU DDR4 based on access patterns alone. Once a page has migrated to GPU HBM it will remain there even if the CPU accesses it, and all accesses which do not resolve in the CPU cache will occur over the Infinity Fabric between the AMD ""Optimized 3rd Gen EPYC"" CPU and AMD MI250X GPU. Pages",4.428512516645903
"Can pages automatically migrate back to CPU DDR4 on Frontier?
",page must forcibly be evicted back to CPU DDR4 to make room for other data.,4.362053080454386
"Can pages automatically migrate back to CPU DDR4 on Frontier?
",page must forcibly be evicted back to CPU DDR4 to make room for other data.,4.362053080454386
"What is the email address to contact for requesting exemption to certain system policies?
","How the software is obtained.

Explanation of why the software is needed. If OLCF provides equivalent software, what is different about this software? If requesting an upgrade, describe new features or bug fixes.

Who will be using this software? Approximately how many people will be using it? If possible, list individual users of this software.

To request exemption to certain system policies, contact help@olcf.ornl.gov with an overview of the exemption you need. Example requests include:

Relaxed queue limits for one or more jobs (longer walltime, higher priority)",4.228140039622787
"What is the email address to contact for requesting exemption to certain system policies?
","System reservation (a dedicated set of nodes at a specific date/time)

Increased disk quota

Purge exemption for User/Group/World Work areas

Special requests are reviewed weekly by the OLCF Resource Utilization Council. Please contact help@olcf.ornl.gov for more information.



This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title: Computing Policy Version: 12.10",4.1095286449373125
"What is the email address to contact for requesting exemption to certain system policies?
","these prohibited data types or information that falls under Export Control. For questions, contact help@nccs.gov.",4.095721941032236
"What is the purpose of the ""Security Enclave"" field on the OLCF training event registration form?
","OLCF training events can go through a streamlined version of the approval process before gaining access to the system. The remainder of this section of the user guide describes ""Ascent-specific"" information intended for participants of OLCF training events.",4.33195842933825
"What is the purpose of the ""Security Enclave"" field on the OLCF training event registration form?
","This sub-section describes the process of obtaining access to Ascent for an OLCF training event. Please follow the steps below to request access.

Once on the form, linked above, fill in the project ID in the ""Enter the Project ID of the project you wish to join"" field and click ""Next"".



After you enter the Project ID, use the sliders to select ""Yes"" for OLCF as the Project Organization and select ""Yes"" for Open as the Security Enclave.

<string>:5: (INFO/1) Enumerated list start value not ordinal-1: ""2"" (ordinal 2)",4.312647068220396
"What is the purpose of the ""Security Enclave"" field on the OLCF training event registration form?
","Area - The general name of the storage area.

Path - The path (symlink) to the storage area's directory.

Enclave - The security enclave where the path is available. There are several security enclaves:

- Open (O) - Ascent and other OLCF machines accessible with a username/password

- Moderate Projects not subject to export control (M1) - These are projects on machines such as Summit or Andes that require 2-factor authentication but are not subject to export controll restrictions.",4.177763215613775
"How does SBCAST handle OLCF-provided libraries in the /sw directory?
","OLCF systems provide many software packages and scientific libraries pre-installed at the system-level for users to take advantage of. In order to link these libraries into an application, users must direct the compiler to their location. The module show command can be used to determine the location of a particular library. For example",4.16019728813681
"How does SBCAST handle OLCF-provided libraries in the /sw directory?
","# If you SBCAST **all** your libraries (ie, `--exclude=NONE`), you may use the following line:
#export LD_LIBRARY_PATH=""/mnt/bb/$USER/${exe}_libs:$(pkg-config --variable=libdir libfabric)""
# Use with caution -- certain libraries may use ``dlopen`` at runtime, and that is NOT covered by sbcast
# If you use this option, we recommend you contact OLCF Help Desk for the latest list of additional steps required",4.158476059358663
"How does SBCAST handle OLCF-provided libraries in the /sw directory?
","# If you SBCAST **all** your libraries (ie, `--exclude=NONE`), you may use the following line:
#export LD_LIBRARY_PATH=""/mnt/bb/$USER/${exe}_libs:$(pkg-config --variable=libdir libfabric)""
# Use with caution -- certain libraries may use ``dlopen`` at runtime, and that is NOT covered by sbcast
# If you use this option, we recommend you contact OLCF Help Desk for the latest list of additional steps required",4.158476059358663
"How can users access their data on OLCF resources?
","OLCF resources are federal computer systems, and as such, users should have no explicit or implicit expectation of privacy. OLCF employees and authorized vendor personnel with “root” privileges have access to all data on OLCF systems. Such employees can also login to OLCF systems as other users. As a general rule, OLCF employees will not discuss your data with any unauthorized entities nor grant access to data files to any person other than the UNIX “owner” of the data file, except in the following situations:",4.436382422275268
"How can users access their data on OLCF resources?
","New to the Oak Ridge Leadership Computing Facility?

Welcome! The information below introduces how we structure user accounts, projects, and system allocations. It's all you need to know about getting to work. In general, OLCF resources are granted to projects in allocations, and are made available to the users associated with each project.",4.433402325704811
"How can users access their data on OLCF resources?
","The OLCF provides a comprehensive suite of hardware and software resources for the creation, manipulation, and retention of scientific data. This document comprises guidelines for acceptable use of those resources. It is an official policy of the OLCF, and as such, must be agreed to by relevant parties as a condition of access to and use of OLCF computational resources.",4.429865465647168
"How can I ensure that my job script executes the next command immediately after backgrounding a process in Summit?
","This method on Summit requires the user to be present until the job completes. For users who have long scripts or are unable to monitor the job, you can submit the above lines in a batch script. However, you will wait in the queue twice, so this is not recommended. Alternatively, one can use Andes.

For Andes/Frontier (Slurm Script):

Andes

.. code-block:: bash


   #!/bin/bash
   #SBATCH -A XXXYYY
   #SBATCH -J visit_test
   #SBATCH -N 1
   #SBATCH -p gpu
   #SBATCH -t 0:05:00

   cd $SLURM_SUBMIT_DIR
   date

   module load visit",4.198766145235267
"How can I ensure that my job script executes the next command immediately after backgrounding a process in Summit?
","To execute multiple job steps concurrently, standard UNIX process backgrounding can be used by adding a & at the end of the command. This will return control to the job script and execute the next command immediately, allowing multiple job launches to start at the same time. The jsruns will not share core/gpu resources in this configuration. The batch node allocation is equal to the sum of those of each jsrun, and the total walltime must be equal to or greater than that of the longest running jsrun task.",4.161871677393648
"How can I ensure that my job script executes the next command immediately after backgrounding a process in Summit?
","A wait command must follow all backgrounded processes to prevent the job from appearing completed and exiting prematurely.



The following example executes three backgrounded job steps and waits for them to finish before the job ends.

#!/bin/bash
#BSUB -P ABC123
#BSUB -W 3:00
#BSUB -nnodes 1
#BSUB -J RunSim123
#BSUB -o RunSim123.%J
#BSUB -e RunSim123.%J

cd $MEMBERWORK/abc123
jsrun <options> ./a.out &
jsrun <options> ./a.out &
jsrun <options> ./a.out &
wait",4.151262874274732
"How do I run a job on Frontier using srun?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.448919770055249
"How do I run a job on Frontier using srun?
","In addition to holding, releasing, and updating the job, the scontrol command can show detailed job information via the show job subcommand. For example, scontrol show job 12345.



The default job launcher for Frontier is srun . The srun command is used to execute an MPI binary on one or more compute nodes in parallel.

srun  [OPTIONS... [executable [args...]]]

Single Command (non-interactive)

$ srun -A <project_id> -t 00:05:00 -p <partition> -N 2 -n 4 --ntasks-per-node=2 ./a.out
<output printed to terminal>",4.436349698456455
"How do I run a job on Frontier using srun?
","Unlike Summit and Titan, there are no launch/batch nodes on Frontier. This means your batch script runs on a node allocated to you rather than a shared node. You still must use the job launcher (srun) to run parallel jobs across all of your nodes, but serial tasks need not be launched with srun.



To easily visualize job examples (see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-mapping further below), the compute node diagram has been simplified to the picture shown below.

Simplified Frontier node architecture diagram",4.402208926050651
"What is the client-server architecture used by VisIt?
","VisIt uses a client-server architecture. You will obtain the best performance by running the VisIt client on your local computer and running the server on OLCF resources. VisIt for your local computer can be obtained here: VisIt Installation.

Recommended VisIt versions on our systems:

Summit: VisIt 3.1.4

Andes: VisIt 3.3.3

Frontier: VisIt 3.3.3

Using a different version than what is listed above is not guaranteed to work properly.",4.441074147764595
"What is the client-server architecture used by VisIt?
","Navigate to the appropriate file.

Once you choose a file, you will be prompted for the number of nodes and processors you would like to use (remember that each node of Andes contains 32 processors, or 28 if using the high-memory GPU partition) and the Project ID, which VisIt calls a ""Bank"" as shown below.



Once specified, the server side of VisIt will be launched, and you can interact with your data.",4.221405399025446
"What is the client-server architecture used by VisIt?
","Although in a single machine setup both the ParaView client and server run on the same host, this need not be the case. It is possible to run a local ParaView client to display and interact with your data while the ParaView server runs in an Andes or Summit batch job, allowing interactive analysis of very large data sets.",4.2186117788585
"Can I use the Cray compiler wrappers with GPU-aware MPICH on Crusher?
","| Implementation | Module | Compiler | Header Files & Linking | | --- | --- | --- | --- | | Cray MPICH | cray-mpich | cc, CC, ftn (Cray compiler wrappers) | MPI header files and linking is built into the Cray compiler wrappers | | hipcc |  |

To use GPU-aware Cray MPICH, users must set the following modules and environment variables:

module load craype-accel-amd-gfx90a
module load rocm

export MPICH_GPU_SUPPORT_ENABLED=1

There are extra steps needed to enable GPU-aware MPI on Crusher, which depend on the compiler that is used (see 1. and 2. below).",4.471692248102171
"Can I use the Cray compiler wrappers with GPU-aware MPICH on Crusher?
","To use GPU-aware Cray MPICH with the Cray compiler wrappers, users must load specific modules, set some environment variables, and include appropriate headers and libraries. The following modules and environment variables must be set:

Setting MPICH_SMP_SINGLE_COPY_MODE=CMA is required as a temporary workaround due to a known issue. Users should make a note of where they set this environment variable (if e.g., set in a script) since it should NOT be set once the known issue has been resolved.

module load craype-accel-amd-gfx908
module load PrgEnv-cray
module load rocm",4.377904273030366
"Can I use the Cray compiler wrappers with GPU-aware MPICH on Crusher?
","Use the -craype-verbose flag to display the full include and link information used by the Cray compiler wrappers. This must be called on a file to see the full output (e.g., CC -craype-verbose test.cpp).

The MPI implementation available on Crusher is Cray's MPICH, which is ""GPU-aware"" so GPU buffers can be passed directly to MPI calls.



This section covers how to compile for different programming models using the different compilers covered in the previous section.

<string>:230: (INFO/1) Duplicate implicit target name: ""mpi"".",4.360724098103967
"How can I launch 8 MPI ranks across a node with each rank assigned its own logical core?
","The intent with both of the following examples is to launch 8 MPI ranks across the node where each rank is assigned its own logical (and, in this case, physical) core.  Using the -m distribution flag, we will cover two common approaches to assign the MPI ranks -- in a ""round-robin"" (cyclic) configuration and in a ""packed"" (block) configuration. Slurm's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive method was used to request an allocation of 1 compute node for these examples: salloc -A <project_id> -t 30 -p <parition> -N 1",4.516687838894407
"How can I launch 8 MPI ranks across a node with each rank assigned its own logical core?
","This example launches 16 MPI ranks (-n16), each with 4 physical CPU cores (-c4) to launch 1 OpenMP thread (OMP_NUM_THREADS=1) on. Because it is using 4 physical CPU cores per task, core specialization needs to be disabled (-S 0). The MPI ranks will be assigned to GPUs in a packed fashion so that each of the 8 GPUs on the node are shared by 2 MPI ranks. Similar to Example 5, -ntasks-per-gpu=2 will be used, but a new srun flag will be used to change the default round-robin (cyclic) distribution of MPI ranks across NUMA domains:",4.4223271023699375
"How can I launch 8 MPI ranks across a node with each rank assigned its own logical core?
","This example launches 8 MPI ranks (-n8), each with 8 physical CPU cores (-c8) to launch 1 OpenMP thread (OMP_NUM_THREADS=1) on. The MPI ranks will be assigned to GPUs in a packed fashion so that each of the 4 GPUs on the node are shared by 2 MPI ranks. Similar to Example 5, -ntasks-per-gpu=2 will be used, but a new srun flag will be used to change the default round-robin (cyclic) distribution of MPI ranks across NUMA domains:",4.416832866692265
"What is the purpose of the ""vampir &"" command?
","Vampir is a software performance visualizer focused on highly parallel applications. It presents a unified view on an application run including the use of programming paradigms like MPI, OpenMP, PThreads, CUDA, OpenCL and OpenACC. It also incorporates file I/O, hardware performance counters and other performance data sources. Various interactive displays offer detailed insight into the performance behavior of the analyzed application. Vampir's scalable analysis server and visualization engine enable interactive navigation of large amounts of performance data. Score-P and TAU generate OTF2",4.25365317487784
"What is the purpose of the ""vampir &"" command?
","Vampir is a software performance visualizer focused on highly parallel applications. It presents a unified view on an application run including the use of programming paradigms like MPI, OpenMP, PThreads, CUDA, OpenCL and OpenACC. It also incorporates file I/O, hardware performance counters and other performance data sources. Various interactive displays offer detailed insight into the performance behavior of the analyzed application. Vampir’s scalable analysis server and visualization engine enable interactive navigation of large amounts of performance data. Score-P and TAU generate OTF2",4.247023874597859
"What is the purpose of the ""vampir &"" command?
","For detailed information about using Vampir on Summit and the builds available, please see the Vampir Software Page.",4.219969706278473
"How many cabinets does Crusher have?
","Crusher is an National Center for Computational Sciences (NCCS) moderate-security system that contains identical hardware and similar software as the upcoming Frontier system. It is used as an early-access testbed for Center for Accelerated Application Readiness (CAAR) and Exascale Computing Project (ECP) teams as well as NCCS staff and our vendor partners. The system has 2 cabinets, the first with 128 compute nodes and the second with 64 compute nodes, for a total of 192 compute nodes.",4.177269483255298
"How many cabinets does Crusher have?
",Crusher is connected to the center-wide IBM Spectrum Scale™ filesystem providing 250 PB of storage capacity with a peak write speed of 2.5 TB/s. Crusher also has access to the center-wide NFS-based filesystem (which provides user and project home areas). While Crusher does not have direct access to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#dtn-user-guide to move data to/from HPSS.,4.15349217774043
"How many cabinets does Crusher have?
","Crusher contains a total of 768 AMD MI250X. The AMD MI250X has a peak performance of 53 TFLOPS in double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 26.5 TFLOPS (double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in both directions simultaneously).



To connect to Crusher, ssh to crusher.olcf.ornl.gov. For example:

$ ssh <username>@crusher.olcf.ornl.gov",4.07774353326387
"What is the timeline for the decommissioning of Alpine?
","We highly encourage all teams to start migrating and/or deleting data from the Alpine filesystem now.  If you wait too late in the year to begin the transition, you will run the risk of running out of time to move your data before the system is decommissioned.  It is important to note that any data remaining on the Alpine filesystem after December 31, 2023, will truly be unavailable and not recoverable in any way as the system will be dismantled and the drives will be shredded.

Moving data off-site

Globus is the suggested tool to move data off-site",4.323217674341066
"What is the timeline for the decommissioning of Alpine?
","Summit will accept batch jobs prior to 08:00 on December 18, but only batch jobs that will complete prior to 08:00 Dec 18 will run.  All batch jobs remaining in the queue at 08:00, Dec 18 will be deleted.



Alpine will be unmounted from Andes on December 19.  Jobs must be modified to use Orion as their scratch filesystem prior to this day.



In preparation for Alpine's decommission on January 01, Alpine will become read-only from all OLCF systems on December 19.",4.295250174800579
"What is the timeline for the decommissioning of Alpine?
","Due to the large amount of data on the filesystems, we strongly urge you to start transferring your data now, and do not wait until later in the year.

Jan 01, all remaining Alpine data will be PERMANENTLY DELETED.  Do not wait to move needed data.



Summit will be returned to service early 2024.

Projects awarded a 2024 Summit allocation will be able to log into Summit and submit batch jobs once the system has been made available.",4.2492579921597615
"How can I check which processes were recently killed by cgroup limits on Summit?
","If a process from any of a user’s login sessions reaches 4 hours of CPU-time, all login sessions will be limited to .5 hardware-thread. After 8 hours of CPU-time, the process is automatically killed. To reset the cgroup limits on a node to default once the 4 hour CPU-time reduction has been reached, kill the offending process and start a new login session to the node.

Users can run command check_cgroup_user on login nodes to check what processes were recently killed by cgroup limits.",4.264357185133399
"How can I check which processes were recently killed by cgroup limits on Summit?
","Because the login nodes are resources shared by all Summit users, we utilize cgroups to help better ensure resource availability for all users of the shared nodes. By default each user is limited to 16 hardware-threads, 16GB of memory, and 1 GPU.  Please note that limits are set per user and not individual login sessions. All user processes on a node are contained within a single cgroup and share the cgroup's limits.",4.230859001058825
"How can I check which processes were recently killed by cgroup limits on Summit?
","Compute-intensive, memory-intensive, or otherwise disruptive processes running on login nodes may be killed without warning.

Slurm

Most OLCF resources now use the Slurm batch scheduler. Previously, most OLCF resources used the Moab scheduler. Summit and other IBM hardware use the LSF scheduler. Below is a comparison table of useful commands among the three schedulers.",4.079824436869558
"How do I launch my R script using jsrun?
","jsrun -n4 -r2 Rscript hw.r

Before continuing, a few comments. First you need to replace the example project identifiers (ABC123 above) with your project. Second, load the appropriate modules (here we just need R). Third, make sure that you change directory to the appropriate place on gpfs. Fourth, add your jsrun call. Finally, you should also change the name of your job from rhw to something else by modifying the #BSUB -J line.",4.360243162176072
"How do I launch my R script using jsrun?
","There are two ways we can run this. One is with an interactive job, and one is with a batch job. The interactive job doesn’t provide us with the ability to run interactive R jobs on the compute nodes (using R interactively on the compute node is complicated, so we do not discuss that here.). However, it does allow us to interactively submit tasks to the compute nodes (launched via jsrun). This can be useful if you are trying to debug a script that unexpectedly dies, without having to continuously submit jobs to the batch queue (more on that in a moment).",4.31376859480997
"How do I launch my R script using jsrun?
","$ jsrun -n 1 Rscript hw.r
## [1] ""hello world""

Our task run, we can enter exit into the terminal.

Of course, this involves no parallelism, since it is a single R session. We will show how to run a basic MPI example with pbdR next.

The R code is:

suppressMessages(library(pbdMPI))

msg = paste0(""Hello from rank "", comm.rank(), "" (local rank "", comm.localrank(), "") of "", comm.size())
comm.print(msg, all.rank=TRUE, quiet=TRUE)

finalize()

As before, save this file as, say, pbdr_hw.r somewhere on gpfs. This time, we will get an interactive node with 2 nodes:",4.298125929486644
"How do I monitor the usage of my node hours on Frontier?
","The requesting project’s allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Frontier would be equivalent to using 16,000 Frontier node-hours of a project’s allocation.

Reservations should not be confused with priority requests. If quick turnaround is needed for a few jobs or for a period of time, a priority boost should be requested. A reservation should only be requested if users need to guarantee availability of a set of nodes at a given time, such as for a live demonstration at a conference.",4.197274226017546
"How do I monitor the usage of my node hours on Frontier?
","The 2 GCDs on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The OLCF Director's Discretionary (DD) program allocates approximately 10% of the available Frontier hours in a calendar year. Frontier is allocated in *node* hours, and a typical DD project is awarded between 15,000 - 20,000 *node* hours. For more information about Frontier, please visit the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#frontier-user-guide.",4.170064628038641
"How do I monitor the usage of my node hours on Frontier?
","On Frontier, there are two major types of nodes you will encounter: Login and Compute. While these are similar in terms of hardware (see: https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes), they differ considerably in their intended use.",4.1358129091582985
"What is the purpose of the --smpiargs=""off"" argument in jsrun?
","--smpiargs=""-disable_gpu_hooks""

as an argument to jsrun. Note that this is not compatible with the -gpu argument to --smpiargs, since that is what enables CUDA-aware MPI and the CUDA-aware MPI functionality depends on the CUDA hook.",4.481452083447511
"What is the purpose of the --smpiargs=""off"" argument in jsrun?
","jsrun --smpiargs=""-gpu"" ...

Not using the --smpiargs=""-gpu"" flag might result in confusing segmentation faults. If you see a segmentation fault when trying to do GPU aware MPI, check to see if you have the flag set correctly.

LSF provides several utilities with which you can monitor jobs. These include monitoring the queue, getting details about a particular job, viewing STDOUT/STDERR of running jobs, and more.",4.236803929766412
"What is the purpose of the --smpiargs=""off"" argument in jsrun?
","$ jsrun  nvprof --openmp-profiling off

Users should not use JSMD_LAUNCH_MODE=csm in their ~/.jsm.conf file at this time. A bug has been filed with IBM to address this issue.



In some cases with large number of MPI processes when there is not enough memory available on the compute node, the Abstract-Device Interface for I/O (ADIO) driver can break with this error:

Out of memory in file ../../../../../../../opensrc/ompi/ompi/mca/io/romio321/romio/adio/ad_gpfs/ad_gpfs_rdcoll.c, line 1178

The solution is to declare in your submission script:

export GPFSMPIO_COMM=1",4.23584063491851
"How can I change the default compiler environment on Andes?
","Changing compilers

If a different compiler is required, it is important to use the correct environment for each compiler. To aid users in pairing the correct compiler and environment, the module system on andes automatically pulls in libraries compiled with a given compiler when changing compilers. The compiler modules will load the correct pairing of compiler version, message passing libraries, and other items required to build and run code. To change the default loaded intel environment to the gcc environment for example, use:

$ module load gcc",4.441467843442968
"How can I change the default compiler environment on Andes?
","If a different compiler is required, it is important to use the correct environment for each compiler. To aid users in pairing the correct compiler and environment, the module system on andes automatically pulls in libraries compiled with a given compiler when changing compilers. The compiler modules will load the correct pairing of compiler version, message passing libraries, and other items required to build and run code. To change the default loaded intel environment to the gcc environment for example, use:

$ module load gcc",4.434041991253811
"How can I change the default compiler environment on Andes?
","The OLCF provides hundreds of pre-installed software packages and scientific libraries for your use, in addition to taking software installation requests. See the software page for complete details on existing installs.

Compiling code on andes is typical of commodity or Beowulf-style HPC Linux clusters.

The following compilers are available on Andes:

intel, intel composer xe (default)

pgi, the portland group compilar suite

gcc, the gnu compiler collection",4.230376819811877
"What is the advantage of using OpenShift to build the new image?
","Building an image in Openshift is the act of transferring a set of input parameters into an object. That object is typically an image. Openshift contains all of the necessary components to build a Docker image or a piece of source code an image that will run in a container.

A Docker build is achieved by linking to a source repository that contains a docker image and all of the necessary Docker artifacts. A build is then triggered through the standard $ docker build command. More specific documentation on docker builds can be found here.",4.367749846866177
"What is the advantage of using OpenShift to build the new image?
","OpenShift has an integrated container image build service that users interact with through BuildConfig objects. BuildConfig's are very powerful, builds can be triggered by git repo or image tag pushes and connected into a pipeline to do automated deployments of newly built images. While powerful, these mechanisms can be cumbersome when starting out so we will be using a BuildConfig in a slightly simpler setup.

We will create a BuildConfig that will take a Binary (Local) source which will stream the contents of our local filesystem to the builder.",4.342695078094387
"What is the advantage of using OpenShift to build the new image?
","Openshift will automatically pull and build your source. If you make a change and need an updated build simply navigate to the build option on the menu to the left and select your project and click Start Build in the upper right.

The first step to creating the the build is to create a build config. This is done in one of three ways depending on how your code is structured. If you have a git repository already configured and it is public then the command

oc new-build .

will create your build config from that repository.",4.272139824235445
"What happens if an individual violates OLCF policies and procedures?
","authorized individual or investigative agency is in effect during the period of your access to information on a DOE computer and for a period of three years thereafter. OLCF personnel and users are required to address, safeguard against, and report misuse, abuse and criminal activities. Misuse of OLCF resources can lead to temporary or permanent disabling of accounts, loss of DOE allocations, and administrative or legal actions. Users who have not accessed a OLCF computing resource in at least 6 months will be disabled. They will need to reapply to regain access to their account. All users",4.315029211927547
"What happens if an individual violates OLCF policies and procedures?
","The requirements outlined in this document apply to all individuals who have an OLCF account. It is your responsibility to ensure that all individuals have the proper need-to-know before allowing them access to the information on OLCF computing resources. This document will outline the main security concerns.

OLCF computing resources are for business use only. Installation or use of software for personal use is not allowed. Incidents of abuse will result in account termination. Inappropriate uses include, but are not limited to:

Sexually oriented information",4.275838337869985
"What happens if an individual violates OLCF policies and procedures?
","Users are prohibited from taking unauthorized actions to intentionally modify or delete information or programs.

The OLCF reserves the right to remove any data at any time and/or transfer data to other users working on the same or similar project once a user account is deleted or a person no longer has a business association with the OLCF. After a sensitive project has ended or has been terminated, all data related to the project must be purged from all OLCF computing resources within 30 days.",4.248775585767622
"How can I use TAU to analyze the performance of my MPI+OpenMP applications?
","For Fortran: replace the compiler with the TAU wrapper tau_f90.sh / tau_f77.sh

Even if you don't compile your application with a TAU wrapper, you can profile some basic functionalities with tau_exec, for example:

jsrun -n 4 –r 4 –a 1 –c 1 tau_exec -T mpi ./test

The above command profiles MPI for the binary test, which was not compiled with the TAU wrapper.

The following TAU environment variables may be useful in job submission scripts.",4.402550682971588
"How can I use TAU to analyze the performance of my MPI+OpenMP applications?
","Below, we'll look at using TAU to profile each case.

Edit the cmake_summit_pgi.sh and replace mpic++ with tau_cxx.sh. This applies only for the non-GPU versions.

TAU works with special TAU makefiles to declare what programming models are expected from the application:

The available makefiles are located inside TAU installation:",4.387328916484644
"How can I use TAU to analyze the performance of my MPI+OpenMP applications?
","TAU is installed with Program Database Toolkit (PDT) on Summit. PDT is a framework for analyzing source code written in several programming languages. Moreover, Performance Application Programming Interface (PAPI) is supported. PAPI counters are used to assess the CPU performance. In this section, some approaches for profiling and tracing will be presented.

In most cases, we need to use wrappers to recompile the application:

For C: replace the compiler with the TAU wrapper tau_cc.sh

For C++: replace the compiler with the TAU wrapper tau_cxx.sh",4.384452176548431
"What is the myOLCF site used for?
","Project members

The myOLCF site is provided to aid in the utilization and management of OLCF allocations. See the https://docs.olcf.ornl.gov/services_and_applications/myolcf/index.html for more information.

If you have any questions or have a request for additional data, please contact the OLCF User Assistance Center.",4.541025721881555
"What is the myOLCF site used for?
","myOLCF is currently available to OLCF Moderate user accounts; i.e., users that authenticate to OLCF systems with an RSA SecurID token. Visit https://my.olcf.ornl.gov and authenticate with your OLCF Moderate username and RSA SecurID PASSCODE (PIN followed by the 6-digit tokencode).

The myOLCF login page

OLCF Open user accounts, i.e., users that authenticate to OLCF systems with a password, cannot access myOLCF at this time, as we are still investigating the feasibility of supporting password-only authentication.",4.308449252236002
"What is the myOLCF site used for?
","Project members

The myOLCF site is provided to aid in the utilization and management of OLCF allocations. See the https://docs.olcf.ornl.gov/services_and_applications/myolcf/index.html for more information.

If you have any questions or have a request for additional data, please contact the OLCF User Assistance Center.





<string>:1129: (INFO/1) Duplicate implicit target name: ""debugging"".",4.282019721032532
"How do I monitor the status of my application in libEnsemble?
","libEnsemble is a complete https://docs.olcf.ornl.gov/systems/libensemble.html#Python<py-index> toolkit for steering dynamic ensembles of calculations. Workflows are highly portable and detect/integrate heterogeneous resources with little effort. For instance, libEnsemble can automatically detect, assign, and reassign allocated processors and GPUs to ensemble members.",4.172987413693559
"How do I monitor the status of my application in libEnsemble?
","Upon initialization, libEnsemble will detect available nodes and GPUs from the Slurm environment, and allocate those resources towards application-launches.

Start an interactive session:

$ salloc --nodes=2 -A <project_id> --time=00:10:00

Within the session (multiprocessing comms, all processes on first node):

$ python my_libensemble_script.py --comms local --nworkers 8",4.130210387454443
"How do I monitor the status of my application in libEnsemble?
","Users select or supply generator and simulator functions to express their ensembles; the generator typically steers the ensemble based on prior simulator results. Such functions can also launch and monitor external executables at any scale.

Begin by loading the python module:

$ module load cray-python

libEnsemble is available on PyPI, conda-forge, the xSDK, and E4S. Most users install libEnsemble via pip:

$ pip install libensemble",4.118375816979639
"How can I contact IBM support for issues related to Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.224263246402623
"How can I contact IBM support for issues related to Summit?
","Summit is connected to an IBM Spectrum Scale™ filesystem providing 250PB of storage capacity with a peak write speed of 2.5 TB/s. Summit also has access to the center-wide NFS-based filesystem (which provides user and project home areas) and has access to the center’s High Performance Storage System (HPSS) for user and project archival storage.

Summit is running Red Hat Enterprise Linux (RHEL) version 8.2.",4.216311160004556
"How can I contact IBM support for issues related to Summit?
",https://vimeo.com/306435487 | | 2018-12-03 | Experiences Porting/Optimizing Codes for Acceptance Testing | Bob Walkup (IBM) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording 1 recording 2) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_walkup.pdf https://vimeo.com/306890861 https://vimeo.com/306890949 | | 2018-12-03 | Practical Tips for Running on Summit | David Appelhans (IBM) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording),4.212164183558598
"What is the equivalent LSF command for the PBS/Torque/Moab command qsub?
",Users of other OLCF resources are likely familiar with PBS-like commands which are used by the Torque/Moab instances on other systems. The table below summarizes the equivalent LSF command for various PBS/Torque/Moab commands.,4.655149509912869
"What is the equivalent LSF command for the PBS/Torque/Moab command qsub?
","The table below shows shows LSF (bsub) command-line/batch script options and the PBS/Torque/Moab (qsub) options that provide similar functionality.

| LSF Option | PBS/Torque/Moab Option | Description | | --- | --- | --- | | #BSUB -W 60 | #PBS -l walltime=1:00:00 | Request a walltime of 1 hour | | #BSUB -nnodes 1024 | #PBS -l nodes=1024 | Request 1024 nodes | | #BSUB -P ABC123 | #PBS -A ABC123 | Charge the job to project ABC123 | | #BSUB -alloc_flags gpumps | No equivalent (set via environment variable) | Enable multiple MPI tasks to simultaneously access a GPU |",4.559124323921707
"What is the equivalent LSF command for the PBS/Torque/Moab command qsub?
","| LSF Command | PBS/Torque/Moab Command | Description | | --- | --- | --- | | bsub job.sh | qsub job.sh | Submit the job script job.sh to the batch system | | bsub -Is /bin/bash | qsub -I | Submit an interactive batch job | | bjobs -u all | qstat showq | Show jobs currently in the queue NOTE: without the -u all argument, bjobs will only show your jobs | | bjobs -l | checkjob | Get information about a specific job | | bjobs -d | showq -c | Get information about completed jobs | | bjobs -p | showq -i showq -b checkjob | Get information about pending jobs | | bjobs -r | showq -r | Get",4.308809825958664
"What is the purpose of the #!/bin/bash line at the top of the script?
","The first line of a script can be used to specify the script’s interpreter; this line is optional. If not used, the submitter’s default shell will be used. The line uses the hash-bang syntax, i.e., #!/path/to/shell.

The Slurm submission options are preceded by the string #SBATCH, making them appear as comments to a shell. Slurm will look for #SBATCH options in a batch script from the script’s first line through the first non-comment line. A comment line begins with #. #SBATCH options entered after the first non-comment line will not be read by Slurm.",4.2928253979729005
"What is the purpose of the #!/bin/bash line at the top of the script?
","Interpreter Line

The first line of a script can be used to specify the script’s interpreter; this line is optional. If not used, the submitter’s default shell will be used. The line uses the hash-bang syntax, i.e., #!/path/to/shell.

Slurm Submission Options",4.247715646031282
"What is the purpose of the #!/bin/bash line at the top of the script?
","The execution section of a script will be interpreted by a shell and can contain multiple lines of executables, shell commands, and comments.  when the job's queue wait time is finished, commands within this section will be executed on the primary compute node of the job's allocated resources. Under normal circumstances, the batch job will exit the queue after the last line of the script is executed.

#!/bin/bash
#SBATCH -A XXXYYY
#SBATCH -J test
#SBATCH -N 2
#SBATCH -t 1:00:00

cd $SLURM_SUBMIT_DIR
date
srun -n 8 ./a.out",4.220304945314496
"How can I build an image in Openshift?
","Building an image in Openshift is the act of transferring a set of input parameters into an object. That object is typically an image. Openshift contains all of the necessary components to build a Docker image or a piece of source code an image that will run in a container.

A Docker build is achieved by linking to a source repository that contains a docker image and all of the necessary Docker artifacts. A build is then triggered through the standard $ docker build command. More specific documentation on docker builds can be found here.",4.578297131867571
"How can I build an image in Openshift?
","What happens is that oc pulls in the provided repository, in this example Django, and automatically configures everything needed to build the image. You should now be able to go to the Openshift web GUI and under the builds tab see your newly built build.

Now, since everything has been configured, you can click the Start Build button in the upper right hand side of the Web GUI anytime that you need to make another build. You can also start a another build from the command line with either:

oc start-build <buildconfig_name>

Or, if you would like to receive logs from the build:",4.4669599950525365
"How can I build an image in Openshift?
","Openshift will automatically pull and build your source. If you make a change and need an updated build simply navigate to the build option on the menu to the left and select your project and click Start Build in the upper right.

The first step to creating the the build is to create a build config. This is done in one of three ways depending on how your code is structured. If you have a git repository already configured and it is public then the command

oc new-build .

will create your build config from that repository.",4.466361257854499
"Can you use Cupy to perform operations on arrays located on multiple GPUs?
","most of the array operations that NumPy provides, including array indexing, math, and transformations. Most operations provide an immediate speed-up out of the box, and some operations are sped up by over a factor of 100 (see CuPy benchmark timings below, from the Single-GPU CuPy Speedups article).",4.284754943395202
"Can you use Cupy to perform operations on arrays located on multiple GPUs?
",">>> with cp.cuda.Device(0):
...    x_gpu_0 = cp.array([1, 2, 3, 4, 5]) # create an array in GPU 0
...
>>> with cp.cuda.Device(1):
...    x_gpu_0 * 2  # ERROR: trying to use x_gpu_0 on GPU 1
...
PerformanceWarning: The device where the array resides (0) is different from the current device (1). Peer access has been activated automatically.

To solve the above warning/error, you must transfer x_gpu_0 to ""Device 1"". A CuPy array can be transferred to a specific GPU using the cupy.asarray() function while on the specific device:",4.252322661607817
"Can you use Cupy to perform operations on arrays located on multiple GPUs?
","CuPy is a library that implements NumPy arrays on NVIDIA GPUs by utilizing CUDA Toolkit libraries like cuBLAS, cuRAND, cuSOLVER, cuSPARSE, cuFFT, cuDNN and NCCL. Although optimized NumPy is a significant step up from Python in terms of speed, performance is still limited by the CPU (especially at larger data sizes) -- this is where CuPy comes in. Because CuPy's interface is nearly a mirror of NumPy, it acts as a replacement to run existing NumPy/SciPy code on NVIDIA CUDA platforms, which helps speed up calculations further. CuPy supports most of the array operations that NumPy provides,",4.197869782491922
"Can I manually kill a job in the killable queue?
","To submit a job to the killable queue, add the -q killable option to your bsub command or #BSUB -q killable to your job script.

To prevent a preempted job from being automatically requeued, the BSUB -rn flag can be used at submit time.",4.280656739202861
"Can I manually kill a job in the killable queue?
","The killable queue is a preemptable queue that allows jobs in bins 4 and 5 to request walltimes up to 24 hours. Jobs submitted to the killable queue will be preemptable once the job reaches the guaranteed runtime limit as shown in the table below. For example, a job in bin 5 submitted to the killable queue can request a walltime of 24 hours. The job will be preemptable after two hours of run time. Similarly, a job in bin 4 will be preemptable after six hours of run time. Once a job is preempted, the job will be resubmitted by default with the original limits as requested in the job script and",4.251127498678201
"Can I manually kill a job in the killable queue?
","Like bstop and bresume, bkill command also supports identifying the job(s) to be signaled by criteria other than the job id. These include some/all jobs with a given name, in a particular queue, etc. See man bkill for more information.",4.196046880331605
"Are there any restrictions on the types of kernels that can run on Frontier's MI250X GPU?
","Each Frontier compute node contains 4 AMD MI250X. The AMD MI250X has a peak performance of 53 TFLOPS in double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 26.5 TFLOPS (double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in each direction simultaneously).

To connect to Frontier, ssh to frontier.olcf.ornl.gov. For example:

$ ssh <username>@frontier.olcf.ornl.gov",4.270798342469559
"Are there any restrictions on the types of kernels that can run on Frontier's MI250X GPU?
","The Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.239938023965729
"Are there any restrictions on the types of kernels that can run on Frontier's MI250X GPU?
",Programming Environment  Frontier utilizes the Cray Programming Environment.  Many aspects including LMOD are similar to Summit's environment.  But Cray's compiler wrappers and the Cray and AMD compilers are worth noting.  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more information.  AMD GPUs  Each frontier node has 4 AMD MI250X accelerators with two Graphic Compute Dies (GCDs) in each accelerator. The system identifies each GCD as an independent device (so for simplicity we use the term GPU when we talk about a GCD) for a total of 8,4.226757114865548
"How can I compile a C program using the Cray compiler on Crusher?
","Cray, AMD, and GCC compilers are provided through modules on Crusher. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers.

It is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) whenever possible. See the next section for more details.",4.340240237894085
"How can I compile a C program using the Cray compiler on Crusher?
","Use the -craype-verbose flag to display the full include and link information used by the Cray compiler wrappers. This must be called on a file to see the full output (e.g., CC -craype-verbose test.cpp).

The MPI implementation available on Crusher is Cray's MPICH, which is ""GPU-aware"" so GPU buffers can be passed directly to MPI calls.



This section covers how to compile for different programming models using the different compilers covered in the previous section.

<string>:230: (INFO/1) Duplicate implicit target name: ""mpi"".",4.294487939385134
"How can I compile a C program using the Cray compiler on Crusher?
","| Implementation | Module | Compiler | Header Files & Linking | | --- | --- | --- | --- | | Cray MPICH | cray-mpich | cc, CC, ftn (Cray compiler wrappers) | MPI header files and linking is built into the Cray compiler wrappers | | hipcc |  |

To use GPU-aware Cray MPICH, users must set the following modules and environment variables:

module load craype-accel-amd-gfx90a
module load rocm

export MPICH_GPU_SUPPORT_ENABLED=1

There are extra steps needed to enable GPU-aware MPI on Crusher, which depend on the compiler that is used (see 1. and 2. below).",4.260515996762075
"What is the rank of the first node in the system?
","On Summit, there are three major types of nodes you will encounter: Login, Launch, and Compute. While all of these are similar in terms of hardware (see: https://docs.olcf.ornl.gov/systems/summit_user_guide.html#summit-nodes), they differ considerably in their intended use.",4.096932079847501
"What is the rank of the first node in the system?
","Login nodes have (2) 16-core Power9 CPUs and (4) V100 GPUs. Compute nodes have (2) 22-core Power9 CPUs and (6) V100 GPUs.

Summit nodes are connected to a dual-rail EDR InfiniBand network providing a node injection bandwidth of 23 GB/s. Nodes are interconnected in a Non-blocking Fat Tree topology. This interconnect is a three-level tree implemented by a switch to connect nodes within each cabinet (first level) along with Director switches (second and third level) that connect cabinets together.",4.08203409717185
"What is the rank of the first node in the system?
","Recall from the System Overview that Frontier contains two node types: Login and Compute. When you connect to the system, you are placed on a login node. Login nodes are used for tasks such as code editing, compiling, etc. They are shared among all users of the system, so it is not appropriate to run tasks that are long/computationally intensive on login nodes. Users should also limit the number of simultaneous tasks on login nodes (e.g. concurrent tar commands, parallel make",4.065017363789295
"What is the version of ESSL installed on Summit?
","summit$ module show essl
------------------------------------------------------------------------------------
   /sw/summit/modulefiles/core/essl/6.1.0-1:
------------------------------------------------------------------------------------
whatis(""ESSL 6.1.0-1 "")
prepend_path(""LD_LIBRARY_PATH"",""/sw/summit/essl/6.1.0-1/essl/6.1/lib64"")
append_path(""LD_LIBRARY_PATH"",""/sw/summit/xl/16.1.1-beta4/lib"")
prepend_path(""MANPATH"",""/sw/summit/essl/6.1.0-1/essl/6.1/man"")
setenv(""OLCF_ESSL_ROOT"",""/sw/summit/essl/6.1.0-1/essl/6.1"")
help([[ESSL 6.1.0-1

]])",4.349356607031551
"What is the version of ESSL installed on Summit?
",List of installed packages on Summit for E4S release 21.05:,4.331054843012792
"What is the version of ESSL installed on Summit?
","]])

When this module is loaded, the $OLCF_ESSL_ROOT environment variable holds the path to the ESSL installation, which contains the lib64/ and include/ directories:

summit$ module load essl
summit$ echo $OLCF_ESSL_ROOT
/sw/summit/essl/6.1.0-1/essl/6.1
summit$ ls $OLCF_ESSL_ROOT
FFTW3  READMES  REDIST.txt  include  iso-swid  ivps  lap  lib64  man  msg

The following screencast shows an example of linking two libraries into a simple program on Summit. https://vimeo.com/292015868",4.25460053091788
"What is the purpose of the ""vglrun vmd"" command?
","/sw/andes/spack-envs/base/opt/linux-rhel8-x86_64/gcc-8.3.1/vmd-1.9.3-javakxxmgnha3ah4hqcv2rpx4paunyzf/lib/vmd_LINUXAMD64: /lib64/libGL.so.1: no version information available (required by /sw/andes/spack-envs/base/opt/linux-rhel8-x86_64/gcc-8.3.1/vmd-1.9.3-javakxxmgnha3ah4hqcv2rpx4paunyzf/lib/vmd_LINUXAMD64)
Info) VMD for LINUXAMD64, version 1.9.3 (November 30, 2016)
Info) http://www.ks.uiuc.edu/Research/vmd/
Info) Email questions and bug reports to vmd@ks.uiuc.edu
Info) Please include this reference in published work using VMD:",4.000496083943043
"What is the purpose of the ""vglrun vmd"" command?
","/sw/andes/spack-envs/base/opt/linux-rhel8-x86_64/gcc-8.3.1/vmd-1.9.3-javakxxmgnha3ah4hqcv2rpx4paunyzf/lib/vmd_LINUXAMD64: /lib64/libGL.so.1: no version information available (required by /sw/andes/spack-envs/base/opt/linux-rhel8-x86_64/gcc-8.3.1/vmd-1.9.3-javakxxmgnha3ah4hqcv2rpx4paunyzf/lib/vmd_LINUXAMD64)
Info) VMD for LINUXAMD64, version 1.9.3 (November 30, 2016)
Info) http://www.ks.uiuc.edu/Research/vmd/
Info) Email questions and bug reports to vmd@ks.uiuc.edu
Info) Please include this reference in published work using VMD:",4.000496083943043
"What is the purpose of the ""vglrun vmd"" command?
","Step 1 (local system)

<string>:1329: (INFO/1) Duplicate implicit target name: ""step 1 (local system)"".

Install a vncviewer (turbovnc, tigervnc, etc.) on your local machine.  When running vncviewer for the first time, it will ask to set a password for this and future vnc sessions.

Step 2 (terminal 1)

<string>:1336: (INFO/1) Duplicate implicit target name: ""step 2 (terminal 1)"".

From an Andes connection launch a batch job and execute the below vmd-vgl.sh script to start the vncserver and run vmd within:

localsytem: ssh -X username@andes.olcf.ornl.gov",3.980290062854325
"How can I list all currently loaded modules in Frontier?
","below), you can choose between the rendering options by loading its corresponding module on the system you're connected to. For example, to see these modules on Summit:",4.174959519024669
"How can I list all currently loaded modules in Frontier?
","<p style=""font-size:20px""><b>Frontier: Darshan Runtime 3.4.0 (May 10, 2023)</b></p>

The Darshan Runtime modulefile darshan-runtime/3.4.0 on Frontier is now loaded by default. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>.

Unloading darshan-runtime modulefile is recommended for users profiling their applications with other profilers to prevent conflicts.",4.15905098982167
"How can I list all currently loaded modules in Frontier?
",| Command | Description | | --- | --- | | module -t list | Shows a terse list of the currently loaded modules. | | module avail | Shows a table of the currently available modules | | module help <modulename> | Shows help information about <modulename> | | module show <modulename> | Shows the environment changes made by the <modulename> modulefile | | module spider <string> | Searches all possible modules according to <string> | | module load <modulename> [...] | Loads the given <modulename>(s) into the current environment | | module use <path> | Adds <path> to the modulefile search cache and,4.095774152136921
"Can CUTLASS be used for other operations besides matrix multiplication?
","The CUTLASS library provides a variety of primitives that are optimized for proper data layout and movement to achieve the maximum possible performance of a matrix multiplation on an NVIDIA GPU. These include iterators for blocking, loading, and storing matrix tiles, plus optimized classes for transforming the data and performing the actual multiplication. CUTLASS provides extensive documentation of these features and examples have been provided. Interested developers are also encouraged to watch the CUTLASS introduction video from GTC2018.",4.477663228647588
"Can CUTLASS be used for other operations besides matrix multiplication?
","<string>:5: (INFO/1) Duplicate implicit target name: ""cutlass"".

CUTLASS is an open-source library provided by NVIDIA for building matrix multiplication operations using C++ templates. The goal is to provide performance that is nearly as good as the hand-tuned cuBLAS library, but in a more expressive, composible manner.",4.293570851266443
"Can CUTLASS be used for other operations besides matrix multiplication?
","Even if iterative techniques are not available for an application, direct use of Tensor Cores may be beneficial if at least the A and B matrices can be constructed from the input data without significant loss of precision. Since the C and D matrices may be 32-bit, the output may have a higher degree of precision than the input. It may be possible to try these operations automatically by setting the math mode in cuBLAS, as detailed above, to determine whether the loss of precision is an acceptable trade-off for increased performance in a given application. If it is, the cublasGemmEx API allows",4.172192518627958
"How do you set a password for the initial connection or enter the created password for subsequent connections?
","This will give you a new opportunity to enter your PIN + token code and your username will appear in login request box as shown below. If you want you OLCF username to be filled in by default, go to ""Options→Host profiles"" and enter it under ""Username"".",4.091578551197154
"How do you set a password for the initial connection or enter the created password for subsequent connections?
","Access to systems is provided via Secure Shell version 2 (sshv2). You will need to ensure that your ssh client supports keyboard-interactive authentication. The method of setting up this authentication varies from client to client, so you may need to contact your local administrator for assistance. Most new implementations support this authentication type, and many ssh clients are available on the web. Login sessions will be automatically terminated after a period of inactivity. When you apply for an account, you will be mailed an RSA SecurID token. You will also be sent a request to complete",4.077670241159007
"How do you set a password for the initial connection or enter the created password for subsequent connections?
","If the above does not apply to you, double check that you set up your host profile exactly as how it is outlined in the https://docs.olcf.ornl.gov/systems/visit.html#visit-host-profiles section. It may be helpful to delete and remake your host profile, but just remember to always save your settings via ""Options/Save Settings"".

If VisIt keeps asking for your ""Password"" in the dialog box below, and you are entering your correct PIN + RSA token code, you might need to select ""Change username"" and then enter your OLCF username when prompted.",4.066287439028014
"Are backups enabled for the member work directory on Slate?
","Files within ""Work"" directories (i.e., Member Work, Project Work, World Work) are not backed up and are purged on a regular basis according to the timeframes listed above.",4.223830524095846
"Are backups enabled for the member work directory on Slate?
","Footnotes

1

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

2

Retention is not applicable as files will follow purge cycle.",4.152152506232577
"Are backups enabled for the member work directory on Slate?
","3

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

4

Retention is not applicable as files will follow purge cycle.",4.134616435742115
"What is dockershim on Slate?
","Jenkins

OpenShift Pipelines

GitLab Runners

Deploying a GitLab Runner into a Slate project may be found in the https://docs.olcf.ornl.gov/systems/overview.html#slate_gitlab_runners document which leverages a localized Slate Helm Chart. The GitLab Pipelines documentation as well as GitLab CI/CD Examples provide more details on GitLab Runner capabilities and usage.

Jenkins

For Jenkins deployment, a Slate Helm Chart is planned to be released. Documentation concerning Jenkins Pipelines as well as Jenkins Pipeline Tutorials are available.

OpenShift Pipelines",4.137252444545775
"What is dockershim on Slate?
","The following items need to be established before deploying applications on Slate systems (Marble or Onyx OpenShift clusters).

Follow https://docs.olcf.ornl.gov/systems/helm_prerequisite.html#slate_getting_started if you do not already have a project/namespace established on Onyx or Marble.

It is recommended to use Helm version 3.

Helm enables application deployment via Helm Charts. The high level Helm Architecture docs are also a great reference for understanding Helm.

Like oc, Helm is a single binary executable.

This can be installed on macOS with Homebrew :

$ brew install helm",4.04052786558576
"What is dockershim on Slate?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",4.039075113551787
"What is the purpose of the HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query?
","$ AMD_LOG_LEVEL=1 HSA_XNACK=0 srun -n 1 -N 1 -t 1 ./xnack_plus.exe
:1:rocdevice.cpp            :1573: 43966598070 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966598762 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966599392 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966599970 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.",4.111459037760401
"What is the purpose of the HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query?
","$ AMD_LOG_LEVEL=1 HSA_XNACK=0 srun -n 1 -N 1 -t 1 ./xnack_plus.exe
:1:rocdevice.cpp            :1573: 43966598070 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966598762 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966599392 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966599970 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.",4.111133900639063
"What is the purpose of the HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query?
",":1:rocdevice.cpp            :1573: 43966600550 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966601109 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966601673 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966602248 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:hip_code_object.cpp      :460 : 43966602806 us: hipErrorNoBinaryForGpu: Unable to find code object for all current devices!",4.10132064559265
"What is the GPU allocation for rank 5 in Summit?
","Rank:   20; NumRanks: 24; RankCore:  96; Hostname: a01n01; GPU: 4
Rank:   21; NumRanks: 24; RankCore: 100; Hostname: a01n01; GPU: 4

Rank:   22; NumRanks: 24; RankCore: 104; Hostname: a01n01; GPU: 5
Rank:   23; NumRanks: 24; RankCore: 108; Hostname: a01n01; GPU: 5

summit>",4.298261732024279
"What is the GPU allocation for rank 5 in Summit?
","Rank:   18; NumRanks: 24; RankCore:  88; Hostname: a33n05; GPU: 3, 4, 5
Rank:   19; NumRanks: 24; RankCore:  92; Hostname: a33n05; GPU: 3, 4, 5
Rank:   20; NumRanks: 24; RankCore:  96; Hostname: a33n05; GPU: 3, 4, 5
Rank:   21; NumRanks: 24; RankCore: 100; Hostname: a33n05; GPU: 3, 4, 5
Rank:   22; NumRanks: 24; RankCore: 104; Hostname: a33n05; GPU: 3, 4, 5
Rank:   23; NumRanks: 24; RankCore: 108; Hostname: a33n05; GPU: 3, 4, 5
summit>",4.273485011400919
"What is the GPU allocation for rank 5 in Summit?
","Each Summit Compute node has 6 NVIDIA V100 GPUs.  The NVIDIA Tesla V100 accelerator has a peak performance of 7.8 TFLOP/s (double-precision) and contributes to a majority of the computational work performed on Summit. Each V100 contains 80 streaming multiprocessors (SMs), 16 GB (32 GB on high-memory nodes) of high-bandwidth memory (HBM2), and a 6 MB L2 cache that is available to the SMs. The GigaThread Engine is responsible for distributing work among the SMs and (8) 512-bit memory controllers control access to the 16 GB (32 GB on high-memory nodes) of HBM2 memory. The V100 uses NVIDIA's",4.248355269253413
"What is the deadline for migrating data from Alpine to Orion?
","To assist you with moving your data off of Alpine, the DTN's mount the new Orion filesystem and all projects with access to Alpine have now been granted access to the Orion filesystem.

Please do not wait to migrate needed data, begin migrating all needed data now.",4.477948564920844
"What is the deadline for migrating data from Alpine to Orion?
","We highly encourage all teams to start migrating and/or deleting data from the Alpine filesystem now.  If you wait too late in the year to begin the transition, you will run the risk of running out of time to move your data before the system is decommissioned.  It is important to note that any data remaining on the Alpine filesystem after December 31, 2023, will truly be unavailable and not recoverable in any way as the system will be dismantled and the drives will be shredded.

Moving data off-site

Globus is the suggested tool to move data off-site",4.42389445070533
"What is the deadline for migrating data from Alpine to Orion?
",Data will not be automatically transferred from Alpine to Orion. Users should consider the data needed from Alpine and transfer it. Globus is the preferred method and there is access to both Orion and Alpine through the Globus OLCF DTN endpoint. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-transferring-data-globus. Use of HPSS to specifically stage data for the Alpine to Orion transfer is discouraged.,4.362032100167729
"What is the name of the user who owns the scratch directory?
","$ ls /gpfs/wolf/[projid]
proj-shared  scratch  world-shared

proj-shared can be accessed by all members of a project.

scratch contains directories for each user of a project and only that user can access their own directory.

world-shared can be accessed by any users on the system in any project.

Ascent is a training system that is not intended to be used as an OLCF user resource. Access to the system is only obtained through OLCF training events.",4.063858890874858
"What is the name of the user who owns the scratch directory?
","Each project has a World Work directory that resides in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. World Work areas can be accessed by all users of the system and are intended for sharing of data between projects. World Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but rather should be",4.02426571505206
"What is the name of the user who owns the scratch directory?
","Each project is granted a Project Work directory; these reside in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. Project Work directories can be accessed by all members of a project and are intended for sharing data within a project. Project Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but",4.009533509854863
"How do I launch a script on Summit using bsub?
","For Summit (module):

$ module load visit
$ visit -nowin -cli -v 3.1.4 -l bsub/jsrun -p batch -b XXXYYY -t 00:05 -np 42 -nn 1 -s visit_example.py

Due to the nature of the custom VisIt launcher for Summit, users are unable to solely specify -l jsrun for VisIt to work properly. Instead of manually creating a batch script, as seen in the Andes method outlined below, VisIt submits its own through -l bsub/jsrun. The -t flag sets the time limit, -b specifies the project to be charged, and -p designates the queue the job will submit to.",4.406346234323911
"How do I launch a script on Summit using bsub?
","If you’ve previously used LSF, you’re probably used to submitting a job with input redirection (i.e. bsub < myjob.lsf). This is not needed (and will not work) on Summit.

As an example, consider the following batch script:

#!/bin/bash
# Begin LSF Directives
#BSUB -P ABC123
#BSUB -W 3:00
#BSUB -nnodes 2048
#BSUB -alloc_flags gpumps
#BSUB -J RunSim123
#BSUB -o RunSim123.%J
#BSUB -e RunSim123.%J

cd $MEMBERWORK/abc123
cp $PROJWORK/abc123/RunData/Input.123 ./Input.123
date
jsrun -n 4092 -r 2 -a 12 -g 3 ./a.out
cp my_output_file /ccs/proj/abc123/Output.123",4.345289119486868
"How do I launch a script on Summit using bsub?
","As pointed out in https://docs.olcf.ornl.gov/systems/summit_user_guide.html#login-launch-and-compute-nodes, you will be placed on a launch (a.k.a. ""batch"") node upon launching an interactive job and as usual need to use jsrun to access the compute node(s):

$ bsub -Is -W 0:10 -nnodes 1 -P STF007 $SHELL
Job <779469> is submitted to default queue <batch>.
<<Waiting for dispatch ...>>
<<Starting on batch2>>

$ hostname
batch2

$ jsrun -n1 hostname
a35n03",4.329477515420812
"Can a product be approved for use in the OLCF environment if it does not meet the requirements for providing modules and a statement of support?
","Products must provide a statement of support, to be displayed via the module system and in other appropriate contexts/locations.

The statement should clearly indicate that the product is not supported or maintained by the OLCF, but is supported by the UMS project applicant and/or the UMS project team.

The statement should clearly indicate the organization that is providing support and maintenance, and clearly indicate the preferred method(s) of reporting issues or requesting support.

Product modules will be grouped under project-level modules.",4.527306447625648
"Can a product be approved for use in the OLCF environment if it does not meet the requirements for providing modules and a statement of support?
","Order is for convenience and no implication of priority is implied.

Products installed should be limited to those explicitly listed in the project application and approved by the OLCF.

The project application is reviewed by the Export Control Office. If you would like to install additional packages not listed in your original application, the Project PI must contact the OLCF at help@olcf.ornl.gov before making changes.

Products must provide appropriate modules for their software.",4.396713883169357
"Can a product be approved for use in the OLCF environment if it does not meet the requirements for providing modules and a statement of support?
","Users will be advised to do a module load <UMS project>, which will expose modules for the individual products associated with that project, accessed by a second module load <product>.

Project PI must ensure that support is provided for users of the product, as documented in the statement of support.

Project PI must ensure that the product is updated in response to changes in the system software environment (e.g. updates to OS, compiler, library, or other key tools) and in a timely fashion.  If this commitment is not met, the OLCF may remove the software from UMS.",4.329412159704659
"How can I build the square.hipref.cpp file using the hipcc compiler?
","Last Updated: 07 February 2023

Using the hip-cuda/5.1.0 module on Summit, applications cannot build using a CMakeLists.txt that requires HIP language support or references the hip::host and hip::device identifiers. There is no known workaround for this issue. Applications wishing to compile HIP code with CMake need to avoid using HIP language support or hip::host and hip::device identifiers.",4.160615819202144
"How can I build the square.hipref.cpp file using the hipcc compiler?
","export CXX='hipcc'
export CXXFLAGS=""$(pat_opts include hipcc) \
  $(pat_opts pre_compile hipcc) -g -O3 -std=c++17 -Wall \
  --offload-arch=gfx90a -I${CRAY_MPICH_DIR}/include \
  $(pat_opts post_compile hipcc)""
export LD='hipcc'
export LDFLAGS=""$(pat_opts pre_link hipcc) ${CXXFLAGS} \
  -L${CRAY_MPICH_DIR}/lib ${PE_MPICH_GTL_DIR_amd_gfx908}""
export LIBS=""-lmpi ${PE_MPICH_GTL_LIBS_amd_gfx908} \
  $(pat_opts post_link hipcc)""

make clean
make

pat_build -g hip,io,mpi -w -f <executable>",4.116258966003248
"How can I build the square.hipref.cpp file using the hipcc compiler?
","export CXX='hipcc'
export CXXFLAGS=""$(pat_opts include hipcc) \
  $(pat_opts pre_compile hipcc) -g -O3 -std=c++17 -Wall \
  --offload-arch=gfx90a -I${CRAY_MPICH_DIR}/include \
  $(pat_opts post_compile hipcc)""
export LD='hipcc'
export LDFLAGS=""$(pat_opts pre_link hipcc) ${CXXFLAGS} \
  -L${CRAY_MPICH_DIR}/lib ${PE_MPICH_GTL_DIR_amd_gfx908}""
export LIBS=""-lmpi ${PE_MPICH_GTL_LIBS_amd_gfx908} \
  $(pat_opts post_link hipcc)""

make clean
make

pat_build -g hip,io,mpi -w -f <executable>",4.116258966003248
"How do I submit the FireWorks demo as a batch job to LSF from a Summit node?
","module load workflows
module load fireworks/2.0.2

# Edit the following line to match your own MongoDB connection string.
export MONGODB_URI=""mongodb://admin:password@apps.marble.ccs.ornl.gov:32767/test""

jsrun -n 1 python3 demo.py

Finally, submit the batch job to LSF by executing the following command from a Summit login node:

$ bsub fireworks_demo.lsf

Congratulations! Once the batch job completes, you will find new directories beginning with launcher_ and containing FW.json files that detail exactly what happened.",4.5275395500128415
"How do I submit the FireWorks demo as a batch job to LSF from a Summit node?
","Finally, create an LSF batch script called mlflow_demo.lsf, and change abc123 to match your own project identifier:

#BSUB -P abc123
#BSUB -W 10
#BSUB -nnodes 1

#BSUB -J mlflow_demo
#BSUB -o mlflow_demo.o%J
#BSUB -e mlflow_demo.e%J

module load git
module load workflows
module load mlflow/1.22.0

jsrun -n 1 mlflow run ./mlflow-example --no-conda

Finally, submit the batch job to LSF by executing the following command from a Summit login node:

$ bsub mlflow_demo.lsf",4.421967710539327
"How do I submit the FireWorks demo as a batch job to LSF from a Summit node?
","Run the following command to verify that FireWorks is available:

$ rlaunch -v
rlaunch v2.0.2

To run this FireWorks demo on Summit, you will create a Python file and then submit it as a batch job to LSF from a Summit node.

The contents for demo.py follow:

import os

from fireworks import Firework, Workflow, LaunchPad, ScriptTask
from fireworks.core.rocket_launcher import rapidfire

# Set up and reset the LaunchPad using MongoDB URI string.
launchpad = LaunchPad(host = os.getenv(""MONGODB_URI""), uri_mode = True)
launchpad.reset('', require_password=False)",4.39379844557588
"How can I verify that the GitLab runner was properly deployed on Slate?
","To deploy a GitLab Runner from the Developer Perspective, navigate to ""Topology"" -> ""Helm Chart"" and select ""GitLab Runner vX.X.X Provided by Slate Helm Charts"". From the new window, select ""Install Helm Chart"".

On the resulting screen, one can customize the installation of the GitLab Runner helm chart. The chart has been forked from upstream to allow for customized deployment to Slate. From the Form View, expand the ""GitLab Runner"" fields. Using the registration token retrieved in the prior section, enter token for adding the runner to the GitLab server in the empty field.",4.351789431496935
"How can I verify that the GitLab runner was properly deployed on Slate?
","Jenkins

OpenShift Pipelines

GitLab Runners

Deploying a GitLab Runner into a Slate project may be found in the https://docs.olcf.ornl.gov/systems/overview.html#slate_gitlab_runners document which leverages a localized Slate Helm Chart. The GitLab Pipelines documentation as well as GitLab CI/CD Examples provide more details on GitLab Runner capabilities and usage.

Jenkins

For Jenkins deployment, a Slate Helm Chart is planned to be released. Documentation concerning Jenkins Pipelines as well as Jenkins Pipeline Tutorials are available.

OpenShift Pipelines",4.32234921322385
"How can I verify that the GitLab runner was properly deployed on Slate?
","GitLab CI/CD jobs may be accomplished using the GitLab Runner application. An open-source application written in Go, a GitLab Runner may be installed with a variety of executors. This documentation focuses on the installation and configuration for use of the GitLab Runner Kubernetes Executor on Slate.

https://docs.gitlab.com/runner/

https://docs.gitlab.com/runner/executors/kubernetes.html

https://docs.gitlab.com/runner/install/kubernetes.html",4.273967775160026
"What is the purpose of enabling Low-Noise Mode on Crusher?
","Frontier uses low-noise mode and core specialization (-S flag at job allocation, e.g., sbatch).  Low-noise mode constrains all system processes to core 0.  Core specialization (by default, -S 8) reserves the first core in each L3 region.  This prevents the user running on the core that system processes are constrained to.  This also means that there are only 56 allocatable cores by default instead of 64. Therefore, this modifies the simplified node layout to:

Simplified Frontier node architecture diagram (low-noise mode)",4.063026220256232
"What is the purpose of enabling Low-Noise Mode on Crusher?
","The Alpine GPFS file system remains available but will be permanently unmounted from Crusher on Tuesday, April 18, 2023. Please begin moving your data to the Orion file system as soon as possible.

On Thursday, December 29, 2022 the following system configuration settings will be updated on Crusher:

Low-Noise Mode will be enabled: as a result, system processes will be constrained to core 0 on every node.",4.053802545626362
"What is the purpose of enabling Low-Noise Mode on Crusher?
","Crusher users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",3.9658695963617103
"Who reviews and approves Director's Discretion (DD) projects?
","DD – Director’s Discretion (DD) projects are dedicated to leadership computing preparation, INCITE and ALCC scaling, and application performance to maximize scientific application efficiency and productivity on leadership computing platforms. The OLCF Resource Utilization Council, as well as independent referees, review and approve all DD requests. Applications are accepted year-round via the OLCF Director's Discretion Project Application. Select ""OLCF Director's Discretionary Project"" from the drop down menu to begin.",4.390959088835891
"Who reviews and approves Director's Discretion (DD) projects?
","Request a Director's Discretionary (DD) Project Use this form to request a Director's Discretionary (DD) Project. Select ""OLCF Director's Discretionary Program"" from the drop down menu.

Principal Investigator Agreement The Oak Ridge Leadership Computing Facility must have a signed copy of this form on file from the project's principal investigator(s) before any accounts for the project will be processed.",4.302234798857149
"Who reviews and approves Director's Discretion (DD) projects?
","Access to OLCF resources is limited to approved projects and their users. The type of project (INCITE, ALCC, or Director's Discretion) will determine the application and review procedures. *Quarterly reports are required from industrial Director's Discretion projects only.",4.185453647048724
"What is the relationship between cray-mpich and ROCm in Crusher?
",The compatibility table below was determined by linker testing with all current combinations of cray-mpich and rocm modules on Crusher.,4.447197215080124
"What is the relationship between cray-mpich and ROCm in Crusher?
","| Implementation | Module | Compiler | Header Files & Linking | | --- | --- | --- | --- | | Cray MPICH | cray-mpich | cc, CC, ftn (Cray compiler wrappers) | MPI header files and linking is built into the Cray compiler wrappers | | hipcc |  |

To use GPU-aware Cray MPICH, users must set the following modules and environment variables:

module load craype-accel-amd-gfx90a
module load rocm

export MPICH_GPU_SUPPORT_ENABLED=1

There are extra steps needed to enable GPU-aware MPI on Crusher, which depend on the compiler that is used (see 1. and 2. below).",4.261538504011318
"What is the relationship between cray-mpich and ROCm in Crusher?
","The system was upgraded to Cray OS 2.5, Slingshot Host Software 2.0.2-112, and the AMD GPU 6.0.5 device driver (ROCm 5.5.1 release).

ROCm 5.5.1 is now available via the rocm/5.5.1 modulefile.

HPE/Cray Programming Environments (PE) 23.05 is now available via the cpe/23.05 modulefile.

HPE/Cray PE 23.05 introduces support for ROCm 5.5.1. However, due to issues identified during testing, ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.

On Wednesday, April 5, 2023, the Crusher TDS was upgraded to a new software stack.  A summary of the changes is included below.",4.24557806773075
"How do I request an allocation of 2 compute nodes for my job in Frontier?
","The requesting project’s allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Frontier would be equivalent to using 16,000 Frontier node-hours of a project’s allocation.

Reservations should not be confused with priority requests. If quick turnaround is needed for a few jobs or for a period of time, a priority boost should be requested. A reservation should only be requested if users need to guarantee availability of a set of nodes at a given time, such as for a live demonstration at a conference.",4.374772271258055
"How do I request an allocation of 2 compute nodes for my job in Frontier?
","Due to the unique architecture of Frontier compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.

Before jumping into the examples, it is helpful to understand the output from the hello_jobstep program:",4.276613477628677
"How do I request an allocation of 2 compute nodes for my job in Frontier?
","$ salloc -A <project_id> -J <job_name> -t 00:05:00 -p <partition> -N 2
salloc: Granted job allocation 4258
salloc: Waiting for resource configuration
salloc: Nodes crusher[010-011] are ready for job

$ srun -n 4 --ntasks-per-node=2 ./a.out
<output printed to terminal>

$ srun -n 2 --ntasks-per-node=1 ./a.out
<output printed to terminal>

Here, salloc is used to request an allocation of 2 compute nodes for 5 minutes. Once the resources become available, the user is granted access to the compute nodes (crusher010 and crusher011 in this case) and can launch job steps on them using srun.",4.239951919182092
"How can I add a Persistent Volume Claim (PVC) to a pod using the web GUI?
","To add the PVC to a pod using the web GUI first select Workloads and then Deployments in the hamburger menu on the left had side.

Application Deployments

Next, select the deployment that contains the pod you wish to add the storage to.

Select Actions in the upper left and then and then Add Storage.

Edit YAML

Fill out your Mount point and other options if you need them to be non-default values. Otherwise, hit the Add button at the bottom.

Add Storage Menu",4.545229136904701
"How can I add a Persistent Volume Claim (PVC) to a pod using the web GUI?
","From the main screen of a project go to the Storage option in the hamburger menu on the left hand side of the screen, then click Persistent Volume Claims

Persistent Volume Claim Menu

In the upper right click the Create Persistent Volume Claim button.

Create Storage

This will take you to a screen that allows you to select what you need for your PVC. Give your claim a name and request an amount of storage and click Create. You can also click Edit YAML to edit the PVC object directly.

PV Settings",4.509183902772579
"How can I add a Persistent Volume Claim (PVC) to a pod using the web GUI?
","PV Settings

This will redirect you to the status page of your new PVC. You should see Bound in the Status field.

PV Status

Once the claim has been granted to the project a pod within that project can access that storage. To do this, edit the YAML of the pod and under the volume portion of the file add a name and the name of the claim. It will look similar to the following.

volumes:
  name: pvol
  persistentVolumeClaim:
    claimName: storage-1

Then all that is left to do is mount the storage into a container:

volumeMounts:
  - mountPath: /tmp/
    name: pvol",4.455700156038608
"Can I use a configuration transformation to modify the resulting resources in Slate?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",4.060282222577563
"Can I use a configuration transformation to modify the resulting resources in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.045180937296513
"Can I use a configuration transformation to modify the resulting resources in Slate?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",4.023703308277958
"Can I use a newer version of libffi than 3.2.1 for quantum software development on Frontier?
","Newer: https://github.com/libffi/libffi/releases/

ZMQ download: https://github.com/zeromq/libzmq/releases

Forest SDK download: https://qcs.rigetti.com/sdk-downloads

Below are example instructions for installing the above packages into your $HOME directory. Versions may vary.

Newer versions than those used in the install instructions below are known to work on Andes; however, on Frontier, newer versions of libffi than 3.2.1 are known to cause problems.

Andes

.. code-block:: bash

    $ module load gcc cmake

Frontier

.. code-block:: bash",4.362871646049344
"Can I use a newer version of libffi than 3.2.1 for quantum software development on Frontier?
","https://docs.rigetti.com/qcs/getting-started/installing-locally

The bare-bones installation only contains the executable binaries and manual pages, and doesn’t contain any of the requisite dynamic libraries. As such, installation doesn’t require administrative or sudo privileges. This method of installation requires one, through whatever means, to install shared libraries for BLAS, LAPACK, libffi, and libzmq3. Some download methods are listed here:

Lapack (with BLAS) download: http://www.netlib.org/lapack/

libffi download:

Older versions: https://sourceware.org/ftp/libffi/",4.161200338177619
"Can I use a newer version of libffi than 3.2.1 for quantum software development on Frontier?
","Currently, the install steps listed below only work for our x86_64 based systems (Andes, Frontier, etc.). The steps can be explored on Summit, but -- due to Summit's Power architecture -- is not recommended or guaranteed to work.

Both Qiskit and pyQuil can live in the same Python environment if desired. However, as this may not always be the case, it is highly recommened to use separate environments if possible or test if packages still function after modifying your environment.",4.085536035154679
"How do I ensure that my patch matches an object in Slate?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",4.000056605617118
"How do I ensure that my patch matches an object in Slate?
","when compared to the output from the build command ran against the base directory.

The apiVersion/kind/metadata.name must match exactly the object to modify. If the patch does not match an object, an error similar to: Error: no matches for Id ~G_v1_ConfigMap|~X|themap; failed to find unique target for patch ~G_v1_ConfigMap|themap will be generated will instead pointing to the problem. In the case, the metadata.name field was themap instead of the-map.",3.985737206157784
"How do I ensure that my patch matches an object in Slate?
","This section will describe the project allocation process for Slate. This is applicable to both the Onyx and Marble OLCF OpenShift clusters.

In addition, we will go over the process to log into Onyx and Marble, and how namespaces work within the OpenShift context.

Please fill out the Slate Request Form in order to use Slate resources. This must be done in order to proceed.

If you have any question please contact User Assistance, via a Help Ticket Submission or by emailing help@olcf.ornl.gov.

Required information:

- Existing OLCF Project ID

- Project PI",3.913231373701564
"How do I optimize the performance of Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi-processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations.

For detailed information about using Score-P on Summit and the builds available, please see the Score-P Software Page.",4.324597399704054
"How do I optimize the performance of Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations. It works in combination with Periscope, Scalasca, Vampir, and Tau.

Steps in a typical Score-P workflow to run on https://docs.olcf.ornl.gov/systems/Scorep.html#summit-user-guide:",4.320821106285239
"How do I optimize the performance of Score-P?
","To instrument your code, you need to compile the code using the Score-P instrumentation command (scorep), which is added as a prefix to your compile statement. In most cases the Score-P instrumentor is able to automatically detect the programming paradigm from the set of compile and link options given to the compiler. Some cases will, however, require some additional link options within the compile statement e.g. CUDA instrumentation.

Below are some basic examples of the different instrumentation scenarios:",4.263667820669243
"What type of GPUs are available on compute nodes?
","We are targeting use cases that need GPUs for long running services. For batch access to GPUs we recommend using the standard HPC clusters in NCCS

The Slate Marble cluster has nodes with three NVIDIA Tesla V100 GPUs per node available for scheduling so a single pod could request from 1 to 3 GPUs",4.34967855799662
"What type of GPUs are available on compute nodes?
","Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.",4.248193280301643
"What type of GPUs are available on compute nodes?
","Login nodes have (2) 16-core Power9 CPUs and (4) V100 GPUs. Compute nodes have (2) 22-core Power9 CPUs and (6) V100 GPUs.

Summit nodes are connected to a dual-rail EDR InfiniBand network providing a node injection bandwidth of 23 GB/s. Nodes are interconnected in a Non-blocking Fat Tree topology. This interconnect is a three-level tree implemented by a switch to connect nodes within each cabinet (first level) along with Director switches (second and third level) that connect cabinets together.",4.235547241519649
"What is the advantage of using a pre-loaded conda environment for deep learning on Summit?
","This guide introduces a user to the basic workflow of using conda environments, as well as providing an example of how to create a conda environment that uses a different version of Python than the base environment uses on Summit. Although Summit is being used in this guide, all of the concepts still apply to other OLCF systems. If using Frontier, this assumes you already have installed your own version of conda (e.g., by https://docs.olcf.ornl.gov/software/python/miniconda.html).

OLCF Systems this guide applies to:

Summit

Andes

Frontier (if using conda)",4.305015544566154
"What is the advantage of using a pre-loaded conda environment for deep learning on Summit?
","IBM Watson Machine Learning Community Edition (ibm-wml-ce) has been replaced by Open-CE. The Open-CE environment is provided on Summit through the module open-ce, which is built based on the Open Cognitive Environment. Open-CE is a Python Anaconda environment that is pre-loaded with many popular machine learning frameworks and tuned to Summit's Power9+NVIDIA Volta hardware.

To access the latest analytics packages use the module load command:

module load open-ce",4.215071245622268
"What is the advantage of using a pre-loaded conda environment for deep learning on Summit?
","Summit

Andes

Frontier (if using conda)

First, load the python module and the gnu compiler module on Summit (most Python packages assume use of GCC)

$ module load gcc
$ module load python

The above module load python does not apply to Frontier (since you will be using a personal Miniconda instead).

This puts you in the ""base"" conda environment, which is the default Python environment after loading the module. To see a list of environments, use the command conda env list:

$ conda env list

# conda environments:
#
base                  *  /sw/summit/python/3.8/anaconda3/2020.07-rhel8",4.202714866073018
"How do I get a list of nodes in my Spark cluster?
","Spark also provides a web UI to monitor cluster, and you can access it on your local machine by port forwarding the master node to local machine.

For example, if master node is running on andes338, you can run the following code on your local machine terminal.

ssh -N <USERNAME>@andes-login1.olcf.ornl.gov -L 8080:andes338.olcf.ornl.gov:8080

Then access the Spark dashboard using address http://localhost:8080/ on a web browser on your local machine.

The spark documentation is very useful tool, go through it to find the Spark capabilities.",4.246695865155793
"How do I get a list of nodes in my Spark cluster?
","To setup a Spark cluster, we use the following Slurm script to request compute nodes. The slurm script requests four nodes and spawns a Spark cluster having a master node and three worker nodes. Number of worker nodes can be increased or decreased by changing the value of -N option in the Slurm script.

#!/bin/bash
#SBATCH --mem=0
#SBATCH -A <ABC1234>
#SBATCH -t 1:00:00
#SBATCH -N 4
#SBATCH -J spark_test
#SBATCH -o o.spark_test
#SBATCH -e e.spark_test

module load spark
module load python",4.044764917411078
"How do I get a list of nodes in my Spark cluster?
","module load spark
module load python

nodes=($(scontrol show hostnames ${SLURM_JOB_NODELIST} | sort | uniq ))
numnodes=${#nodes[@]}
last=$(( $numnodes - 1 ))
export SCRATCH=<SCRATCH_DIRECTORY>
export SPARK_HOME=<PATH/WHERE/SPARK/DIRECTORY/IS>/spark
master=${nodes[0]}
masterurl=""spark://${master}.olcf.ornl.gov:7077""
ssh ${nodes[0]} ""cd ${SPARK_HOME}; source /etc/profile ; module load spark; ./sbin/start-master.sh""
for i in $( seq 1 $last )
do
    ssh ${nodes[$i]} ""cd ${SPARK_HOME}; source /etc/profile ; module load spark; ./sbin/start-worker.sh ${masterurl}""
done",4.024991988429936
"Can I run a parallel job on pbdR?
","For parallelism, you should use pbdR packages, Rmpi directly, or an interface which can use Rmpi as a backend. We address GPUs specifically next.

There are some R packages which can use GPUs, such as xgboost. There is also the gpuR series of packages. Several pbdR packages support GPU computing. It is also possible to offload some linear algebra computations (specifically matrix-matrix products, and methods which are computationally dominated by them) to the GPU using NVIDIA’s NVBLAS.",4.1916203187704895
"Can I run a parallel job on pbdR?
","If you want to do GPU computing on Summit with R, we would love to collaborate with you (see contact details at the bottom of this document).

For more information about using R and pbdR effectively in an HPC environment such as Summit, please see the R and pbdR articles. These are long-form articles that introduce HPC concepts like MPI programming in much more detail than we do here.

Also, if you want to use R and/or pbdR on Summit, please feel free to contact us directly:

Mike Matheson - mathesonma AT ornl DOT gov

George Ostrouchov - ostrouchovg AT ornl DOT gov",4.103827638618044
"Can I run a parallel job on pbdR?
","Once resources have been allocated through the batch system, users have the option of running commands on the allocated resources' primary compute node (a serial job) and/or running an MPI/OpenMP executable across all the resources in the allocated resource pool simultaneously (a parallel job).

The executable portion of batch scripts is interpreted by the shell specified on the first line of the script. If a shell is not specified, the submitting user’s default shell will be used.",4.0605115107466
"How do I set the number of threads using OpenMP on Summit?
","summit> setenv OMP_NUM_THREADS 4
summit> jsrun -n12 -a1 -c4 -g1 -b packed:4 -d packed ./a.out
Rank: 0; RankCore: 0; Thread: 0; ThreadCore: 0; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}
Rank: 0; RankCore: 0; Thread: 1; ThreadCore: 4; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}
Rank: 0; RankCore: 0; Thread: 2; ThreadCore: 8; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}
Rank: 0; RankCore: 0; Thread: 3; ThreadCore: 12; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}",4.397766930314482
"How do I set the number of threads using OpenMP on Summit?
","In addition to specifying the SMT level, you can also control the number of threads per MPI task by exporting the OMP_NUM_THREADS environment variable. If you don't export it yourself, Jsrun will automatically set the number of threads based on the number of cores requested (-c) and the binding (-b) option. It is better to be explicit and set the OMP_NUM_THREADS value yourself rather than relying on Jsrun constructing it for you. Especially when you are using Job Step Viewer which relies on the presence of that environment variable to give you visual thread assignment information.",4.339830819071505
"How do I set the number of threads using OpenMP on Summit?
","In this example, the intent is to launch 2 MPI ranks, each of which spawn 2 OpenMP threads, and have all of the 4 OpenMP threads run on different physical CPU cores.

First (INCORRECT) attempt

To set the number of OpenMP threads spawned per MPI rank, the OMP_NUM_THREADS environment variable can be used. To set the number of MPI ranks launched, the srun flag -n can be used.

$ export OMP_NUM_THREADS=2
$ srun -N1 -n2 ./hello_mpi_omp | sort",4.270614016811092
"How do I submit the LSF batch script to the queue?
","| Task | LSF (Summit) | Slurm | | --- | --- | --- | | View batch queue | jobstat | squeue | | Submit batch script | bsub | sbatch | | Submit interactive batch job | bsub -Is $SHELL | salloc | | Run parallel code within batch job | jsrun | srun |

Writing Batch Scripts

Batch scripts, or job submission scripts, are the mechanism by which a user configures and submits a job for execution. A batch script is simply a shell script that also includes commands to be interpreted by the batch scheduling software (e.g. Slurm).",4.3908872956741805
"How do I submit the LSF batch script to the queue?
","Finally, create an LSF batch script called mlflow_demo.lsf, and change abc123 to match your own project identifier:

#BSUB -P abc123
#BSUB -W 10
#BSUB -nnodes 1

#BSUB -J mlflow_demo
#BSUB -o mlflow_demo.o%J
#BSUB -e mlflow_demo.e%J

module load git
module load workflows
module load mlflow/1.22.0

jsrun -n 1 mlflow run ./mlflow-example --no-conda

Finally, submit the batch job to LSF by executing the following command from a Summit login node:

$ bsub mlflow_demo.lsf",4.3637184330299
"How do I submit the LSF batch script to the queue?
","Sometimes it’s necessary to interact with a batch job after it has been submitted. LSF provides several commands for interacting with already-submitted jobs.

Many of these commands can operate on either one job or a group of jobs. In general, they only operate on the most recently submitted job that matches other criteria provided unless “0” is specified as the job id.",4.357243857728266
"How can I view the details of the Deployment object?
","As with other objects in OpenShift, you can create this Deployment with oc create -f {FILE_NAME}.

Once created, you can check the status of the ongoing deployment process with the command

oc rollout status deploy/{NAME}

To get basic information about the available revisions, if you had done any updates to the deployment since, run

oc rollout history deploy/{NAME}

You can view a specific revision with

oc rollout history deploy/{NAME} --revision=1

For more detailed information about a Deployment, use

oc describe deploy {NAME}

To roll back a deployment, run",4.194076508855978
"How can I view the details of the Deployment object?
","To roll back a deployment, run

oc rollout undo deploy/{NAME}

When using the web interface, you can view and edit a Deployment, from the sidebar, go to Applications, then Deployments.

Deployment Menu

You can get info on any deployment by clicking on it.

To edit this configuration, click Actions in the upper right hand corner, then Edit on whatever you wish to edit, or Edit Deployment if you'd rather edit the YAML directly.

Edit Deployment Config",4.175853335033911
"How can I view the details of the Deployment object?
","You should now be on the 'Pod' screen with the 'Overview' tab selected From here you can get a quick idea of the amount of resources (memory, CPU etc) that your  Pod is using.

Click on the 'Logs' tab to get the logs from your pod. This will display ""Hello World!"" in our example because of our echo command. There will be a dropdown here that for our example will contain only one item named 'hello-openshift'. This is the name of the container that you are viewing the logs for inside your pod.",4.116440426421301
"How do I run a batch job on Frontier?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.528628331064788
"How do I run a batch job on Frontier?
","Unlike Summit and Titan, there are no launch/batch nodes on Frontier. This means your batch script runs on a node allocated to you rather than a shared node. You still must use the job launcher (srun) to run parallel jobs across all of your nodes, but serial tasks need not be launched with srun.



To easily visualize job examples (see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-mapping further below), the compute node diagram has been simplified to the picture shown below.

Simplified Frontier node architecture diagram",4.283299090949091
"How do I run a batch job on Frontier?
","Most users will find batch jobs an easy way to use the system, as they allow you to ""hand off"" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.",4.249316543687992
"What should I do if I encounter issues with signing in to my quantum account?
","After submitting the OLCF quantum account application and receiving approval, proceed to https://quantum-computing.ibm.com/ and click on ""Create an IBMid account"". Your IBM Quantum Hub account email will be the email associated with your OLCF account. If sign-in fails, contact help@olcf.ornl.gov. Once logged in, users will have access to the IBM Quantum Hub, IBM’s online platform for QPU access, forums for quantum computing discussion, etc. From the IBM Quantum Hub Dashboard, users can manage system reservations, view system (backend) statuses, and view the results of your past jobs. More",4.229614170185341
"What should I do if I encounter issues with signing in to my quantum account?
","with the project will need to apply for a https://docs.olcf.ornl.gov/systems/quantum_access.html#User Account <quantum-user>. After your user account is approved, you can then move on to accessing the quantum resources offered by our vendors (see https://docs.olcf.ornl.gov/systems/quantum_access.html#quantum-vendors).",4.189982703834495
"What should I do if I encounter issues with signing in to my quantum account?
","After submitting the OLCF quantum account application and receiving approval, you will receive an email from support@rigetti.com inviting you to create your quantum account. If you did not receive this, proceed to https://qcs.rigetti.com/sign-in and click “Sign In”. It is necessary that the email you use for sign in be associated with an affiliated subscribing institution, i.e. ORNL, ANL, etc. If sign in fails, contact help@olcf.ornl.gov. Once logged in, users will have access to Quantum Cloud Services (QCS), Rigetti’s online platform for accessing the hybrid infrastructure of available",4.189138446573077
"What is the path to the project home directory for a project with the ID ""myproject""?
","Open and Moderate Projects are provided with a Project Home storage area in the NFS-mounted filesystem. This area is intended for storage of data, code, and other files that are of interest to all members of a project. Since Project Home is an NFS-mounted filesystem, its performance will not be as high as other filesystems.

Moderate Enhanced projects are not provided with Project Home spaces, just Project Work spaces.

Project Home area is accessible at /ccs/proj/abc123 (where abc123 is your project ID).",4.2018436569040105
"What is the path to the project home directory for a project with the ID ""myproject""?
","To check your project’s current usage, run df -h /ccs/proj/abc123 (where abc123 is your project ID). Quotas are enforced on project home directories. The current limit is shown in the table above.

The default permissions for project home directories are 0770 (full access to the user and group). The directory is owned by root and the group includes the project’s group members. All members of a project should also be members of that group-specific project. For example, all members of project “ABC123” should be members of the “abc123” UNIX group.",4.191120938321748
"What is the path to the project home directory for a project with the ID ""myproject""?
","Upon logging into Ascent, users will be placed in their own personal home (NFS) directory, /ccsopen/home/[userid], which can only be accessed by that user. Users also have access to an NFS project directory, /ccsopen/proj/[projid], which is visible to all members of a project. Both of these NFS directories are commonly used to store source code and build applications.

Users also have access to a (GPFS) parallel file system, called wolf, which is where data should be written when running on Ascent's compute nodes. Under /gpfs/wolf/[projid], there are 3 directories:",4.124507406085565
"How do subprojects differ from individual user accounts in OLCF?
","primary project users must be associated with a subproject(s). If you have any questions, or would like to request a subproject, please contact the OLCF Accounts Team at accounts@ccs.ornl.gov.",4.480764010943702
"How do subprojects differ from individual user accounts in OLCF?
","Collaborators involved with an approved and activated OLCF project can apply for a user account associated with it. There are several steps in receiving a user account, and we're here to help you through them.

Project PIs do not receive a user account with project creation, and must also apply.",4.3841663770873724
"How do subprojects differ from individual user accounts in OLCF?
","If you already have a user account at the OLCF, your existing credentials can be leveraged across multiple projects.

If your user account has an associated RSA SecurID (i.e. you have an ""OLCF Moderate"" account), you gain access to another project by logging in to the myOLCF self-service portal and filling out the application under My Account > Join Another Project. For more information, see the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#myOLCF self-service portal documentation<myolcf-overview>.",4.361621206395776
"Can pmake create a target that depends on multiple sources?
","<string>:5: (INFO/1) Duplicate implicit target name: ""pmake"".

pmake is a parallel make developed for use within batch jobs.  A rules.yaml file specifies extended make-rules with:

multiple input and multiple output files

a resource-set specification

a multi-line shell script that can use variable substitution (e.g. {mpirun} expands to {jsrun -g -c ...} on summit).

Full documentation and examples are available in https://code.ornl.gov/99R/pmake.",4.208315478022
"Can pmake create a target that depends on multiple sources?
","Inside the simulation directory, you should see 3 new files, simulate.sh, which contains the shell script pmake built from the simulate rule, simulate.log, containing the log output from running simulate.sh, and run.log, the file written during rule execution.

Extending pmake using your own rules is straightforward. pmake acts like make, running rules to create output files (that do not yet exist) from input files (that must exist before the rule is run).

Unlike make, pmake does not run a rule unless its output is requested by some target.",4.04271737618099
"Can pmake create a target that depends on multiple sources?
","pmake is a standard python package.  It is recommended to install it in a virtual environment.  One easy way to create a virtual environment is to load an available python module, and then put a new environment into /ccs/proj/<projid>/<systemname>.  This way, the project can share environments, and each system gets its own install location.

$ module load python/3.8-anaconda3
$ python -m venv /path/to/new-venv
$ source /path/to/new-venv/bin/activate

On subsequent logins, remember to load the same python module and run the source /path/to/new-venv/bin/activate command again.",4.0223471669009045
"What is the recommended way to map GPUs to MPI ranks?
","While this level of control over mapping MPI ranks to GPUs might be useful for some applications, it is always important to consider the implication of the mapping. For example, if the order of the GPU IDs in the map_gpu option is reversed, the MPI ranks and the GPUs they are mapped to would be in different L3 cache regions, which could potentially lead to poorer performance.

Example 4: 16 MPI ranks - each with 2 OpenMP threads and 1 *specific* GPU (multi-node)",4.533695363066613
"What is the recommended way to map GPUs to MPI ranks?
","In general, GPU mapping can be accomplished in different ways. For example, an application might map GPUs to MPI ranks programmatically within the code using, say, hipSetDevice. In this case, there might not be a need to map GPUs using Slurm (since it can be done in the code itself). However, many applications expect only 1 GPU to be available to each rank. It is this latter case that the following examples refer to.",4.505432117782182
"What is the recommended way to map GPUs to MPI ranks?
","As mentioned previously, all GPUs are accessible by all MPI ranks by default, so it is possible to programatically map any combination of MPI ranks to GPUs. However, there is currently no way to use Slurm to map multiple GPUs to a single MPI rank. If this functionality is needed for an application, please submit a ticket by emailing help@olcf.ornl.gov.

There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_jobstep program and test whether or not processes and threads are running where intended.",4.5008979451462485
"Can I use resource sets with other resource management tools on Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.1978189323651725
"Can I use resource sets with other resource management tools on Summit?
",For Summit:,4.147222188999422
"Can I use resource sets with other resource management tools on Summit?
","This section covers differences between SPI and non-SPI workflows, but the existing resource user guides cover the majority of system use methods.  Please use the https://docs.olcf.ornl.gov/systems/index.html#Summit User Guide<summit-documentation-resources> and https://docs.olcf.ornl.gov/systems/index.html#Frontier User Guide<frontier-user-guide> for resource use details.



To help separate data and processes, the Citadel framework provides separate login nodes to reach Summit and Frontier's compute resources:",4.146741059282094
"What is the name of the function that has the highest inclusive time?
","NODE 0;CONTEXT 0;THREAD 0:
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call
---------------------------------------------------------------------------------------
100.0        0.038     1:10.733           1           1   70733442 .TAU application
100.0            9     1:10.733           1        4654   70733404 int main(int, char **)",3.9689865579676864
"What is the name of the function that has the highest inclusive time?
","Explanation:

One process was running as it is a serial application, even MPI calls are executed from a single thread.

The total execution time is 70.733 seconds and only 9 msec are exclusive for the main routine. The rest are caused by subroutines.

The exclusive time is the time caused by the mentioned routine, and the inclusive is with the execution time from the subroutines.

The #Subrs is the number of the called subroutines.

There is also information about the parallel I/O if any exists, the bytes, and the bandwidth.",3.947568357636558
"What is the name of the function that has the highest inclusive time?
","Click on the new metric, ""PAPI_TOT_INS / PAPI_TOT_CYC"" to see the instructions per cycle (IPC) across the various routines.



Click on the label mean:



For the non-MPI routines/calls, an IPC that is lower than 1.5 means that there is a potential for performance improvement.

Menu Windows -> 3D Visualization (3D demands OpenGL) will not work on Summit, and you will need to download the data on your laptop and install TAU locally to use this feature.

You can see per MPI rank, per routine, the exclusive time and the floating operations.",3.900669010026914
"How does the restructuring of the matrix affect performance?
","matrix, just rearranged. Although you may not expect it, the restructuring results in a big performance boost as well.",4.562687775461415
"How does the restructuring of the matrix affect performance?
","precision results. One such place where this technique has been applied is in calculating an LU factorization of the linear system Ax = B. This operation is dominated by a matrix multiplication operation, which is illustrated in green in the image below. It is possible to perform the GEMM operations at a reduced precision, while leaving the panel and trailing matrices in a higher precision. This technique allows for the majority of the math operations to be done at the higher FP16 throughput. The matrix used in the GEMM is generally not square, which is often the best performing GEMM",4.059903367985893
"How does the restructuring of the matrix affect performance?
","Tensor Cores provide the potential for an enormous performance boost over full-precision operations, but when their use is appropriate is highly application and even problem independent. Iterative Refinement techniques can suffer from slow or possible a complete lack of convergence if the condition number of the matrix is very large. By using Tensor Cores, which support 32-bit accumulation, rather than strict 16-bit math operations, iterative refinement becomes a viable option in a much larger number of cases, so it should be attempted when an application is already using a supported solver.",4.032387942767215
"Can I use jslist to view the status of jobs launched using different batch submission commands?
","To view the status of multiple jobs launched sequentially or concurrently within a batch script, you can use jslist to see which are completed, running, or still queued. If you are using it outside of an interactive batch job, use the -c option to specify the CSM allocation ID number. The following example shows how to obtain the CSM allocation number for a non interactive job and then check its status.

$ bsub test.lsf
Job <26238> is submitted to default queue <batch>.

$ bjobs -l 26238 | grep CSM_ALLOCATION_ID
Sun Feb 16 19:01:18: CSM_ALLOCATION_ID=34435",4.426600744962316
"Can I use jslist to view the status of jobs launched using different batch submission commands?
","As pointed out in https://docs.olcf.ornl.gov/systems/summit_user_guide.html#login-launch-and-compute-nodes, you will be placed on a launch (a.k.a. ""batch"") node upon launching an interactive job and as usual need to use jsrun to access the compute node(s):

$ bsub -Is -W 0:10 -nnodes 1 -P STF007 $SHELL
Job <779469> is submitted to default queue <batch>.
<<Waiting for dispatch ...>>
<<Starting on batch2>>

$ hostname
batch2

$ jsrun -n1 hostname
a35n03",4.236141943936766
"Can I use jslist to view the status of jobs launched using different batch submission commands?
","summit>



Jsrun provides the ability to launch multiple jsrun job launches within a single batch job allocation. This can be done within a single node, or across multiple nodes.

By default, multiple invocations of jsrun in a job script will execute serially in order. In this configuration, jobs will launch one at a time and the next one will not start until the previous is complete. The batch node allocation is equal to the largest jsrun submitted, and the total walltime must be equal to or greater then the sum of all jsruns issued.",4.232802603616692
"How do I use kustomize to deploy my Kubernetes application to multiple clusters?
","directory of YAML or JSON files

kustomize applications

helm charts

This section will focus on the deployment of Kubernetes resources using kustomize. If the use of helm is preferred, refer to the Continuous Delivery with Helm and ArgoCD blog post as well as the App of Apps Pattern discussed on the ArgoCD Cluster Bootstrapping page.

References to ksonnet for deployment of Kubernetes resources may be mentioned in some documentation. However, the use if ksonnet is no longer supported by ArgoCD.

In order to deploy resources, one should have the following to start with:",4.326754781210321
"How do I use kustomize to deploy my Kubernetes application to multiple clusters?
","With a git repository defined in the ArgoCD Repositories settings that has kustomize environments ready for use, one can start using ArgoCD to deploy and manage kubernetes resources.",4.253935766398854
"How do I use kustomize to deploy my Kubernetes application to multiple clusters?
","base directory of YAML files that specify one or multiple kubernetes resources

kustomization.yaml file

one or more overlay directories

Unlike helm which is a template framework for deployment of kubernetes resources, kustomize is a patching framework. Once the base directory of YAML files is in place, kustomize patches those files to modify kubernetes resources for deployment with custom configurations for one or multiple environments such as dev, test, and prod.",4.232363445014481
"How do I ensure that a workload uses the correct shared filesystems in a cluster in Slate?
","OLCF shared filesystems can be mounted into a container running in Slate. The mountpoints will be the same as a cluster node. The Kubernetes object will need to be annotated in order to get the necessary configuration injected into the container at runtime.

| Cluster | Annotation | Value | Mounts | | --- | --- | --- | --- | | Marble | ccs.ornl.gov/fs | olcf | /ccs/sw, /ccs/home, /ccs/sys, /ccs/proj, /gpfs/alpine | | Onyx | ccs.ornl.gov/fs | ccsopen | /ccsopen/sw, /ccsopen/home, /ccsopen/proj, /gpfs/wolf |

If you already have a Deployment running you can add the annotation with the client",4.180081686247054
"How do I ensure that a workload uses the correct shared filesystems in a cluster in Slate?
","We hope this provides a starting point for more robust implementations of MinIO, if your workload/project benefits from that. In addition, it gives insight into some of the core building blocks for establishing your own, different, applications on Slate.

This example deployment of MinIO enables two possible configurations (which we configure in the following sections):

MinIO running on a NCCS filesystem (GPFS or NFS - exposing filesystem project space through the MinIO GUI).",4.173369950632761
"How do I ensure that a workload uses the correct shared filesystems in a cluster in Slate?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",4.130276919485344
"Can I join an existing QCUP project?
","To gain access, you must first submit a project proposal to the OLCF QCUP (see https://docs.olcf.ornl.gov/systems/quantum_access.html#quantum-proj) or join an existing QCUP project (see https://docs.olcf.ornl.gov/systems/quantum_access.html#quantum-user). The Quantum Resource Utilization Council (QRUC), as well as independent referees, review and approve all QCUP project proposals.  Applications to QCUP are accepted year-round via the project application form found below. Once a project is approved, then all of the users associated with the project will need to apply for a",4.381166147210131
"Can I join an existing QCUP project?
","A QCUP proposal describes the nature, methodology, and merits of the project, explains why it requires access to QCUP resources, and outlines any other essential information that might be needed for its consideration. Project applications are submitted using the Project Application Form. Select ""OLCF Quantum Computing User Program"" from the dropdown menu.

For QCUP Projects, all proposed work must be open, fundamental research and no Export Control, PHI, or other controlled data can be used.",4.369221115300407
"Can I join an existing QCUP project?
","The OLCF will then establish a QCUP project and notify the PI of its creation along with the 6-character OLCF QCUP Project ID and resources allocation details. At this time project participants may proceed with applying for their individual user accounts.

QCUP Projects have a finite duration; when starting, projects get however many months are left in that allocation period and then must be renewed for subsequent 6 month intervals. Projects can be renewed by filling out a renewal form (:download:`Accounts Renewal Form <Quantum-Renewal-Form.docx>`) and emailing it to accounts@ccs.ornl.gov.",4.314001493673009
"What are some differences between using OLCF resources for SPI and non-SPI processes?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads. Because of this, many processes used within the SPI are very similar to those used for standard non-SPI. This page lists the differences you may see when using OLCF resources to execute SPI resources.",4.738384267266255
"What are some differences between using OLCF resources for SPI and non-SPI processes?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.  This page lists the differences you may see when using OLCF resources to execute SPI workflows.  The page will also point to sections within the site where more information on standard non-SPI use can be found.",4.731006058950952
"What are some differences between using OLCF resources for SPI and non-SPI processes?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.

The SPI provides access to the OLCF's Summit resource for compute.  To safely separate SPI and non-SPI workflows, SPI workflows must use a separate login node named https://docs.olcf.ornl.gov/systems/citadel_user_guide.html#Citadel<spi-compute-citadel>.  Citadel provides a login node specifically for SPI workflows.",4.466160780719253
"Can the AMD MI250X GPU access memory allocated through standard system allocators like malloc with XNACK disabled?
","Disabling XNACK will not necessarily result in an application failure, as most types of memory can still be accessed by the AMD ""Optimized 3rd Gen EPYC"" CPU and AMD MI250X GPU. In most cases, however, the access will occur in a zero-copy fashion over the Infinity Fabric. The exception is memory allocated through standard system allocators such as malloc, which cannot be accessed directly from GPU kernels without previously being registered via a HIP runtime call such as hipHostRegister. Access to malloc'ed and unregistered memory from GPU kernels will result in fatal unhandled page faults.",4.548998667787853
"Can the AMD MI250X GPU access memory allocated through standard system allocators like malloc with XNACK disabled?
","Disabling XNACK will not necessarily result in an application failure, as most types of memory can still be accessed by the AMD ""Optimized 3rd Gen EPYC"" CPU and AMD MI250X GPU. In most cases, however, the access will occur in a zero-copy fashion over the Infinity Fabric. The exception is memory allocated through standard system allocators such as malloc, which cannot be accessed directly from GPU kernels without previously being registered via a HIP runtime call such as hipHostRegister. Access to malloc'ed and unregistered memory from GPU kernels will result in fatal unhandled page faults.",4.548998667787853
"Can the AMD MI250X GPU access memory allocated through standard system allocators like malloc with XNACK disabled?
","The accessibility of memory from GPU kernels and whether pages may migrate depends three factors: how the memory was allocated; the XNACK operating mode of the GPU; whether the kernel was compiled to support page migration. The latter two factors are intrinsically linked, as the MI250X GPU operating mode restricts the types of kernels which may run.",4.447873133581645
"How do I submit a job to Summit with MPS enabled?
",of how to start an MPS server process for a job: https://vimeo.com/292016149,4.262170219598442
"How do I submit a job to Summit with MPS enabled?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.218239481235903
"How do I submit a job to Summit with MPS enabled?
","The Multi-Process Service (MPS) enables multiple processes (e.g. MPI ranks) to concurrently share the resources on a single GPU. This is accomplished by starting an MPS server process, which funnels the work from multiple CUDA contexts (e.g. from multiple MPI ranks) into a single CUDA context. In some cases, this can increase performance due to better utilization of the resources. As mentioned in the Common bsub Options section above, MPS can be enabled with the -alloc_flags ""gpumps"" option to bsub. The following screencast shows an example of how to start an MPS server process for a job:",4.198355320369397
"Can I use PuTTY to create an ssh connection in Paraview for macOS clients?
","After installing, you must give ParaView the relevant server information to be able to connect to OLCF systems (comparable to VisIt's system of host profiles). The following provides an example of doing so. Although several methods may be used, the one described should work in most cases.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.

For Windows clients, it is necessary to install PuTTY to create an ssh connection in step 2.

Step 1: Save the following servers.pvsc file to your local computer",4.3154210980877865
"Can I use PuTTY to create an ssh connection in Paraview for macOS clients?
","If ParaView is unable to connect to our systems after trying to initiate a connection via the GUI and you see a ""The process failed to start. Either the invoked program is missing, or you may have insufficient permissions to invoke the program"" error, make sure that you have XQuartz (X11) installed.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.",4.15130968266663
"Can I use PuTTY to create an ssh connection in Paraview for macOS clients?
","In a new terminal, open a tunneling connection with andes79.olcf.ornl.gov and port 5901
example:
     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov
     andes: ssh -4L 5901:localhost:5901 andes79

**************************************************************************

MATLAB is selecting SOFTWARE OPENGL rendering.

Step 3 (terminal 2)

In a second terminal on your local system open a tunneling connection following the instructions given by the vnc start-up script:

localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov",4.1398332535770095
"What is the consequence of intentionally introducing or using malicious software on OLCF computing resources?
","Users are not allowed to reconstruct information or software for which they are not authorized. This includes but is not limited to any reverse engineering of copyrighted software or firmware present on OLCF computing resources.

Users are accountable for their actions and may be held accountable to applicable administrative or legal sanctions.",4.3978290332455
"What is the consequence of intentionally introducing or using malicious software on OLCF computing resources?
",The Oak Ridge Leadership Computing Facility (OLCF) computing resources are provided to users for research purposes. All users must agree to abide by all security measures described in this document. Failure to comply with security procedures will result in termination of access to OLCF computing resources and possible legal actions.,4.368224989108408
"What is the consequence of intentionally introducing or using malicious software on OLCF computing resources?
","All software used on OLCF computers must be appropriately acquired and used according to the appropriate software license agreement. Possession, use, or transmission of illegally obtained software is prohibited. Likewise, users shall not copy, store, or transfer copyrighted software, except as permitted by the owner of the copyright. Only export-controlled codes approved by the Export Control Office may be run by parties with sensitive data agreements.

Users must not intentionally introduce or use malicious software such as computer viruses, Trojan horses, or worms.",4.337711039013624
"How can I specify the project ID for my job on Crusher?
","Select the host in the left side of the window.

Select the ""Launch Profiles"" tab in the right side of the window. This will display the known launch profiles for this host.

Select a ""Launch Profile"" and the settings are displayed in the tabs below.

You can set your Project ID in the ""Default Bank/Account"" field in the ""Parallel"" tab.

You can change the queue used by modifying the ""Partition/Pool/Queue"" field in the ""Parallel"" tab.

Once you have made your changes, press the ""Apply"" button, and then save the settings (Options/Save Settings).",4.120886868765303
"How can I specify the project ID for my job on Crusher?
","Before you can do anything you need a Project to do your things in. Fortunately, when you get an allocation on one of our clusters a Project is automatically created for you with the same name as the allocation. By using our own distinct Project we are ensuring that we will not interfere with anyone else's work.

NOTE: Everywhere that you see <cluster> replace that with the cluster that you will be running on (marble or onyx).",4.112275227281636
"How can I specify the project ID for my job on Crusher?
","If you have problems or need helping running on Crusher, please submit a ticket by emailing help@olcf.ornl.gov.



JIRA_CONTENT_HERE",4.070115235793796
"What is the name of the storage volume associated with the storage-1 Persistent Volume Claim?
",a desired size for a PersistentVolume. The cluster administrator or some automated mechanism will provision the storage on the backend and make it available to the cluster via the PersistentVolumeClaim.,4.385779846262554
"What is the name of the storage volume associated with the storage-1 Persistent Volume Claim?
","Once the Persistent Volume is created its allocated memory is available to be claimed by a project. This is done through a Persistent Volume Claim. PVC's are created using a YAML file in the same manner that PV's are created. An example of a PVC that claims the PV created in the previous section follows:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: storage-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

The claim is then placed using:

oc create -f storage-1.yaml",4.352920943453037
"What is the name of the storage volume associated with the storage-1 Persistent Volume Claim?
","From the main screen of a project go to the Storage option in the hamburger menu on the left hand side of the screen, then click Persistent Volume Claims

Persistent Volume Claim Menu

In the upper right click the Create Persistent Volume Claim button.

Create Storage

This will take you to a screen that allows you to select what you need for your PVC. Give your claim a name and request an amount of storage and click Create. You can also click Edit YAML to edit the PVC object directly.

PV Settings",4.339995801899914
"What is the name of the directory where the dynamically loaded plugins are located in the Andes environment?
",For Andes:,4.052396934458948
"What is the name of the directory where the dynamically loaded plugins are located in the Andes environment?
","Some libraries still resolved to paths outside of /mnt/bb, and the reason for that is that the executable may have several paths in RPATH.



Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-user-guide.",4.049986160190109
"What is the name of the directory where the dynamically loaded plugins are located in the Andes environment?
","On Summit, Andes, and the DTNs, additional paths to the various project-centric work areas are available via the following symbolic links and/or environment variables:

Member Work Directory:  /gpfs/alpine/scratch/[userid]/[projid] or $MEMBERWORK/[projid]

Project Work Directory: /gpfs/alpine/proj-shared/[projid] or $PROJWORK/[projid]

World Work Directory: /gpfs/alpine/world-shared/[projid] or $WORLDWORK/[projid]",4.0463482887187
"Can I use a persistent volume claim in multiple pods?
","Once the Persistent Volume is created its allocated memory is available to be claimed by a project. This is done through a Persistent Volume Claim. PVC's are created using a YAML file in the same manner that PV's are created. An example of a PVC that claims the PV created in the previous section follows:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: storage-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

The claim is then placed using:

oc create -f storage-1.yaml",4.338027642487189
"Can I use a persistent volume claim in multiple pods?
","By design pods are ephemeral. They are meant to be something that can be killed with relatively little impact to the application. Since containers are ephemeral, we need a way to consume storage that is persistent and can hold data for our applications. One option that Kubernetes has for storing data is with Persistent Volumes. PersistentVolume objects are created by the cluster administrator and reference storage that is on a resilient storage platform such as NetApp. A user can request storage by creating a PersistentVolumeClaim which requests a desired size for a PersistentVolume. The",4.336005262975736
"Can I use a persistent volume claim in multiple pods?
","Now, if the data in the PersistentVolume becomes corrupted or lost in some way, we can create a new PersistentVolume that is identical to the original PersistentVolume at the time the VolumeSnapshot was created.

To do this, create a PersistentVolumeClaim that contains a dataSource field in the Pod's spec. An example follows:",4.318866858231099
"How do I set the ROMIO hints for a specific job on Summit?
","A performance issue has been identified using parallel HDF5 with the default version of ROMIO provided in spectrum-mpi/10.2.0.10-20181214. To fully take advantage of parallel HDF5, users need to switch to the newer version of ROMIO and use ROMIO hints. The following shows recommended variables and hints for a 2 node job. Please note that hints must be tuned for a specific job.

$ module unload darshan-runtime
$ export OMPI_MCA_io=romio321
$ export ROMIO_HINTS=./my_romio_hints
$ cat $ROMIO_HINTS
romio_cb_write enable
romio_ds_write enable
cb_buffer_size 16777216
cb_nodes 2",4.233321397742468
"How do I set the ROMIO hints for a specific job on Summit?
","<string>:17: (INFO/1) Duplicate explicit target name: ""summit"".

<string>:17: (INFO/1) Duplicate explicit target name: ""x11 forwarding"".

After logging onto Summit (with X11 forwarding), execute the series of commands below:

$ module load vampir

$ vampir &

Once the GUI pops up (might take a few seconds), you can load a file resident on the file system by selecting Local File for file selection.",4.041770522370281
"How do I set the ROMIO hints for a specific job on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.039013527095261
"What is the relationship between the Roofline performance model and the AQL profiling libraries?
","The Roofline performance model is an increasingly popular way to demonstrate and understand application performance. This section documents how to construct a simple roofline model for a single kernel using rocprof. This roofline model is designed to be comparable to rooflines constructed by NVIDIA's NSight Compute. A roofline model plots the achieved performance (in floating-point operations per second, FLOPS/s) as a function of arithmetic (or operational) intensity (in FLOPS per Byte). The model detailed here calculates the bytes moved as they move to and from the GPU's HBM.",4.310326710535397
"What is the relationship between the Roofline performance model and the AQL profiling libraries?
","The Roofline performance model is an increasingly popular way to demonstrate and understand application performance. This section documents how to construct a simple roofline model for a single kernel using rocprof. This roofline model is designed to be comparable to rooflines constructed by NVIDIA's NSight Compute. A roofline model plots the achieved performance (in floating-point operations per second, FLOPS/s) as a function of arithmetic (or operational) intensity (in FLOPS per Byte). The model detailed here calculates the bytes moved as they move to and from the GPU's HBM.",4.310326710535397
"What is the relationship between the Roofline performance model and the AQL profiling libraries?
","Attainable peak rooflines are constructed using microbenchmarks, and are not currently discussed here. Attainable rooflines consider the limitations of cooling and power consumption and are more representative of what an application can achieve.",4.173646530813173
"Can I use environment variables in my batch script to specify job options in Frontier?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.293490829754899
"Can I use environment variables in my batch script to specify job options in Frontier?
","When using Python environments with SLURM, it is always recommended to submit a batch script using the export=NONE flag to avoid $PATH issues and use unset SLURM_EXPORT_ENV in your job script (before calling srun); however, this means that previously set environment variables are NOT passed into the batch job, so you will have to set them again (and load modules again) if they are required by your workflow. Alternatively, you can try submitting your batch script from a fresh login shell.

$ sbatch --export=NONE submit.sl

Below are example batch scripts for running on Andes and Frontier:",4.254145789319283
"Can I use environment variables in my batch script to specify job options in Frontier?
","LSF provides a number of environment variables in your job’s shell environment. Many job parameters are stored in environment variables and can be queried within the batch job. Several of these variables are summarized in the table below. This is not an all-inclusive list of variables available to your batch job; in particular only LSF variables are discussed, not the many “standard” environment variables that will be available (such as $PATH).",4.220965208405387
"How do I know if my memory region is coarse-grained or fine-grained?
","AMD GPUs can allocate two different types of memory locations: 1) Coarse grained and 2) Fine grained.

Coarse grained memory is only guaranteed to be coherent outside of GPU kernels that modify it, enabling higher performance memory operations. Changes applied to coarse-grained memory by a GPU kernel are  only visible to the rest of the system (CPU or other GPUs) when the kernel has completed. A GPU kernel is only guaranteed to see changes applied to coarse grained memory by the rest of the system (CPU or other GPUs) if those changes were made before the kernel launched.",4.306586591826991
"How do I know if my memory region is coarse-grained or fine-grained?
","AMD GPUs can allocate two different types of memory locations: 1) Coarse grained and 2) Fine grained.

Coarse grained memory is only guaranteed to be coherent outside of GPU kernels that modify it, enabling higher performance memory operations. Changes applied to coarse-grained memory by a GPU kernel are  only visible to the rest of the system (CPU or other GPUs) when the kernel has completed. A GPU kernel is only guaranteed to see changes applied to coarse grained memory by the rest of the system (CPU or other GPUs) if those changes were made before the kernel launched.",4.306586591826991
"How do I know if my memory region is coarse-grained or fine-grained?
","Fine grained memory allows CPUs and GPUs to synchronize (via atomics) and coherently communicate with each other while the GPU kernel is running, allowing more advanced programming patterns. The additional visibility impacts the performance of fine grained allocated memory.",4.252231126195492
"What is the advantage of using SBCAST?
","Slurm contains a utility called sbcast. This program takes a file and broadcasts it to each node's node-local storage (ie, /tmp, NVMe). This is useful for sharing large input files, binaries and shared libraries, while reducing the overhead on shared file systems and overhead at startup. This is highly recommended at scale if you have multiple shared libraries on Lustre/NFS file systems.

Here is a simple example of a file sbcast from a user's scratch space on Lustre to each node's NVMe drive:",4.053111984686325
"What is the advantage of using SBCAST?
","Slurm contains a utility called sbcast. This program takes a file and broadcasts it to each node's node-local storage (ie, /tmp, NVMe). This is useful for sharing large input files, binaries and shared libraries, while reducing the overhead on shared file systems and overhead at startup. This is highly recommended at scale if you have multiple shared libraries on Lustre/NFS file systems.

Here is a simple example of a file sbcast from a user's scratch space on Lustre to each node's NVMe drive:",4.053111984686325
"What is the advantage of using SBCAST?
","Most users will find batch jobs an easy way to use the system, as they allow you to ""hand off"" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.",4.031874444727056
"What is the path to the project home directory on Slate?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",4.198795798378438
"What is the path to the project home directory on Slate?
","Open and Moderate Projects are provided with a Project Home storage area in the NFS-mounted filesystem. This area is intended for storage of data, code, and other files that are of interest to all members of a project. Since Project Home is an NFS-mounted filesystem, its performance will not be as high as other filesystems.

Moderate Enhanced projects are not provided with Project Home spaces, just Project Work spaces.

Project Home area is accessible at /ccs/proj/abc123 (where abc123 is your project ID).",4.19193482760971
"What is the path to the project home directory on Slate?
","This section will describe the project allocation process for Slate. This is applicable to both the Onyx and Marble OLCF OpenShift clusters.

In addition, we will go over the process to log into Onyx and Marble, and how namespaces work within the OpenShift context.

Please fill out the Slate Request Form in order to use Slate resources. This must be done in order to proceed.

If you have any question please contact User Assistance, via a Help Ticket Submission or by emailing help@olcf.ornl.gov.

Required information:

- Existing OLCF Project ID

- Project PI",4.135923964279447
"What is the namespace of the GitLab runner deployment on Slate?
","Jenkins

OpenShift Pipelines

GitLab Runners

Deploying a GitLab Runner into a Slate project may be found in the https://docs.olcf.ornl.gov/systems/overview.html#slate_gitlab_runners document which leverages a localized Slate Helm Chart. The GitLab Pipelines documentation as well as GitLab CI/CD Examples provide more details on GitLab Runner capabilities and usage.

Jenkins

For Jenkins deployment, a Slate Helm Chart is planned to be released. Documentation concerning Jenkins Pipelines as well as Jenkins Pipeline Tutorials are available.

OpenShift Pipelines",4.395633576137609
"What is the namespace of the GitLab runner deployment on Slate?
","To deploy a GitLab Runner from the Developer Perspective, navigate to ""Topology"" -> ""Helm Chart"" and select ""GitLab Runner vX.X.X Provided by Slate Helm Charts"". From the new window, select ""Install Helm Chart"".

On the resulting screen, one can customize the installation of the GitLab Runner helm chart. The chart has been forked from upstream to allow for customized deployment to Slate. From the Form View, expand the ""GitLab Runner"" fields. Using the registration token retrieved in the prior section, enter token for adding the runner to the GitLab server in the empty field.",4.282510355494802
"What is the namespace of the GitLab runner deployment on Slate?
","GitLab CI/CD jobs may be accomplished using the GitLab Runner application. An open-source application written in Go, a GitLab Runner may be installed with a variety of executors. This documentation focuses on the installation and configuration for use of the GitLab Runner Kubernetes Executor on Slate.

https://docs.gitlab.com/runner/

https://docs.gitlab.com/runner/executors/kubernetes.html

https://docs.gitlab.com/runner/install/kubernetes.html",4.267169052569908
"How can I access the Rigetti Quantum Computing queue and simulators?
","Access to the IBM Quantum Computing queues, reservations, and simulators can be obtained via multiple methods -- either through the https://docs.olcf.ornl.gov/systems/ibm_quantum.html#cloud <ibm-cloud> or https://docs.olcf.ornl.gov/systems/ibm_quantum.html#locally <ibm-local>.",4.450880350164095
"How can I access the Rigetti Quantum Computing queue and simulators?
","accessing the hybrid infrastructure of available quantum processors and classical computational framework via the cloud. From the QCS, users can view system status and availability, initiate and manage quantum infrastructure reservations (either executing programs manually or adding them to the queue). Information on using this resource is available on the Rigetti's Documentation or our https://docs.olcf.ornl.gov/quantum/quantum_systems/rigetti.html.",4.430407840130066
"How can I access the Rigetti Quantum Computing queue and simulators?
","Rigetti provides system access via a cloud-based JupyterLab development environment: https://docs.rigetti.com/qcs/getting-started/jupyterlab-ide.  From there, users can access a JupyterLab server loaded with Rigetti's PyQuil programming framework, Rigetti's Forest Software Development Kit, and associated program examples and tutorials.  This is the method that allows access to Rigetti's QPU's directly, as opposed to simulators.",4.37200077491326
"What is the maximum number of wavefronts that can be assigned to a single CU in the Frontier system?
","Each CU has 4 Matrix Core Units (the equivalent of NVIDIA's Tensor core units) and 4 16-wide SIMD units. For a vector instruction that uses the SIMD units, each wavefront (which has 64 threads) is assigned to a single 16-wide SIMD unit such that the wavefront as a whole executes the instruction over 4 cycles, 16 threads per cycle. Since other wavefronts occupy the other three SIMD units at the same time, the total throughput still remains 1 instruction per cycle. Each CU maintains an instructions buffer for 10 wavefronts and also maintains 256 registers where each register is 64 4-byte wide",4.260335701713815
"What is the maximum number of wavefronts that can be assigned to a single CU in the Frontier system?
","The 2 GCDs on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The OLCF Director's Discretionary (DD) program allocates approximately 10% of the available Frontier hours in a calendar year. Frontier is allocated in *node* hours, and a typical DD project is awarded between 15,000 - 20,000 *node* hours. For more information about Frontier, please visit the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#frontier-user-guide.",4.22573043693488
"What is the maximum number of wavefronts that can be assigned to a single CU in the Frontier system?
","Each GPU contains 110 Compute Units (CUs) grouped in 4 Compute Engines (CEs). Physically, each GPU contains 112 CUs, but two are disabled. A command processor in each GPU receives API commands and transforms them into compute tasks. Compute tasks are managed by the 4 compute engines, which dispatch wavefronts to compute units. All wavefronts from a single workgroup are assigned to the same CU. In CUDA terminology, workgroups are ""blocks"", wavefronts are ""warps"", and work-items are ""threads"". The terms are often used interchangeably.

Block diagram of the AMD Instinct MI200 multi-chip module",4.2189109975067325
"How can I list the libraries in Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.115075691381018
"How can I list the libraries in Frontier?
",This section of the Summit User Guide is intended to show current OLCF users how to start preparing their applications to run on the upcoming Frontier system. We will continue to add more topics to this section in the coming months. Please see the topics below to get started.,4.109334801038557
"How can I list the libraries in Frontier?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers. Please see the following https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more detailed inforation on how to compile using these modules.",4.070332474158615
"How can I enable verbose debugging messages in TAU?
",Environment variables to be used during compilation through the environment variable TAU_OPTIONS,4.168674718170679
"How can I enable verbose debugging messages in TAU?
",| Variable | Default | Description | | --- | --- | --- | | TAU_TRACE | 0 | Setting to 1 turns on tracing | | TAU_CALLPATH | 0 | Setting to 1 turns on callpath profiling | | TAU_TRACK_MEMORY_LEAKS | 0 | Setting to 1 turns on leak detection | | TAU_TRACK_HEAP | 0 | Setting to 1 turns on heap memory routine entry/exit | | TAU_CALLPATH_DEPTH | 2 | Specifies depth of callpath | | TAU_TRACK_IO_PARAMS | 0 | Setting 1 with -optTrackIO | | TAU_SAMPLING | 1 | Generates sample based profiles | | TAU_COMM_MATRIX | 0 | Setting to 1 generates communication matrix | | TAU_THROTTLE | 1 | Setting to 0 turns,4.157094283035874
"How can I enable verbose debugging messages in TAU?
","For Fortran: replace the compiler with the TAU wrapper tau_f90.sh / tau_f77.sh

Even if you don't compile your application with a TAU wrapper, you can profile some basic functionalities with tau_exec, for example:

jsrun -n 4 –r 4 –a 1 –c 1 tau_exec -T mpi ./test

The above command profiles MPI for the binary test, which was not compiled with the TAU wrapper.

The following TAU environment variables may be useful in job submission scripts.",4.137880217263156
"What is the difference between sbatch and salloc commands?
","Since all compute resources are managed and scheduled by Slurm, it is not possible to simply log into the system and immediately begin running parallel codes interactively. Rather, you must request the appropriate resources from Slurm and, if necessary, wait for them to become available. This is done through an ""interactive batch"" job. Interactive batch jobs are submitted with the salloc command. Resources are requested via the same options that are passed via #SBATCH in a regular batch script (but without the #SBATCH prefix). For example, to request an interactive batch job with the same",4.351682149559883
"What is the difference between sbatch and salloc commands?
","All of the above can also be achieved in an interactive batch job through the use of the salloc command on Andes or the bsub -Is command on Summit. Recall that login nodes should not be used for memory- or compute-intensive tasks, including VisIt.",4.332454039020147
"What is the difference between sbatch and salloc commands?
","different commands.  Notable are the separation in batch script submission (sbatch) and interactive batch submission (salloc).  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-slurm section for more infomation including a LSF to Slurm command comparison.  Srun job launcher  Frontier uses Slurm's job launcher, srun, instead of Summit's jsrun to launch parallel jobs within a batch script.  Overall functionality is similar, but commands are notably different. Frontier's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#compute node layout <frontier-simple>",4.323788560766318
"How do I unload darshan-runtime on Frontier?
","<p style=""font-size:20px""><b>Frontier: Darshan Runtime 3.4.0 (May 10, 2023)</b></p>

The Darshan Runtime modulefile darshan-runtime/3.4.0 on Frontier is now loaded by default. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>.

Unloading darshan-runtime modulefile is recommended for users profiling their applications with other profilers to prevent conflicts.",4.397895588746219
"How do I unload darshan-runtime on Frontier?
","On Tuesday, May 9, 2023, the darshan-runtime modulefile was added to DefApps and is now loaded by default on Frontier. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>. Unloading darshan-runtime is recommended for users profiling their applications with other profilers to prevent conflicts.



JIRA_CONTENT_HERE",4.310388303694223
"How do I unload darshan-runtime on Frontier?
","You will need to unload the darshan-runtime module. In some instances you may need to unload the xalt and xl modules.  $ module unload darshan-runtime

Serial

..  C

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load scorep
        $ module load gcc
        $ scorep gcc -c test.c
        $ scorep gcc -o test test.o

..  C++

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load scorep
        $ module load gcc
        $ scorep g++ -c test.cpp main.cpp
        $ scorep g++ -o test test.o main.o

..  Fortran",4.304442677388486
"How long does the installation of CuPy typically take on Summit?
","The CC and NVCC flags ensure that you are passing the correct wrappers, while the various flags for Frontier tell CuPy to build for AMD GPUs. Note that, on Summit, if you are using the instructions for installing CuPy with OpenCE below, the cuda/11.0.3 module will automatically be loaded. This installation takes, on average, 10-20 minutes to complete (due to building everything from scratch), so don't panic if it looks like the install timed-out. Eventually you should see output similar to this (versions will vary):

Successfully installed cupy-12.2.0 fastrlock-0.8.1",4.355985097719508
"How long does the installation of CuPy typically take on Summit?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.

This guide teaches you how to build CuPy from source into a custom virtual environment. On Summit and Andes, this is done using conda, while on Frontier this is done using venv.

In this guide, you will:

Learn how to install CuPy

Learn the basics of CuPy

Compare speeds to NumPy

OLCF Systems this guide applies to:

Summit

Frontier

Andes",4.244797950857556
"How long does the installation of CuPy typically take on Summit?
","As noted in AMD+CuPy limitations, data sizes explored here hang. So, this section currently does not apply to Frontier.

Now that you know how to use CuPy, time to see the actual benefits that CuPy provides for large datasets. More specifically, let's see how much faster CuPy can be than NumPy on Summit. You won't need to fix any errors; this is mainly a demonstration on what CuPy is capable of.

There are a few things to consider when running on GPUs, which also apply to using CuPy:

Higher precision means higher cost (time and space)

The structuring of your data is important",4.223211510926416
"What is the goal of the mentorship at the GPU hackathons?
","If you want/need to get your code running (or optimized) on a GPU-accelerated system, these hackathons offer a unique opportunity to set aside the time, surround yourself with experts in the field, and push toward your development goals. By the end of the event, each team should have part of their code running (or more optimized) on GPUs, or at least have a clear roadmap of how to get there.

<p style=""font-size:20px""><b>Target audience</b></p>",4.490904678094541
"What is the goal of the mentorship at the GPU hackathons?
","A GPU hackathon is a multi-day coding event in which teams of developers port their own applications to run on GPUs, or optimize their applications that already run on GPUs. Each team consists of 3 or more developers who are intimately familiar with (some part of) their application, and they work alongside 1 or more mentors with GPU programming expertise. Our mentors come from universities, national laboratories, supercomputing centers, and industry partners.",4.469665841495223
"What is the goal of the mentorship at the GPU hackathons?
","If you would like more information about how you can volunteer to mentor a team at an upcoming Open/GPU hackathon, please visit our Become a Mentor page.

<p style=""font-size:20px""><b>Who can I contact with questions?</b></p>

If you have any questions about the OLCF GPU Hackathon Series, please contact OLCF Help at (help@olcf.ornl.gov).",4.360560704856469
"How can I share data with users outside my project?
",data for archival access that's shared with those outside your project | World Archive | /hpss/prod/[projid]/world-shared |,4.135306990224354
"How can I share data with users outside my project?
","Project Work: Short-term project data for fast, batch-job access that's shared with other project members.

World Work: Short-term project data for fast, batch-job access that's shared with users outside your project.

Member Archive: Long-term project data for archival access that is not shared with other project members.

Project Archive: Long-term project data for archival access that's shared with other project members.

World Archive: Long-term project data for archival access that's shared with users outside your project.",4.127823950756978
"How can I share data with users outside my project?
","Never allow access to your sensitive data to anyone outside of your group.

Transfer of sensitive data must be through the use encrypted methods (scp, sftp, etc).

All sensitive data must be removed from all OLCF resources when your project has concluded.",4.093689092976978
"What is the maximum number of processors that can be used for a simulation on Summit?
","Summit is an IBM system located at the Oak Ridge Leadership Computing Facility. With a theoretical peak double-precision performance of approximately 200 PF, it is one of the most capable systems in the world for a wide range of traditional computational science applications. It is also one of the ""smartest"" computers in the world for deep learning applications with a mixed-precision capability in excess of 3 EF.



Summit node architecture diagram",4.277655460073028
"What is the maximum number of processors that can be used for a simulation on Summit?
","Each Summit Compute node has 6 NVIDIA V100 GPUs.  The NVIDIA Tesla V100 accelerator has a peak performance of 7.8 TFLOP/s (double-precision) and contributes to a majority of the computational work performed on Summit. Each V100 contains 80 streaming multiprocessors (SMs), 16 GB (32 GB on high-memory nodes) of high-bandwidth memory (HBM2), and a 6 MB L2 cache that is available to the SMs. The GigaThread Engine is responsible for distributing work among the SMs and (8) 512-bit memory controllers control access to the 16 GB (32 GB on high-memory nodes) of HBM2 memory. The V100 uses NVIDIA's",4.272570756440489
"What is the maximum number of processors that can be used for a simulation on Summit?
","Summit node architecture diagram

The basic building block of Summit is the IBM Power System AC922 node. Each of the approximately 4,600 compute nodes on Summit contains two IBM POWER9 processors and six NVIDIA Tesla V100 accelerators and provides a theoretical double-precision capability of approximately 40 TF. Each POWER9 processor is connected via dual NVLINK bricks, each capable of a 25GB/s transfer rate in each direction.",4.2723942744900265
"How do I specify the number of nodes to use for a Crusher job step?
","Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.",4.353571178360744
"How do I specify the number of nodes to use for a Crusher job step?
","The first step to creating resource sets is understanding how a code would like the node to appear. For example, the number of tasks/threads per GPU. Once this is understood, the next step is to simply calculate the number of resource sets that can fit on a node. From here, the number of needed nodes can be calculated and passed to the batch job request.

The basic steps to creating resource sets:

Understand how your code expects to interact with the system.

How many tasks/threads per GPU?",4.178097272890575
"How do I specify the number of nodes to use for a Crusher job step?
","Each Crusher compute node has [2x] 1.92 TB NVMe devices (SSDs) with a peak sequential performance of 5500 MB/s (read) and 2000 MB/s (write). To use the NVMe, users must request access during job allocation using the -C nvme option to sbatch, salloc, or srun. Once the devices have been granted to a job, users can access them at /mnt/bb/<userid>. Users are responsible for moving data to/from the NVMe before/after their jobs. Here is a simple example script:",4.177227385023424
"What is the version of the CUDA runtime installed in Nvidia Rapids?
","RAPIDS is a suite of libraries to execute end-to-end data science and analytics pipelines on GPUs. RAPIDS utilizes NVIDIA CUDA primitives for low-level compute optimization through user-friendly Python interfaces. An overview of the RAPIDS libraries available at OLCF is given next.

cuDF is a Python GPU DataFrame library (built on the Apache Arrow columnar memory format) for loading, joining, aggregating, filtering, and otherwise manipulating data.",4.154924770187961
"What is the version of the CUDA runtime installed in Nvidia Rapids?
","Although there are newer CUDA modules on Summit, cuda/11.0.3 is the latest version that is officially supported by the version of IBM's software stack installed on Summit. When loading the newer CUDA modules, a message is printed to the screen stating that the module is for “testing purposes only”. These newer unsupported CUDA versions might work with some users’ applications, but importantly, they are known not to work with GPU-aware MPI.",4.1379177058799135
"What is the version of the CUDA runtime installed in Nvidia Rapids?
","Using the hip-cuda/5.1.0 module on Summit requires CUDA v11.4.0 or later. However, only CUDA v11.0.3 and earlier currently support GPU-aware MPI, so GPU-aware MPI is currently not available when using HIP on Summit.

Any codes compiled with post 11.0.3 CUDA (cuda/11.0.3) will not work with MPS enabled (-alloc_flags ""gpumps"") on Summit. The code will hang indefinitely. CUDA v11.0.3 is the latest version that is officially supported by IBM's software stack installed on Summit. We are continuing to look into this issue.",4.119774307726056
"How do I get more information about using Frontier for high performance computing?
","Frontier connects to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#dtn-user-guide to move data to/from HPSS.

Frontier is running Cray OS 2.4 based on SUSE Linux Enterprise Server (SLES) version 15.4.",4.3520601753047865
"How do I get more information about using Frontier for high performance computing?
","Each Frontier compute node contains 4 AMD MI250X. The AMD MI250X has a peak performance of 53 TFLOPS in double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 26.5 TFLOPS (double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in each direction simultaneously).

To connect to Frontier, ssh to frontier.olcf.ornl.gov. For example:

$ ssh <username>@frontier.olcf.ornl.gov",4.345004040721824
"How do I get more information about using Frontier for high performance computing?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.32110840530383
"How can I check the current environment variables set for Score-P?
","Once the code has been instrumented, it is time to begin the measurement run of the newly compiled code. The measurement calls will gather information during the runtime of the code where this information will be stored for later analysis.

By default Score-P is configured to run with profiling set to true and tracing set to false. Measurement types are configured via environment variables.

##Environment variable setup examples

export SCOREP_ENABLE_TRACING=true

You can check what current Score-P environment variables are set:

$ scorep-info config-vars --full

#Output",4.2450698572125125
"How can I check the current environment variables set for Score-P?
","In addition to the trace, Score-P requires some additional memory to maintain internal data structures. Thus, it provides also an estimation for the total amount of required memory on each process. The memory size per process that Score-P reserves is set via the environment variable SCOREP_TOTAL_MEMORY. In the given example the per process memory is about 10GB. When defining a filter, it is recommended to exclude short, frequently called functions from measurement since they require a lot of buffer space (represented by a high value under max_tbc) but incur a high measurement overhead. MPI",4.142222902345111
"How can I check the current environment variables set for Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations. It works in combination with Periscope, Scalasca, Vampir, and Tau.

Steps in a typical Score-P workflow to run on https://docs.olcf.ornl.gov/systems/Scorep.html#summit-user-guide:",4.126778885718223
"How do I launch R?
","Several versions of R are available on Summit. You can see which by entering the command module spider r. Throughout this example, we will be using R version 3.6.1.

If you have logged in with the default modules, then you need to swap xl for gcc and the load R:

module swap xl gcc/6.4.0
module load r/3.6.1

If we do that and launch R, then we see:",4.2685412599748
"How do I launch R?
","So far we have seen how to launch jobs interactively. The other way to run our script is to submit a batch job. To do that, we need to create a batch script:

#!/bin/bash
#BSUB -P ABC123
#BSUB -W 10
#BSUB -nnodes 2
#BSUB -J rhw

module load gcc/6.4.0
module load r/3.6.1

cd /gpfs/alpine/abc123/proj-shared/my_hw_path/

jsrun -n4 -r2 Rscript hw.r",4.152760016655756
"How do I launch R?
","If we do that and launch R, then we see:

version
## platform       powerpc64le-unknown-linux-gnu
## arch           powerpc64le
## os             linux-gnu
## system         powerpc64le, linux-gnu
## status
## major          3
## minor          6.1
## year           2019
## month          07
## day            05
## svn rev        76782
## language       R
## version.string R version 3.6.1 (2019-07-05)
## nickname       Action of the Toes",4.140718773991742
"How are blocks of threads scheduled in Frontier?
","By default, Frontier reserves the first core in each L3 cache region. Frontier uses low-noise mode, which constrains all system processes to core 0. Low-noise mode cannot be disabled by users. In addition, Frontier uses SLURM core specialization (-S 8 flag at job allocation time, e.g., sbatch) to reserve one core from each L3 cache region, leaving 56 allocatable cores. Set -S 0 at job allocation to override this setting.",4.148845424830563
"How are blocks of threads scheduled in Frontier?
","The 2 GCDs on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The OLCF Director's Discretionary (DD) program allocates approximately 10% of the available Frontier hours in a calendar year. Frontier is allocated in *node* hours, and a typical DD project is awarded between 15,000 - 20,000 *node* hours. For more information about Frontier, please visit the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#frontier-user-guide.",4.141878874312121
"How are blocks of threads scheduled in Frontier?
","Because a Frontier compute node has two hardware threads available (2 logical cores per physical core), this enables the possibility of multithreading your application (e.g., with OpenMP threads). Although the additional hardware threads can be assigned to additional MPI tasks, this is not recommended. It is highly recommended to only use 1 MPI task per physical core and to use OpenMP threads instead on any additional logical cores gained when using both hardware threads.",4.132665252338823
"Are there any best practices for optimizing the performance of SPI workflows?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.  This page lists the differences you may see when using OLCF resources to execute SPI workflows.  The page will also point to sections within the site where more information on standard non-SPI use can be found.",4.261085180943117
"Are there any best practices for optimizing the performance of SPI workflows?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads. Because of this, many processes used within the SPI are very similar to those used for standard non-SPI. This page lists the differences you may see when using OLCF resources to execute SPI resources.",4.173316080853917
"Are there any best practices for optimizing the performance of SPI workflows?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.

The SPI provides access to the OLCF's Summit resource for compute.  To safely separate SPI and non-SPI workflows, SPI workflows must use a separate login node named https://docs.olcf.ornl.gov/systems/citadel_user_guide.html#Citadel<spi-compute-citadel>.  Citadel provides a login node specifically for SPI workflows.",4.17201825192709
"How can I create a route in Slate that points to multiple services?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.315923600926742
"How can I create a route in Slate that points to multiple services?
","Route Exposed

While a route usually points to one service through the to parameter in the configuration, it is possible to have as many as four services to load balance between. This is used with A/B deployments.

Here is an example route which points to 3 services:",4.267431259638894
"How can I create a route in Slate that points to multiple services?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.245525422258138
"What command can you use to display the shared libraries used by an executable in Crusher?
","OLCF systems provide many software packages and scientific libraries pre-installed at the system-level for users to take advantage of. In order to link these libraries into an application, users must direct the compiler to their location. The module show command can be used to determine the location of a particular library. For example",4.169199007628685
"What command can you use to display the shared libraries used by an executable in Crusher?
","Crusher users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.128808792185214
"What command can you use to display the shared libraries used by an executable in Crusher?
","Notice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable. Once libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.

As mentioned above, you can alternatively use --exclude=NONE on sbcast to send all libraries along with the binary. Using --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time. A job script for the previous example, modified for sending all libraries is shown below.",4.1200792158743536
"Can I use a Slate deployment to roll back to a previous revision?
","Versioned and Immutable: Desired state is stored in a way that enforces immutability, versioning and retains a complete version history.

Pulled Automatically: Software agents automatically pull the desired declarations from the source.

Continuously Reconciled: Software agents continuously observe actual system state and attempt to apply the desired state.

On Slate, Red Hat OpenShift GitOps is based on the open source ArgoCD project. For more information as well as how to install and use ArgoCD on Slate, see: https://docs.olcf.ornl.gov/systems/overview.html#slate_openshift_gitops.",4.129950460264647
"Can I use a Slate deployment to roll back to a previous revision?
","To roll back a deployment, run

oc rollout undo deploy/{NAME}

When using the web interface, you can view and edit a Deployment, from the sidebar, go to Applications, then Deployments.

Deployment Menu

You can get info on any deployment by clicking on it.

To edit this configuration, click Actions in the upper right hand corner, then Edit on whatever you wish to edit, or Edit Deployment if you'd rather edit the YAML directly.

Edit Deployment Config",4.104528231491391
"Can I use a Slate deployment to roll back to a previous revision?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.094225393802829
"How can I run my R script on the compute nodes using an interactive job?
","There are two ways we can run this. One is with an interactive job, and one is with a batch job. The interactive job doesn’t provide us with the ability to run interactive R jobs on the compute nodes (using R interactively on the compute node is complicated, so we do not discuss that here.). However, it does allow us to interactively submit tasks to the compute nodes (launched via jsrun). This can be useful if you are trying to debug a script that unexpectedly dies, without having to continuously submit jobs to the batch queue (more on that in a moment).",4.615256111396921
"How can I run my R script on the compute nodes using an interactive job?
","After running this command, the job will wait until enough compute nodes are available, just as any other batch job must. However, once the job starts, the user will be given an interactive prompt on the primary compute node within the allocated resource pool. Commands may then be executed directly (instead of through a batch script).",4.383053759653784
"How can I run my R script on the compute nodes using an interactive job?
","After running this command, the job will wait until enough compute nodes are available, just as any other batch job must. However, once the job starts, the user will be given an interactive prompt on the primary compute node within the allocated resource pool. Commands may then be executed directly (instead of through a batch script).

Debugging",4.370649473945019
"How can I modify a resource in Slate using the `patchesStrategicMerge` option?
","If more advanced patching is needed of a resources or field does not support the strategic merge process, use patchesJson6902 instead of patchesStrategicMerge as this provides for more operations and control over the merge process. Additionally, one may also be able to use a configuration transformation to modify the resulting resources. While not utilized the the helloWorld kustomize application, these are illustrated in some of the other examples it the same repository.

Review of the production environment is left as an exercise for the reader.",4.321899337096945
"How can I modify a resource in Slate using the `patchesStrategicMerge` option?
","there are a few meta information blocks present: namePrefix, commonLabels, and commonAnnotations. Additionally, we see that there is a patch specified with the patchesStrategicMerge block where a patch file to be merged is specified:

$ cat overlays/staging/map.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: the-map
data:
  altGreeting: ""Have a pineapple!""
  enableRisky: ""true""",4.0697234177284365
"How can I modify a resource in Slate using the `patchesStrategicMerge` option?
","In this case, the patch will use a merge strategy to change the data entries for the specified apiVersion/kind/metadata.name object. Running kustomize build on the staging environment shows the result of the patch as well as the added meta:

$ kustomize build overlays/staging
apiVersion: v1
data:
  altGreeting: Have a pineapple!
  enableRisky: ""true""
kind: ConfigMap
metadata:
  annotations:
    note: Hello, I am staging!
  labels:
    app: hello
    org: acmeCorporation
    variant: staging
  name: staging-the-map
---
apiVersion: v1
kind: Service
...",4.02944436296836
"What are the penalties for violating export control regulations related to open source software?
","All software used on OLCF computers must be appropriately acquired and used according to the appropriate software license agreement. Possession, use, or transmission of illegally obtained software is prohibited. Likewise, users shall not copy, store, or transfer copyrighted software, except as permitted by the owner of the copyright. Only export-controlled codes approved by the Export Control Office may be run by parties with sensitive data agreements.

Users must not intentionally introduce or use malicious software such as computer viruses, Trojan horses, or worms.",4.244346488486222
"What are the penalties for violating export control regulations related to open source software?
","All software used on OLCF computers must be appropriately acquired and used according to the appropriate software license agreement. Possession, use, or transmission of illegally obtained software is prohibited. Likewise, users shall not copy, store, or transfer copyrighted software, except as permitted by the owner of the copyright. Only export-controlled codes approved by the Export Control Office may be run by parties with sensitive data agreements.

<string>:645: (INFO/1) Duplicate implicit target name: ""malicious software"".",4.23325797028382
"What are the penalties for violating export control regulations related to open source software?
","Applicants who appear on a restricted foreign country listing in section 15 CFR 740.7 License Exceptions for Computers are denied access based on US Foreign Policy. The countries cited are Cuba, Iran, North Korea, Sudan, and Syria. Additionally, no work may be performed on OLCF computers on behalf of foreign nationals from these countries.

Users may not deliberately interfere with other users accessing system resources.",4.178292640821498
"What is the name of the port being exposed by the service?
","Note that a NodePort value will automatically be given by the service controller.

Your service can then be accessed by the scheme apps.{cluster}.ccs.ornl.gov:{nodePort}.

In this example, if the service was running on marble, I could access it with apps.marble.ccs.ornl.gov:30298",4.25361692286391
"What is the name of the port being exposed by the service?
","name: the-container
        ports:
        - containerPort: 8080",4.228769730212138
"What is the name of the port being exposed by the service?
","In general, using FQDNs to access a service is more convenient. If your service communicates over HTTP or HTTPS, you can set up a https://docs.olcf.ornl.gov/systems/services.html#slate_routes to achieve this. If your service uses another protocol, you can use https://docs.olcf.ornl.gov/systems/services.html#slate_nodeports.

NodePort is a type of Service that reserves a port across the cluster that can route traffic to your pods. This is more flexible than a Route and can handle any TCP or UDP traffic.",4.224263373433325
"What is the type of service described in the example?
","Here is an example service definition:

apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    name: my-app
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080

This definition tells Kubernetes that all pods with the label ""my-app"" are associated with this service. Any traffic to the service should be distributed among these pods.

The port parameter contains what port the service listens on, and the targetPort parameter contains the port to which the service forwards connections.",4.089368522792858
"What is the type of service described in the example?
","For example, let's look at service that was created in the https://docs.olcf.ornl.gov/systems/nodeport.html#slate_services document. This document assumes that it was deployed to the my-project project.

If you run oc get services, you should see your service in the list.

$ oc get services
NAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)     AGE
my-service            ClusterIP   172.25.170.246   <none>        8080/TCP    8s

Then, get some information about the service with oc describe service my-service.",4.086575453013966
"What is the type of service described in the example?
","In Kubernetes, a Service is an internal load balancer which identifies a set of pods and can proxy traffic to them. This set of pods is determined by a label selector.

A service is a stable way of accessing a set of pods, which are ephemeral.

When a service is created, it is granted a ClusterIP, which is an IP address internal to the Kubernetes cluster. Other pods can use this ClusterIP to access the service.

Here is an example service definition:",4.02313488336827
"What is the name of the library that has a size of 5.2M and is located in the lmp_libs directory on Frontier?
","liblzma.so.5 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/xz-5.2.5-zwra4gpckelt5umczowf3jtmeiz3yd7u/lib/liblzma.so.5 (0x00007fffdb82a000)
    libiconv.so.2 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libiconv-1.16-coz5dzhtoeq5unhjirayfn2xftnxk43l/lib/libiconv.so.2 (0x00007fffdb52e000)
    librocfft-device-0.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-0.so.0 (0x00007fffa11a0000)
    librocfft-device-1.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-1.so.0 (0x00007fff6491b000)",4.067093652799559
"What is the name of the library that has a size of 5.2M and is located in the lmp_libs directory on Frontier?
","liblzma.so.5 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/xz-5.2.5-zwra4gpckelt5umczowf3jtmeiz3yd7u/lib/liblzma.so.5 (0x00007fffdb82a000)
    libiconv.so.2 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libiconv-1.16-coz5dzhtoeq5unhjirayfn2xftnxk43l/lib/libiconv.so.2 (0x00007fffdb52e000)
    librocfft-device-0.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-0.so.0 (0x00007fffa11a0000)
    librocfft-device-1.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-1.so.0 (0x00007fff6491b000)",4.065482047029738
"What is the name of the library that has a size of 5.2M and is located in the lmp_libs directory on Frontier?
","Frontier is connected to Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Data will not be automatically transferred from Alpine to Orion. Frontier also has access to the center-wide NFS-based filesystem (which provides user and project home areas). Each compute node has two 1.92TB Non-Volatile Memory storage devices. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-data-storage for more",4.020428461650084
"What is the benefit of using SPI resources for my project?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads. Because of this, many processes used within the SPI are very similar to those used for standard non-SPI. This page lists the differences you may see when using OLCF resources to execute SPI resources.",4.320509311379282
"What is the benefit of using SPI resources for my project?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.  This page lists the differences you may see when using OLCF resources to execute SPI workflows.  The page will also point to sections within the site where more information on standard non-SPI use can be found.",4.294300794449982
"What is the benefit of using SPI resources for my project?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.

The SPI provides access to the OLCF's Summit resource for compute.  To safely separate SPI and non-SPI workflows, SPI workflows must use a separate login node named https://docs.olcf.ornl.gov/systems/citadel_user_guide.html#Citadel<spi-compute-citadel>.  Citadel provides a login node specifically for SPI workflows.",4.224428097807126
"Can I request a reservation for a specific backend on IBM Quantum Services?
","IBM Quantum Services provides access to more than 20 currently available quantum systems (known as backends).  IBM's quantum processors are made up of superconducting transmon qubits, and users can utilize these systems via the universal, gate-based, circuit model of quantum computation.  Additionally, users have access to 5 different types of simulators, simulating from 32 up to 5000 qubits to represent different aspects of the quantum backends.",4.299925498805473
"Can I request a reservation for a specific backend on IBM Quantum Services?
","User can submit jobs to IBM Quantum backends both via a fair-sharing queue system as well as via priority reservation system.  As discussed below, the dynamic fair-sharing queue system determines the queuing order of jobs so as to fairly balance system time between access providers, of which the OLCF QCUP is only one.  Because of this, the order of when a user's job in the fair-share queue will run varies dynamically, and can't be predicted. In light of this, for time-critical applications or iterative algorithms, IBM Quantum recommends users consider making a priority reservation.",4.282519569396756
"Can I request a reservation for a specific backend on IBM Quantum Services?
","Access to the IBM Quantum Computing queues, reservations, and simulators can be obtained via multiple methods -- either through the https://docs.olcf.ornl.gov/systems/ibm_quantum.html#cloud <ibm-cloud> or https://docs.olcf.ornl.gov/systems/ibm_quantum.html#locally <ibm-local>.",4.272676728673208
"How do I inspect the available versions of a compiler family on Summit?
","The following compilers are available on Summit:

XL: IBM XL Compilers (loaded by default)

LLVM: LLVM compiler infrastructure

PGI: Portland Group compiler suite

NVHPC: Nvidia HPC SDK compiler suite

GNU: GNU Compiler Collection

NVCC: CUDA C compiler

PGI was bought out by Nvidia and have rebranded their compilers, incorporating them into the NVHPC compiler suite. There will be no more new releases of the PGI compilers.",4.250657051736017
"How do I inspect the available versions of a compiler family on Summit?
","The E4S software list is installed along side the existing software on Summit and can be access via lmod modulefiles.

To access the installed software, load the desired compiler via:

module load < compiler/version >
.. ie ..
module load gcc/9.1.0
.. or ..
module load gcc/7.5.0

Then use module avail to see the installed list of packages.

List of installed packages on Summit for E4S release 21.08:",4.242832749977284
"How do I inspect the available versions of a compiler family on Summit?
","Upon login, the default versions of the XL compiler suite and Spectrum Message Passing Interface (MPI) are added to each user's environment through the modules system. No changes to the environment are needed to make use of the defaults.

Multiple versions of each compiler family are provided, and can be inspected using the modules system:

summit$ module -t avail gcc
/sw/summit/spack-envs/base/modules/site/Core:
gcc/7.5.0
gcc/9.1.0
gcc/9.3.0
gcc/10.2.0
gcc/11.1.0

type char is unsigned by default",4.240200840806408
"What information do I need to provide to start the VampirServer?
","$ module load vampir

#Start the VampirServer

$ vampirserver start -- -P <projectID> -w <walltime> -q <queue>

Once you have successfully authenticated, you will need the information printed on the https://docs.olcf.ornl.gov/systems/Vampir.html#terminal window. <vampserpw> That includes:

Node ID

Port Number

password

Once the VampirServer is started, in a fresh terminal window on your Local machine you can then initiate the port forward command. This will open a secure tunnel from your local machine to the backend server running VampirServer.

Port Forwarding",4.301447033179066
"What information do I need to provide to start the VampirServer?
","After connecting to Summit using X11 forwarding you will need to load the Vampir module and start the https://docs.olcf.ornl.gov/systems/Vampir.html#VampirServer. <vamps>

$ module load vampir

$ vampirserver start -- -P <projectID> -w <walltime> -q <queue>

#Example: vampirserver start -- -P 123456 -w 60 -q debug



Successful VampirServer startup message should appear in terminal window. You will need this information!",4.2773218337872905
"What information do I need to provide to start the VampirServer?
","Launch the Vampir GUI on your local machine

Similar to how we have connected Vampir to the VampirServer in the https://docs.olcf.ornl.gov/systems/Vampir.html#previous section, <vampauth> you will follow the same steps except you will use localhost for the server name and your local machine port number you selected. Press 'Connect' and this should open the authentication window where you will enter your UserID and the https://docs.olcf.ornl.gov/systems/Vampir.html#VampirServer password <vampserpw> printed after a successful connection.",4.2649864148505205
"Can I use the `oc` command to get the details of a specific GitLab runner pod on Slate?
","Jenkins

OpenShift Pipelines

GitLab Runners

Deploying a GitLab Runner into a Slate project may be found in the https://docs.olcf.ornl.gov/systems/overview.html#slate_gitlab_runners document which leverages a localized Slate Helm Chart. The GitLab Pipelines documentation as well as GitLab CI/CD Examples provide more details on GitLab Runner capabilities and usage.

Jenkins

For Jenkins deployment, a Slate Helm Chart is planned to be released. Documentation concerning Jenkins Pipelines as well as Jenkins Pipeline Tutorials are available.

OpenShift Pipelines",4.2741545133987415
"Can I use the `oc` command to get the details of a specific GitLab runner pod on Slate?
","Once the pod has a Status of ""Running"", navigate to the GitLab web interface and ensure that the runner has registered to either the group or repository per the token given, and that it is listed as available.

Finally, if batch scheduler integration was enabled, one can verify functionality in the pod with:

$ oc exec -it pods/gitlab-runner-gitlab-runner-687486d94-lpmhs -- bash
bash-5.0$ squeue
          JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
           9951 ...",4.225041402523325
"Can I use the `oc` command to get the details of a specific GitLab runner pod on Slate?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.215427368831441
"What are some common pitfalls to avoid when programming for Frontier's AMD GPUs?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.37080952224566
"What are some common pitfalls to avoid when programming for Frontier's AMD GPUs?
",Programming Environment  Frontier utilizes the Cray Programming Environment.  Many aspects including LMOD are similar to Summit's environment.  But Cray's compiler wrappers and the Cray and AMD compilers are worth noting.  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more information.  AMD GPUs  Each frontier node has 4 AMD MI250X accelerators with two Graphic Compute Dies (GCDs) in each accelerator. The system identifies each GCD as an independent device (so for simplicity we use the term GPU when we talk about a GCD) for a total of 8,4.31373429639411
"What are some common pitfalls to avoid when programming for Frontier's AMD GPUs?
","The below code block should not be run on Frontier, as it causes problems for the subsequent code blocks further below. With recent updates to CuPy, peer access is enabled by default, which ""passes"" the below error. This causes problems with AMD GPUs, resulting in inaccurate data.",4.224354007410028
"How can I request GPU MPS and set SMT levels for a job in Summit using a single command?
","""gpumps smt1"" | Used to request GPU Multi-Process Service (MPS) and to set SMT (Simultaneous Multithreading) levels. Only one ""#BSUB alloc_flags"" command is recognized so multiple alloc_flags options need to be enclosed in quotes and space-separated. Setting gpumps enables NVIDIA’s Multi-Process Service, which allows multiple MPI ranks to simultaneously access a GPU. Setting smtn (where n is 1, 2, or 4) sets different SMT levels. To run with 2 hardware threads per physical core, you’d use smt2. The default level is smt4. |",4.387462615690291
"How can I request GPU MPS and set SMT levels for a job in Summit using a single command?
","NVIDIA recommends using the EXCLUSIVE_PROCESS compute mode (the default on Summit) when using the Multi-Process Service, but both MPS and the compute mode can be changed by providing both values: -alloc_flags ""gpumps gpudefault"".",4.253638763898245
"How can I request GPU MPS and set SMT levels for a job in Summit using a single command?
","The Multi-Process Service (MPS) enables multiple processes (e.g. MPI ranks) to concurrently share the resources on a single GPU. This is accomplished by starting an MPS server process, which funnels the work from multiple CUDA contexts (e.g. from multiple MPI ranks) into a single CUDA context. In some cases, this can increase performance due to better utilization of the resources. As mentioned in the Common bsub Options section above, MPS can be enabled with the -alloc_flags ""gpumps"" option to bsub. The following screencast shows an example of how to start an MPS server process for a job:",4.213055366487702
"How do I ensure that my data is secure on Summit?
",For Summit:,4.240776476793002
"How do I ensure that my data is secure on Summit?
","Due to the large amount of data on the filesystems, we strongly urge you to start transferring your data now, and do not wait until later in the year.

Jan 01, all remaining Alpine data will be PERMANENTLY DELETED.  Do not wait to move needed data.



Summit will be returned to service early 2024.

Projects awarded a 2024 Summit allocation will be able to log into Summit and submit batch jobs once the system has been made available.",4.208742586305555
"How do I ensure that my data is secure on Summit?
","Summit is connected to an IBM Spectrum Scale™ filesystem providing 250PB of storage capacity with a peak write speed of 2.5 TB/s. Summit also has access to the center-wide NFS-based filesystem (which provides user and project home areas) and has access to the center’s High Performance Storage System (HPSS) for user and project archival storage.

Summit is running Red Hat Enterprise Linux (RHEL) version 8.2.",4.185685189391356
"What is the location of the user scratch space?
","Each project has a World Work directory that resides in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. World Work areas can be accessed by all users of the system and are intended for sharing of data between projects. World Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but rather should be",4.203221989115949
"What is the location of the user scratch space?
","Each project is granted a Project Work directory; these reside in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. Project Work directories can be accessed by all members of a project and are intended for sharing data within a project. Project Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but",4.16423774104466
"What is the location of the user scratch space?
","Project members get an individual Member Work directory for each associated project; these reside in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. Member Work areas are not shared with other users of the system and are intended for project data that the user does not want to make available to other users. Member Work directories are provided commonly across all systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular",4.146834581289512
"What is the default TERM arg2 in Paraview for Unix?
","<Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Windows"">
            <Set name=""TERM_PATH"" value=""cmd"" />
            <Set name=""TERM_ARG1"" value=""/C"" />
            <Set name=""TERM_ARG2"" value=""start"" />
            <Set name=""TERM_ARG3"" value="""" />
            <Set name=""SSH_PATH"" value=""plink.exe"" />
          </Case>
          <Case value=""Unix"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.246102370013636
"What is the default TERM arg2 in Paraview for Unix?
","<Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Windows"">
            <Set name=""TERM_PATH"" value=""cmd"" />
            <Set name=""TERM_ARG1"" value=""/C"" />
            <Set name=""TERM_ARG2"" value=""start"" />
            <Set name=""TERM_ARG3"" value="""" />
            <Set name=""SSH_PATH"" value=""plink.exe"" />
          </Case>
          <Case value=""Unix"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.246102370013636
"What is the default TERM arg2 in Paraview for Unix?
","</Option>
      </Options>
      <Command exec=""$TERM_PATH$"" delay=""5"">
        <Arguments>
          <Argument value=""$TERM_ARG1$""/>
          <Argument value=""$TERM_ARG2$""/>
          <Argument value=""$TERM_ARG3$""/>
          <Argument value=""$SSH_PATH$""/>
          <Argument value=""-t""/>
          <Argument value=""-R""/>
          <Argument value=""$PV_SERVER_PORT$:localhost:$PV_SERVER_PORT$""/>
          <Argument value=""$USER$@$HOST$""/>
          <Argument value=""/sw/andes/paraview/pvsc/ORNL/login_node.sh""/>
          <Argument value=""$NUM_NODES$""/>",4.193679135918288
"What is the benefit of using multiple physical CPU cores in Crusher?
","If CAAR or ECP teams require a temporary exception to this policy, please email help@olcf.ornl.gov with your request and it will be given to the OLCF Resource Utilization Council (RUC) for review.

This section describes how to map processes (e.g., MPI ranks) and process threads (e.g., OpenMP threads) to the CPUs and GPUs on Crusher. The https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#crusher-compute-nodes diagram will be helpful when reading this section to understand which physical CPU cores (and hardware threads) your processes and threads run on.",4.242485847138923
"What is the benefit of using multiple physical CPU cores in Crusher?
","Crusher contains a total of 768 AMD MI250X. The AMD MI250X has a peak performance of 53 TFLOPS in double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 26.5 TFLOPS (double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in both directions simultaneously).



To connect to Crusher, ssh to crusher.olcf.ornl.gov. For example:

$ ssh <username>@crusher.olcf.ornl.gov",4.177424678171131
"What is the benefit of using multiple physical CPU cores in Crusher?
","Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.",4.170251115373439
"How many nodes are requested for the job?
","node-hours = nodes requested * ( batch job endtime - batch job starttime )

Where batch job starttime is the time the job moves into a running state, and batch job endtime is the time the job exits a running state.",4.307971704908598
"How many nodes are requested for the job?
","A batch job's usage is calculated solely on requested nodes and the batch job's start and end time. The number of cores actually used within any particular node within the batch job is not used in the calculation. For example, if a job requests (6) nodes through the batch script, runs for (1) hour, uses only (2) CPU cores per node, the job will still be charged for 6 nodes * 1 hour = 6 node-hours.

Viewing Usage",4.296871765536267
"How many nodes are requested for the job?
","Compute nodes are only allocated to one job at a time; they are not shared. This is why users request nodes (instead of some other resource such as cores or GPUs) in batch jobs and is why projects are charged based on the number of nodes allocated multiplied by the amount of time for which they were allocated. Thus, a job using only 1 core on each of its nodes is charged the same as a job using every core and every GPU on each of its nodes.



<string>:2475: (INFO/1) Duplicate implicit target name: ""debugging"".",4.231465906442248
"How can I ensure that my job is running in low-noise mode on Frontier?
","Frontier uses low-noise mode and core specialization (-S flag at job allocation, e.g., sbatch).  Low-noise mode constrains all system processes to core 0.  Core specialization (by default, -S 8) reserves the first core in each L3 region.  This prevents the user running on the core that system processes are constrained to.  This also means that there are only 56 allocatable cores by default instead of 64. Therefore, this modifies the simplified node layout to:

Simplified Frontier node architecture diagram (low-noise mode)",4.304348344653044
"How can I ensure that my job is running in low-noise mode on Frontier?
","By default, Frontier reserves the first core in each L3 cache region. Frontier uses low-noise mode, which constrains all system processes to core 0. Low-noise mode cannot be disabled by users. In addition, Frontier uses SLURM core specialization (-S 8 flag at job allocation time, e.g., sbatch) to reserve one core from each L3 cache region, leaving 56 allocatable cores. Set -S 0 at job allocation to override this setting.",4.250504615048906
"How can I ensure that my job is running in low-noise mode on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.182969350508285
"Are there any restrictions on the use of resources allocated through the Director's Discretionary (DD) Project?
","DD – Director’s Discretion (DD) projects are dedicated to leadership computing preparation, INCITE and ALCC scaling, and application performance to maximize scientific application efficiency and productivity on leadership computing platforms. The OLCF Resource Utilization Council, as well as independent referees, review and approve all DD requests. Applications are accepted year-round via the OLCF Director's Discretion Project Application. Select ""OLCF Director's Discretionary Project"" from the drop down menu to begin.",4.395803129762687
"Are there any restrictions on the use of resources allocated through the Director's Discretionary (DD) Project?
","Request a Director's Discretionary (DD) Project Use this form to request a Director's Discretionary (DD) Project. Select ""OLCF Director's Discretionary Program"" from the drop down menu.

Principal Investigator Agreement The Oak Ridge Leadership Computing Facility must have a signed copy of this form on file from the project's principal investigator(s) before any accounts for the project will be processed.",4.3239332189650295
"Are there any restrictions on the use of resources allocated through the Director's Discretionary (DD) Project?
","Access to OLCF resources is limited to approved projects and their users. The type of project (INCITE, ALCC, or Director's Discretion) will determine the application and review procedures. *Quarterly reports are required from industrial Director's Discretion projects only.",4.255059327074001
"Can I use the conda create command to create an environment for Cupy on Andes?
","Summit

.. code-block:: bash

   $ conda create -p /ccs/proj/<project_id>/<user_id>/envs/summit/cupy-summit python=3.10

Frontier

.. code-block:: bash

   $ python3 -m venv /ccs/proj/<project_id>/<user_id>/envs/frontier/cupy-frontier

Andes

.. code-block:: bash

   $ conda create -p /ccs/proj/<project_id>/<user_id>/envs/andes/cupy-andes python=3.10",4.33827115552757
"Can I use the conda create command to create an environment for Cupy on Andes?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.

This guide teaches you how to build CuPy from source into a custom virtual environment. On Summit and Andes, this is done using conda, while on Frontier this is done using venv.

In this guide, you will:

Learn how to install CuPy

Learn the basics of CuPy

Compare speeds to NumPy

OLCF Systems this guide applies to:

Summit

Frontier

Andes",4.318808634730479
"Can I use the conda create command to create an environment for Cupy on Andes?
","Andes

.. code-block:: bash

   $ source activate /ccs/proj/<project_id>/<user_id>/envs/andes/h5pympi-andes

Frontier

.. code-block:: bash

   $ source activate /ccs/proj/<project_id>/<user_id>/envs/frontier/h5pympi-frontier

Now that you have a fresh environment, you will next install mpi4py from source into your new environment. To make sure that you are building from source, and not a pre-compiled binary, use pip:

Summit

.. code-block:: bash

   $ MPICC=""mpicc -shared"" pip install --no-cache-dir --no-binary=mpi4py mpi4py

Andes

.. code-block:: bash",4.300202107669742
"How can I run the osubenchmarks on Summit without using the preinstalled images?
","$ module load otf2
$ module load cubew

#!/bin/bash
# Begin LFS Directives
#BSUB -P ABC123        #Project Account
#BSUB -W 3:00          #Walltime
#BSUB -nnodes 1        #Number of Nodes
#BSUB -J RunSim123     #Job Name
#BSUB -o RunSim123.%J  #Job System Out
#BSUB -e RunSim123.%J  #Job System Error Out

cd <path to instrumented code>

jsrun -n 1 ./<binary to run>

For more information on launching jobs on Summit, please see the https://docs.olcf.ornl.gov/systems/Scorep.html#Running Jobs <running-jobs> section of the Summit User Guide.",4.09774641611157
"How can I run the osubenchmarks on Summit without using the preinstalled images?
","Login to https://docs.olcf.ornl.gov/systems/Scorep.html#Summit <connecting-to-olcf>: ssh <user_id>@summit.olcf.ornl.gov

Instrument your code with Score-P

Perform a measurement run with profiling enabled

Perform a profile analysis with CUBE or cube_stat

Use scorep-score to define a filter

Perform a measurement run with tracing enabled and the filter applied

Perform in-depth analysis on the trace data with Vampir",4.089261399009398
"How can I run the osubenchmarks on Summit without using the preinstalled images?
","You can view the Dockerfiles used to build the MPI base image at the code.ornl.gov repository. These Dockerfiles are buildable on Summit yourself by cloning the repository and running the ./build in the individual directories in the repository. This allows you the freedom to modify these base images to your own needs if you don't need all the components in the base images. You may run into the cgroup memory limit when building so kill the podman process, log out, and try running the build again if that happens when building.",4.086028689089459
"How do I install the OpenShift CLI tool on MacOS using Homebrew?
","It is a single binary that can be downloaded from a number of places (the choice is yours):

Direct from the cluster (preferred):

Marble Command Line Tools

Onyx Command Line Tools

Homebrew on MacOS (need Homebrew setup first):

The Homebrew package is not always kept up to date with the latest version of OpenShift so some client features may not be available

$ brew install openshift-cli

RHEL/CentOS (requires openshift-origin repo):

$ yum install origin-clients

From Source

https://github.com/openshift/oc",4.472451250605118
"How do I install the OpenShift CLI tool on MacOS using Homebrew?
","$ brew install helm

Or can be pulled from the Helm Release Page. If downloading from the GitHub release page, you can copy this executable into /usr/local/bin to add it to $PATH.

NOTE: One nice feature of Helm is that it uses the underlying authentication credentials used with oc, so once you login with oc login, the helm client will authenticate automatically.

Once oc and helm are setup and you are logged in with oc login, test Helm:

$ helm ls",4.1731494327944025
"How do I install the OpenShift CLI tool on MacOS using Homebrew?
","The following items need to be established before deploying applications on Slate systems (Marble or Onyx OpenShift clusters).

Follow https://docs.olcf.ornl.gov/systems/helm_prerequisite.html#slate_getting_started if you do not already have a project/namespace established on Onyx or Marble.

It is recommended to use Helm version 3.

Helm enables application deployment via Helm Charts. The high level Helm Architecture docs are also a great reference for understanding Helm.

Like oc, Helm is a single binary executable.

This can be installed on macOS with Homebrew :

$ brew install helm",4.17278518021103
"Can I access the Project Archive from outside the OLCF?
","To facilitate collaboration among researchers, the OLCF provides (3) distinct types of project-centric archival storage areas: Member Archive directories, Project Archive directories, and World Archive directories. These directories should be used for storage of data not immediately needed in either the Project Home (NFS) areas or Project Work (Alpine) areas and to serve as a location to store backup copies of project-related files.",4.296189606916362
"Can I access the Project Archive from outside the OLCF?
","Project Archive directories may only be accessed via utilities called HSI and HTAR. For more information on using HSI or HTAR, see the https://docs.olcf.ornl.gov/systems/project_centric.html#data-hpss section.",4.294085094471305
"Can I access the Project Archive from outside the OLCF?
","not be retained in this file system for long, but rather should be migrated to Project Home or Project Archive space as soon as the files are not actively being used. If a file system associated with Project Work storage is nearing capacity, the OLCF may contact the PI of the project to request that he or she reduce the size of the Project Work directory. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for more details on applicable quotas, backups, purge, and retention timeframes.",4.276283509351288
"What is the difference between ""RAPIDS basic execution"" and ""simultaneous job steps""?
","Note the ""RAPIDS basic execution"" option is for illustrative purposes and not recommended to run RAPIDS on Summit since it underutilizes resources. If your RAPIDS code is single GPU, consider Jupyter or the concurrent job steps option.

<string>:3: (INFO/1) Duplicate explicit target name: ""simultaneous job steps"".

In cases when a set of time steps need to be processed by single-GPU RAPIDS codes and each time step fits comfortably in GPU memory, it is recommended to execute simultaneous job steps.

The following script provides a general pattern to run job steps simultaneously with RAPIDS:",4.43238748078296
"What is the difference between ""RAPIDS basic execution"" and ""simultaneous job steps""?
","The output shows that each independent process ran on its own CPU core and GPU on the same single node. To show that the ranks ran simultaneously, date was called before each job step and a 20 second sleep was added to the end of the hello_jobstep program. So the output also shows that the first job step was submitted at :45 and the subsequent job steps were all submitted between :46 and :52. But because each hello_jobstep sleeps for 20 seconds, the subsequent job steps must have all been running while the first job step was still sleeping (and holding up its resources). And the same argument",4.0941538323426405
"What is the difference between ""RAPIDS basic execution"" and ""simultaneous job steps""?
","To execute multiple job steps concurrently, standard UNIX process backgrounding can be used by adding a & at the end of the command. This will return control to the job script and execute the next command immediately, allowing multiple job launches to start at the same time. The jsruns will not share core/gpu resources in this configuration. The batch node allocation is equal to the sum of those of each jsrun, and the total walltime must be equal to or greater than that of the longest running jsrun task.",4.083737022485596
"How can I check the status of a job on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.243858293463526
"How can I check the status of a job on Summit?
",For Summit:,4.215657377537371
"How can I check the status of a job on Summit?
","For bug reports or suggestions, please email help@olcf.ornl.gov.

Request a Summit allocation

bsub -W 10 -nnodes 2 -P $OLCF_PROJECT_ID -Is $SHELL

Load the job-step-viewer module

module load job-step-viewer

Test out a jsrun line by itself, or provide an executable as normal

jsrun -n12 -r6 -c7 -g1 -a1 -EOMP_NUM_THREADS=7 -brs

Visit the provided URL

https://jobstepviewer.olcf.ornl.gov/summit/871957-1",4.179004430336736
"What is the estimated time required for the trace?
","The first line of the output gives an estimation of the total size of the trace, aggregated over all processes. This information is useful for estimating the space required on disk. In the given example, the estimated total size of the event trace is 40GB. The second line prints an estimation of the memory space required by a single process for the trace. Since flushes heavily disturb measurements, the memory space that Score-P reserves on each process at application start must be large enough to hold the process’ trace in memory in order to avoid flushes during runtime.",4.215910490926294
"What is the estimated time required for the trace?
","Estimated aggregate size of event trace:                   40GB
Estimated requirements for largest trace buffer (max_buf): 10GB
Estimated memory requirements (SCOREP_TOTAL_MEMORY):       10GB
(warning: The memory requirements can not be satisfied by Score-P to avoid
intermediate flushes when tracing. Set SCOREP_TOTAL_MEMORY=4G to get the
maximum supported memory or reduce requirements using USR regions filters.)",4.177773568292257
"What is the estimated time required for the trace?
","$ export SCOREP_ENABLE_TRACING=true

Since tracing measurements acquire significantly more output data than profiling, we need to design a filter to remove some of the most visited calls within your instrumented code. There is a tool developed by Score-P that allows us to estimate the size of the trace file (OTF2) based on information attained from the profiling generated cube file.

To gather the needed information to design a filter file, first run scorep-score:

$ scorep-score -r <profile cube dir>/profile.cubex",3.995168559957627
"How can I switch the orientation of the colorbar in Paraview?
","# Set Colorbar Properties
display.SetScalarBarVisibility(curr_view,True) # Show bar
scalarBar = GetScalarBar(cmap, curr_view)      # Get bar's properties
scalarBar.WindowLocation = 'Any Location'       # Allows free movement
scalarBar.Orientation = 'Horizontal'           # Switch from Vertical to Horizontal
scalarBar.Position = [0.15,0.80]               # Bar Position in [x,y]
scalarBar.LabelFormat = '%.0f'                 # Format of tick labels
scalarBar.RangeLabelFormat = '%.0f'            # Format of min/max tick labels
scalarBar.ScalarBarLength = 0.7                # Set length of bar",4.053801646286076
"How can I switch the orientation of the colorbar in Paraview?
","After setting up and installing ParaView, you can connect to OLCF systems remotely to visualize your data interactively through ParaView's GUI. To do so, go to File→Connect and select either ORNL Andes or ORNL Summit (provided they were successfully imported -- as outlined in https://docs.olcf.ornl.gov/systems/paraview.html#paraview-install-setup). Next, click on Connect and change the values in the Connection Options box.",4.040246297501642
"How can I switch the orientation of the colorbar in Paraview?
","Alternatively, the ParaView installations in /sw/andes/paraview (i.e., the paraview/5.9.1-egl and paraview/5.9.1-osmesa modules) can also be loaded to avoid this issue.



The ParaView at OLCF Tutorial highlights how to get started on Andes with example datasets.

The Official ParaView User's Guide and the Python API Documentation contain all information regarding the GUI and Python interfaces.

A full list of ParaView Documentation can be found on ParaView's website.

The ParaView Wiki contains extensive information about all things ParaView.",3.9963041150232455
"Can I manually adjust the file striping on Orion?
","On Alpine, there was no user-exposed concept of file striping, the process of dividing a file between the storage elements of the filesystem. Orion uses a feature called Progressive File Layout (PFL) that changes the striping of files as they grow. Because of this, we ask users not to manually adjust the file striping. If you feel the default striping behavior of Orion is not meeting your needs, please contact help@olcf.ornl.gov.

As with Alpine, files older than 90 days are purged from Orion.  Please plan your data management and lifecycle at OLCF before generating the data.",4.458034644255094
"Can I manually adjust the file striping on Orion?
","To keep the Spectrum Scale file system exceptionally performant, files that have not been accessed in the project and user areas are purged at the intervals shown in the table above. Please make sure that valuable data is moved off of these systems regularly. See https://docs.olcf.ornl.gov/systems/policies.html#data-hpss. for information about using the HSI and HTAR utilities to archive data on HPSS. Just to note that when you read a file, then the 90 days counter restarts.",4.058597010458998
"Can I manually adjust the file striping on Orion?
","On Summit, there is no concept of striping from the user point of view, the user uses the Alpine storage without the need to declare the striping for files/directories. The GPFS will handle the workload, the file system was tuned during the installation.",4.045165599567594
"How do I optimize my code for performance on Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.261747823342169
"How do I optimize my code for performance on Andes?
",For Andes:,4.259358702808116
"How do I optimize my code for performance on Andes?
","For example, this is how you would modify the Andes profile to use the gpu partition:

Under Andes' ""Launch Profiles"":

Click on ""New Profile""

Name the profile something like ""gpu"" (arbitrary)

Click on ""Parallel""

Check ""Launch Parallel Engine""

Set ""Launch Method"" to sbatch/srun (required)

Set ""Partition/Pool/Queue"" to gpu (required)

Set default number of processors to 28 (max without hyperthreading) (arbitrary)

Set default number of nodes to 1 (arbitrary)

Set default ""Bank/Account"" to your OLCF project with Andes allocation

Set a default ""Time Limit"" in format of (HH:MM:SS)",4.186076365707016
"How many bytes are used for message size in all-reduce operations?
","755          8          8          8          0  Message size for all-reduce
       302  2.621E+05          4  1.302E+05  1.311E+05  Message size for broadcast
---------------------------------------------------------------------------------------",4.488780118430433
"How many bytes are used for message size in all-reduce operations?
","The first line of the output gives an estimation of the total size of the trace, aggregated over all processes. This information is useful for estimating the space required on disk. In the given example, the estimated total size of the event trace is 40GB. The second line prints an estimation of the memory space required by a single process for the trace. Since flushes heavily disturb measurements, the memory space that Score-P reserves on each process at application start must be large enough to hold the process’ trace in memory in order to avoid flushes during runtime.",3.9840643062243135
"How many bytes are used for message size in all-reduce operations?
","For best performance on the IBM Spectrum Scale filesystem, use large page aligned I/O and asynchronous reads and writes. The filesystem blocksize is 16MB, the minimum fragment size is 16K so when a file under 16K is stored, it will still use 16K of the disk. Writing files of 16 MB or larger, will achieve better performance. All files are striped across LUNs which are distributed across all IO servers.",3.965977153797789
"How can I set the number of OpenMP threads per core on Frontier?
","Because a Frontier compute node has two hardware threads available (2 logical cores per physical core), this enables the possibility of multithreading your application (e.g., with OpenMP threads). Although the additional hardware threads can be assigned to additional MPI tasks, this is not recommended. It is highly recommended to only use 1 MPI task per physical core and to use OpenMP threads instead on any additional logical cores gained when using both hardware threads.",4.387905224511699
"How can I set the number of OpenMP threads per core on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.310010001807016
"How can I set the number of OpenMP threads per core on Frontier?
","Now the output shows that each OpenMP thread ran on its own physical CPU core. More specifically (see the Frontier Compute Node diagram), OpenMP thread 000 of MPI rank 000 ran on logical core 001 (i.e., physical CPU core 01), OpenMP thread 001 of MPI rank 000 ran on logical core 002 (i.e., physical CPU core 02), OpenMP thread 000 of MPI rank 001 ran on logical core 009 (i.e., physical CPU core 09), and OpenMP thread 001 of MPI rank 001 ran on logical core 010 (i.e., physical CPU core 10) - as intended.

Third attempt - Using multiple threads per core",4.292323255523619
"How does the XNACK mode affect the types of kernels that can run on the MI250X GPU?
","The accessibility of memory from GPU kernels and whether pages may migrate depends three factors: how the memory was allocated; the XNACK operating mode of the GPU; whether the kernel was compiled to support page migration. The latter two factors are intrinsically linked, as the MI250X GPU operating mode restricts the types of kernels which may run.",4.485627582467293
"How does the XNACK mode affect the types of kernels that can run on the MI250X GPU?
","The accessibility of memory from GPU kernels and whether pages may migrate depends three factors: how the memory was allocated; the XNACK operating mode of the GPU; whether the kernel was compiled to support page migration. The latter two factors are intrinsically linked, as the MI250X GPU operating mode restricts the types of kernels which may run.",4.485627582467293
"How does the XNACK mode affect the types of kernels that can run on the MI250X GPU?
","Although XNACK is a capability of the MI250X GPU, it does require that kernels be able to recover from page faults. Both the ROCm and CCE HIP compilers will default to generating code that runs correctly with both XNACK enabled and disabled. Some applications may benefit from using the following compilation options to target specific XNACK modes.

hipcc --amdgpu-target=gfx90a or CC --offload-arch=gfx90a -x hip

Kernels are compiled to a single ""xnack any"" binary, which will run correctly with both XNACK enabled and XNACK disabled.",4.460310660627164
"What is the default core specialization in Frontier?
","By default, Frontier reserves the first core in each L3 cache region. Frontier uses low-noise mode, which constrains all system processes to core 0. Low-noise mode cannot be disabled by users. In addition, Frontier uses SLURM core specialization (-S 8 flag at job allocation time, e.g., sbatch) to reserve one core from each L3 cache region, leaving 56 allocatable cores. Set -S 0 at job allocation to override this setting.",4.227274629623056
"What is the default core specialization in Frontier?
","This example assumes the use of a core specialization of -S 0.  Because Frontier's default core specialization (-S 8) reserves the first core in each L3 region, the ""packed"" mode can be problematic because the 7 cores available in each L3 region won't necessarily divide evenly. This can lead to tasks potentially spanning multiple L3 regions with its assigned cores, which creates problems when Slurm tries to assign GPUs to a given task.",4.210145800484212
"What is the default core specialization in Frontier?
","This example assumes the use of a core specialization of -S 0.  Because Frontier's default core specialization (-S 8) reserves the first core in each L3 region, the ""packed"" mode can be problematic because the 7 cores available in each L3 region won't necessarily divide evenly. This can lead to tasks potentially spanning multiple L3 regions with its assigned cores, which creates problems when Slurm tries to assign GPUs to a given task.",4.210145800484212
"Can I transfer data between SPI projects?
","https://docs.olcf.ornl.gov/systems/index.html#Transfer needed data<spi-data-transfer> to the SPI filesystems.  The SPI resources mount filesystems unique to the SPI.  Needed data, code, and libraries must be transferred into the SPI using the SPI's Data Transfer Nodes.",4.265231675756923
"Can I transfer data between SPI projects?
","In order to help ensure data separation, each SPI user is given a unique userid for each project. SPI user names use the format: <userid>_<proj>_mde . For example: usera_prj123_mde.

SPI usernames will not overlap with usernames used in the non-SPI enclaves. Unlike non-SPI usernames, SPI usernames only exist in a single project. Users on multiple SPI projects will have a unique username for each SPI project.  You must specify your unique SPI username matching the target project when connecting.",4.1495051713482765
"Can I transfer data between SPI projects?
","The SPI provides separate https://docs.olcf.ornl.gov/systems/index.html#Data Transfer Nodes<spi-data-transfer> configured specifically for SPI workflows.  The nodes are not directly accessible for login but are accessible through the Globus tool.  The SPI DTNs mount the same Arx filesystem available on the SPI compute resources.  Globus is the preferred method to transfer data into and out of the SPI resources.

Please see the https://docs.olcf.ornl.gov/systems/index.html#Data Transfer Nodes<spi-data-transfer> section for more details.",4.144941629426236
"What is the purpose of the -m flag when using MPI?
","Instead, you can assign MPI ranks so that the L3 regions are filled in a ""packed"" (block) manner.  This mode will assign consecutive MPI tasks to the same L3 region (socket) until it is ""filled up"" or ""packed"" before assigning a task to a different socket.

Recall that the -m flag behaves like: -m <node distribution>:<socket distribution>.  Hence, the key setting to achieving the round-robin nature is the -m block:block flag, specifically the block setting provided for the ""socket distribution"". This ensures that the MPI tasks will be distributed in a packed manner.",4.314963587366046
"What is the purpose of the -m flag when using MPI?
","Recall that the -m flag behaves like: -m <node distribution>:<socket distribution>.  Hence, the key setting to achieving the round-robin nature is the -m block:cyclic flag, specifically the cyclic setting provided for the ""socket distribution"". This ensures that the MPI tasks will be distributed across sockets in a cyclic (round-robin) manner.

The below srun command will achieve the intended 8 MPI ""round-robin"" layout:

$ export OMP_NUM_THREADS=1
$ srun -N1 -n8 -c1 --cpu-bind=threads --threads-per-core=1 -m block:cyclic ./hello_mpi_omp | sort



Breaking down the srun command, we have:",4.300295756041624
"What is the purpose of the -m flag when using MPI?
","The intent with both of the following examples is to launch 8 MPI ranks across the node where each rank is assigned its own logical (and, in this case, physical) core.  Using the -m distribution flag, we will cover two common approaches to assign the MPI ranks -- in a ""round-robin"" (cyclic) configuration and in a ""packed"" (block) configuration. Slurm's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive method was used to request an allocation of 1 compute node for these examples: salloc -A <project_id> -t 30 -p <parition> -N 1",4.275887847905041
"How does the StatefulSet keep track of its objects?
","We will be deploying MongoDB with a StatefulSet. This is a special kind of deployment controller that is different from a normal Deployment in a few distinct ways and is primarily meant for for applications that rely on well known names for each pod.

We will create a Service with the StatefulSet because the StatefulSet controller requires a headless service in order to provide well-known DNS identifiers.",4.243100196790295
"How does the StatefulSet keep track of its objects?
","apiVersion: apps/v1
kind: StatefulSet
metadata:
  # statefulset name
  name: test-pod-stateful-set
spec:
  # number of replicas
  replicas: 3
  selector:
    # this sets the label the stateful set is looking for
    matchLabels:
      app: test-pod
  template:
    metadata:
      # labels are how the stateful set keep track of their objects. This sets a label on the pod
      labels:
        app: test-pod
    spec:
      containers:
        # container name
      - name: test-pod
        # using the base image",4.192741558400087
"How does the StatefulSet keep track of its objects?
","By design pods are ephemeral. They are meant to be something that can be killed with relatively little impact to the application. Since containers are ephemeral, we need a way to consume storage that is persistent and can hold data for our applications. One option that Kubernetes has for storing data is with Persistent Volumes. PersistentVolume objects are created by the cluster administrator and reference storage that is on a resilient storage platform such as NetApp. A user can request storage by creating a PersistentVolumeClaim which requests a desired size for a PersistentVolume. The",4.179991714951846
"What is the purpose of the help command in the ESSL module on Summit?
","summit$ module show essl
------------------------------------------------------------------------------------
   /sw/summit/modulefiles/core/essl/6.1.0-1:
------------------------------------------------------------------------------------
whatis(""ESSL 6.1.0-1 "")
prepend_path(""LD_LIBRARY_PATH"",""/sw/summit/essl/6.1.0-1/essl/6.1/lib64"")
append_path(""LD_LIBRARY_PATH"",""/sw/summit/xl/16.1.1-beta4/lib"")
prepend_path(""MANPATH"",""/sw/summit/essl/6.1.0-1/essl/6.1/man"")
setenv(""OLCF_ESSL_ROOT"",""/sw/summit/essl/6.1.0-1/essl/6.1"")
help([[ESSL 6.1.0-1

]])",4.236873405826115
"What is the purpose of the help command in the ESSL module on Summit?
","]])

When this module is loaded, the $OLCF_ESSL_ROOT environment variable holds the path to the ESSL installation, which contains the lib64/ and include/ directories:

summit$ module load essl
summit$ echo $OLCF_ESSL_ROOT
/sw/summit/essl/6.1.0-1/essl/6.1
summit$ ls $OLCF_ESSL_ROOT
FFTW3  READMES  REDIST.txt  include  iso-swid  ivps  lap  lib64  man  msg

The following screencast shows an example of linking two libraries into a simple program on Summit. https://vimeo.com/292015868",4.150091394051759
"What is the purpose of the help command in the ESSL module on Summit?
","The E4S software list is installed along side the existing software on Summit and can be access via lmod modulefiles.

To access the installed software, load the desired compiler via:

module load < compiler/version >
.. ie ..
module load gcc/9.1.0
.. or ..
module load gcc/7.5.0

Then use module avail to see the installed list of packages.

List of installed packages on Summit for E4S release 21.08:",4.1352398902407765
"Can I use the Trace tool to create Python scripts that use advanced Paraview features?
","One of the most convenient tools available in the GUI is the ability to convert (or ""trace"") interactive actions in ParaView to Python code. Users that repeat a sequence of actions in ParaView to visualize their data may find the Trace tool useful. The Trace tool creates a Python script that reflects most actions taken in ParaView, which then can be used by either PvPython or PvBatch (ParaView's Python interfaces) to accomplish the same actions. See section https://docs.olcf.ornl.gov/systems/paraview.html#paraview-command-line for an example of how to run a Python script using PvBatch on",4.3613640164942025
"Can I use the Trace tool to create Python scripts that use advanced Paraview features?
","To start tracing from the GUI, click on Tools→Start Trace. An options window will pop up and prompt for specific Trace settings other than the default. Upon starting the trace, any time you modify properties, create filters, open files, and hit Apply, etc., your actions will be translated into Python syntax. Once you are finished tracing the actions you want to script, click Tools→Stop Trace. A Python script should then be displayed to you and can be saved.",4.252004780582833
"Can I use the Trace tool to create Python scripts that use advanced Paraview features?
",of how to run a Python script using PvBatch on Andes and Summit.,4.158252281227471
"How can I disable instrumentation for a specific phase in TAU?
","export TAU_OPTIONS=“-optTauSelectFile=phase.tau”

Now when you instrument your application, the phase called phase 1 are the lines 300-327 of the file miniWeather_mpi.cpp. Every call will be instrumented. This could create signiificant overhead, thus you should be careful when you use it.

Create a file called phases.tau.

BEGIN_INSTRUMENT_SECTION
static phase name=""phase1"" file=""miniWeather_mpi.cpp"" line=300 to line=327
static phase name=""phase2"" file=""miniWeather_mpi.cpp"" line=333 to line=346
END_INSTRUMENT_SECTION

Declare the TAU_OPTIONS variable.",4.359537089365137
"How can I disable instrumentation for a specific phase in TAU?
","Declare the TAU_OPTIONS variable

export TAU_OPTIONS=“-optTauSelectFile=select.tau”

Now, the routine sort*(int *) is excluded from the instrumentation.

Create a file called phase.tau.

BEGIN_INSTRUMENT_SECTION
dynamic phase name=“phase1” file=“miniWeather_mpi.cpp” line=300 to line=327
END_INSTRUMENT_SECTION

Declare the TAU_OPTIONS variable.

export TAU_OPTIONS=“-optTauSelectFile=phase.tau”",4.306081271659777
"How can I disable instrumentation for a specific phase in TAU?
","TAU is a portable profiling and tracing toolkit that supports many programming languages. The instrumentation can be done by inserting in the source code using an automatic tool based on the Program Database Toolkit (PDT), on the compiler instrumentation, or manually using the instrumentation API.

Webpage: https://www.cs.uoregon.edu/research/tau/home.php",4.182400139854487
"What is the benefit of using Conda on Frontier?
","Currently, Crusher and Frontier do NOT have Anaconda/Conda modules. If your workflow better suits conda environments, you can install your own Miniconda on Frontier.

The install process is rather simple (with a few notable warnings, see https://docs.olcf.ornl.gov/systems/miniconda.html#Cautionary Notes <miniconda-notes> further below):

mkdir miniconda_frontier/
cd miniconda_frontier/
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
chmod u+x Miniconda3-latest-Linux-x86_64.sh
./Miniconda3-latest-Linux-x86_64.sh -u -p ~/miniconda_frontier",4.405135505085881
"What is the benefit of using Conda on Frontier?
","Because there is no conda module on Frontier, this guide only applies if you installed a personal Miniconda first. See our https://docs.olcf.ornl.gov/software/python/miniconda.html for more details.

This guide has been shortened and adapted from a challenge in OLCF's Hands-On with Summit GitHub repository (Python: Conda Basics).",4.34276416426959
"What is the benefit of using Conda on Frontier?
","Currently, Frontier does NOT have an Anaconda/Conda module.  To use conda, you will have to download and install Miniconda on your own (see our https://docs.olcf.ornl.gov/software/python/miniconda.html). Alternatively, you can use Python's native virtual environments venv feature with the cray-python module (as we will explore in the guides below).  For more details on venv, see Python's Official Documentation.  Contact help@olcf.ornl.gov if conda is required for your workflow, or if you have any issues.",4.331936560934477
"How can I run a single command on Crusher's compute nodes without creating a batch script?
","After running this command, the job will wait until enough compute nodes are available, just as any other batch job must. However, once the job starts, the user will be given an interactive prompt on the primary compute node within the allocated resource pool. Commands may then be executed directly (instead of through a batch script).",4.309512120727734
"How can I run a single command on Crusher's compute nodes without creating a batch script?
","After running this command, the job will wait until enough compute nodes are available, just as any other batch job must. However, once the job starts, the user will be given an interactive prompt on the primary compute node within the allocated resource pool. Commands may then be executed directly (instead of through a batch script).

Debugging",4.2859879759707615
"How can I run a single command on Crusher's compute nodes without creating a batch script?
","Slurm provides 3 ways of submitting and launching jobs on Crusher's compute nodes: batch scripts, interactive, and single-command. The Slurm commands associated with these methods are shown in the table below and examples of their use can be found in the related subsections. Please note that regardless of the submission method used, the job will launch on compute nodes, with the first compute in the allocation serving as head-node.

| sbatch |  | | --- | --- | | salloc |  | | srun |  |",4.228423607981707
"How does Valgrind4hpc manage starting and redirecting output from multiple copies of Valgrind?
","Valgrind4hpc is a Valgrind-based debugging tool to aid in the detection of memory leaks and errors in parallel applications. Valgrind4hpc aggregates any duplicate messages across ranks to help provide an understandable picture of program behavior. Valgrind4hpc manages starting and redirecting output from many copies of Valgrind, as well as deduplicating and filtering Valgrind messages. If your program can be debugged with Valgrind, it can be debugged with Valgrind4hpc.

Valgrind4hpc is available on Frontier under all compiler families:

module load valgrind4hpc",4.5353164863595445
"How does Valgrind4hpc manage starting and redirecting output from multiple copies of Valgrind?
","module load valgrind4hpc

Additional information about Valgrind4hpc usage can be found on the HPE Cray Programming Environment User Guide Page.

The Performance Analysis Tools (PAT), formerly CrayPAT, are a suite of utilities that enable users to capture and analyze performance data generated during program execution. These tools provide an integrated infrastructure for measurement, analysis, and visualization of computation, communication, I/O, and memory utilization to help users optimize programs for faster execution and more efficient computing resource usage.",4.177779547550436
"How does Valgrind4hpc manage starting and redirecting output from multiple copies of Valgrind?
","Valgrind

Valgrind is an instrumentation framework for building dynamic analysis tools. There are Valgrind tools that can automatically detect many memory management and threading bugs, and profile your programs in detail. You can also use Valgrind to build new tools.

The Valgrind distribution currently includes five production-quality tools: a memory error detector, a thread error detector, a cache and branch-prediction profiler, a call-graph generating cache profiler, and a heap profiler. It also includes two experimental tools: a data race detector, and an instant memory leak detector.",4.139692444396136
"How can I run a job on the System Model H1 family and System Model H2 hardware using Quantinuum Credits?
","Running a job on the System Model H1 family and System Model H2 hardware requires Quantinuum Credits. Additional information on credit usage can be found in the Quantinuum Systems User Guide under the Examples tab on the Quantinuum User Portal. Due to increased demand and to make the most efficient use of credits, the following allocating policy will go into effect starting October 1st 2022:

Any request for credits must be submitted by the project Principle Investigator (PI) to help@olcf.ornl.gov",4.688139291966492
"How can I run a job on the System Model H1 family and System Model H2 hardware using Quantinuum Credits?
","Information on submitting jobs to Quantinuum systems, system availability, checking job status, and tracking usage can be found in the Quantinuum Systems User Guide under the Examples tab on the Quantinuum User Portal.

Users have access to the API validator to check program syntax, and to the Quantinuum System Model H1 emulator, which returns actual results back as if users submitted code to the real quantum hardware.",4.362695554751347
"How can I run a job on the System Model H1 family and System Model H2 hardware using Quantinuum Credits?
","Allocations will be granted on a monthly basis to maximize the availability of the H1 family and H2 machines. Please note that allocations do not carry over to the next month and must be consumed in the month granted.

Allocation requests requiring 20 qubits and under will be considered for H1 family machines, and allocation requests requiring 21-32 qubits will be considered for H2.

Allocation requests for the following month must be submitted no later than the 25th of the preceding month.  The uptime schedule is available on the Calendar tab of the Quantinuum User Portal.",4.274903761424627
"How does MPS accomplish this?
","The Multi-Process Service (MPS) enables multiple processes (e.g. MPI ranks) to concurrently share the resources on a single GPU. This is accomplished by starting an MPS server process, which funnels the work from multiple CUDA contexts (e.g. from multiple MPI ranks) into a single CUDA context. In some cases, this can increase performance due to better utilization of the resources. The figure below illustrates MPS on a pre-Volta GPU.",4.29733460894845
"How does MPS accomplish this?
",of how to start an MPS server process for a job: https://vimeo.com/292016149,4.256928599895041
"How does MPS accomplish this?
","The Multi-Process Service (MPS) enables multiple processes (e.g. MPI ranks) to concurrently share the resources on a single GPU. This is accomplished by starting an MPS server process, which funnels the work from multiple CUDA contexts (e.g. from multiple MPI ranks) into a single CUDA context. In some cases, this can increase performance due to better utilization of the resources. As mentioned in the Common bsub Options section above, MPS can be enabled with the -alloc_flags ""gpumps"" option to bsub. The following screencast shows an example of how to start an MPS server process for a job:",4.25505872886519
"What is the purpose of the moderate enhanced projects home directory permissions?
","The default permissions for user home directories are 0750 (full access to the user, read and execute for the group). Users have the ability to change permissions on their home directories, although it is recommended that permissions be set to as restrictive as possible (without interfering with your work).

Moderate enhanced projects have home directory permissions set to 0700 and are automatically reset to that if changed by the user.

There are no backups for moderate enhanced project home directories.",4.501222732742882
"What is the purpose of the moderate enhanced projects home directory permissions?
","Open and Moderate Projects are provided with a Project Home storage area in the NFS-mounted filesystem. This area is intended for storage of data, code, and other files that are of interest to all members of a project. Since Project Home is an NFS-mounted filesystem, its performance will not be as high as other filesystems.

Moderate Enhanced projects are not provided with Project Home spaces, just Project Work spaces.

Project Home area is accessible at /ccs/proj/abc123 (where abc123 is your project ID).",4.406271143186975
"What is the purpose of the moderate enhanced projects home directory permissions?
","3

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

4

Retention is not applicable as files will follow purge cycle.",4.188682138280301
"How can users ensure that they are using the latest version of a product?
","Blue-green deployment requires that your application can handle both old and new versions running at the same time. Be sure to think about your application and if it can handle this. For example, if the new version of the software changes how a certain field in a database is read and written, then the old version of the software won't be able to read the database changes, and your production instance could break. This is known as ""N-1 compatibility"" or ""backward compatibility"".",4.130458780570447
"How can users ensure that they are using the latest version of a product?
",It is the user’s responsibility to insure the appropriate level of backup and integrity checks on critical data and programs.,4.081957730616367
"How can users ensure that they are using the latest version of a product?
","Users will be advised to do a module load <UMS project>, which will expose modules for the individual products associated with that project, accessed by a second module load <product>.

Project PI must ensure that support is provided for users of the product, as documented in the statement of support.

Project PI must ensure that the product is updated in response to changes in the system software environment (e.g. updates to OS, compiler, library, or other key tools) and in a timely fashion.  If this commitment is not met, the OLCF may remove the software from UMS.",4.068474485631327
"How do I use the `address_by_interface` function in Parsl to specify the address of my high-throughput computing environment?
","from parsl.addresses import address_by_interface
from parsl.config import Config
from parsl.executors import HighThroughputExecutor
from parsl.launchers import JsrunLauncher
from parsl.providers import LSFProvider

from parsl import python_app

import parsl

parsl.set_stream_logger()",4.213434188121884
"How do I use the `address_by_interface` function in Parsl to specify the address of my high-throughput computing environment?
","config = Config(
    executors = [
        HighThroughputExecutor(
            label = 'Summit_HTEX',
            address = address_by_interface('ib0'),
            worker_port_range = (50000, 55000),
            provider = LSFProvider(
                launcher = JsrunLauncher(),
                walltime = '00:10:00',
                nodes_per_block = 1,
                init_blocks = 1,
                max_blocks = 1,
                worker_init = 'source activate parsl-py36',
                project = 'abc123', # replace this line
                cmd_timeout = 30
            )
        )",4.070485864397677
"How do I use the `address_by_interface` function in Parsl to specify the address of my high-throughput computing environment?
","Parsl needs to be able to write to the working directory from compute nodes, so we will work from within the member work directory and assume a project ID ABC123:

$ mkdir -p ${MEMBERWORK}/abc123/parsl-demo/
$ cd ${MEMBERWORK}/abc123/parsl-demo/

To run an example ""Hello world"" program with Parsl on Summit, create a file called hello-parsl.py with the following contents, but with your own project ID in the line specified:",3.994964856828607
"How do I create a Persistent Volume Claim (PVC) in OpenShift?
","From the main screen of a project go to the Storage option in the hamburger menu on the left hand side of the screen, then click Persistent Volume Claims

Persistent Volume Claim Menu

In the upper right click the Create Persistent Volume Claim button.

Create Storage

This will take you to a screen that allows you to select what you need for your PVC. Give your claim a name and request an amount of storage and click Create. You can also click Edit YAML to edit the PVC object directly.

PV Settings",4.5454766881289
"How do I create a Persistent Volume Claim (PVC) in OpenShift?
","Once the Persistent Volume is created its allocated memory is available to be claimed by a project. This is done through a Persistent Volume Claim. PVC's are created using a YAML file in the same manner that PV's are created. An example of a PVC that claims the PV created in the previous section follows:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: storage-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

The claim is then placed using:

oc create -f storage-1.yaml",4.533886679210074
"How do I create a Persistent Volume Claim (PVC) in OpenShift?
","PV Settings

This will redirect you to the status page of your new PVC. You should see Bound in the Status field.

PV Status

Once the claim has been granted to the project a pod within that project can access that storage. To do this, edit the YAML of the pod and under the volume portion of the file add a name and the name of the claim. It will look similar to the following.

volumes:
  name: pvol
  persistentVolumeClaim:
    claimName: storage-1

Then all that is left to do is mount the storage into a container:

volumeMounts:
  - mountPath: /tmp/
    name: pvol",4.4647093459641605
"How can I ensure that my job is submitted to the correct queue on Citadel?
","If you submit a job to a ""normal"" Summit queue while on Citadel, such as -q batch, your job will be unable to launch.",4.340204907428901
"How can I ensure that my job is submitted to the correct queue on Citadel?
","There are special queue names when submitting jobs to citadel.ccs.ornl.gov (the Moderate Enhanced version of Summit). These queues are: batch-spi, batch-hm-spi, and debug-spi.  For example, to submit a job to the batch-spi queue on Citadel, you would need -q batch-spi when using the bsub command or #BSUB -q batch-spi when using a job script.

Except for the enhanced security policies for jobs in these queues, all other queue properties are the same as the respective Summit queues described above, such as maximum walltime and number of eligible running jobs.",4.304623593175501
"How can I ensure that my job is submitted to the correct queue on Citadel?
","The Citadel framework allows use of the Summit/Frontier compute resources but adds additional layers of security to ensure data protection.  To ensure proper configuration and protection access to the compute resources, the following batch queue(s) must be used from the https://docs.olcf.ornl.gov/systems/index.html#Citadel login nodes<citadel-login-nodes>:

batch-spi",4.186150604905108
"How can I submit a job to run on Crusher's compute nodes at a later time?
","This section describes how to run programs on the Crusher compute nodes, including a brief overview of Slurm and also how to map processes and threads to CPU cores and GPUs.

Slurm is the workload manager used to interact with the compute nodes on Crusher. In the following subsections, the most commonly used Slurm commands for submitting, running, and monitoring jobs will be covered, but users are encouraged to visit the official documentation and man pages for more information.",4.332039075872264
"How can I submit a job to run on Crusher's compute nodes at a later time?
","Slurm provides 3 ways of submitting and launching jobs on Crusher's compute nodes: batch scripts, interactive, and single-command. The Slurm commands associated with these methods are shown in the table below and examples of their use can be found in the related subsections. Please note that regardless of the submission method used, the job will launch on compute nodes, with the first compute in the allocation serving as head-node.

| sbatch |  | | --- | --- | | salloc |  | | srun |  |",4.323511712962342
"How can I submit a job to run on Crusher's compute nodes at a later time?
","Prepare executables and input files.

Write a batch script.

Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on commodity clusters.



Login vs Compute Nodes on Commodity Clusters

Login Nodes

<string>:403: (INFO/1) Duplicate implicit target name: ""login nodes"".",4.250974328206909
"What is the name of the MinIO standalone application?
","MinIO is a high-performance software-defined object storage suite that enables the ability to easily deploy cloud-native data infrastructure for various data workloads. In this example we are deploying a simple, standalone implementation of MinIO on our cloud-native platform, Slate (https://docs.olcf.ornl.gov/systems/minio.html#slate_overview).

This service will only be accessible from inside of ORNL's network.

If your project requires an externally facing service available to the Internet, please contact User Assistance by submitting a help ticket. There is a process to get such approval.",4.388930192772024
"What is the name of the MinIO standalone application?
","MinIO running on a dedicated volume, allocated automatically from the NetApp storage server, isolated to the MinIO server.

It is important to note that we are also launching MinIO in standalone mode, which is a single MinIO server instance. MinIO also supports distributed mode for more robust implementations, but we are not setting that up in this example.

<string>:5: (INFO/1) Duplicate explicit target name: ""user assistance"".",4.363279754446794
"What is the name of the MinIO standalone application?
","What do you need to consider?

What should I name my host value? (This will be the URL in which you access your MinIO instance)

What should I name my application? (This is the name value and should be unique to you or your project)

Do I want MinIO to run on an OLCF filesytem? (It can run on NFS or GPFS project spaces. If you do not run it on an OLCF filesystem it uses an isolated volume dedicated to the MinIO server)

What do you need to configure?

host (Set the URL of your application)

name (Set the name of your application)",4.278524554668076
"How can I initiate a tunneling connection with a GPU node on the Andes cluster?
","In a new terminal, open a tunneling connection with andes-gpu5.olcf.ornl.gov and                                                                              port 5901
example:
     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov
     andes: ssh -4L 5901:localhost:5901 andes-gpu5

**************************************************************************",4.433346184326392
"How can I initiate a tunneling connection with a GPU node on the Andes cluster?
","In a new terminal, open a tunneling connection with andes-gpu5.olcf.ornl.gov and                                                                              port 5901
example:
     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov
     andes: ssh -4L 5901:localhost:5901 andes-gpu5

**************************************************************************",4.433346184326392
"How can I initiate a tunneling connection with a GPU node on the Andes cluster?
","Nice DCV is currently undergoing maintenance. Instead, please use the VNC options detailed above.

Step 1 (terminal 1)

Launch an interactive job:

localsytem: ssh username@andes.olcf.ornl.gov
andes: salloc -A PROJECT_ID -p gpu -N 1 -t 60:00 -M andes --constraint=DCV

Run the following commands:

$ xinit &
$ export DISPLAY=:0
$ dcv create-session --gl-display :0 mySessionName
$ hostname  // will be used to open a tunneling connection with this node
$ andes-gpuN

Step 2 (terminal 2)

Open a tunneling connection with gpu node N, given by hostname:",4.341884417433547
"How can I verify that the change has been made?
","On the ""Policies & Agreements"" page click the links to read the polices and then answer ""Yes"" to affirm you have read each. Certify your application by selecting ""Yes"" as well. Then Click ""Submit.""

<string>:5: (INFO/1) Enumerated list start value not ordinal-1: ""8"" (ordinal 8)

After submitting your application, it will need to pass through the approval process. Depending on when you submit, approval might not occur until the next business day.",3.992781498750553
"How can I verify that the change has been made?
","When our accounts team begins processing your application, you will receive an automated email containing an unique 36-character confirmation code. Make note of it; you can use it to check the status of your application at any time.

The principal investigator (PI) of the project must approve your account and system access. We will make the project PI aware of your request.",3.9629585714161903
"How can I verify that the change has been made?
","Form. You will need your Application Confirmation Number that was emailed to you by our accounts team to schedule in this manner. If you do not possess a confirmation number (you are verifying a replacement token, for example), please email us at help@olcf.ornl.gov to schedule.",3.928946665900444
"How do the Frontier nodes communicate with each other?
","On Frontier, there are two major types of nodes you will encounter: Login and Compute. While these are similar in terms of hardware (see: https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes), they differ considerably in their intended use.",4.331789267037155
"How do the Frontier nodes communicate with each other?
","Simplified Frontier node architecture diagram

In the diagram, each physical core on a Frontier compute node is composed of two logical cores that are represented by a pair of blue and grey boxes. For a given physical core, the blue box represents the logical core of the first hardware thread, where the grey box represents the logical core of the second hardware thread.",4.3203187950736535
"How do the Frontier nodes communicate with each other?
",The Frontier nodes are connected with [4x] HPE Slingshot 200 Gbps (25 GB/s) NICs providing a node-injection bandwidth of 800 Gbps (100 GB/s).,4.274132928353639
"How can I monitor the progress of my Rapids job on Summit?
","Large workloads.

Long runtimes on Summit's high memory nodes.

Your Python script has support for multi-gpu/multi-node execution via dask-cuda.

Your Python script is single GPU but requires simultaneous job steps.

RAPIDS is provided in Jupyter following  these instructions.

Note that Python scripts prepared on Jupyter can be deployed on Summit if they use the same RAPIDS version. Use !jupyter nbconvert --to script my_notebook.ipynb to convert notebook files to Python scripts.

RAPIDS is provided on Summit through the module load command:",4.164732548377494
"How can I monitor the progress of my Rapids job on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.164204901489765
"How can I monitor the progress of my Rapids job on Summit?
","To access Summit and Frontier's compute resources for SPI workflows, you must first log into a https://docs.olcf.ornl.gov/systems/index.html#Citadel login node<citadel-login-nodes> and then submit a batch job to one of the SPI specific batch queues.",4.129212723499653
"How do I access the Workloads tab in the Slate tutorial?
","From here, in the left hand hamburger menu click on the 'Workloads' tab and then the 'pods' tab:



Here you will be able to view all of the Pods in your Project. Since this is a new Project there will be no Pods in it. To create a  Pod click the 'Create Pod' button.

This will bring you to a screen of pre populated YAML that you can edit in the browser. This YAML is the basis of a podspec that will be sent to the API server once you click the 'Create' button in the lower left to create a  Pod in your Project. Here we will make a few slight modifications to the podspec.",4.205866630257739
"How do I access the Workloads tab in the Slate tutorial?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.087912780205948
"How do I access the Workloads tab in the Slate tutorial?
","To learn more about FireWorks, please refer to its extensive online documentation.

Before using FireWorks itself, you will need a MongoDB service running on https://docs.olcf.ornl.gov/systems/fireworks.html#Slate<slate>. A tutorial to deploy MongoDB is available in the Slate documentation.

You will need to know the connection information for both MongoDB so that FireWorks can be configured to connect to it.

Then, to use FireWorks on Summit, load the module as shown below:

$ module load workflows
$ module load fireworks/2.0.2",4.062870898347998
"How can we avoid the singularity build step from getting killed due to reaching cgroup memory limit?
","podman build -f gpuexample.dockerfile -t gpuexample:latest .;
podman save -o gpuexampleimage.tar localhost/gpuexample:latest;
singularity build --disable-cache gpuexampleimage.sif docker-archive://gpuexampleimage.tar;

It's possible the singularity build step might get killed due to reaching cgroup memory limit. To get around this, you can start an interactive job and build the singularity image with

jsrun -n1 -c42 -brs singularity build --disable-cache gpuexampleimage.sif docker-archive://gpuexampleimage.tar;",4.285886941980946
"How can we avoid the singularity build step from getting killed due to reaching cgroup memory limit?
","module purge
module load DefApps
module load gcc/9.1.0
module -t list
podman build -v $MPI_ROOT:$MPI_ROOT -f mpiexample.dockerfile -t mpiexample:latest .;
podman save -o mpiexampleimage.tar localhost/mpiexample:latest;
singularity build --disable-cache mpiexampleimage.sif docker-archive://mpiexampleimage.tar;

It's possible the singularity build step might get killed due to reaching cgroup memory limit. To get around this, you can start an interactive job and build the singularity image with",4.162624554118835
"How can we avoid the singularity build step from getting killed due to reaching cgroup memory limit?
","You can view the Dockerfiles used to build the MPI base image at the code.ornl.gov repository. These Dockerfiles are buildable on Summit yourself by cloning the repository and running the ./build in the individual directories in the repository. This allows you the freedom to modify these base images to your own needs if you don't need all the components in the base images. You may run into the cgroup memory limit when building so kill the podman process, log out, and try running the build again if that happens when building.",4.159596087527392
"What is a registration token, and why is it needed for the GitLab Runner?
","Prior to installation of the GitLab Runner, a registration token for the runner is needed from the GitLab server. This token will allow a GitLab runner to register to the server in the needed location for running CI/CD jobs. Runners themselves may be registered to either a group as a shared runner or a project as a repository specific runner.",4.660733001273076
"What is a registration token, and why is it needed for the GitLab Runner?
","of the CI/CD settings. In the ""Specific Runners"" area, the registration token should be available for retrieval.",4.337502442657793
"What is a registration token, and why is it needed for the GitLab Runner?
","If the runner is to be a group shared runner, navigate to the group in GitLab and then go to Settings -> CI/CD. Expand the Runners section of the CI/CD Settings panel. Ensure that the ""Enable shared runners for this group"" toggle is enabled. The registration token should also be available for retrieval from ""Group Runners"" area.",4.305906647081104
"What is the effect of the reduced priority on the job's apparent submit time?
","number of processors requested above, this is an adjustment to the

apparent submit time of the job. However, this adjustment has the effect

of making jobs appear much younger than jobs submitted under projects

that have not exceeded their allocation. In addition to the priority

change, these jobs are also limited in the amount of wall time that can

be used. For example, consider that ``job1`` is submitted at the same

time as ``job2``. The project associated with ``job1`` is over its

allocation, while the project for ``job2`` is not. The batch system will",4.363005694572376
"What is the effect of the reduced priority on the job's apparent submit time?
","consider ``job2`` to have been waiting for a longer time than ``job1``.

Also projects that are at 125% of their allocated time will be limited

to only one running job at a time. The adjustment to the apparent submit

time depends upon the percentage that the project is over its

allocation, as shown in the table below:



+------------------------+----------------------+--------------------------+------------------+

| % Of Allocation Used   | Priority Reduction   | Number eligible-to-run   | Number running   |",4.358455783010802
"What is the effect of the reduced priority on the job's apparent submit time?
","Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.305144135105942
"What is the advantage of using AMP in Summit?
",For Summit:,4.2206331011998826
"What is the advantage of using AMP in Summit?
","For detailed information about using Vampir on Summit and the builds available, please see the Vampir Software Page.",4.069928510246538
"What is the advantage of using AMP in Summit?
","SummitPLUS is one of the new allocation programs that will be used to allocate a significant portion of the system for 2024. The program is open to researchers from academia, government laboratories, federal agencies, and industry. We welcome proposals for computationally ready projects from investigators who are new to Summit, as well as from previous INCITE, ALCC, DD, ECP awardees and projects. We encourage proposals on emerging paradigms for computational campaigns including data-intensive science and AI/ML.",4.062604217178263
"Can I use the graphical client on Frontier to debug my application?
",One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Frontier from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Frontier. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.,4.394477873878154
"Can I use the graphical client on Frontier to debug my application?
",One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Frontier from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Frontier. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.,4.394477873878154
"Can I use the graphical client on Frontier to debug my application?
","- **Use cluster’s graphics cards**: Unchecked

Click “Apply” and make sure to save the settings (Options/Save Settings).
Exit and re-launch VisIt.

.. note::
    If you want to use the ``debug`` QOS on Frontier, you can add ``-q debug``
    to the ""Launcher arguments"" section under the ""Advanced"" tab (make sure
    to also check the ""Launcher arguments"" box).

Summit

**For Summit:**",4.147852114076421
"How can I access the Persistent Volume (PV) that is mounted to a pod?
","PV Settings

This will redirect you to the status page of your new PVC. You should see Bound in the Status field.

PV Status

Once the claim has been granted to the project a pod within that project can access that storage. To do this, edit the YAML of the pod and under the volume portion of the file add a name and the name of the claim. It will look similar to the following.

volumes:
  name: pvol
  persistentVolumeClaim:
    claimName: storage-1

Then all that is left to do is mount the storage into a container:

volumeMounts:
  - mountPath: /tmp/
    name: pvol",4.383862162979383
"How can I access the Persistent Volume (PV) that is mounted to a pod?
","Once the Persistent Volume is created its allocated memory is available to be claimed by a project. This is done through a Persistent Volume Claim. PVC's are created using a YAML file in the same manner that PV's are created. An example of a PVC that claims the PV created in the previous section follows:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: storage-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

The claim is then placed using:

oc create -f storage-1.yaml",4.344813772024469
"How can I access the Persistent Volume (PV) that is mounted to a pod?
","By design pods are ephemeral. They are meant to be something that can be killed with relatively little impact to the application. Since containers are ephemeral, we need a way to consume storage that is persistent and can hold data for our applications. One option that Kubernetes has for storing data is with Persistent Volumes. PersistentVolume objects are created by the cluster administrator and reference storage that is on a resilient storage platform such as NetApp. A user can request storage by creating a PersistentVolumeClaim which requests a desired size for a PersistentVolume. The",4.332057663381622
"Can I use Crusher to run a job step that requires a specific software environment?
","Crusher users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.26024827259712
"Can I use Crusher to run a job step that requires a specific software environment?
","This section describes how to run programs on the Crusher compute nodes, including a brief overview of Slurm and also how to map processes and threads to CPU cores and GPUs.

Slurm is the workload manager used to interact with the compute nodes on Crusher. In the following subsections, the most commonly used Slurm commands for submitting, running, and monitoring jobs will be covered, but users are encouraged to visit the official documentation and man pages for more information.",4.129333754144381
"Can I use Crusher to run a job step that requires a specific software environment?
","Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.",4.110172078757778
"What is GDB and what programming languages does it support?
","GDB, the GNU Project Debugger, is a command-line debugger useful for traditional debugging and investigating code crashes. GDB lets you debug programs written in Ada, C, C++, Objective-C, Pascal (and many other languages).

GDB is available on Summit under all compiler families:

module load gdb

To use GDB to debug your application run:

gdb ./path_to_executable

Additional information about GDB usage can befound on the GDB Documentation Page.",4.430836016468128
"What is GDB and what programming languages does it support?
","GDB, the GNU Project Debugger, is a command-line debugger useful for traditional debugging and investigating code crashes. GDB lets you debug programs written in Ada, C, C++, Objective-C, Pascal (and many other languages).

GDB is availableon Summit under all compiler families:

module load gdb

To use GDB to debug your application run:

gdb ./path_to_executable

Additional information about GDB usage can befound on the GDB Documentation Page.",4.4270901456081
"What is GDB and what programming languages does it support?
","GDB, the GNU Project Debugger, is a command-line debugger useful for traditional debugging and investigating code crashes. GDB lets you debug programs written in Ada, C, C++, Objective-C, Pascal (and many other languages).

GDB is available on andes via the gdb module:

module load gdb

To use GDB to debug your application run:

gdb ./path_to_executable

Additional information about GDB usage can befound on the GDB Documentation Page.",4.4210422412128425
"How many Infinity Fabric GPU-GPU connections are available on a single Crusher compute node?
","The 2 GCDs on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The GCDs on different MI250X are connected with Infinity Fabric GPU-GPU in the arrangement shown in the Crusher Node Diagram below, where the peak bandwidth ranges from 50-100 GB/s based on the number of Infinity Fabric connections between individual GCDs.",4.454828987690847
"How many Infinity Fabric GPU-GPU connections are available on a single Crusher compute node?
","on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The GCDs on different MI250X are connected with Infinity Fabric GPU-GPU in the arrangement shown in the Frontier Node Diagram below, where the peak bandwidth ranges from 50-100 GB/s based on the number of Infinity Fabric connections between individual GCDs.",4.40355263649474
"How many Infinity Fabric GPU-GPU connections are available on a single Crusher compute node?
","Each Crusher compute node consists of [1x] 64-core AMD EPYC 7A53 ""Optimized 3rd Gen EPYC"" CPU (with 2 hardware threads per physical core) with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X, each with 2 Graphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can think of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth memory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU, allowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of 36+36 GB/s. The 2 GCDs on the same MI250X are connected with",4.40022788048452
"How can I run a parallel h5py job on Andes?
","source activate /ccs/proj/<project_id>/<user_id>/envs/summit/h5pympi-summit

   jsrun -n1 -r1 -a42 -c42 python3 hdf5_parallel.py

Andes

.. code-block:: bash

   #!/bin/bash
   #SBATCH -A <PROJECT_ID>
   #SBATCH -J h5py
   #SBATCH -N 1
   #SBATCH -p gpu
   #SBATCH -t 0:05:00

   unset SLURM_EXPORT_ENV

   cd $SLURM_SUBMIT_DIR
   date

   module load gcc
   module load hdf5
   module load python

   source activate /ccs/proj/<project_id>/<user_id>/envs/andes/h5pympi-andes

   srun -n42 python3 hdf5_parallel.py

Frontier

.. code-block:: bash",4.390712088366581
"How can I run a parallel h5py job on Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.362854913882909
"How can I run a parallel h5py job on Andes?
","Andes

.. code-block:: bash

   #!/bin/bash
   #SBATCH -A <PROJECT_ID>
   #SBATCH -J mpi4py
   #SBATCH -N 1
   #SBATCH -p gpu
   #SBATCH -t 0:05:00

   unset SLURM_EXPORT_ENV

   cd $SLURM_SUBMIT_DIR
   date

   module load gcc
   module load hdf5
   module load python

   source activate /ccs/proj/<project_id>/<user_id>/envs/andes/h5pympi-andes

   srun -n42 python3 hello_mpi.py

Frontier

.. code-block:: bash

   #!/bin/bash
   #SBATCH -A <PROJECT_ID>
   #SBATCH -J mpi4py
   #SBATCH -N 1
   #SBATCH -p batch
   #SBATCH -t 0:05:00

   unset SLURM_EXPORT_ENV

   cd $SLURM_SUBMIT_DIR
   date",4.261281425312861
"What is the purpose of the ""podman save"" command?
","If you already have a ""image.tar"" file created with podman save from earlier that you are trying to replace, you will need to delete it first before running any other podman save to replace it. podman save won't overwrite the tar file for you.

Not using the --disable-cache flag in your singularity build commands could cause your home directory to get quickly filled by singularity caching image data. You can set the cache to a location in /tmp/containers with export SINGULARITY_CACHEDIR=/tmp/containers/<username>/singularitycache if you want to avoid using the --disable-cache flag.",4.262028572643502
"What is the purpose of the ""podman save"" command?
","Podman is a container framework from Red Hat that functions as a drop in replacement for Docker. On Summit, we will use Podman for building container images and converting them into tar files for Singularity to use. Podman makes use of Dockerfiles to describe the images to be built. We cannot use Podman to run containers as it doesn't properly support MPI on Summit, and Podman does not support storing its container images on GPFS or NFS.",4.207016650710764
"What is the purpose of the ""podman save"" command?
","Due to Podman's lack of support for storage on GPFS and NFS, container images will be built on the login nodes using the node-local NVMe on the login node. This NVMe is mounted in /tmp/containers. Users should treat this storage as temporary. Any data (container image layers or otherwise) in this storage will be purged if the node is ever rebooted or when it gets full.  So any images created with Podman need to be converted to tar files using podman save and stored elsewhere if you wish to preserve your image.",4.189469372449854
"Can I use the same username for multiple SPI projects?
","In order to help ensure data separation, each SPI user is given a unique userid for each project. SPI user names use the format: <userid>_<proj>_mde . For example: usera_prj123_mde.

SPI usernames will not overlap with usernames used in the non-SPI enclaves. Unlike non-SPI usernames, SPI usernames only exist in a single project. Users on multiple SPI projects will have a unique username for each SPI project.  You must specify your unique SPI username matching the target project when connecting.",4.4584257308795685
"Can I use the same username for multiple SPI projects?
","In order to help ensure data separation, each SPI user is given a unique userID for each project. SPI userIDs use the format: <userid>_<proj>_mde . For example: userx_abc123_mde. SPI usernames will not overlap with usernames used in the non-SPI enclaves. Unlike non-SPI usernames, SPI usernames only exist in a single project. Users on multiple SPI projects will have a unique username for each SPI project.",4.415609480259524
"Can I use the same username for multiple SPI projects?
","Once a project has been approved and created, the next step will be to request user accounts.  A user account will allow an individual to access the project's allocated resources.    This process to request an account is very similar to the process used for non-SPI projects.  One notable difference between SPI and non-SPI accounts: SPI usernames are unique to a project.  SPI usernames use the format: <userid>_<proj>_mde.  If you have access to three SPI projects, you will have three userIDs with three separate home areas.",4.383411062738444
"How do I access my repository settings in Slate?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",4.073886469561336
"How do I access my repository settings in Slate?
","Image of the repositories area.

and then add the ""Repository URL"", ""Username"" for the deploy token, and the deploy token itself as the password. If Git-LFS support is needed, click the ""Enable LFS support"" at the bottom of the page. Once entries look correct:

Image of the connect to repo using https parameters.

click the ""CONNECT"" button in the upper left. Once entered and ArgoCD is able to access the server, the connection should have a status of ""Successful"" with a green check mark:

Image of a successful git repository configuration.",4.043104224898669
"How do I access my repository settings in Slate?
","This section will describe the project allocation process for Slate. This is applicable to both the Onyx and Marble OLCF OpenShift clusters.

In addition, we will go over the process to log into Onyx and Marble, and how namespaces work within the OpenShift context.

Please fill out the Slate Request Form in order to use Slate resources. This must be done in order to proceed.

If you have any question please contact User Assistance, via a Help Ticket Submission or by emailing help@olcf.ornl.gov.

Required information:

- Existing OLCF Project ID

- Project PI",4.03511013174454
"How can I ensure that each MPI rank has access to a unique GPU?
","As mentioned previously, all GPUs are accessible by all MPI ranks by default, so it is possible to programatically map any combination of MPI ranks to GPUs. However, there is currently no way to use Slurm to map multiple GPUs to a single MPI rank. If this functionality is needed for an application, please submit a ticket by emailing help@olcf.ornl.gov.

There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_jobstep program and test whether or not processes and threads are running where intended.",4.387081093483393
"How can I ensure that each MPI rank has access to a unique GPU?
","When a CUDA program begins, each MPI rank creates a separate CUDA context on the GPU, but the scheduler on the GPU only allows one CUDA context (and so one MPI rank) at a time to launch on the GPU. This means that multiple MPI ranks can share access to the same GPU, but each rank gets exclusive access while the other ranks wait (time-slicing). This can cause the GPU to become underutilized if a rank (that has exclusive access) does not perform enough work to saturate the resources of the GPU. The following figure depicts such time-sliced access to a pre-Volta GPU.",4.382902932533016
"How can I ensure that each MPI rank has access to a unique GPU?
","has access to ""global"" GPU 4, MPI 001 has access to ""global"" GPU 5, etc., but all MPI ranks show a HIP runtime GPU ID of 0. The reason is that each MPI rank only ""sees"" one GPU and so the HIP runtime labels it as ""0"", even though it might be global GPU ID 0, 1, 2, 3, 4, 5, 6, or 7. The GPU's bus ID is included to definitively show that different GPUs are being used.",4.379097089696998
"What is the purpose of the `--device-memory-limit` option when running the `dask-cuda-worker` command?
","echo ""Done!""

Note twelve dask-cuda-workers are executed, one per each available GPU, --memory-limit is set to 82 GB and  --device-memory-limit is set to 16 GB. If using Summit's high-memory nodes --memory-limit can be increased and setting --device-memory-limit to 32 GB  and --rmm-pool-size to 30 GB or so is recommended. Also note it is recommeded to wait some seconds for the dask-scheduler and dask-cuda-workers to start.",4.298428431540551
"What is the purpose of the `--device-memory-limit` option when running the `dask-cuda-worker` command?
","Note the ""RAPIDS basic execution"" option is for illustrative purposes and not recommended to run RAPIDS on Summit since it underutilizes resources. If your RAPIDS code is single GPU, consider Jupyter or the concurrent job steps option.

<string>:3: (INFO/1) Duplicate explicit target name: ""simultaneous job steps"".

In cases when a set of time steps need to be processed by single-GPU RAPIDS codes and each time step fits comfortably in GPU memory, it is recommended to execute simultaneous job steps.

The following script provides a general pattern to run job steps simultaneously with RAPIDS:",3.957886619752637
"What is the purpose of the `--device-memory-limit` option when running the `dask-cuda-worker` command?
",new CUDA API functions introduced in CUDA8 allow users to fine tune the use of unified memory.,3.936884388944852
"How do I optimize my code to utilize the AMD GPUs on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.449905770876743
"How do I optimize my code to utilize the AMD GPUs on Frontier?
",Programming Environment  Frontier utilizes the Cray Programming Environment.  Many aspects including LMOD are similar to Summit's environment.  But Cray's compiler wrappers and the Cray and AMD compilers are worth noting.  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more information.  AMD GPUs  Each frontier node has 4 AMD MI250X accelerators with two Graphic Compute Dies (GCDs) in each accelerator. The system identifies each GCD as an independent device (so for simplicity we use the term GPU when we talk about a GCD) for a total of 8,4.3299197183574805
"How do I optimize my code to utilize the AMD GPUs on Frontier?
","GPU when we talk about a GCD) for a total of 8 GPUs per node (compared to Summit's 6 Nvidia V100 GPUs per node). Each pair of GPUs is associated with a particular NUMA domain (see node diagram in https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes section) which might affect how your application should lay out data and computation.  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#amd-gpus section for more information.  Programming Models  Since Frontier uses AMD GPUs, code written in Nvidia's CUDA language will not work as is. They need to be",4.266915709944657
"What is the purpose of the `USER` command in the Dockerfile?
","We will use OpenShift to build a new image based on the upstream one and change owner of the directories that need to be writable during container execution. Here is an example Dockerfile which derives from an upstream image and changes ownership of directories to the user id that the container will run as in the cluster.

For example, if we are using the UID 63114 for our NCCS project user and we need to write to /opt/application-data during the runtime of the container image we could do this:

FROM upstream-image:tag
USER 0
RUN chown -R 63114 /opt/application-data
USER 63114",4.044086297831014
"What is the purpose of the `USER` command in the Dockerfile?
","We will use this Dockerfile to generate a BuildConfig and then build a new image in our project that has the correct permissions.

cat Dockerfile | oc new-build --dockerfile=- --to=my-image:tag

The build should start automatically, monitor it with oc logs bc/my-image -f.

Now that we have a new image with our /opt/application-data directory owned by the right user we can either update an existing deployment or create a new one with the image.",4.032127202864249
"What is the purpose of the `USER` command in the Dockerfile?
","[storage]
driver = ""overlay""
graphroot = ""/tmp/containers/<user>""

[storage.options]
additionalimagestores = [
]

[storage.options.overlay]
ignore_chown_errors = ""true""
mount_program = ""/usr/bin/fuse-overlayfs""
mountopt = ""nodev,metacopy=on""

[storage.options.thinpool]

<user> in the graphroot = ""/tmp/containers/<user>"" in the above file should be replaced with your username. This will ensure that Podman will use the NVMe mounted in /tmp/containers for storage during container image builds.

As an example, let's build and run a very simple container image to demonstrate the workflow.",4.004267626277742
"How can I create a service in Slate that uses Passthrough Termination?
","With Passthrough Termination, the encrypted traffic goes straight to the pod with no TLS termination. This is useful if you are running a service such as HTTPD that is handling TLS termination itself. Another use case example could be doing mutual TLS authentication from a pod.

The following command will create a secured route with passthrough termination.

oc create route passthrough --service=my-project \
  --hostname=my-project.apps.<cluster>.ccs.ornl.gov

The produced yaml will look like this:",4.224485978814126
"How can I create a service in Slate that uses Passthrough Termination?
","The produced yaml will look like this:

apiVersion: v1
kind: Route
metadata:
  name: my-service
spec:
  host: my-project.apps.<cluster>.ccs.ornl.gov
  to:
    kind: Service
    name: service-name
  tls:
    termination: passthrough

Note that with passthrough termination, no keys are provided to the route.

Re-encryption termination combines edge termination and passthrough termination, in that the router terminates TLS, then re-encrypts its connection. The endpoint may have a different certificate. With re-encryption termination, both the internal and external network paths are encrypted.",4.110921372483064
"How can I create a service in Slate that uses Passthrough Termination?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.075403552797138
"How can I use burst buffers (NVMes) by default on Summit?
","The following example illustrates how to use the burst buffers (NVMes) by default on Summit. This example uses a submission script, check_nvme.lsf. It is assumed that the files are saved in the user's GPFS scratch area, /gpfs/alpine/scratch/$USER/projid, and that the user is operating from there as well. Do not forget that for all the commands on NVMe, it is required to use jsrun. This will submit a job to run on one node.

Job submssion script: check_nvme.lsf.

#!/bin/bash
#BSUB -P project123
#BSUB -J name_test
#BSUB -o nvme_test.o%J
#BSUB -W 2
#BSUB -nnodes 1
#BSUB -alloc_flags NVME",4.462685231699122
"How can I use burst buffers (NVMes) by default on Summit?
","Each compute node on Summit has a 1.6TB Non-Volatile Memory (NVMe) storage device (high-memory nodes have a 6.4TB NVMe storage device), colloquially known as a ""Burst Buffer"" with theoretical performance peak of 2.1 GB/s for writing and 5.5 GB/s for reading. The NVMes could be used to reduce the time that applications wait for I/O.  More information can be found later in the Burst Buffer section.



<string>:208: (INFO/1) Duplicate implicit target name: ""software"".",4.413922710524425
"How can I use burst buffers (NVMes) by default on Summit?
",NVMes will need to be copied back to the parallel filesystem before the job ends. This largely manual mode of usage will not be the recommended way to use the burst buffer for most applications because tools are actively being developed to automate and improve the NVMe transfer and data management process. Here are the basic steps for using the BurstBuffers in their current limited mode of usage:,4.345627349020682
"What is the benefit of using resource sets on Summit?
",For Summit:,4.240540039386151
"What is the benefit of using resource sets on Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.183437917277252
"What is the benefit of using resource sets on Summit?
","Resource sets allow each jsrun to control how the node appears to a code. This method is unique to jsrun, and requires thinking of each job launch differently than aprun or mpirun. While the method is unique, the method is not complicated and can be reasoned in a few basic steps.",4.1299186950210105
"How do I create a new directory called ""data"" inside the ""swift-work"" directory?
","The suggested directory structure is to have a outer directory say swift-work that has the swift source and shell scripts. Inside of swift-work create a new directory called data.

Additionally, we will need two terminals open. In the first terminal window, navigate to the swift-work directory and invoke the data generation script like so:

$ ./gendata.sh

In the second terminal, we will run the swift workflow as follows (make sure to change the project name per your allocation):",4.570112517501051
"How do I create a new directory called ""data"" inside the ""swift-work"" directory?
","This example demonstrates a continuously running cross-facility workflow. The idea is that there is a science facility (eg. SNS at ORNL) that produces scientific data to be processed by the remote compute facility (eg. OLCF at ORNL). The data is continuously arriving in a designated directory at the compute facility from science facility. The workflow picks data from that directory and does the processing to the data to produce some output. The Swift source file workflow.swift looks as follows:

import files;
import io;",4.110476378316813
"How do I create a new directory called ""data"" inside the ""swift-work"" directory?
","World Work Directory: 775

For example, if you have data that must be restricted only to yourself, keep them in your Member Work directory for that project (and leave the default permissions unchanged). If you have data that you intend to share with researchers within your project, keep them in the project’s Project Work directory. If you have data that you intend to share with researchers outside of a project, keep them in the project’s World Work directory.",4.103383231391858
"Can I use pat_build with perftools-lite-gpu?
","pat_report hello_jobstep+pat+39545-2t

The resulting report includes profiles of functions, profiles of maximum function times, details on load imbalance, details on program energy and power usages, details on memory high water mark, and more.

More detailed information on the HPE Performance Analysis Tools can be found in the HPE Performance Analysis Tools User Guide.

When using perftools-lite-gpu, there is a known issue causing ld.lld not to be found. A workaround this issue can be found here.",4.2426581634999785
"Can I use pat_build with perftools-lite-gpu?
","pat_report hello_jobstep+pat+39545-2t

The resulting report includes profiles of functions, profiles of maximum function times, details on load imbalance, details on program energy and power usages, details on memory high water mark, and more.

More detailed information on the HPE Performance Analysis Tools can be found in the HPE Performance Analysis Tools User Guide.

When using perftools-lite-gpu, there is a known issue causing ld.lld not to be found. A workaround this issue can be found here.",4.2426581634999785
"Can I use pat_build with perftools-lite-gpu?
","There are three programming interfaces available: (1) Perftools-lite, (2) Perftools, and (3) Perftools-preload.

Below are two examples that generate an instrumented executable using Perftools, which is an advanced interface that provides full-featured data collection and analysis capability, including full traces with timeline displays.

The first example generates an instrumented executable using a PrgEnv-amd build:

module load PrgEnv-amd
module load craype-accel-amd-gfx90a
module load rocm
module load perftools",4.166961171630505
"Can I enable instrumentation of headers in TAU?
","TAU is a portable profiling and tracing toolkit that supports many programming languages. The instrumentation can be done by inserting in the source code using an automatic tool based on the Program Database Toolkit (PDT), on the compiler instrumentation, or manually using the instrumentation API.

Webpage: https://www.cs.uoregon.edu/research/tau/home.php",4.272767683074155
"Can I enable instrumentation of headers in TAU?
","export TAU_OPTIONS=“-optTauSelectFile=phase.tau”

Now when you instrument your application, the phase called phase 1 are the lines 300-327 of the file miniWeather_mpi.cpp. Every call will be instrumented. This could create signiificant overhead, thus you should be careful when you use it.

Create a file called phases.tau.

BEGIN_INSTRUMENT_SECTION
static phase name=""phase1"" file=""miniWeather_mpi.cpp"" line=300 to line=327
static phase name=""phase2"" file=""miniWeather_mpi.cpp"" line=333 to line=346
END_INSTRUMENT_SECTION

Declare the TAU_OPTIONS variable.",4.235731338136778
"Can I enable instrumentation of headers in TAU?
","A similar approach for other metrics, not all of them can be used. TAU provides a tool called tau_cupti_avail, where we can see the list of available metrics, then we have to figure out which CUPTI metrics use these ones.

Activate tracing and declare the data format to OTF2. OTF2 format is supported only by MPI and OpenSHMEM applications.

$ export TAU_TRACE=1
$ export TAU_TRACE_FORMAT=otf2

Use Vampir for visualization.

For example, do not instrument routine sort*(int *)

Create a file select.tau

BEGIN_EXCLUDE_LIST
void sort_#(int *)
END_EXCLUDE_LIST

Declare the TAU_OPTIONS variable",4.207157061972952
"Are backups enabled for the project home directory on Slate?
","The backups are accessed via the .snapshot subdirectory. Note that ls alone (or even ls -a) will not show the .snapshot subdirectory exists, though ls .snapshot will show its contents. The .snapshot feature is available in any subdirectory of your project home directory and will show the online backups available for that subdirectory.

To retrieve a backup, simply copy it into your desired destination with the cp command.",4.216358676491308
"Are backups enabled for the project home directory on Slate?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",4.180994620428322
"Are backups enabled for the project home directory on Slate?
","If you accidentally delete files from your project home directory (/ccs/proj/[projid]), you may be able to retrieve them. Online backups are performed at regular intervals.  Hourly backups for the past 24 hours, daily backups for the last 7 days, and once-weekly backups are available. It is possible that the deleted files are available in one of those backups. The backup directories are named hourly.*, daily.*, and weekly.* where * is the date/time stamp of backup creation. For example, hourly.2020-01-01-0905 is an hourly backup made on January 1st, 2020 at 9:05 AM.",4.153423753775956
"Can accesses be granted to other project members on Member Work directories?
","3

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

4

Retention is not applicable as files will follow purge cycle.",4.56173864895125
"Can accesses be granted to other project members on Member Work directories?
","3

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

4

Retention is not applicable as files will follow purge cycle.",4.56173864895125
"Can accesses be granted to other project members on Member Work directories?
","Footnotes

1

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

2

Retention is not applicable as files will follow purge cycle.",4.527116578476836
"How can I ensure that my job starts after a specific job has completed?
","Sometimes you may need to place a hold on a job to keep it from starting. For example, you may have submitted it assuming some needed data was in place but later realized that data is not yet available. This can be done with the scontrol hold command. Later, when the data is ready, you can release the job (i.e. tell the system that it's now OK to run the job) with the scontrol release command. For example:

| scontrol hold 12345 | Place job 12345 on hold | | --- | --- | | scontrol release 12345 | Release job 12345 (i.e. tell the system it's OK to run it) |",4.10697282777499
"How can I ensure that my job starts after a specific job has completed?
","Oftentimes, a job will need data from some other job in the queue, but it's nonetheless convenient to submit the second job before the first finishes. Slurm allows you to submit a job with constraints that will keep it from running until these dependencies are met. These are specified with the -d option to Slurm. Common dependency flags are summarized below. In each of these examples, only a single jobid is shown but you can specify multiple job IDs as a colon-delimited list (i.e. #SBATCH -d afterok:12345:12346:12346). For the after dependency, you can optionally specify a +time value for",4.104794672533336
"How can I ensure that my job starts after a specific job has completed?
","In a simple batch queue system, jobs run in a first-in, first-out (FIFO) order. This often does not make effective use of the system. A large job may be next in line to run. If the system is using a strict FIFO queue, many processors sit idle while the large job waits to run. Backfilling would allow smaller, shorter jobs to use those otherwise idle resources, and with the proper algorithm, the start time of the large job would not be delayed. While this does make more effective use of the system, it indirectly encourages the submission of smaller jobs.",4.0779665343088425
"What kind of usage information can I find on the myOLCF site?
","Project members

The myOLCF site is provided to aid in the utilization and management of OLCF allocations. See the https://docs.olcf.ornl.gov/services_and_applications/myolcf/index.html for more information.

If you have any questions or have a request for additional data, please contact the OLCF User Assistance Center.",4.4995014301030185
"What kind of usage information can I find on the myOLCF site?
","At any time, you can view account pages by clicking on the ""My Account"" link in the top navigation menu:

link to my account page

There is only (1) account context in myOLCF: ""you"" as the currently-authenticated user. This account context is linked to the OLCF Moderate account that you used to authenticate to myOLCF.

account page left navigation menu

The left navigation menu also includes an expandable item with links to account-centric pages.",4.285761204115805
"What kind of usage information can I find on the myOLCF site?
","You can check the general status of your application at any time using the myOLCF self-service portal's account status page. For more information, see the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#myOLCF self-service portal documentation<myolcf-overview>. If you need to make further inquiries about your application, you may email our Accounts Team at accounts@ccs.ornl.gov.",4.282566108584719
"How do I run a Python script generated by the Trace tool using PvBatch?
",of how to run a Python script using PvBatch on Andes and Summit.,4.489957537460198
"How do I run a Python script generated by the Trace tool using PvBatch?
","One of the most convenient tools available in the GUI is the ability to convert (or ""trace"") interactive actions in ParaView to Python code. Users that repeat a sequence of actions in ParaView to visualize their data may find the Trace tool useful. The Trace tool creates a Python script that reflects most actions taken in ParaView, which then can be used by either PvPython or PvBatch (ParaView's Python interfaces) to accomplish the same actions. See section https://docs.olcf.ornl.gov/systems/paraview.html#paraview-command-line for an example of how to run a Python script using PvBatch on",4.313218801687385
"How do I run a Python script generated by the Trace tool using PvBatch?
","(specified with module load) and execute a python script called para_example.py using PvBatch. The example python script is detailed below, and users are highly encouraged to use this script (especially after version upgrades) for testing purposes.",4.3130786758278825
"How do I load the scorep module using Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations. It works in combination with Periscope, Scalasca, Vampir, and Tau.

Steps in a typical Score-P workflow to run on https://docs.olcf.ornl.gov/systems/Scorep.html#summit-user-guide:",4.214399293843635
"How do I load the scorep module using Score-P?
","To instrument your code, you need to compile the code using the Score-P instrumentation command (scorep), which is added as a prefix to your compile statement. In most cases the Score-P instrumentor is able to automatically detect the programming paradigm from the set of compile and link options given to the compiler. Some cases will, however, require some additional link options within the compile statement e.g. CUDA instrumentation.

Below are some basic examples of the different instrumentation scenarios:",4.210818527771656
"How do I load the scorep module using Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi-processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations.

For detailed information about using Score-P on Summit and the builds available, please see the Score-P Software Page.",4.176230002144756
"How can I extract all files from a directory in an HPSS archive?
","htar will overwrite files of the same name in the target directory.  When possible, extract only the files you need from large archives. To display the names of the files in the project1.tar archive file within the HPSS home directory:

htar -vtf  /hpss/prod/[projid]/users/[userid]/project1.tar

To extract only one file, executable.out, from the project1 directory in the Archive file called `` /hpss/prod/[projid]/users/[userid]/project1.tar``:

htar -xm -f project1.tar project1/ executable.out",4.351125842074096
"How can I extract all files from a directory in an HPSS archive?
","Project Archive directories may only be accessed via utilities called HSI and HTAR. For more information on using HSI or HTAR, see the https://docs.olcf.ornl.gov/systems/project_centric.html#data-hpss section.",4.347266212135561
"How can I extract all files from a directory in an HPSS archive?
","To extract all files from the project1/src directory in the archive file called project1.tar, and use the time of extraction as the modification time, use the following command:

htar -xm -f  /hpss/prod/[projid]/users/[userid]/project1.tar project1/src

The htar utility has several limitations.

You cannot add or append files to an existing archive.

File path names within an htar archive of the form prefix/name are limited to 154 characters for the prefix and 99 characters for the file name. Link names cannot exceed 99 characters.",4.304172225937243
"Can I use the jsrun command to create resource sets with different numbers of MPI tasks and threads?
","The following example will create 12 resource sets each with 1 task, 4 threads, and 1 GPU. Each MPI task will start 4 threads and have access to 1 GPU. Rank 0 will have access to GPU 0 and start 4 threads on the first socket of the first node ( red resource set). Rank 2 will have access to GPU 1 and start 4 threads on the second socket of the first node ( green resource set). This pattern will continue until 12 resource sets have been created. The following jsrun command will create 12 resource sets (-n12). Each resource set will contain 1 MPI task (-a1), 1 GPU (-g1), and 4 cores (-c4).",4.531894755858693
"Can I use the jsrun command to create resource sets with different numbers of MPI tasks and threads?
","The following jsrun command will request 12 resource sets (-n12). Each resource set will contain 2 MPI tasks (-a2), 1 GPU (-g1), and 2 cores (-c2). 2 MPI tasks will have access to a single GPU. Ranks 0 - 1 will have access to GPU 0 on the first node ( red resource set). Ranks 2 - 3 will have access to GPU 1 on the first node ( green resource set). This pattern will continue until 12 resource sets have been created.",4.488483261702443
"Can I use the jsrun command to create resource sets with different numbers of MPI tasks and threads?
","summit>

The following example will create 4 resource sets each with 6 tasks and 3 GPUs. Each set of 6 MPI tasks will have access to 3 GPUs. Ranks 0 - 5 will have access to GPUs 0 - 2 on the first socket of the first node ( red resource set). Ranks 6 - 11 will have access to GPUs 3 - 5 on the second socket of the first node ( green resource set). This pattern will continue until 4 resource sets have been created. The following jsrun command will request 4 resource sets (-n4). Each resource set will contain 6 MPI tasks (-a6), 3 GPUs (-g3), and 6 cores (-c6).",4.450426237661578
"What is the purpose of the TAU_METRICS setting?
","A similar approach for other metrics, not all of them can be used. TAU provides a tool called tau_cupti_avail, where we can see the list of available metrics, then we have to figure out which CUPTI metrics use these ones.

Activate tracing and declare the data format to OTF2. OTF2 format is supported only by MPI and OpenSHMEM applications.

$ export TAU_TRACE=1
$ export TAU_TRACE_FORMAT=otf2

Use Vampir for visualization.

For example, do not instrument routine sort*(int *)

Create a file select.tau

BEGIN_EXCLUDE_LIST
void sort_#(int *)
END_EXCLUDE_LIST

Declare the TAU_OPTIONS variable",4.237698171671426
"What is the purpose of the TAU_METRICS setting?
","Add the following in your submission file:

export TAU_METRICS=TIME
export TAU_PROFILE=1
export TAU_TRACK_MESSAGE=1
export TAU_COMM_MATRIX=1
jsrun -n 6 -r 6 --smpiargs=""-gpu"" -g 1  tau_exec -T mpi,pgi,pdt -openacc ./miniWeather_mpi_openacc

We declare to TAU to profile the MPI with PDT support through the -T parameters, as well as using the pgi tag for the TAU makefile and OpenACC.

CUPTI metrics for OpenACC are not yet supported for TAU.

When the execution of the instrumented application finishes, there is one directory for each TAU_METRICS declaration with the format MULTI__",4.21759892657752
"What is the purpose of the TAU_METRICS setting?
","matrix | | TAU_THROTTLE | 1 | Setting to 0 turns off throttling, by default removes overhead | | TAU_THROTTLE_NUMCALLS | 100000 | Number of calls before testing throttling | | TAU_THROTTLE_PERCALL | 10 | If a routine is called more than 100000 times and it takes less than 10 usec of inclusive time, throttle it | | TAU_COMPENSATE | 10 | Setting to 1 enables runtime compensation of instrumentation overhead | | TAU_PROFILE_FORMAT | Profile | Setting to ""merged"" generates a single file, ""snapshot"" generates a snapshot per thread | | TAU_METRICS | TIME | Setting to a comma separated list",4.20782498232103
"Can you give an example of how utilization is calculated on Summit?
","The requesting project's allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Summit would be equivalent to using 16,000 Summit node-hours of a project's allocation.



As is the case with many other queuing systems, it is possible to place dependencies on jobs to prevent them from running until other jobs have started/completed/etc. Several possible dependency settings are described in the table below:",4.263863811668729
"Can you give an example of how utilization is calculated on Summit?
",For Summit:,4.215079027336564
"Can you give an example of how utilization is calculated on Summit?
","Utilization is calculated daily using batch jobs which complete between 00:00 and 23:59 of the previous day. For example, if a job moves into a run state on Tuesday and completes Wednesday, the job's utilization will be recorded Thursday. Only batch jobs which write an end record are used to calculate utilization. Batch jobs which do not write end records due to system failure or other reasons are not used when calculating utilization. Jobs which fail because of run-time errors (e.g. the user's application causes a segmentation fault) are counted against the allocation.",4.18373841717476
"Can I use Nsight Systems for remote debugging?
","The profiler will print several sections including information about the CUDA API calls made by the application, as well as any GPU kernels that were launched. Nsight Systems can be used for CUDA C++, CUDA Fortran, OpenACC, OpenMP offload, and other programming models that target NVIDIA GPUs, because under the hood they all ultimately take the same path for generating the binary code that runs on the GPU.",4.197118889466555
"Can I use Nsight Systems for remote debugging?
","One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Andes from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Andes. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.

GDB",4.121677288703634
"Can I use Nsight Systems for remote debugging?
",One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Frontier from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Frontier. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.,4.121564902650025
"Where are the GPU hackathons held?
","A GPU hackathon is a multi-day coding event in which teams of developers port their own applications to run on GPUs, or optimize their applications that already run on GPUs. Each team consists of 3 or more developers who are intimately familiar with (some part of) their application, and they work alongside 1 or more mentors with GPU programming expertise. Our mentors come from universities, national laboratories, supercomputing centers, and industry partners.",4.515854709517516
"Where are the GPU hackathons held?
","If you want/need to get your code running (or optimized) on a GPU-accelerated system, these hackathons offer a unique opportunity to set aside the time, surround yourself with experts in the field, and push toward your development goals. By the end of the event, each team should have part of their code running (or more optimized) on GPUs, or at least have a clear roadmap of how to get there.

<p style=""font-size:20px""><b>Target audience</b></p>",4.515035569122313
"Where are the GPU hackathons held?
","I used html for the section headings to avoid individual entries in the associated menu (TP)



Each year, the Oak Ridge Leadership Computing Facility (OLCF) works with our vendor partners to organize a series of GPU hackathons at a number of host locations around the world.

<p style=""font-size:20px""><b>What is a GPU hackathon?</b></p>",4.328096671004664
"How can I update the cache of commonly used modules in Andes?
","Changing compilers

If a different compiler is required, it is important to use the correct environment for each compiler. To aid users in pairing the correct compiler and environment, the module system on andes automatically pulls in libraries compiled with a given compiler when changing compilers. The compiler modules will load the correct pairing of compiler version, message passing libraries, and other items required to build and run code. To change the default loaded intel environment to the gcc environment for example, use:

$ module load gcc",4.102512578527861
"How can I update the cache of commonly used modules in Andes?
","If a different compiler is required, it is important to use the correct environment for each compiler. To aid users in pairing the correct compiler and environment, the module system on andes automatically pulls in libraries compiled with a given compiler when changing compilers. The compiler modules will load the correct pairing of compiler version, message passing libraries, and other items required to build and run code. To change the default loaded intel environment to the gcc environment for example, use:

$ module load gcc",4.08467279903912
"How can I update the cache of commonly used modules in Andes?
","Alternatively, the ParaView installations in /sw/andes/paraview (i.e., the paraview/5.9.1-egl and paraview/5.9.1-osmesa modules) can also be loaded to avoid this issue.



The ParaView at OLCF Tutorial highlights how to get started on Andes with example datasets.

The Official ParaView User's Guide and the Python API Documentation contain all information regarding the GUI and Python interfaces.

A full list of ParaView Documentation can be found on ParaView's website.

The ParaView Wiki contains extensive information about all things ParaView.",4.0605053255635495
"How many devices are listed as not having a code object in the provided code snippet?
",":1:hip_code_object.cpp      :461 : 43966602810 us:   Devices:
:1:hip_code_object.cpp      :464 : 43966602811 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602811 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602812 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602813 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]",3.913808313461084
"How many devices are listed as not having a code object in the provided code snippet?
",":1:hip_code_object.cpp      :461 : 43966602810 us:   Devices:
:1:hip_code_object.cpp      :464 : 43966602811 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602811 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602812 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]
:1:hip_code_object.cpp      :464 : 43966602813 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]",3.913808313461084
"How many devices are listed as not having a code object in the provided code snippet?
","The information below the dashes which we omitted can be occasionally helpful for debugging, say if there is some kind of hardware problem..",3.910389525586483
"How do I debug my code on Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.295309873402813
"How do I debug my code on Summit?
",For Summit:,4.272587549789486
"How do I debug my code on Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.211007785583525
"Can users expect privacy in their email communications on DOE computers?
","Users are advised that there is no expectation of privacy of your activities on any system that is owned by, leased or operated by UT-Battelle on behalf of the U.S. Department of Energy (DOE). The Company retains the right to monitor all activities on these systems, to access any computer files or electronic mail messages, and to disclose all or part of information gained to authorized individuals or investigative agencies, all without prior notice to, or consent from, any user, sender, or addressee. This access to information or a system by an authorized individual or investigative agency is",4.408111561958641
"Can users expect privacy in their email communications on DOE computers?
","authorized individual or investigative agency is in effect during the period of your access to information on a DOE computer and for a period of three years thereafter. OLCF personnel and users are required to address, safeguard against, and report misuse, abuse and criminal activities. Misuse of OLCF resources can lead to temporary or permanent disabling of accounts, loss of DOE allocations, and administrative or legal actions. Users who have not accessed a OLCF computing resource in at least 6 months will be disabled. They will need to reapply to regain access to their account. All users",4.199248973500954
"Can users expect privacy in their email communications on DOE computers?
","in Arms Regulations, and other types of data that require privacy.",4.144258494623126
"How do I ensure that my application is multithreaded on Frontier?
","Because a Frontier compute node has two hardware threads available (2 logical cores per physical core), this enables the possibility of multithreading your application (e.g., with OpenMP threads). Although the additional hardware threads can be assigned to additional MPI tasks, this is not recommended. It is highly recommended to only use 1 MPI task per physical core and to use OpenMP threads instead on any additional logical cores gained when using both hardware threads.",4.311647581598665
"How do I ensure that my application is multithreaded on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.178553092953512
"How do I ensure that my application is multithreaded on Frontier?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.148475085305952
"What is the name of the user who owns the file?
","*****ORIGINAL FILE*****
This is my file. There are many like it but this one is mine.
***********************

*****UPDATED FILE******
This is my file. There are many like it but this one is mine.
spock25
***********************



If you have problems or need helping running on Spock, please submit a ticket by emailing help@olcf.ornl.gov.



JIRA_CONTENT_HERE",4.003275658547641
"What is the name of the user who owns the file?
","When the owner of the data requests a change of ownership for any reason, e.g., the owner is leaving the project and grants the PI ownership of the data.

In situations of suspected abuse/misuse computational resources, criminal activity, or cyber-security violations.",3.989835931155339
"What is the name of the user who owns the file?
","Users interested in sharing files publicly via the World Wide Web can request a user website directory be created for their account. User website directories (~/www) have a 5GB storage quota and allow access to files at http://users.nccs.gov/~user (where user is your userid). If you are interested in having a user website directory created, please contact the User Assistance Center at help@olcf.ornl.gov.",3.9885077967675633
"Can you use hipMemAdvise() to change the memory allocation granularity of a memory block allocated with malloc()?
","Finally, the following table summarizes the nature of the memory returned based on the flag passed as argument to hipMallocManaged() and the use of CPU regular malloc() routine with the possible use of hipMemAdvise().

| API | MemAdvice | Result | | --- | --- | --- | | hipMallocManaged() |  | Fine grained | | hipMallocManaged() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained | | malloc() |  | Fine grained | | malloc() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained |",4.389467951374344
"Can you use hipMemAdvise() to change the memory allocation granularity of a memory block allocated with malloc()?
","Finally, the following table summarizes the nature of the memory returned based on the flag passed as argument to hipMallocManaged() and the use of CPU regular malloc() routine with the possible use of hipMemAdvise().

| API | MemAdvice | Result | | --- | --- | --- | | hipMallocManaged() |  | Fine grained | | hipMallocManaged() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained | | malloc() |  | Fine grained | | malloc() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained |",4.389467951374344
"Can you use hipMemAdvise() to change the memory allocation granularity of a memory block allocated with malloc()?
","Users applying floating point atomic operations (e.g., atomicAdd) on memory regions allocated via regular hipMalloc() can safely apply the -munsafe-fp-atomics flags to their codes to get the best possible performance and leverage hardware supported floating point atomics. Atomic operations supported in hardware on non-FP datatypes  (e.g., INT32) will work correctly regardless of the nature of the memory region used.",4.142730730918329
"How do I ensure that my project complies with the OLCF's policies and procedures when using the SPI?
","CITADEL, having undergone comprehensive technical-, legal-, and policy-oriented reviews and received third-party accreditation, has presented new possibilities for research projects that previously could not access utilize OLCF systems due to the nature of their data.

If you are new to the OLCF or the OLCF's SPI, this page can help get you started.  Below are some of the high-level steps needed to begin use of the OLCF's SPI:",4.430367391137394
"How do I ensure that my project complies with the OLCF's policies and procedures when using the SPI?
","Similar to standard OLCF workflows, to run SPI workflows on OLCF resources, you will first need an approved project.  The project will be awarded an allocation(s) that will allow resource access and use.  The first step to requesting an allocation is to complete the project request form.  The form will initiate the process that includes peer review, export control review, agreements, and others.  Once submitted, members of the OLCF accounts team will help walk you through the process.

Please see the OLCF Accounts and Projects section of this site to request a new project.",4.422421639800761
"How do I ensure that my project complies with the OLCF's policies and procedures when using the SPI?
","https://docs.olcf.ornl.gov/systems/index.html#Whitelist your IPs<spi-whitelisting-ip>.  Access to the SPI resources is limited to IPs that have been whitelisted by the OLCF.  The only exception is for projects using KDI resources.  If your project also uses KDI resources, you will use the KDI access procedures and do not need to provide your IP to the OLCF.",4.323963738974374
"How can I ensure the confidentiality of my data on OLCF systems?
","The OLCF systems provide protections to maintain the confidentiality, integrity, and availability of user data. Measures include: the availability of file permissions, archival systems with access control lists, and parity/CRC checks on data paths/files. It is the user’s responsibility to set access controls appropriately for data. In the event of system failure or malicious actions, the OLCF makes no guarantee against loss of data nor makes a guarantee that a user’s data could not be potentially accessed, changed, or deleted by another individual. It is the user’s responsibility to insure",4.475887199557985
"How can I ensure the confidentiality of my data on OLCF systems?
","The OLCF systems provide protections to maintain the confidentiality, integrity, and availability of user data. Measures include the availability of file permissions, archival systems with access control lists, and parity and CRC checks on data paths and files. It is the user’s responsibility to set access controls appropriately for the data. In the event of system failure or malicious actions, the OLCF makes no guarantee against loss of data or that a user’s data can be accessed, changed, or deleted by another individual. It is the user’s responsibility to insure the appropriate level of",4.461224475537552
"How can I ensure the confidentiality of my data on OLCF systems?
","Never allow access to your sensitive data to anyone outside of your group.

Transfer of sensitive data must be through the use encrypted methods (scp, sftp, etc).

All sensitive data must be removed from all OLCF resources when your project has concluded.",4.450146058266243
"How do I add the necessary lines in my job script to work around the ERF issue on Summit?
","Explicit Resource Files provide even more fine-granied control over how processes are mapped onto compute nodes. Users have reported errors when using ERF on Summit:

Failed to bind process to ERF smt array, err: Invalid argument

This is a known issue with the current version of jsrun. A workaround is to add the following lines in your job script.

export JSM_ROOT=/gpfs/alpine/stf007/world-shared/vgv/inbox/jsm_erf/jsm-10.4.0.4/opt/ibm/jsm
$JSM_ROOT/bin/jsm &
$JSM_ROOT/bin/jsrun --erf_input=Your_erf ./Your_app

If you get an error message that looks like:",4.290050915780713
"How do I add the necessary lines in my job script to work around the ERF issue on Summit?
","This method on Summit requires the user to be present until the job completes. For users who have long scripts or are unable to monitor the job, you can submit the above lines in a batch script. However, you will wait in the queue twice, so this is not recommended. Alternatively, one can use Andes.

For Andes/Frontier (Slurm Script):

Andes

.. code-block:: bash


   #!/bin/bash
   #SBATCH -A XXXYYY
   #SBATCH -J visit_test
   #SBATCH -N 1
   #SBATCH -p gpu
   #SBATCH -t 0:05:00

   cd $SLURM_SUBMIT_DIR
   date

   module load visit",4.135942598013076
"How do I add the necessary lines in my job script to work around the ERF issue on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.135596144447061
"How many tensor cores does each SM on the V100 contain?
","Each SM on the V100 contains 32 FP64 (double-precision) cores, 64 FP32 (single-precision) cores, 64 INT32 cores, and 8 tensor cores. A 128-KB combined memory block for shared memory and L1 cache can be configured to allow up to 96 KB of shared memory. In addition, each SM has 4 texture units which use the (configured size of the) L1 cache.",4.516974692828421
"How many tensor cores does each SM on the V100 contain?
","For more information, please see the following section of NVIDIA's CUDA Programming Guide: http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#independent-thread-scheduling-7-x

The Tesla V100 contains 640 tensor cores (8 per SM) intended to enable faster training of large neural networks. Each tensor core performs a D = AB + C operation on 4x4 matrices. A and B are FP16 matrices, while C and D can be either FP16 or FP32:",4.418082788855716
"How many tensor cores does each SM on the V100 contain?
",section provides information for using the V100 Tensor Cores.,4.362691132843625
"How do I run a shell command before building a target in pmake?
","Inside the simulation directory, you should see 3 new files, simulate.sh, which contains the shell script pmake built from the simulate rule, simulate.log, containing the log output from running simulate.sh, and run.log, the file written during rule execution.

Extending pmake using your own rules is straightforward. pmake acts like make, running rules to create output files (that do not yet exist) from input files (that must exist before the rule is run).

Unlike make, pmake does not run a rule unless its output is requested by some target.",4.218576771311177
"How do I run a shell command before building a target in pmake?
","Once you have entered the virtual environment, pmake can be installed with:

$ python -m pip install git+https://code.ornl.gov/99R/pmake.git@latest

Run the following command to verify that pmake is available:

$ pmake --help

To run a pmake demo on Summit, you will create a pmake-example directory with its preferred file layout, then submit a batch job to LSF from a Summit login node.

First, create the directories,

$ mkdir -p pmake-example/simulation

Next, create pmake's two configuration files, rules.yaml and targets.yaml:

# pmake-example/targets.yaml",4.13691100973075
"How do I run a shell command before building a target in pmake?
","<string>:5: (INFO/1) Duplicate implicit target name: ""pmake"".

pmake is a parallel make developed for use within batch jobs.  A rules.yaml file specifies extended make-rules with:

multiple input and multiple output files

a resource-set specification

a multi-line shell script that can use variable substitution (e.g. {mpirun} expands to {jsrun -g -c ...} on summit).

Full documentation and examples are available in https://code.ornl.gov/99R/pmake.",4.128216757497577
"How do I navigate to the ""Manage your repositories, projects, settings"" tab in ArgoCD?
","connect using https

connect using GitHub App

Each of these methods are described in the ArgoCD Private Repositories document. For example, to connect to an OLCF or NCCS GitLab instance, create a deploy token per the GitLab documentation for use by ArgoCD copying the username and token value. Then, in ArgoCD, navigate to ""Manage your repositories, projects, settings"" tab and select ""Repositories"".

Image of the Manage your repositories, project, settings tab.

Once into the ""Repositories"" area, select ""CONNECT REPO USING HTTPS"":

Image of the repositories area.",4.370672643034067
"How do I navigate to the ""Manage your repositories, projects, settings"" tab in ArgoCD?
","Image of the ArgoCD applications tab.

Prior to using ArgoCD, a git repository containing Kubernetes custom resources needs to be added for use.

The git repository containing Kubernetes custom resources is typically not the same git repository used to build the application.

There are three ways for ArgoCD to connect to a git repository:

connect using ssh

connect using https

connect using GitHub App",4.340907518308935
"How do I navigate to the ""Manage your repositories, projects, settings"" tab in ArgoCD?
","Image of the repositories area.

and then add the ""Repository URL"", ""Username"" for the deploy token, and the deploy token itself as the password. If Git-LFS support is needed, click the ""Enable LFS support"" at the bottom of the page. Once entries look correct:

Image of the connect to repo using https parameters.

click the ""CONNECT"" button in the upper left. Once entered and ArgoCD is able to access the server, the connection should have a status of ""Successful"" with a green check mark:

Image of a successful git repository configuration.",4.302316941245988
"How can I set the TERM_ARG3 environment variable for the ORNL Summit server in Paraview?
","<Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Windows"">
            <Set name=""TERM_PATH"" value=""cmd"" />
            <Set name=""TERM_ARG1"" value=""/C"" />
            <Set name=""TERM_ARG2"" value=""start"" />
            <Set name=""TERM_ARG3"" value="""" />
            <Set name=""SSH_PATH"" value=""plink.exe"" />
          </Case>
          <Case value=""Unix"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.241959422884617
"How can I set the TERM_ARG3 environment variable for the ORNL Summit server in Paraview?
","<Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Windows"">
            <Set name=""TERM_PATH"" value=""cmd"" />
            <Set name=""TERM_ARG1"" value=""/C"" />
            <Set name=""TERM_ARG2"" value=""start"" />
            <Set name=""TERM_ARG3"" value="""" />
            <Set name=""SSH_PATH"" value=""plink.exe"" />
          </Case>
          <Case value=""Unix"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.241959422884617
"How can I set the TERM_ARG3 environment variable for the ORNL Summit server in Paraview?
","</Option>
      </Options>
      <Command exec=""$TERM_PATH$"" delay=""5"">
        <Arguments>
          <Argument value=""$TERM_ARG1$""/>
          <Argument value=""$TERM_ARG2$""/>
          <Argument value=""$TERM_ARG3$""/>
          <Argument value=""$SSH_PATH$""/>
          <Argument value=""-t""/>
          <Argument value=""-R""/>
          <Argument value=""$PV_SERVER_PORT$:localhost:$PV_SERVER_PORT$""/>
          <Argument value=""$USER$@$HOST$""/>
          <Argument value=""/sw/andes/paraview/pvsc/ORNL/login_node.sh""/>
          <Argument value=""$NUM_NODES$""/>",4.201107471141032
"What are the requirements for handling and transferring export controlled information at the ORNL User Facility?
","I will use best efforts to publish the results from my use of the ORNL User Facility in an open scientific journal or significant industry technical journal or conference proceedings. I will acknowledge use of the ORNL User Facility in the publication and notify the ORNL User Facility of any publications that result from my use of the facility.

I will comply with all U.S. Export Control laws and regulations and be responsible for the appropriate handling and transfer of any export controlled information, which may require advance U.S. Government authorization.",4.435065333479314
"What are the requirements for handling and transferring export controlled information at the ORNL User Facility?
","I understand that my access is limited to certain designated areas and/or systems, and my access may be revoked if I pose a security, safety, or operational risk.

I will follow the applicable ORNL rules, regulations and requirements, including those requirements of the ORNL User Facility. I will follow the requirements set forth in training if assigned to me by the ORNL User Facility.

I will take all reasonable precautions to protect the safety and health of others and comply with all applicable safety and health requirements.",4.407500802861591
"What are the requirements for handling and transferring export controlled information at the ORNL User Facility?
","Export Control: The project request will be reviewed by ORNL Export Control to determine whether sensitive or proprietary data will be generated or used. The results of this review will be forwarded to the PI. If the project request is deemed sensitive and/or proprietary, the OLCF Security Team will schedule a conference call with the PI to discuss the data protection needs.",4.38815241370648
"What is the purpose of the `module load open-ce/1.5.2-py39-0` command?
","[user@login2.summit ~]$ module avail open-ce

---------------------------------- /sw/summit/modulefiles/core ---------------------------------
open-ce/1.2.0-py36-0        open-ce/1.4.0-py37-0    open-ce/1.5.0-py37-0    open-ce/1.5.2-py37-0
open-ce/1.2.0-py37-0        open-ce/1.4.0-py38-0    open-ce/1.5.0-py38-0    open-ce/1.5.2-py38-0
open-ce/1.2.0-py38-0 (D)    open-ce/1.4.0-py39-0    open-ce/1.5.0-py39-0    open-ce/1.5.2-py39-0

[user@login2.summit ~]$ module load open-ce/1.5.0-py39-0",4.371769677984656
"What is the purpose of the `module load open-ce/1.5.2-py39-0` command?
","module load open-ce

Loading a specific version of the module is recommended to future-proof scripts against software updates. The following commands can be used to find and load specific module versions:

[user@login2.summit ~]$ module avail open-ce",4.303645700138481
"What is the purpose of the `module load open-ce/1.5.2-py39-0` command?
","As seen above, there are also different Python versions of each Open-CE release available on Summit (indicated by -pyXY- in the module name, where ""X"" and ""Y"" are the major and minor Python version numbers, respectively.)

For more information on loading modules, including loading specific verions, see: https://docs.olcf.ornl.gov/systems/ibm-wml-ce.html#environment-management-with-lmod

Loading an Open-CE module will activate a conda environment which is pre-loaded with the following packages, and their dependencies:",4.260510724413172
"How do I access the OLCF systems?
","When all of the above steps are completed, your user account will be created and you will be notified by email. Now that you have a user account and it has been associated with a project, you're ready to get to work. This website provides extensive documentation for OLCF systems, and can help you efficiently use your project's allocation. We recommend reading the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#System User Guides<system-user-guides> for the machines you will be using often.",4.466007775061071
"How do I access the OLCF systems?
","Please see the https://docs.olcf.ornl.gov/systems/index.html#OLCF Applying for a User Account<applying-for-a-user-account> section of this site to request a new account and join an existing project.  Once submitted, the OLCF Accounts team will help walk you through the process.",4.452136444027742
"How do I access the OLCF systems?
","New to the Oak Ridge Leadership Computing Facility?

Welcome! The information below introduces how we structure user accounts, projects, and system allocations. It's all you need to know about getting to work. In general, OLCF resources are granted to projects in allocations, and are made available to the users associated with each project.",4.42223669550568
"What is the advantage of using the Citadel framework for SPI workflows on OLCF resources?
","The Citadel framework allows use of the OLCF's existing HPC resources Summit and Frontier for SPI workflows.  Citadel adds measures to ensure separation of SPI and non-SPI workflows and data. This section provides differences when using OLCF resources for SPI and non-SPI workflows.  Because the Citadel framework just adds another security layer to existing HPC resources, many system use methods are the same between SPI and non-SPI workflows.  For example, compiling, batch scheduling, and job layout are the same between the two security enclaves.  Because of this, the existing resource user",4.6869063480524495
"What is the advantage of using the Citadel framework for SPI workflows on OLCF resources?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.

The SPI provides access to the OLCF's Summit resource for compute.  To safely separate SPI and non-SPI workflows, SPI workflows must use a separate login node named https://docs.olcf.ornl.gov/systems/citadel_user_guide.html#Citadel<spi-compute-citadel>.  Citadel provides a login node specifically for SPI workflows.",4.487667510602074
"What is the advantage of using the Citadel framework for SPI workflows on OLCF resources?
","CITADEL, having undergone comprehensive technical-, legal-, and policy-oriented reviews and received third-party accreditation, has presented new possibilities for research projects that previously could not access utilize OLCF systems due to the nature of their data.

If you are new to the OLCF or the OLCF's SPI, this page can help get you started.  Below are some of the high-level steps needed to begin use of the OLCF's SPI:",4.465969512578326
"Can I access HPSS from outside of OLCF?
","Each OLCF user receives an HPSS account automatically. Users can transfer data to HPSS from any OLCF system using the HSI or HTAR utilities. For more information on using HSI or HTAR, see the https://docs.olcf.ornl.gov/systems/user_centric.html#data-hpss .

Each file and directory on HPSS is associated with an HPSS storage allocation. For information on HPSS storage allocations, please visit the https://docs.olcf.ornl.gov/systems/user_centric.html#data-policy section.",4.331885349764419
"Can I access HPSS from outside of OLCF?
","System Overview





The High Performance Storage System (HPSS) provides tape storage for large amounts of data created on OLCF systems. The HPSS can be accessed from any OLCF system through the hsi utility. More information about using HPSS can be found in the https://docs.olcf.ornl.gov/systems/hpss_user_guide.html#data-hpss section of the https://docs.olcf.ornl.gov/systems/hpss_user_guide.html#data-storage-and-transfers page.",4.294318086111822
"Can I access HPSS from outside of OLCF?
","For information on usage and best practices for HPSS, please see the https://docs.olcf.ornl.gov/systems/user_centric.html#data-hpss documentation.",4.267794737328854
"What is the name of the third device listed in the variable ""devices""?
","The information below the dashes which we omitted can be occasionally helpful for debugging, say if there is some kind of hardware problem..",3.9306034957834015
"What is the name of the third device listed in the variable ""devices""?
","The above log messages indicate the type of image required by each device, given its current mode (amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack-) and the images found in the binary (hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+).",3.900494017055802
"What is the name of the third device listed in the variable ""devices""?
","The above log messages indicate the type of image required by each device, given its current mode (amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack-) and the images found in the binary (hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+).",3.897277352189642
"How do I complete identity verification?
","If you are processing sensitive or proprietary data, additional paperwork is required and will be sent to you.

If you need an RSA SecurID token from our facility, the token and additional paperwork will be sent to you via email to complete identity proofing.",4.255420184485101
"How do I complete identity verification?
","Notary Token Verification Form (See Notary Instructions) For certain resources, ORNL requires identity proofing to authenticate a user’s identity and possession of the token prior to activation. The Notary Token Verification Form verifies the identity of the applicant and maintains records of the identity proofing credentials that may be disclosed in the future to an authorized individual or investigative agency. Alternatively, you may schedule this verification with a member of our staff virtually my filling out the Conference Scheduler Form. You will need your Application Confirmation",4.222090912267199
"How do I complete identity verification?
","When our accounts team begins processing your application, you will receive an automated email containing an unique 36-character confirmation code. Make note of it; you can use it to check the status of your application at any time.

The principal investigator (PI) of the project must approve your account and system access. We will make the project PI aware of your request.",4.149081504214894
"How can you optimize the performance of a kernel on Crusher?
","If CAAR or ECP teams require a temporary exception to this policy, please email help@olcf.ornl.gov with your request and it will be given to the OLCF Resource Utilization Council (RUC) for review.

This section describes how to map processes (e.g., MPI ranks) and process threads (e.g., OpenMP threads) to the CPUs and GPUs on Crusher. The https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#crusher-compute-nodes diagram will be helpful when reading this section to understand which physical CPU cores (and hardware threads) your processes and threads run on.",4.17027448824012
"How can you optimize the performance of a kernel on Crusher?
","Crusher contains a total of 768 AMD MI250X. The AMD MI250X has a peak performance of 53 TFLOPS in double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 26.5 TFLOPS (double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in both directions simultaneously).



To connect to Crusher, ssh to crusher.olcf.ornl.gov. For example:

$ ssh <username>@crusher.olcf.ornl.gov",4.166305469764508
"How can you optimize the performance of a kernel on Crusher?
","Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.",4.15545909069343
"How does the Red Hat OpenShift GitOps operator handle configuration management?
","In addition to multiple operators, Red Hat OpenShift GitOps provides RBAC roles and bindings, default resource request limits, integration with Red Hat SSO, integration with OpenShift cluster logging as well as cluster metrics, ability to manage resources across multiple OpenShift clusters with a single OpenShift GitOps instance, automatic remediation if resource configuration changes from desired configuration, and promotion of configurations from dev to test/staging to production.

Resources

Installation

Configuration

Multiple Project Management

Application Deployment",4.502884210288239
"How does the Red Hat OpenShift GitOps operator handle configuration management?
","From the release notes:

Red Hat OpenShift GitOps is a declarative way to implement continuous deployment for cloud native applications. Red Hat OpenShift GitOps ensures consistency in applications when you deploy them to different clusters in different environments, such as: development, staging, and production.",4.471511001173758
"How does the Red Hat OpenShift GitOps operator handle configuration management?
","The Red Hat OpenShift GitOps operator is already installed onto each of the Slate clusters. However, to use ArgoCD an instance will need to be added into a project namespace. If one is to use ArgoCD to manage resources in a single project, then the ArgoCD instance could be located in the same project as the resources being managed. However, if the application being managed is resource (CPU and memory) sensitive or if resources in multiple projects are to be managed, it may be better to have the ArgoCD instance deployed into a separate project to allow for better control of resources allocated",4.318643501776146
"How can a project manager ensure that their project does not exceed its allocation?
","For example, consider that job1 is submitted at the same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. In addition, projects that are at 125% of their allocated time will be limited to only one running job at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.224254137980508
"How can a project manager ensure that their project does not exceed its allocation?
","For example, consider that job1 is submitted at the same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. In addition, projects that are at 125% of their allocated time will be limited to only one running job at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.224254137980508
"How can a project manager ensure that their project does not exceed its allocation?
","Before you can do anything you need a Project to do your things in. Fortunately, when you get an allocation on one of our clusters a Project is automatically created for you with the same name as the allocation. By using our own distinct Project we are ensuring that we will not interfere with anyone else's work.

NOTE: Everywhere that you see <cluster> replace that with the cluster that you will be running on (marble or onyx).",4.222434838178119
"How many calls are made to the semi_discrete_step function?
","97.1           15     1:08.668        4501       27006      15256 void perform_timestep(double *, double *, double *, double *, double)
 97.1        1,167     1:08.653       27006       54012       2542 void semi_discrete_step(double *, double *, double *, double, int, double *, double *)
 48.4       34,240       34,240       13503           0       2536 void compute_tendencies_z(double *, double *, double *)
 46.9       33,199       33,199       13503           0       2459 void compute_tendencies_x(double *, double *, double *)",4.2093769169422695
"How many calls are made to the semi_discrete_step function?
","The output shows that each independent process ran on its own CPU core and GPU on the same single node. To show that the ranks ran simultaneously, date was called before each job step and a 20 second sleep was added to the end of the hello_jobstep program. So the output also shows that the first job step was submitted at :45 and the subsequent job steps were all submitted between :46 and :52. But because each hello_jobstep sleeps for 20 seconds, the subsequent job steps must have all been running while the first job step was still sleeping (and holding up its resources). And the same argument",3.855917564259767
"How many calls are made to the semi_discrete_step function?
","A summary of the algorithm used for calculating in mixed precision is in the following image.



We see in the graph below that it is possible to achieved a 3-4X performance improvement over the double-precision solver, while achieving the same level of accuracy. It has also been observed that the use of Tensor Cores makes the problem more likely to converge than strict half-precision GEMMs due to the ability to accumulate into 32-bit results.",3.853222232571504
"How do I set the cache directory for singularity builds on Summit?
","The reason we include the --disable-cache flag is because Singularity's caching can fill up your home directory without you realizing it. And if the home directory is full, Singularity builds will fail. If you wish to make use of the cache, you can set the environment variable SINGULARITY_CACHEDIR=/tmp/containers/<user>/singularitycache or something like that so that the NVMe storage is used as the cache.

As a simple example, we will run hostname with the Singularity container.

Create a file submit.lsf with the contents below.",4.303861002879047
"How do I set the cache directory for singularity builds on Summit?
","Users will be building and running containers on Summit without root permissions i.e. containers on Summit are rootless.  This means users can get the benefits of containers without needing additional privileges. This is necessary for a shared system like Summit. And this is part of the reason why Docker doesn't work on Summit. Podman and Singularity provides rootless support but to different extents hence why users need to use a combination of both.

Users will need to set up a file in their home directory /ccs/home/<username>/.config/containers/storage.conf with the following content:",4.209857834097059
"How do I set the cache directory for singularity builds on Summit?
","Singularity is a container framework from Sylabs. On Summit, we will use Singularity solely as the runtime. We will convert the tar files of the container images Podman creates into sif files, store those sif files on GPFS, and run them with Jsrun. Singularity also allows building images but ordinary users cannot utilize that on Summit due to additional permissions not allowed for regular users.",4.194478326977737
"What is the standard deviation of MPI-IO Bytes Written for the profile?
","USER EVENTS Profile :NODE 0, CONTEXT 0, THREAD 0
---------------------------------------------------------------------------------------
NumSamples   MaxValue   MinValue  MeanValue  Std. Dev.  Event Name
---------------------------------------------------------------------------------------
      1058    1.6E+05          4  9.134E+04  7.919E+04  MPI-IO Bytes Written
       454        284          4      5.947       13.2  MPI-IO Bytes Written : int main(int, char **) => void output(double *, double) => MPI_File_write_at()",4.3207589421163295
"What is the standard deviation of MPI-IO Bytes Written for the profile?
","604    1.6E+05    1.6E+05    1.6E+05          0  MPI-IO Bytes Written : int main(int, char **) => void output(double *, double) => MPI_File_write_at_all()
      1058       9412     0.1818       3311       3816  MPI-IO Write Bandwidth (MB/s)
       454      1.856     0.1818     0.5083     0.1904  MPI-IO Write Bandwidth (MB/s) : int main(int, char **) => void output(double *, double) => MPI_File_write_at()
       604       9412      2.034       5799       3329  MPI-IO Write Bandwidth (MB/s) : int main(int, char **) => void output(double *, double) => MPI_File_write_at_all()",4.31420635147832
"What is the standard deviation of MPI-IO Bytes Written for the profile?
","Click on the new metric, ""PAPI_TOT_INS / PAPI_TOT_CYC"" to see the instructions per cycle (IPC) across the various routines.



Click on the label mean:



For the non-MPI routines/calls, an IPC that is lower than 1.5 means that there is a potential for performance improvement.

Menu Windows -> 3D Visualization (3D demands OpenGL) will not work on Summit, and you will need to download the data on your laptop and install TAU locally to use this feature.

You can see per MPI rank, per routine, the exclusive time and the floating operations.",4.154630880901731
"How can I optimize my application's performance using rocprof?
","This provides the minimum set of metrics used to construct a roofline model, in the minimum number of passes. Each line that begins with pmc indicates that the application will be re-run, and the metrics in that line will be collected. rocprof can collect up to 8 counters from each block (SQ, TCC) in each application re-run. To gather metrics across multiple MPI ranks, you will need to use a command that redirects the output of rocprof to a unique file for each task. For example:",4.294470454758369
"How can I optimize my application's performance using rocprof?
","This provides the minimum set of metrics used to construct a roofline model, in the minimum number of passes. Each line that begins with pmc indicates that the application will be re-run, and the metrics in that line will be collected. rocprof can collect up to 8 counters from each block (SQ, TCC) in each application re-run. To gather metrics across multiple MPI ranks, you will need to use a command that redirects the output of rocprof to a unique file for each task. For example:",4.294470454758369
"How can I optimize my application's performance using rocprof?
","Integer instructions and cache levels are currently not documented here.

To get started, you will need to make an input file for rocprof, to be passed in through rocprof -i <input_file> --timestamp on -o my_output.csv <my_exe>. Below is an example, and contains the information needed to roofline profile GPU 0, as seen by each rank:",4.258553391108306
"How do I join a project on the OLCF system?
","When all of the above steps are completed, your user account will be created and you will be notified by email. Now that you have a user account and it has been associated with a project, you're ready to get to work. This website provides extensive documentation for OLCF systems, and can help you efficiently use your project's allocation. We recommend reading the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#System User Guides<system-user-guides> for the machines you will be using often.",4.545347231858966
"How do I join a project on the OLCF system?
","If you already have a user account at the OLCF, your existing credentials can be leveraged across multiple projects.

If your user account has an associated RSA SecurID (i.e. you have an ""OLCF Moderate"" account), you gain access to another project by logging in to the myOLCF self-service portal and filling out the application under My Account > Join Another Project. For more information, see the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#myOLCF self-service portal documentation<myolcf-overview>.",4.53101370013796
"How do I join a project on the OLCF system?
","Please see the https://docs.olcf.ornl.gov/systems/index.html#OLCF Applying for a User Account<applying-for-a-user-account> section of this site to request a new account and join an existing project.  Once submitted, the OLCF Accounts team will help walk you through the process.",4.5240961451104456
"How does Lmod protect against conflicts?
","Lmod is a recursive environment module system, meaning it is aware of module compatibility and actively alters the environment to protect against conflicts. Messages to stderr are issued upon Lmod implicitly altering the environment. Environment modules are structured hierarchically by compiler family such that packages built with a given compiler will only be accessible if the compiler family is first present in the environment.

Lmod can interpret both Lua modulefiles and legacy Tcl modulefiles. However, long and logic-heavy Tcl modulefiles may require porting to Lua.",4.260538260251924
"How does Lmod protect against conflicts?
","Lmod is a recursive environment module system, meaning it is aware of module compatibility and actively alters the environment to protect against conflicts. Messages to stderr are issued upon Lmod implicitly altering the environment. Environment modules are structured hierarchically by compiler family such that packages built with a given compiler will only be accessible if the compiler family is first present in the environment.

note: Lmod can interpret both Lua modulefiles and legacy Tcl modulefiles. However, long and logic-heavy Tcl modulefiles may require porting to Lua.",4.251306827903624
"How does Lmod protect against conflicts?
","Modules are changed recursively. Some commands, such as module swap, are available to maintain compatibility with scripts using Tcl Environment Modules, but are not necessary since Lmod recursively processes loaded modules and automatically resolves conflicts.",4.244080617450223
"What is the purpose of the --image flag when creating a pod in Slate?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.159533795941217
"What is the purpose of the --image flag when creating a pod in Slate?
","The -t flag names the container image and the -f flag indicates the file to use for building the image.

Run podman image ls to see the list of images. localhost/simple should be among them. Any container created without an explicit url to a container registry in its name will automatically have the localhost prefix.

$ podman image ls
REPOSITORY             TAG      IMAGE ID      CREATED      SIZE
localhost/simple       latest   e47dbfde3e99  3 hours ago  687 MB
quay.io/centos/centos  stream8  ad6f8b5e7f64  8 days ago   497 MB",4.147931455754713
"What is the purpose of the --image flag when creating a pod in Slate?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",4.134174714686088
"How do I upgrade to a higher version of open-ce?
","module load open-ce

Loading a specific version of the module is recommended to future-proof scripts against software updates. The following commands can be used to find and load specific module versions:

[user@login2.summit ~]$ module avail open-ce",4.2331382120482095
"How do I upgrade to a higher version of open-ce?
",The following packages are available in this release of OpenCE:,4.16236397794467
"How do I upgrade to a higher version of open-ce?
","| Component | Old Version | New Version | | --- | --- | --- | | Red Hat Enterprise Linux | 8.3 | 8.4 | | Mellanox InfiniBand Driver | 5.3-1.0.0.1 | 5.4-1.0.3.0 | | NVIDIA driver | 450.36.06 | 460.106.00-1 | | Slurm | 20.02.6 | 20.02.7-1 |



<p style=""font-size:20px""><b>Summit: OpenCE 1.4.0 (October 13, 2021)</b></p>

OpenCE 1.4.0 is now available on Summit. OpenCE 1.4.0 is available for python versions 3.7, 3.8, and 3.9. These builds can be accessed by loading the open-ce/1.4.0-py37-0, open-ce/1.4.0-py38-0, and open-ce/1.4.0-py39-0 modules, respectively.",4.158367739809005
"How do I access my personal scratch directory?
","Each project is granted a Project Work directory; these reside in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. Project Work directories can be accessed by all members of a project and are intended for sharing data within a project. Project Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but",4.144355728885624
"How do I access my personal scratch directory?
","Each project has a World Work directory that resides in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. World Work areas can be accessed by all users of the system and are intended for sharing of data between projects. World Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but rather should be",4.127861715662066
"How do I access my personal scratch directory?
","$ ls /gpfs/wolf/[projid]
proj-shared  scratch  world-shared

proj-shared can be accessed by all members of a project.

scratch contains directories for each user of a project and only that user can access their own directory.

world-shared can be accessed by any users on the system in any project.

Ascent is a training system that is not intended to be used as an OLCF user resource. Access to the system is only obtained through OLCF training events.",4.103002386505211
"Can I specify the exact hardware threads that I want to use for each OpenMP thread?
","Now the output shows that each OpenMP thread ran on (one of the hardware threads of) its own physical CPU core. More specifically (see the Crusher Compute Node diagram), OpenMP thread 000 of MPI rank 000 ran on hardware thread 001 (i.e., physical CPU core 01), OpenMP thread 001 of MPI rank 000 ran on hardware thread 002 (i.e., physical CPU core 02), OpenMP thread 000 of MPI rank 001 ran on hardware thread 009 (i.e., physical CPU core 09), and OpenMP thread 001 of MPI rank 001 ran on hardware thread 010 (i.e., physical CPU core 10) - as intended.",4.371892020670299
"Can I specify the exact hardware threads that I want to use for each OpenMP thread?
","Now the output shows that each OpenMP thread ran on (one of the hardware threads of) its own physical CPU cores. More specifically (see the Spock Compute Node diagram), OpenMP thread 000 of MPI rank 000 ran on hardware thread 000 (i.e., physical CPU core 00), OpenMP thread 001 of MPI rank 000 ran on hardware thread 001 (i.e., physical CPU core 01), OpenMP thread 000 of MPI rank 001 ran on hardware thread 016 (i.e., physical CPU core 16), and OpenMP thread 001 of MPI rank 001 ran on hardware thread 017 (i.e., physical CPU core 17) - as expected.",4.348524981300389
"Can I specify the exact hardware threads that I want to use for each OpenMP thread?
","physical CPU cores available, in which case the remaining OpenMP threads will share hardware threads and a WARNING will be issued as shown in the previous example.",4.301023435590656
"How can I check the status of my sbcast job?
","To view the status of multiple jobs launched sequentially or concurrently within a batch script, you can use jslist to see which are completed, running, or still queued. If you are using it outside of an interactive batch job, use the -c option to specify the CSM allocation ID number. The following example shows how to obtain the CSM allocation number for a non interactive job and then check its status.

$ bsub test.lsf
Job <26238> is submitted to default queue <batch>.

$ bjobs -l 26238 | grep CSM_ALLOCATION_ID
Sun Feb 16 19:01:18: CSM_ALLOCATION_ID=34435",4.150744028245667
"How can I check the status of my sbcast job?
","Slurm

sbatch

squeue

LSF

bsub

bjobs",4.13692318643987
"How can I check the status of my sbcast job?
","Use the sbatch --test-only command to see when a job of a specific size could be scheduled. For example, the snapshot below shows that a (2) node job would start at 10:54.

$ sbatch --test-only -N2 -t1:00:00 batch-script.slurm

  sbatch: Job 1375 to start at 2019-08-06T10:54:01 using 64 processors on nodes andes[499-500] in partition batch

The queue is fluid, the given time is an estimate made from the current queue state and load. Future job submissions and job completions will alter the estimate.



Common Batch Options to Slurm",4.118952185166757
"How can I optimize page faults in GPU kernels on Frontier?
",Memory can be automatically migrated to GPU from CPU on a page fault if XNACK operating mode is set.  No need to explicitly migrate data or provide managed memory. This is useful if you're migrating code from a programming model that relied on 'unified' or 'managed' memory. See more in https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#enabling-gpu-page-migration. Information about how memory is accessed based on the allocator used and the XNACK mode can be found in https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#migration-of-memory-allocator-xnack.,4.250502436038923
"How can I optimize page faults in GPU kernels on Frontier?
","The accessibility of memory from GPU kernels and whether pages may migrate depends three factors: how the memory was allocated; the XNACK operating mode of the GPU; whether the kernel was compiled to support page migration. The latter two factors are intrinsically linked, as the MI250X GPU operating mode restricts the types of kernels which may run.",4.249586288760189
"How can I optimize page faults in GPU kernels on Frontier?
","The accessibility of memory from GPU kernels and whether pages may migrate depends three factors: how the memory was allocated; the XNACK operating mode of the GPU; whether the kernel was compiled to support page migration. The latter two factors are intrinsically linked, as the MI250X GPU operating mode restricts the types of kernels which may run.",4.249586288760189
"How do I ensure that each OpenMP thread runs on its own physical CPU core on Frontier?
","Now the output shows that each OpenMP thread ran on its own physical CPU core. More specifically (see the Frontier Compute Node diagram), OpenMP thread 000 of MPI rank 000 ran on logical core 001 (i.e., physical CPU core 01), OpenMP thread 001 of MPI rank 000 ran on logical core 002 (i.e., physical CPU core 02), OpenMP thread 000 of MPI rank 001 ran on logical core 009 (i.e., physical CPU core 09), and OpenMP thread 001 of MPI rank 001 ran on logical core 010 (i.e., physical CPU core 10) - as intended.

Third attempt - Using multiple threads per core",4.489237355273048
"How do I ensure that each OpenMP thread runs on its own physical CPU core on Frontier?
","Because a Frontier compute node has two hardware threads available (2 logical cores per physical core), this enables the possibility of multithreading your application (e.g., with OpenMP threads). Although the additional hardware threads can be assigned to additional MPI tasks, this is not recommended. It is highly recommended to only use 1 MPI task per physical core and to use OpenMP threads instead on any additional logical cores gained when using both hardware threads.",4.4378446629641575
"How do I ensure that each OpenMP thread runs on its own physical CPU core on Frontier?
","Now the output shows that each OpenMP thread ran on (one of the hardware threads of) its own physical CPU core. More specifically (see the Crusher Compute Node diagram), OpenMP thread 000 of MPI rank 000 ran on hardware thread 001 (i.e., physical CPU core 01), OpenMP thread 001 of MPI rank 000 ran on hardware thread 002 (i.e., physical CPU core 02), OpenMP thread 000 of MPI rank 001 ran on hardware thread 009 (i.e., physical CPU core 09), and OpenMP thread 001 of MPI rank 001 ran on hardware thread 010 (i.e., physical CPU core 10) - as intended.",4.379700907952098
"How can I create an HTAR archive of a directory with a large number of files and subdirectories, but excluding certain files and subdirectories?
","There are limits to the size and number of files that can be placed in an HTAR archive.

| Individual File Size Maximum | 68GB, due to POSIX limit | | --- | --- | | Maximum Number of Files per Archive | 1 million |

For example, when attempting to HTAR a directory with one member file larger that 64GB, the following error message will appear:

$ htar -cvf  /hpss/prod/[projid]/users/[userid]/hpss_test.tar hpss_test/",4.379397740500751
"How can I create an HTAR archive of a directory with a large number of files and subdirectories, but excluding certain files and subdirectories?
","htar will overwrite files of the same name in the target directory.  When possible, extract only the files you need from large archives. To display the names of the files in the project1.tar archive file within the HPSS home directory:

htar -vtf  /hpss/prod/[projid]/users/[userid]/project1.tar

To extract only one file, executable.out, from the project1 directory in the Archive file called `` /hpss/prod/[projid]/users/[userid]/project1.tar``:

htar -xm -f project1.tar project1/ executable.out",4.356275121631847
"How can I create an HTAR archive of a directory with a large number of files and subdirectories, but excluding certain files and subdirectories?
","To extract all files from the project1/src directory in the archive file called project1.tar, and use the time of extraction as the modification time, use the following command:

htar -xm -f  /hpss/prod/[projid]/users/[userid]/project1.tar project1/src

The htar utility has several limitations.

You cannot add or append files to an existing archive.

File path names within an htar archive of the form prefix/name are limited to 154 characters for the prefix and 99 characters for the file name. Link names cannot exceed 99 characters.",4.355351880710384
"What happens to killed jobs after a system outage completes under the ""killable"" queue policy?
","-------------------------



At the start of a scheduled system outage, a *queue reservation* is used

to ensure that no jobs are running. In the ``batch`` queue, the

scheduler will not start a job if it expects that the job would not

complete (based on the job's user-specified max walltime) before the

reservation's start time. In constrast, the ``killable`` queue allows

the scheduler to start a job even if it will *not* complete before a

scheduled reservation. It enforces the following policies:



-  Jobs will be killed if still running when a system outage begins.",4.334743922957359
"What happens to killed jobs after a system outage completes under the ""killable"" queue policy?
","-  The scheduler will stop scheduling jobs in the ``killable`` queue (1)

hour before a scheduled outage.

-  Maximum-job-per-user limits are the same (i.e., in conjunction with)

the ``batch`` queue.

-  Any killed jobs will be automatically re-queued after a system outage

completes.



``debug`` Queue Policy

----------------------



The ``debug`` queue is intended to provide faster turnaround times for

the code development, testing, and debugging cycle. For example,

interactive parallel work is an ideal use for the debug queue. It

enforces the following policies:",4.3147477033055495
"What happens to killed jobs after a system outage completes under the ""killable"" queue policy?
","The killable queue is a preemptable queue that allows jobs in bins 4 and 5 to request walltimes up to 24 hours. Jobs submitted to the killable queue will be preemptable once the job reaches the guaranteed runtime limit as shown in the table below. For example, a job in bin 5 submitted to the killable queue can request a walltime of 24 hours. The job will be preemptable after two hours of run time. Similarly, a job in bin 4 will be preemptable after six hours of run time. Once a job is preempted, the job will be resubmitted by default with the original limits as requested in the job script and",4.236507350372255
"Can you explain why the GPU IDs in the --gpu-bind=map_gpu option are in the format of <gpu_id_for_task_0>, <gpu_id_for_task_1>,...?
","This example will be very similar to Example 1, but instead of using --gpu-bind=closest to map each MPI rank to the closest GPU, --gpu-bind=map_gpu will be used to map each MPI rank to a specific GPU. The map_gpu option takes a comma-separated list of GPU IDs to specify how the MPI ranks are mapped to GPUs, where the form of the comma-separated list is <gpu_id_for_task_0>, <gpu_id_for_task_1>,....

$ export OMP_NUM_THREADS=2
$ srun -N1 -n8 -c2 --gpus-per-node=8 --gpu-bind=map_gpu:4,5,2,3,6,7,0,1 ./hello_jobstep | sort",4.287130501601214
"Can you explain why the GPU IDs in the --gpu-bind=map_gpu option are in the format of <gpu_id_for_task_0>, <gpu_id_for_task_1>,...?
","| --gpus-per-task | Specify the number of GPUs required for the job on each task to be spawned in the job's resource allocation. | | --- | --- | | --gpu-bind=closest | Binds each task to the GPU which is on the same NUMA domain as the CPU core the MPI rank is running on. | | --gpu-bind=map_gpu:<list> | Bind tasks to specific GPUs by setting GPU masks on tasks (or ranks) as specified where <list> is <gpu_id_for_task_0>,<gpu_id_for_task_1>,.... If the number of tasks (or ranks) exceeds the number of elements in this list, elements in the list will be reused as needed starting from the beginning",4.270894440616021
"Can you explain why the GPU IDs in the --gpu-bind=map_gpu option are in the format of <gpu_id_for_task_0>, <gpu_id_for_task_1>,...?
","rank is running on. | | --gpu-bind=map_gpu:<list> | Bind tasks to specific GPUs by setting GPU masks on tasks (or ranks) as specified where <list> is <gpu_id_for_task_0>,<gpu_id_for_task_1>,.... If the number of tasks (or ranks) exceeds the number of elements in this list, elements in the list will be reused as needed starting from the beginning of the list. To simplify support for large task counts, the lists may follow a map with an asterisk and repetition count. (For example map_gpu:0*4,1*4) | | --ntasks-per-gpu=<ntasks> | Request that there are ntasks tasks invoked for every GPU. |",4.257644523362569
"How can I access the Member Work directory for a specific project on Slate?
","Project members get an individual Member Work directory for each associated project; these reside in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. Member Work areas are not shared with other users of the system and are intended for project data that the user does not want to make available to other users. Member Work directories are provided commonly across all systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular",4.259346174475756
"How can I access the Member Work directory for a specific project on Slate?
",The difference between the three lies in the accessibility of the data to project members and to researchers outside of the project. Member Work directories are accessible only by an individual project member by default. Project Work directories are accessible by all project members. World Work directories are readable by any user on the system.,4.25082939279773
"How can I access the Member Work directory for a specific project on Slate?
","3

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

4

Retention is not applicable as files will follow purge cycle.",4.213586069637402
"What are the requirements for Institutional User Agreements?
",if you require a copy of your User Agreement.,4.36289121244769
"What are the requirements for Institutional User Agreements?
","Users of DOE-designated User Facilities must understand and agree to the following Institutional User Agreement clause: I understand that my institution has entered into a User Agreement with UT-Battelle, the management and operating contractor for the U.S. Department of Energy’s (DOE) Oak Ridge National Laboratory (ORNL), that governs my research ORNL’s DOE-designated User Facilities. I have read and understand my obligations under that Agreement, including the provisions summarized below. You may check with your institution or contact accounts@ccs.ornl.gov if you require a copy of your User",4.293409246287542
"What are the requirements for Institutional User Agreements?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  All Users  Title: Non-proprietary Institutional User Agreement Policy Version: 12.10",4.291803515336774
"What is the purpose of creating a Deployment in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.239613912399256
"What is the purpose of creating a Deployment in Slate?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",4.122034767215068
"What is the purpose of creating a Deployment in Slate?
","Jenkins

OpenShift Pipelines

GitLab Runners

Deploying a GitLab Runner into a Slate project may be found in the https://docs.olcf.ornl.gov/systems/overview.html#slate_gitlab_runners document which leverages a localized Slate Helm Chart. The GitLab Pipelines documentation as well as GitLab CI/CD Examples provide more details on GitLab Runner capabilities and usage.

Jenkins

For Jenkins deployment, a Slate Helm Chart is planned to be released. Documentation concerning Jenkins Pipelines as well as Jenkins Pipeline Tutorials are available.

OpenShift Pipelines",4.121873044100232
"Can I use Nvidia Rapids for distributed computing?
","Running RAPIDS multi-gpu/multi-node workloads requires a dask-cuda cluster. Setting up a dask-cuda cluster on Summit requires two components:

dask-scheduler.

dask-cuda-workers.

Once the dask-cluster is running, the RAPIDS script should perform four main tasks. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

Reference of multi-gpu/multi-node operation with cuDF, cuML, cuGraph is available in the next links:

10 Minutes to cuDF and Dask-cuDF.

cuML's Multi-Node, Multi-GPU Algorithms.",4.242335608816092
"Can I use Nvidia Rapids for distributed computing?
","We are targeting use cases that need GPUs for long running services. For batch access to GPUs we recommend using the standard HPC clusters in NCCS

The Slate Marble cluster has nodes with three NVIDIA Tesla V100 GPUs per node available for scheduling so a single pod could request from 1 to 3 GPUs",4.188180334571863
"Can I use Nvidia Rapids for distributed computing?
","RAPIDS is a suite of libraries to execute end-to-end data science and analytics pipelines on GPUs. RAPIDS utilizes NVIDIA CUDA primitives for low-level compute optimization through user-friendly Python interfaces. An overview of the RAPIDS libraries available at OLCF is given next.

cuDF is a Python GPU DataFrame library (built on the Apache Arrow columnar memory format) for loading, joining, aggregating, filtering, and otherwise manipulating data.",4.178755930610036
"What is the purpose of the #SBATCH -C nvme directive in the example file?
","#!/bin/bash
#SBATCH -A <projid>
#SBATCH -J sbcast_to_nvme
#SBATCH -o %x-%j.out
#SBATCH -t 00:05:00
#SBATCH -p batch
#SBATCH -N 2
#SBATCH -C nvme

date

# Change directory to user scratch space (Orion)
cd /lustre/orion/<projid>/scratch/<userid>

echo ""This is an example file"" > test.txt
echo
echo ""*****ORIGINAL FILE*****""
cat test.txt
echo ""***********************""",4.314490541102504
"What is the purpose of the #SBATCH -C nvme directive in the example file?
","#!/bin/bash
#SBATCH -A <projid>
#SBATCH -J sbcast_to_nvme
#SBATCH -o %x-%j.out
#SBATCH -t 00:05:00
#SBATCH -p batch
#SBATCH -N 2
#SBATCH -C nvme

date

# Change directory to user scratch space (Orion)
cd /lustre/orion/<projid>/scratch/<userid>

echo ""This is an example file"" > test.txt
echo
echo ""*****ORIGINAL FILE*****""
cat test.txt
echo ""***********************""",4.314490541102504
"What is the purpose of the #SBATCH -C nvme directive in the example file?
","The NVMes on Frontier are local to each node.

To use the NVMe, users must request access during job allocation using the -C nvme option to sbatch, salloc, or srun. Once the devices have been granted to a job, users can access them at /mnt/bb/<userid>. Users are responsible for moving data to/from the NVMe before/after their jobs. Here is a simple example script:

#!/bin/bash
#SBATCH -A <projid>
#SBATCH -J nvme_test
#SBATCH -o %x-%j.out
#SBATCH -t 00:05:00
#SBATCH -p batch
#SBATCH -N 1
#SBATCH -C nvme

date",4.2779472560457785
"How does Memcheck detect memory errors?
","The Valgrind tool suite provides a number of debugging and profiling tools. The most popular is Memcheck, a memory checking tool which can detect many common memory errors such as:

Touching memory you shouldn’t (eg. overrunning heap block boundaries, or reading/writing freed memory).

Using values before they have been initialized.

Incorrect freeing of memory, such as double-freeing heap blocks.

Memory leaks.

Valgrind is available on Summit under all compiler families:

module load valgrind",4.321307931134443
"How does Memcheck detect memory errors?
","The Valgrind tool suite provides a number of debugging and profiling tools. The most popular is Memcheck, a memory checking tool which can detect many common memory errors such as:

Touching memory you shouldn’t (eg. overrunning heap block boundaries, or reading/writing freed memory).

Using values before they have been initialized.

Incorrect freeing of memory, such as double-freeing heap blocks.

Memory leaks.

Valgrind is available on Andes via the valgrind module:

module load valgrind",4.294813556213462
"How does Memcheck detect memory errors?
","The Valgrind tool suite provides a number of debugging and profiling tools. The most popular is Memcheck, a memory checking tool which can detect many common memory errors such as:

Touching memory you shouldn’t (eg. overrunning heap block boundaries, or reading/writing freed memory).

Using values before they have been initialized.

Incorrect freeing of memory, such as double-freeing heap blocks.

Memory leaks.

Valgrind is available on Andes via the valgrind module:

module load valgrind",4.294813556213462
"What is the formula to calculate the theoretical peak floating-point FLOPS/s for Crusher when using MFMA instructions?
","TheoreticalFLOPS = 128 FLOP/cycle/CU * 110 CU * 1700000000 cycles/second = 23.9 TFLOP/s

However, when using MFMA instructions, the theoretical peak floating-point FLOPS/s is calculated by:

TheoreticalFLOPS = flop\_per\_cycle(precision) FLOP/cycle/CU * 110 CU * 1700000000 cycles/second

where flop_per_cycle(precision) is the published floating-point operations per clock cycle, per compute unit. Those values are:

| Data Type | Flops/Clock/CU | | --- | --- | | FP64 | 256 | | FP32 | 256 | | FP16 | 1024 | | BF16 | 1024 | | INT8 | 1024 |",4.544925982356557
"What is the formula to calculate the theoretical peak floating-point FLOPS/s for Crusher when using MFMA instructions?
","TheoreticalFLOPS = 128 FLOP/cycle/CU * 110 CU * 1700000000 cycles/second = 23.9 TFLOP/s

However, when using MFMA instructions, the theoretical peak floating-point FLOPS/s is calculated by:

TheoreticalFLOPS = flop\_per\_cycle(precision) FLOP/cycle/CU * 110 CU * 1700000000 cycles/second

where flop_per_cycle(precision) is the published floating-point operations per clock cycle, per compute unit. Those values are:

| Data Type | Flops/Clock/CU | | --- | --- | | FP64 | 256 | | FP32 | 256 | | FP16 | 1024 | | BF16 | 1024 | | INT8 | 1024 |",4.544925982356557
"What is the formula to calculate the theoretical peak floating-point FLOPS/s for Crusher when using MFMA instructions?
","When SQ_INSTS_VALU_MFMA_MOPS_*_F64 instructions are used, then 47.8 TF/s is considered the theoretical maximum FLOPS/s. If only SQ_INSTS_VALU_<ADD,MUL,TRANS> are found, then 23.9 TF/s is the theoretical maximum FLOPS/s. Then, we divide the number of FLOPS by the elapsed time of the kernel to find FLOPS per second. This is found from subtracting the rocprof metrics EndNs by BeginNs, provided by --timestamp on, then converting from nanoseconds to seconds by dividing by 1,000,000,000 (power(10,9)).",4.416305804439704
"Can I access MinIO from outside of ORNL's network?
","MinIO is a high-performance software-defined object storage suite that enables the ability to easily deploy cloud-native data infrastructure for various data workloads. In this example we are deploying a simple, standalone implementation of MinIO on our cloud-native platform, Slate (https://docs.olcf.ornl.gov/systems/minio.html#slate_overview).

This service will only be accessible from inside of ORNL's network.

If your project requires an externally facing service available to the Internet, please contact User Assistance by submitting a help ticket. There is a process to get such approval.",4.32961961539223
"Can I access MinIO from outside of ORNL's network?
","Both the web UI and the API endpoint for the oc client are exposed outside of ORNL. However, you must log in with NCCS USERNAME AND PASSWORD rather than NCCS Single Sign On on the Web UI.

For production workloads, it is recommended to learn about https://docs.olcf.ornl.gov/systems/port_forwarding.html#services <slate_services> and https://docs.olcf.ornl.gov/systems/port_forwarding.html#routes <slate_routes> in order to gain access to your internal resources.

However, for testing and development, oc port-forward can be a powerful tool for quick access to internal cluster resources.",4.174514243121178
"Can I access MinIO from outside of ORNL's network?
","You can also go to it by logging into the Marble GUI. Once logged in, go to Networking->Routes and click the URL in the ""Location"" column of your MinIO applications row.

You will be greeted with the NCCS SSO page. Continue through that with your normal NCCS login credentials.

After the NCCS login, you will be greeted with MinIO's login page. Here you will enter the access-key and secret-key you created with the secret-token.yaml file.

At this point, you should be inside the MinIO Browser.",4.160700672171868
"What is the name of the scratch space directory where the job's output files will be saved?
","Each project is granted a Project Work directory; these reside in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. Project Work directories can be accessed by all members of a project and are intended for sharing data within a project. Project Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but",4.222834474837959
"What is the name of the scratch space directory where the job's output files will be saved?
","Each project has a World Work directory that resides in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. World Work areas can be accessed by all users of the system and are intended for sharing of data between projects. World Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but rather should be",4.204440618371171
"What is the name of the scratch space directory where the job's output files will be saved?
","Project members get an individual Member Work directory for each associated project; these reside in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. Member Work areas are not shared with other users of the system and are intended for project data that the user does not want to make available to other users. Member Work directories are provided commonly across all systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular",4.159571161476306
"What is the current limitation of using the BurstBuffers on Summit?
","Each compute node on Summit has a 1.6TB Non-Volatile Memory (NVMe) storage device (high-memory nodes have a 6.4TB NVMe storage device), colloquially known as a ""Burst Buffer"" with theoretical performance peak of 2.1 GB/s for writing and 5.5 GB/s for reading. The NVMes could be used to reduce the time that applications wait for I/O.  More information can be found later in the Burst Buffer section.



<string>:208: (INFO/1) Duplicate implicit target name: ""software"".",4.18806886779314
"What is the current limitation of using the BurstBuffers on Summit?
",| (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/06/20200603_summit_workshop_python.pdf https://vimeo.com/427794043 | | 2020-06-03 | NVMe - Burst Buffers (Part2) | George Markomanolis (OLCF) | 2020 OLCF User Meeting (Summit New User Training) https://www.olcf.ornl.gov/calendar/2020-olcf-user-meeting/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/Burst_Buffer_summit_june_2020.pdf https://vimeo.com/427792243 | | 2020-06-03 | NVMe - Burst Buffers (Part1) | Chris Zimmer (OLCF) | 2020 OLCF User Meeting (Summit New User Training),4.091757071732786
"What is the current limitation of using the BurstBuffers on Summit?
","Most Summit nodes contain 512 GB of DDR4 memory for use by the POWER9 processors, 96 GB of High Bandwidth Memory (HBM2) for use by the accelerators, and 1.6TB of non-volatile memory that can be used as a burst buffer. A small number of nodes (54) are configured as ""high memory"" nodes. These nodes contain 2TB of DDR4 memory, 192GB of HBM2, and 6.4TB of non-volatile memory.",4.090734329601673
"What is the purpose of the -c1 option in Summit?
",For Summit:,4.18563649660795
"What is the purpose of the -c1 option in Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.057568568389492
"What is the purpose of the -c1 option in Summit?
","Adding cores to the RS: The -c flag should be used to request the needed cores for tasks and treads. The default -c core count is 1. In the above example, if -c is not specified both tasks will run on a single core.

summit> jsrun -n12 -a2 -g1 -c2 -dpacked ./a.out | sort
Rank:    0; NumRanks: 24; RankCore:   0; Hostname: a01n05; GPU: 0
Rank:    1; NumRanks: 24; RankCore:   4; Hostname: a01n05; GPU: 0

Rank:    2; NumRanks: 24; RankCore:   8; Hostname: a01n05; GPU: 1
Rank:    3; NumRanks: 24; RankCore:  12; Hostname: a01n05; GPU: 1",4.050687459481469
"How can I specialize my code for the platform using HIP?
",work to learn HIP. See here for a series of tutorials on programming with HIP and also converting existing CUDA code to HIP with the hipify tools .,4.396807133434831
"How can I specialize my code for the platform using HIP?
","Key features include:

HIP is a thin layer and has little or no performance impact over coding directly in CUDA.

HIP allows coding in a single-source C++ programming language including features such as templates, C++11 lambdas, classes, namespaces, and more.

The “hipify” tools automatically convert source from CUDA to HIP.

Developers can specialize for the platform (CUDA or HIP) to tune for performance or handle tricky cases.",4.382408014492581
"How can I specialize my code for the platform using HIP?
","The HIP API is very similar to CUDA, so if you are already familiar with using CUDA, the transition to using HIP should be fairly straightforward. Whether you are already familiar with CUDA or not, the best place to start learning about HIP is this Introduction to HIP webinar that was recently given by AMD:

Introduction to AMD GPU Programming with HIP: (slides | recording)

More useful resources, provided by AMD, can be found here:

HIP Programming Guide

HIP API Documentation

HIP Porting Guide

The OLCF is currently adding some simple HIP tutorials here as well:",4.250357421495772
"Can I view usage for a specific project member on Summit?
","Each user may view usage for projects on which they are members from the command line tool showusage and the myOLCF site.

On the Command Line via showusage

The showusage utility can be used to view your usage from January 01 through midnight of the previous day. For example:

$ showusage
  Usage:
                           Project Totals
  Project             Allocation      Usage      Remaining     Usage
  _________________|______________|___________|____________|______________
  abc123           |  20000       |   126.3   |  19873.7   |   1560.80",4.182664924790044
"Can I view usage for a specific project member on Summit?
","Each user may view usage for projects on which they are members from the command line tool showusage and the myOLCF site.

The showusage utility can be used to view your usage from January 01 through midnight of the previous day. For example:

$ showusage
  Usage:
                           Project Totals
  Project             Allocation      Usage      Remaining     Usage
  _________________|______________|___________|____________|______________
  abc123           |  20000       |   126.3   |  19873.7   |   1560.80

The -h option will list more usage details.",4.176374432705478
"Can I view usage for a specific project member on Summit?
","Each user may view usage for projects on which they are members from the command line tool showusage and the myOLCF site.

The showusage utility can be used to view your usage from January 01 through midnight of the previous day. For example:

$ showusage
  Usage:
                           Project Totals
  Project             Allocation      Usage      Remaining     Usage
  _________________|______________|___________|____________|______________
  abc123           |  20000       |   126.3   |  19873.7   |   1560.80

The -h option will list more usage details.",4.176374432705478
"Can I use this git repository for deployment of resources into a project in Slate?
","Jenkins

OpenShift Pipelines

GitLab Runners

Deploying a GitLab Runner into a Slate project may be found in the https://docs.olcf.ornl.gov/systems/overview.html#slate_gitlab_runners document which leverages a localized Slate Helm Chart. The GitLab Pipelines documentation as well as GitLab CI/CD Examples provide more details on GitLab Runner capabilities and usage.

Jenkins

For Jenkins deployment, a Slate Helm Chart is planned to be released. Documentation concerning Jenkins Pipelines as well as Jenkins Pipeline Tutorials are available.

OpenShift Pipelines",4.295877261272257
"Can I use this git repository for deployment of resources into a project in Slate?
","At this point, it should now be possible to use this git repository for deployment of resources into a project.

By default, OpenShift GitOps will automatically configure the project and add the necessary roles to allow for the deployment of Kubernetes resources to the same project that contains the ArgoCD deployment. If it is desired to manage resources in a project other than where ArgoCD is deployed, please contact the Platforms Group to assistance in configuring the additional projects.

ArgoCD supports multiple methods to deploy Kubernetes resource manifests:",4.197773567829749
"Can I use this git repository for deployment of resources into a project in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.195288470048336
"How do I exit the interactive job?
","Most users will find batch jobs to be the easiest way to interact with the system, since they permit you to hand off a job to the scheduler and then work on other tasks; however, it is sometimes preferable to run interactively on the system. This is especially true when developing, modifying, or debugging a code.",4.255756428765219
"How do I exit the interactive job?
","Most users will find batch jobs an easy way to use the system, as they allow you to ""hand off"" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.",4.235314236710002
"How do I exit the interactive job?
","There are two ways we can run this. One is with an interactive job, and one is with a batch job. The interactive job doesn’t provide us with the ability to run interactive R jobs on the compute nodes (using R interactively on the compute node is complicated, so we do not discuss that here.). However, it does allow us to interactively submit tasks to the compute nodes (launched via jsrun). This can be useful if you are trying to debug a script that unexpectedly dies, without having to continuously submit jobs to the batch queue (more on that in a moment).",4.215983853585724
"How can the nodePort be left blank and randomly assigned by the system?
","Note that a NodePort value will automatically be given by the service controller.

Your service can then be accessed by the scheme apps.{cluster}.ccs.ornl.gov:{nodePort}.

In this example, if the service was running on marble, I could access it with apps.marble.ccs.ornl.gov:30298",4.2748243381115385
"How can the nodePort be left blank and randomly assigned by the system?
","A NodePort reserves a port across all nodes of the cluster. This port routes traffic to a service, which points to the pods that match the service's label selector.

NodePorts are given in the 30000-32767 range. These are ports you can use from outside the cluster to access resources inside of OpenShift.

For the Openshift clusters you will additionally need to create a https://docs.olcf.ornl.gov/systems/nodeport.html#network policy <slate_network_policies> file to allow external traffic into your namespace.",4.2713536215462895
"How can the nodePort be left blank and randomly assigned by the system?
","In general, using FQDNs to access a service is more convenient. If your service communicates over HTTP or HTTPS, you can set up a https://docs.olcf.ornl.gov/systems/services.html#slate_routes to achieve this. If your service uses another protocol, you can use https://docs.olcf.ornl.gov/systems/services.html#slate_nodeports.

NodePort is a type of Service that reserves a port across the cluster that can route traffic to your pods. This is more flexible than a Route and can handle any TCP or UDP traffic.",4.150310964668843
"How can OLCF users transfer files to HPSS using Globus?
","The OLCF users have access to a new functionality, using Globus to transfer files to HPSS through the endpoint ""OLCF HPSS"". Globus has restriction of 8 active transfers across all the users. Each user has a limit of 3 active transfers, so it is required to transfer a lot of data on each transfer than less data across many transfers. If a folder is constituted with mixed files including thousands of small files (less than 1MB each one), it would be better to tar the small files.  Otherwise, if the files are larger, Globus will handle them. To transfer the files, follow these steps:",4.7149918574397525
"How can OLCF users transfer files to HPSS using Globus?
","Each OLCF user receives an HPSS account automatically. Users can transfer data to HPSS from any OLCF system using the HSI or HTAR utilities. For more information on using HSI or HTAR, see the https://docs.olcf.ornl.gov/systems/user_centric.html#data-hpss .

Each file and directory on HPSS is associated with an HPSS storage allocation. For information on HPSS storage allocations, please visit the https://docs.olcf.ornl.gov/systems/user_centric.html#data-policy section.",4.493739892522669
"How can OLCF users transfer files to HPSS using Globus?
","Due to available bandwidth, transferring data through the HPSS will be a slower route than using Globus to transfer directly between Alpine and Orion.

Transferring data through the HPSS is a multi-step process and will be slower than direct transfers using Globus.

Globus is the suggested tool to migrate data off of Alpine.  Please do not use HPSS as a data migration method.



On January 01, data remaining on the GPFS filesystem, Alpine, will no longer be accessible and will be permanently deleted . Following this date, the OLCF will no longer be able to retrieve data remaining on Alpine.",4.4926659913470655
"Can standard tools like rsync and scp be used to move data off-site?
","Standard tools such as rsync and scp can also be used through the DTN, but may be slower and require more manual intervention than Globus

Copying data directly from Alpine (GPFS) to Orion (Lustre)

Globus is the suggested tool to transfer needed data from Alpine to Orion.

Globus should be used when transfer large amounts of data.

Standard tools such as rsync and cp can also be used. The DTN mounts both filesystems and should be used when transferring with rsync and cp tools. These methods should not be used to transfer large amounts of data.

Copying data to the HPSS archive system",4.337574127201888
"Can standard tools like rsync and scp be used to move data off-site?
","rsync -avz --dry-run mydir/ $USER@dtn.ccs.ornl.gov:/path/

See the manual pages for more information:

$ man scp
$ man rsync

Differences:

scp cannot continue if it is interrupted. rsync can.

rsync is optimized for performance.

By default, rsync checks if the transfer of the data was successful.

Standard file transfer protocol (FTP) and remote copy (RCP) should not be used to transfer files to the NCCS high-performance computing (HPC) systems due to security concerns.",4.30003435151347
"Can standard tools like rsync and scp be used to move data off-site?
","This page has been deprecated, and relevant information is now available on https://docs.olcf.ornl.gov/systems/transferring.html#data-storage-and-transfers. Please update any bookmarks to use that page.

In general, when transferring data into or out of the OLCF from the command line, it's best to initiate the transfer from outside the OLCF. If moving many small files, it can be beneficial to compress them into a single archive file, then transfer just the one archive file.

scp and rsync are available for remote transfers.

scp - secure copy (remote file copy program)

Sending a file to OLCF",4.263078956511974
"What is the purpose of the Project Archive?
","World Archive Directory: 775

For example, if you have data that must be restricted only to yourself, keep them in your Member Archive directory for that project (and leave the default permissions unchanged). If you have data that you intend to share with researchers within your project, keep them in the project’s Project Archive directory. If you have data that you intend to share with researchers outside of a project, keep them in the project’s World Archive directory.",4.323531068075196
"What is the purpose of the Project Archive?
","Project Work: Short-term project data for fast, batch-job access that's shared with other project members.

World Work: Short-term project data for fast, batch-job access that's shared with users outside your project.

Member Archive: Long-term project data for archival access that is not shared with other project members.

Project Archive: Long-term project data for archival access that's shared with other project members.

World Archive: Long-term project data for archival access that's shared with users outside your project.",4.30169567467307
"What is the purpose of the Project Archive?
","To facilitate collaboration among researchers, the OLCF provides (3) distinct types of project-centric archival storage areas: Member Archive directories, Project Archive directories, and World Archive directories. These directories should be used for storage of data not immediately needed in either the Project Home (NFS) areas or Project Work (Alpine) areas and to serve as a location to store backup copies of project-related files.",4.267537333637189
"Can I load multiple versions of Open-CE on Summit?
","module load open-ce

Loading a specific version of the module is recommended to future-proof scripts against software updates. The following commands can be used to find and load specific module versions:

[user@login2.summit ~]$ module avail open-ce",4.278099063891473
"Can I load multiple versions of Open-CE on Summit?
","OpenCE 1.5.0 is now available on Summit. OpenCE 1.5.0 is available for python versions 3.7, 3.8, and 3.9. These builds can be accessed by loading the open-ce/1.5.0-py37-0, open-ce/1.5.0-py38-0, and open-ce/1.5.0-py39-0 modules, respectively.

The following packages are available in this release of OpenCE:",4.276499276671513
"Can I load multiple versions of Open-CE on Summit?
","OpenCE 1.5.2 is now available on Summit. OpenCE 1.5.2 is available for python versions 3.9, 3.8, and 3.7. These builds can be accessed by loading the open-ce/1.5.2-py39-0, open-ce/1.5.2-py38-0, and open-ce/1.5.2-py37-0 modules, respectively.

The following packages are available in this release of OpenCE:",4.269021709368995
"How do I launch the Putty Application?
","After installing, you must give ParaView the relevant server information to be able to connect to OLCF systems (comparable to VisIt's system of host profiles). The following provides an example of doing so. Although several methods may be used, the one described should work in most cases.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.

For Windows clients, it is necessary to install PuTTY to create an ssh connection in step 2.

Step 1: Save the following servers.pvsc file to your local computer",4.078507558014352
"How do I launch the Putty Application?
","Login to https://kdivdi.ornl.gov with your KDI issued credentials

Launch the Putty Application

Enter the hostname ""citadel.ccs.ornl.gov"" and click Open

You will then be in an ssh terminal to authenticate with your OLCF credentials as detailed above.

Projects using ORNL's KDI must following KDI access procedures and cannot access SPI resources directly.  If your project uses both KDI and SPI, you will not access the SPI resources directly.",4.070683881965673
"How do I launch the Putty Application?
","Most Terminal applications have built-in shortcuts to directly open web addresses in the default browser.  MacOS Terminal.app: hold Command (⌘) and double-click on the URL  iTerm2: hold Command (⌘) and single-click on the URL

(currently) Compiled with GCC toolchain only

Does not support MPMD-mode via ERF

OpenMP only supported with use of the OMP_NUM_THREADS environment variable.",4.065147715501124
"How can I change my default shell on OLCF resources?
","Shell and programming environments

OLCF systems provide hundreds of software packages and scientific libraries pre-installed at the system-level for users to take advantage of. To facilitate this, environment management tools are employed to handle necessary changes to the shell dynamically. The sections below provide information about using the management tools at the OLCF.



Default shell

A user's default shell is selected when completing the user account request form. The chosen shell is set across all OLCF resources.  Currently, supported shells include:

bash

tsch

csh

ksh",4.49920104078678
"How can I change my default shell on OLCF resources?
","bash

tsch

csh

ksh

If you would like to have your default shell changed, please contact the OLCF user assistance center at help@olcf.ornl.gov.",4.473192124777617
"How can I change my default shell on OLCF resources?
","OLCF systems provide hundreds of software packages and scientific libraries pre-installed at the system-level for users to take advantage of. To facilitate this, environment management tools are employed to handle necessary changes to the shell dynamically. The sections below provide information about using the management tools at the OLCF.



A user's default shell is selected when completing the user account request form. The chosen shell is set across all OLCF resources.  Currently, supported shells include:

bash

tsch

csh

ksh",4.460098930461815
"How can I route requests to multiple services in Slate using a single route?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.309265408172535
"How can I route requests to multiple services in Slate using a single route?
","Route Exposed

While a route usually points to one service through the to parameter in the configuration, it is possible to have as many as four services to load balance between. This is used with A/B deployments.

Here is an example route which points to 3 services:",4.201589615517737
"How can I route requests to multiple services in Slate using a single route?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.195769345985767
"What is the purpose of TAU_TRACK_IO_PARAMS?
",| Variable | Default | Description | | --- | --- | --- | | TAU_TRACE | 0 | Setting to 1 turns on tracing | | TAU_CALLPATH | 0 | Setting to 1 turns on callpath profiling | | TAU_TRACK_MEMORY_LEAKS | 0 | Setting to 1 turns on leak detection | | TAU_TRACK_HEAP | 0 | Setting to 1 turns on heap memory routine entry/exit | | TAU_CALLPATH_DEPTH | 2 | Specifies depth of callpath | | TAU_TRACK_IO_PARAMS | 0 | Setting 1 with -optTrackIO | | TAU_SAMPLING | 1 | Generates sample based profiles | | TAU_COMM_MATRIX | 0 | Setting to 1 generates communication matrix | | TAU_THROTTLE | 1 | Setting to 0 turns,4.074118082352827
"What is the purpose of TAU_TRACK_IO_PARAMS?
","TAU is a portable profiling and tracing toolkit that supports many programming languages. The instrumentation can be done by inserting in the source code using an automatic tool based on the Program Database Toolkit (PDT), on the compiler instrumentation, or manually using the instrumentation API.

Webpage: https://www.cs.uoregon.edu/research/tau/home.php",4.008214328458887
"What is the purpose of TAU_TRACK_IO_PARAMS?
","A similar approach for other metrics, not all of them can be used. TAU provides a tool called tau_cupti_avail, where we can see the list of available metrics, then we have to figure out which CUPTI metrics use these ones.

Activate tracing and declare the data format to OTF2. OTF2 format is supported only by MPI and OpenSHMEM applications.

$ export TAU_TRACE=1
$ export TAU_TRACE_FORMAT=otf2

Use Vampir for visualization.

For example, do not instrument routine sort*(int *)

Create a file select.tau

BEGIN_EXCLUDE_LIST
void sort_#(int *)
END_EXCLUDE_LIST

Declare the TAU_OPTIONS variable",3.990566658478898
"What is the purpose of the ""--namespace"" flag in the MinIO installation command?
","MinIO running on a dedicated volume, allocated automatically from the NetApp storage server, isolated to the MinIO server.

It is important to note that we are also launching MinIO in standalone mode, which is a single MinIO server instance. MinIO also supports distributed mode for more robust implementations, but we are not setting that up in this example.

<string>:5: (INFO/1) Duplicate explicit target name: ""user assistance"".",4.1457714935240775
"What is the purpose of the ""--namespace"" flag in the MinIO installation command?
","name (Set the name of your application)

use_olcf_fs (Controls if NCCS filesystems are used or not - enabled or disabled)

olcf_mount (Set the mount path to your project directory (i.e /ccs/proj/<projectID>/minio/))

pvc_storage (Set the quota for your dedicated storage if use_olcf_fs is disabled)

network_policy_namespace (Set the network policy's namespace to your project name, this will be the output of the oc project command)

The below is not provided in the above configuration, but it must be done for the MinIO application to start properly.",4.121542436888685
"What is the purpose of the ""--namespace"" flag in the MinIO installation command?
","MinIO is a high-performance software-defined object storage suite that enables the ability to easily deploy cloud-native data infrastructure for various data workloads. In this example we are deploying a simple, standalone implementation of MinIO on our cloud-native platform, Slate (https://docs.olcf.ornl.gov/systems/minio.html#slate_overview).

This service will only be accessible from inside of ORNL's network.

If your project requires an externally facing service available to the Internet, please contact User Assistance by submitting a help ticket. There is a process to get such approval.",4.101538562817291
"What is the protocol used by the service?
","In general, using FQDNs to access a service is more convenient. If your service communicates over HTTP or HTTPS, you can set up a https://docs.olcf.ornl.gov/systems/services.html#slate_routes to achieve this. If your service uses another protocol, you can use https://docs.olcf.ornl.gov/systems/services.html#slate_nodeports.

NodePort is a type of Service that reserves a port across the cluster that can route traffic to your pods. This is more flexible than a Route and can handle any TCP or UDP traffic.",4.074822807930817
"What is the protocol used by the service?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.005953127955241
"What is the protocol used by the service?
","Notice the weight parameter on each service. This weight must be in the range 0-256. The default is 1. If the weight is 0, no requests will be passed to the service. If all services have a 0 weight, then all requests will return a 503 error.

The portion of requests sent to each service is determined by its weight divided by the sum of all weights. In the above example, service-name will get 20/40 or 1/2 of the requests, and service-name2 and service-name3 will each get 10/40 or 1/4 of the requests.",4.000386589789523
"How can I set the server username in Paraview?
","After installing, you must give ParaView the relevant server information to be able to connect to OLCF systems (comparable to VisIt's system of host profiles). The following provides an example of doing so. Although several methods may be used, the one described should work in most cases.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.

For Windows clients, it is necessary to install PuTTY to create an ssh connection in step 2.

Step 1: Save the following servers.pvsc file to your local computer",4.1997553419174745
"How can I set the server username in Paraview?
","<String default=""YOURUSERNAME""/>
        </Option>
        <Switch name=""PV_CLIENT_PLATFORM"">
          <Case value=""Apple"">
            <Set name=""TERM_PATH"" value=""/opt/X11/bin/xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />
            <Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Linux"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.159846574222826
"How can I set the server username in Paraview?
","<String default=""YOURUSERNAME""/>
        </Option>
        <Switch name=""PV_CLIENT_PLATFORM"">
          <Case value=""Apple"">
            <Set name=""TERM_PATH"" value=""/opt/X11/bin/xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />
            <Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Linux"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.158502317414695
"How do I create a blue-green deployment in Slate?
","Blue-green deployments are defined as running two versions of an application at the same time, then moving traffic from the old production version (the green version) to the new production version (the blue version). You could use a Rolling Deployment Strategy for this, but for the sake of showing how route-based deployments work, we'll use a route.",4.2956581383529615
"How do I create a blue-green deployment in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.209448413755444
"How do I create a blue-green deployment in Slate?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.160875118764565
"How can I fix the issue with std::complex not being __host__/__device__ annotated in Thrust?
","This issue comes from the fact that std::complex is not __host__/__device__ annotated, so all its functions are implicitly __host__. There is a mostly simple workaround, assuming this is compiled as C++11: in complex.h and complex.inl, annotate the functions that deal with std::complex as __host__ __device__ (they are the ones that are annotated only as __host__ right now), and then compile with --expt-relaxed-constexpr.",4.597583977014968
"How can I fix the issue with std::complex not being __host__/__device__ annotated in Thrust?
","Users that encounter this issue, can use the following workaround. copy the entirety of ${OLCF_CUDA_ROOT}/include/thrust to a private location, make the above edits to thrust/complex.h and thrust/detail/complex/complex.inl, and then add that to your include path:

$ nvcc -ccbin=g++ --expt-relaxed-constexpr assignment.cu -I./

A permanent fix of this issue is expected in the version of Thrust packed with CUDA 10.1 update 1",4.312512679112233
"How can I fix the issue with std::complex not being __host__/__device__ annotated in Thrust?
","Last Updated: 07 February 2023

Using the hip-cuda/5.1.0 module on Summit, applications cannot build using a CMakeLists.txt that requires HIP language support or references the hip::host and hip::device identifiers. There is no known workaround for this issue. Applications wishing to compile HIP code with CMake need to avoid using HIP language support or hip::host and hip::device identifiers.",4.124641144821596
"Implement a quantum algorithm for solving linear systems of equations using Qiskit.
","IBM Quantum provides Qiskit (Quantum Information Software Kit for Quantum Computation) for working with OpenQASM and the IBM Q quantum processors. Qiskit allows users to build quantum circuits, compile them for a particular backend, and run the compiled circuits as jobs. Additional information on using Qiskit is available at https://qiskit.org/learn/ and in our https://docs.olcf.ornl.gov/systems/ibm_quantum.html#Software Section <ibm-soft> below.",4.240390474926891
"Implement a quantum algorithm for solving linear systems of equations using Qiskit.
","Jobs are compiled and submitted via Qiskit in a Python virtual environment or Jupyter notebook (see https://docs.olcf.ornl.gov/systems/ibm_quantum.html#Cloud Access <ibm-cloud> and https://docs.olcf.ornl.gov/systems/ibm_quantum.html#Local Access <ibm-local> sections above).

Circuit jobs comprise jobs of constructed quantum circuits and algorithms submitted to backends in IBM Quantum fair-share queue.

Program jobs utilize a pre-compiled quantum program utilizing the Qiskit Runtime framework.",4.154507254225091
"Implement a quantum algorithm for solving linear systems of equations using Qiskit.
","A recommended workflow for running on Quantinuum's quantum computers is to utilize the syntax checker first, run on the emulator, then run on one of the quantum computers. This is highlighted in the examples.",4.138786028171247
"What information does the profiler provide about the CUDA API calls made by my application on Summit?
","The profiler will print several sections including information about the CUDA API calls made by the application, as well as any GPU kernels that were launched. Nsight Systems can be used for CUDA C++, CUDA Fortran, OpenACC, OpenMP offload, and other programming models that target NVIDIA GPUs, because under the hood they all ultimately take the same path for generating the binary code that runs on the GPU.",4.376109918217793
"What information does the profiler provide about the CUDA API calls made by my application on Summit?
","While using nvprof on the command-line is a quick way to gain insight into your CUDA application, a full visual profile is often even more useful. For information on how to view the output of nvprof in the NVIDIA Visual Profiler, see the NVIDIA Documentation.",4.328699698146069
"What information does the profiler provide about the CUDA API calls made by my application on Summit?
","Prior to Nsight Systems and Nsight Compute, the NVIDIA command line profiling tool was nvprof, which provides both tracing and kernel profiling capabilities. Like with Nsight Systems and Nsight Compute, the profiler data output can be saved and imported into the NVIDIA Visual Profiler for additional graphical analysis. nvprof is in maintenance mode now: it still works on Summit and significant bugs will be fixed, but no new feature development is occurring on this tool.

To use nvprof, the cuda module must be loaded.

summit> module load cuda",4.287986662536173
"Who should I contact for assistance with my QCUP project?
","A QCUP proposal describes the nature, methodology, and merits of the project, explains why it requires access to QCUP resources, and outlines any other essential information that might be needed for its consideration. Project applications are submitted using the Project Application Form. Select ""OLCF Quantum Computing User Program"" from the dropdown menu.

For QCUP Projects, all proposed work must be open, fundamental research and no Export Control, PHI, or other controlled data can be used.",4.3088225729386895
"Who should I contact for assistance with my QCUP project?
","Welcome! The information below introduces how we structure projects and user accounts for access to the Quantum resources within the QCUP program. In general, OLCF QCUP resources are granted to projects, which are in turn made available to the approved users associated with each project.",4.267177327435964
"Who should I contact for assistance with my QCUP project?
","To gain access, you must first submit a project proposal to the OLCF QCUP (see https://docs.olcf.ornl.gov/systems/quantum_access.html#quantum-proj) or join an existing QCUP project (see https://docs.olcf.ornl.gov/systems/quantum_access.html#quantum-user). The Quantum Resource Utilization Council (QRUC), as well as independent referees, review and approve all QCUP project proposals.  Applications to QCUP are accepted year-round via the project application form found below. Once a project is approved, then all of the users associated with the project will need to apply for a",4.262194236765708
"How can I generate plots using VisIt?
","VisIt is now available on Frontier through the UMS022 module.

VisIt 3.3.3 is now available on Andes.

VisIt is an interactive, parallel analysis and visualization tool for scientific data. VisIt contains a rich set of visualization features so you can view your data in a variety of ways. It can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes. Further information regarding VisIt can be found at the links provided in the https://docs.olcf.ornl.gov/systems/visit.html#visit-resources section.",4.279060287473567
"How can I generate plots using VisIt?
","VisIt developers have been notified, and at this time the current workaround is to disable Scalable Rendering from being used. To do this, go to Options→Rendering→Advanced and set the ""Use Scalable Rendering"" option to ""Never"".

However, this workaround has been reported to affect VisIt's ability to save images, as scalable rendering is utilized to save plots as image files (which can result in another compute engine crash). To avoid this, screen capture must be enabled. Go to File→""Set save options"" and check the box labeled ""Screen capture"".",4.241536659273138
"How can I generate plots using VisIt?
","Click ""Apply""

At the top menu click on ""Options""→""Save Settings""



Once you have VisIt installed and set up on your local computer:

Open VisIt on your local computer.

Go to: ""File→Open file"" or click the ""Open"" button on the GUI.

Click the ""Host"" dropdown menu on the ""File open"" window that popped up and choose ""ORNL_Andes"".

This will prompt you for your OLCF password, and connect you to Andes.

Navigate to the appropriate file.",4.227890900382926
"What type of accelerators does Summit use for deep learning applications?
",| | 2020-02-10 | NCCL on Summit | Sylvain Jeaugey (NVIDIA) | Scaling Up Deep Learning Applications on Summit https://www.olcf.ornl.gov/calendar/scaling-up-deep-learning-applications-on-summit/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/12/Summit-NCCL.pdf https://vimeo.com/391520479 | | 2020-02-10 | Introduction to Watson Machine Learning CE | Brad Nemanich & Bryant Nelson (IBM) | Scaling Up Deep Learning Applications on Summit https://www.olcf.ornl.gov/calendar/scaling-up-deep-learning-applications-on-summit/ | (slides | recording),4.28495512065801
"What type of accelerators does Summit use for deep learning applications?
","Summit is an IBM system located at the Oak Ridge Leadership Computing Facility. With a theoretical peak double-precision performance of approximately 200 PF, it is one of the most capable systems in the world for a wide range of traditional computational science applications. It is also one of the ""smartest"" computers in the world for deep learning applications with a mixed-precision capability in excess of 3 EF.



Summit node architecture diagram",4.213457802713027
"What type of accelerators does Summit use for deep learning applications?
","Summit node architecture diagram

The basic building block of Summit is the IBM Power System AC922 node. Each of the approximately 4,600 compute nodes on Summit contains two IBM POWER9 processors and six NVIDIA Tesla V100 accelerators and provides a theoretical double-precision capability of approximately 40 TF. Each POWER9 processor is connected via dual NVLINK bricks, each capable of a 25GB/s transfer rate in each direction.",4.205746683466094
"What should I do if I suspect someone has accessed my account without authorization?
","be disabled immediately. Users are not to attempt to receive unintended messages or access information by some unauthorized means, such as imitating another system, impersonating another user or other person, misuse of legal user credentials (usernames, tokens, etc.), or by causing some system component to function incorrectly. Users are prohibited from changing or circumventing access controls to allow themselves or others to perform actions outside their authorized privileges. Users must notify the OLCF immediately when they become aware that any of the accounts used to access OLCF have",4.079499296635365
"What should I do if I suspect someone has accessed my account without authorization?
",Users are prohibited from taking unauthorized actions to intentionally modify or delete information or programs.,4.058113220282064
"What should I do if I suspect someone has accessed my account without authorization?
","When our accounts team begins processing your application, you will receive an automated email containing an unique 36-character confirmation code. Make note of it; you can use it to check the status of your application at any time.

The principal investigator (PI) of the project must approve your account and system access. We will make the project PI aware of your request.",4.047029684092979
"What are the three programming interfaces available for Perftools?
","There are three programming interfaces available: (1) Perftools-lite, (2) Perftools, and (3) Perftools-preload.

Below are two examples that generate an instrumented executable using Perftools, which is an advanced interface that provides full-featured data collection and analysis capability, including full traces with timeline displays.

The first example generates an instrumented executable using a PrgEnv-amd build:

module load PrgEnv-amd
module load craype-accel-amd-gfx90a
module load rocm
module load perftools",4.524039968236585
"What are the three programming interfaces available for Perftools?
","There are three programming interfaces available: (1) Perftools-lite, (2) Perftools, and (3) Perftools-preload.

Below are two examples that generate an instrumented executable using Perftools, which is an advanced interface that provides full-featured data collection and analysis capability, including full traces with timeline displays.

The first example generates an instrumented executable using a PrgEnv-amd build:

module load PrgEnv-amd
module load craype-accel-amd-gfx90a
module load rocm
module load perftools",4.524039968236585
"What are the three programming interfaces available for Perftools?
","Programming models supported by HPCToolkit include MPI, OpenMP, OpenACC, CUDA, OpenCL, DPC++, HIP, RAJA, Kokkos, and others.

Below is an example that generates a profile and loads the results in their GUI-based viewer.

module use /gpfs/alpine/csc322/world-shared/modulefiles/ppc64le
module load hpctoolkit

# 1. Profile and trace an application using CPU time and GPU performance counters
jsrun <jsrun_options> hpcrun -o <measurement_dir> -t -e CPUTIME -e gpu=nvidia <application>

# 2. Analyze the binary of executables and its dependent libraries
hpcstruct <measurement_dir>",4.118500419279955
"How can I ensure that the GitLab runner has registered with the correct group or repository?
","Prior to installation of the GitLab Runner, a registration token for the runner is needed from the GitLab server. This token will allow a GitLab runner to register to the server in the needed location for running CI/CD jobs. Runners themselves may be registered to either a group as a shared runner or a project as a repository specific runner.",4.42301768032166
"How can I ensure that the GitLab runner has registered with the correct group or repository?
","If the runner is to be a group shared runner, navigate to the group in GitLab and then go to Settings -> CI/CD. Expand the Runners section of the CI/CD Settings panel. Ensure that the ""Enable shared runners for this group"" toggle is enabled. The registration token should also be available for retrieval from ""Group Runners"" area.",4.376076741796904
"How can I ensure that the GitLab runner has registered with the correct group or repository?
","Once the pod has a Status of ""Running"", navigate to the GitLab web interface and ensure that the runner has registered to either the group or repository per the token given, and that it is listed as available.

Finally, if batch scheduler integration was enabled, one can verify functionality in the pod with:

$ oc exec -it pods/gitlab-runner-gitlab-runner-687486d94-lpmhs -- bash
bash-5.0$ squeue
          JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
           9951 ...",4.2790619501933005
"How does Crusher support data types other than floating-point numbers?
",Crusher is connected to the center-wide IBM Spectrum Scale™ filesystem providing 250 PB of storage capacity with a peak write speed of 2.5 TB/s. Crusher also has access to the center-wide NFS-based filesystem (which provides user and project home areas). While Crusher does not have direct access to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#dtn-user-guide to move data to/from HPSS.,4.143189945151996
"How does Crusher support data types other than floating-point numbers?
","Crusher users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",3.979028447029391
"How does Crusher support data types other than floating-point numbers?
","Crusher is an National Center for Computational Sciences (NCCS) moderate-security system that contains identical hardware and similar software as the upcoming Frontier system. It is used as an early-access testbed for Center for Accelerated Application Readiness (CAAR) and Exascale Computing Project (ECP) teams as well as NCCS staff and our vendor partners. The system has 2 cabinets, the first with 128 compute nodes and the second with 64 compute nodes, for a total of 192 compute nodes.",3.971429682686244
"Are there any restrictions on the use of compute nodes on Summit?
","On Summit, there are three major types of nodes you will encounter: Login, Launch, and Compute. While all of these are similar in terms of hardware (see: https://docs.olcf.ornl.gov/systems/summit_user_guide.html#summit-nodes), they differ considerably in their intended use.",4.378247365786211
"Are there any restrictions on the use of compute nodes on Summit?
","Login nodes have (2) 16-core Power9 CPUs and (4) V100 GPUs. Compute nodes have (2) 22-core Power9 CPUs and (6) V100 GPUs.

Summit nodes are connected to a dual-rail EDR InfiniBand network providing a node injection bandwidth of 23 GB/s. Nodes are interconnected in a Non-blocking Fat Tree topology. This interconnect is a three-level tree implemented by a switch to connect nodes within each cabinet (first level) along with Director switches (second and third level) that connect cabinets together.",4.317052196633884
"Are there any restrictions on the use of compute nodes on Summit?
","Recall from the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#system-overview section that Summit has three types of nodes: login, launch, and compute. When you log into the system, you are placed on a login node. When your https://docs.olcf.ornl.gov/systems/summit_user_guide.html#batch-scripts or https://docs.olcf.ornl.gov/systems/summit_user_guide.html#interactive-jobs run, the resulting shell will run on a launch node. Compute nodes are accessed via the jsrun command. The jsrun command should only be issued from within an LSF job (either batch or interactive) on a launch node.",4.301849122613449
"How many compute nodes can my job use on Summit?
","Jobs on Summit are scheduled in full node increments; a node's cores cannot be allocated to multiple jobs. Because the OLCF charges based on what a job makes unavailable to other users, a job is charged for an entire node even if it uses only one core on a node. To simplify the process, users request and are allocated multiples of entire nodes through LSF.

Allocations on Summit are separate from those on Andes and other OLCF resources.

The node-hour charge for each batch job will be calculated as follows:

node-hours = nodes requested * ( batch job endtime - batch job starttime )",4.3899696238852055
"How many compute nodes can my job use on Summit?
","Login nodes have (2) 16-core Power9 CPUs and (4) V100 GPUs. Compute nodes have (2) 22-core Power9 CPUs and (6) V100 GPUs.

Summit nodes are connected to a dual-rail EDR InfiniBand network providing a node injection bandwidth of 23 GB/s. Nodes are interconnected in a Non-blocking Fat Tree topology. This interconnect is a three-level tree implemented by a switch to connect nodes within each cabinet (first level) along with Director switches (second and third level) that connect cabinets together.",4.367281691443213
"How many compute nodes can my job use on Summit?
","Recall from the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#system-overview section that Summit has three types of nodes: login, launch, and compute. When you log into the system, you are placed on a login node. When your https://docs.olcf.ornl.gov/systems/summit_user_guide.html#batch-scripts or https://docs.olcf.ornl.gov/systems/summit_user_guide.html#interactive-jobs run, the resulting shell will run on a launch node. Compute nodes are accessed via the jsrun command. The jsrun command should only be issued from within an LSF job (either batch or interactive) on a launch node.",4.3647614228966
"How can I specify the output file for Paraview?
","Click ""Apply""

At the top menu click on ""Options""→""Save Settings""



Once you have VisIt installed and set up on your local computer:

Open VisIt on your local computer.

Go to: ""File→Open file"" or click the ""Open"" button on the GUI.

Click the ""Host"" dropdown menu on the ""File open"" window that popped up and choose ""ORNL_Andes"".

This will prompt you for your OLCF password, and connect you to Andes.

Navigate to the appropriate file.",4.092476507543381
"How can I specify the output file for Paraview?
","# Render scene and save resulting image
Render()
SaveScreenshot('pvbatch-test.png',ImageResolution=[1080, 1080])

For older versions of ParaView (e.g., 5.9.1), line 23 should be 'AnyLocation' (no space).



If everything is working properly, the above image should be generated after the batch job is complete.

All of the above can also be achieved in an interactive batch job through the use of the salloc command on Andes or the bsub -Is command on Summit. Recall that login nodes should not be used for memory- or compute-intensive tasks, including ParaView.",4.071589036717593
"How can I specify the output file for Paraview?
","Navigate to the appropriate file.

Once you choose a file, you will be prompted for the number of nodes and processors you would like to use (remember that each node of Andes contains 32 processors, or 28 if using the high-memory GPU partition) and the Project ID, which VisIt calls a ""Bank"" as shown below.



Once specified, the server side of VisIt will be launched, and you can interact with your data.",4.046738378044193
"What is the purpose of the node-local NVMe storage mounted in /tmp/containers on Summit?
","Due to Podman's lack of support for storage on GPFS and NFS, container images will be built on the login nodes using the node-local NVMe on the login node. This NVMe is mounted in /tmp/containers. Users should treat this storage as temporary. Any data (container image layers or otherwise) in this storage will be purged if the node is ever rebooted or when it gets full.  So any images created with Podman need to be converted to tar files using podman save and stored elsewhere if you wish to preserve your image.",4.344345880728001
"What is the purpose of the node-local NVMe storage mounted in /tmp/containers on Summit?
",The NVMes on Summit are local to each node.,4.318333446438084
"What is the purpose of the node-local NVMe storage mounted in /tmp/containers on Summit?
","Each compute node on Summit has a 1.6TB Non-Volatile Memory (NVMe) storage device (high-memory nodes have a 6.4TB NVMe storage device), colloquially known as a ""Burst Buffer"" with theoretical performance peak of 2.1 GB/s for writing and 5.5 GB/s for reading. The NVMes could be used to reduce the time that applications wait for I/O.  More information can be found later in the Burst Buffer section.



<string>:208: (INFO/1) Duplicate implicit target name: ""software"".",4.303702283250756
"How can I optimize my visualization workflow on Andes?
","Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-user-guide.

For a full list of software available at the OLCF, please see the Software section (coming soon).",4.395954204369319
"How can I optimize my visualization workflow on Andes?
","ParaView was developed to analyze extremely large datasets using distributed memory computing resources. The OLCF provides ParaView server installs on Andes and Summit to facilitate large scale distributed visualizations. The ParaView server running on Andes and Summit may be used in a headless batch processing mode or be used to drive a ParaView GUI client running on your local machine.

For a tutorial of how to get started with ParaView on Andes, see our ParaView at OLCF Tutorial.",4.29748213046513
"How can I optimize my visualization workflow on Andes?
","Using VisIt on Summit is also an option, as the scalable rendering problem is currently not an issue on Summit (as of Sept. 2021).

As of February 2022, this issue on Andes has been fixed (must use VisIt 3.2.2 or higher).",4.275768880380321
"How are the GPUs associated with the L3 cache regions in Crusher?
","Crusher node architecture diagram

There are [4x] NUMA domains per node and [2x] L3 cache regions per NUMA for a total of [8x] L3 cache regions. The 8 GPUs are each associated with one of the L3 regions as follows:  NUMA 0:  hardware threads 000-007, 064-071 | GPU 4  hardware threads 008-015, 072-079 | GPU 5  NUMA 1:  hardware threads 016-023, 080-087 | GPU 2  hardware threads 024-031, 088-095 | GPU 3  NUMA 2:  hardware threads 032-039, 096-103 | GPU 6  hardware threads 040-047, 104-111 | GPU 7  NUMA 3:  hardware threads 048-055, 112-119 | GPU 0  hardware threads 056-063, 120-127 | GPU 1",4.404388776542903
"How are the GPUs associated with the L3 cache regions in Crusher?
","This example shows an important peculiarity of the Crusher nodes; the ""closest"" GPUs to each MPI rank are not in sequential order. For example, MPI rank 000 and its two OpenMP threads ran on hardware threads 000 and 001. As can be seen in the Crusher node diagram, these two hardware threads reside in the same L3 cache region, and that L3 region is connected via Infinity Fabric (blue line in the diagram) to GPU 4. This is an important distinction that can affect performance if not considered carefully.

Example 2: 16 MPI ranks - each with 2 OpenMP threads and 1 GPU (multi-node)",4.343350313528353
"How are the GPUs associated with the L3 cache regions in Crusher?
","Also, recall that the CPU cores in a given L3 cache region are connected to a specific GPU (see the Frontier Node Diagram and subsequent https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Note on NUMA domains <numa-note> for more information). In the examples below, knowledge of these details will be assumed.

There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_jobstep program and test whether processes and threads are mapped to the CPU cores and GPUs as intended..",4.3070811676899465
"What is the purpose of the destinationCACertificate field in the YAML output for a route in Slate?
","The outputted YAML will look like this example:

apiVersion: v1
kind: Route
metadata:
  name: my-service
spec:
  host: my-service.apps.<cluster>.ccs.ornl.gov
  to:
    kind: Service
    name: my-service
  tls:
    termination: reencrypt
    destinationCACertificate: |-
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----

As with edge encryption, by default the wildcard certificate for the router is used. You can provide your own keys if you like.

OpenShift supports unsecured routes over HTTP, but it is not recommended for use. Use edge encryption if you are unsure.",4.192797312191105
"What is the purpose of the destinationCACertificate field in the YAML output for a route in Slate?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.12386074353961
"What is the purpose of the destinationCACertificate field in the YAML output for a route in Slate?
","The produced yaml will look like this:

apiVersion: v1
kind: Route
metadata:
  name: my-service
spec:
  host: my-project.apps.<cluster>.ccs.ornl.gov
  to:
    kind: Service
    name: service-name
  tls:
    termination: passthrough

Note that with passthrough termination, no keys are provided to the route.

Re-encryption termination combines edge termination and passthrough termination, in that the router terminates TLS, then re-encrypts its connection. The endpoint may have a different certificate. With re-encryption termination, both the internal and external network paths are encrypted.",4.107151651412121
"How can I install CuPy on my system?
","If you wish to use CuPy within a clone of the OpenCE environment, the installation process is very similar to what we do in the regular CuPy installation we saw above.

The open-ce/1.2.0-pyXY-0 (which is the current default) will not support this. So make sure you are using open-ce/1.5.0-pyXY-0 or higher.

The contents of the open-ce module cannot be modified so you need to make your own clone of the open-ce environment.",4.356486825594796
"How can I install CuPy on my system?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.

This guide teaches you how to build CuPy from source into a custom virtual environment. On Summit and Andes, this is done using conda, while on Frontier this is done using venv.

In this guide, you will:

Learn how to install CuPy

Learn the basics of CuPy

Compare speeds to NumPy

OLCF Systems this guide applies to:

Summit

Frontier

Andes",4.349055367863284
"How can I install CuPy on my system?
","Compute nodes equipped with NVIDIA GPUs will be able to take full advantage of CuPy's capabilities on the system, providing significant speedups over NumPy-written code. CuPy with AMD GPUs is still being explored, and the same performance is not guaranteed (especially with larger data sizes). Instructions for Frontier are available in this guide, but users must note that the CuPy developers have labeled this method as experimental and has limitations.



<string>:60: (INFO/1) Duplicate implicit target name: ""installing cupy"".",4.330365933178058
"How can I check the version of CUDA installed on Summit?
","Although there are newer CUDA modules on Summit, cuda/11.0.3 is the latest version that is officially supported by the version of IBM's software stack installed on Summit. When loading the newer CUDA modules, a message is printed to the screen stating that the module is for “testing purposes only”. These newer unsupported CUDA versions might work with some users’ applications, but importantly, they are known not to work with GPU-aware MPI.",4.314434214188363
"How can I check the version of CUDA installed on Summit?
","Using the hip-cuda/5.1.0 module on Summit requires CUDA v11.4.0 or later. However, only CUDA v11.0.3 and earlier currently support GPU-aware MPI, so GPU-aware MPI is currently not available when using HIP on Summit.

Any codes compiled with post 11.0.3 CUDA (cuda/11.0.3) will not work with MPS enabled (-alloc_flags ""gpumps"") on Summit. The code will hang indefinitely. CUDA v11.0.3 is the latest version that is officially supported by IBM's software stack installed on Summit. We are continuing to look into this issue.",4.272604311736521
"How can I check the version of CUDA installed on Summit?
","module load ums
module load ums-gen119
module load nvidia-rapids/21.08

Due different dependecies, cuCIM is available on Summit as a separate module using the next commands:

module load ums
module load ums-gen119
module load nvidia-rapids/cucim_21.08

The RAPIDS and cuCIM modules loads gcc/9.3.0 and cuda/11.0.3 modules. For a complete list of available packages, use conda list command.  After Summit's OS upgrade on August 7th, 2021. Older RAPIDS modules were deprecated.

As an example, the following LSF script will run a single-GPU RAPIDS script in one Summit node:",4.191107795242922
"Can I use the Burst Buffers on Summit for temporary storage?
","Each compute node on Summit has a 1.6TB Non-Volatile Memory (NVMe) storage device (high-memory nodes have a 6.4TB NVMe storage device), colloquially known as a ""Burst Buffer"" with theoretical performance peak of 2.1 GB/s for writing and 5.5 GB/s for reading. The NVMes could be used to reduce the time that applications wait for I/O.  More information can be found later in the Burst Buffer section.



<string>:208: (INFO/1) Duplicate implicit target name: ""software"".",4.252151114576553
"Can I use the Burst Buffers on Summit for temporary storage?
","Each compute node on Summit has a 1.6TB Non-Volatile Memory (NVMe) storage device (high-memory nodes have a 6.4TB NVMe storage device), colloquially known as a ""Burst Buffer"" with theoretical performance peak of 2.1 GB/s for writing and 5.5 GB/s for reading. 100GB of each NVMe is reserved for NFS cache to help speed access to common libraries. When calculating maximum usable storage size, this cache and formatting overhead should be considered; We recommend a maximum storage of 1.4TB (6TB for high-memory nodes). The NVMes could be used to reduce the time that applications wait for I/O. Using",4.182425103158318
"Can I use the Burst Buffers on Summit for temporary storage?
",| (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/06/20200603_summit_workshop_python.pdf https://vimeo.com/427794043 | | 2020-06-03 | NVMe - Burst Buffers (Part2) | George Markomanolis (OLCF) | 2020 OLCF User Meeting (Summit New User Training) https://www.olcf.ornl.gov/calendar/2020-olcf-user-meeting/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/Burst_Buffer_summit_june_2020.pdf https://vimeo.com/427792243 | | 2020-06-03 | NVMe - Burst Buffers (Part1) | Chris Zimmer (OLCF) | 2020 OLCF User Meeting (Summit New User Training),4.150156463050221
"How much high-bandwidth memory (HBM2E) is available on each GCD?
","Most Summit nodes contain 512 GB of DDR4 memory for use by the POWER9 processors, 96 GB of High Bandwidth Memory (HBM2) for use by the accelerators, and 1.6TB of non-volatile memory that can be used as a burst buffer. A small number of nodes (54) are configured as ""high memory"" nodes. These nodes contain 2TB of DDR4 memory, 192GB of HBM2, and 6.4TB of non-volatile memory.",4.360118304680997
"How much high-bandwidth memory (HBM2E) is available on each GCD?
","Each V100 has access to 16 GB (32GB for high-memory nodes) of high-bandwidth memory (HBM2), which can be accessed at speeds of up to 900 GB/s. Access to this memory is controlled by (8) 512-bit memory controllers, and all accesses to the high-bandwidth memory go through the 6 MB L2 cache.

The processors within a node are connected by NVIDIA's NVLink interconnect. Each link has a peak bandwidth of 25 GB/s (in each direction), and since there are 2 links between processors, data can be transferred from GPU-to-GPU and CPU-to-GPU at a peak rate of 50 GB/s.",4.309522668031726
"How much high-bandwidth memory (HBM2E) is available on each GCD?
","Each Crusher compute node consists of [1x] 64-core AMD EPYC 7A53 ""Optimized 3rd Gen EPYC"" CPU (with 2 hardware threads per physical core) with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X, each with 2 Graphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can think of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth memory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU, allowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of 36+36 GB/s. The 2 GCDs on the same MI250X are connected with",4.298973652984473
"What is the purpose of quotas in data storage?
","Each user-centric and project-centric storage area has an associated quota, which could be a hard (systematically-enforceable) quota or a soft (policy-enforceable) quota. Storage usage will be monitored continually. When a user or project exceeds a soft quota for a storage area, the user or project PI will be contacted and will be asked if at all possible to purge data from the offending area. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for details on quotas for each storage area.",4.285584340225609
"What is the purpose of quotas in data storage?
","Although there are no hard quota limits for the project storage, an upper storage limit should be reported in the project request. The available space of a project can be modified upon request.",4.148062238432122
"What is the purpose of quotas in data storage?
","for more details on applicable quotas, backups, purge, and retention timeframes.",4.141776907487186
"How can you view the list of nodes assigned to a job?
","The most straightforward monitoring is with the bjobs command. This command will show the current queue, including both pending and running jobs. Running bjobs -l will provide much more detail about a job (or group of jobs). For detailed output of a single job, specify the job id after the -l. For example, for detailed output of job 12345, you can run bjobs -l 12345 . Other options to bjobs are shown below. In general, if the command is specified with -u all it will show information for all users/all jobs. Without that option, it only shows your jobs. Note that this is not an exhaustive list.",4.161370976427977
"How can you view the list of nodes assigned to a job?
","Provides additional details of given job.

The sview tool provide a graphical queue monitoring tool. To use, you will need an X server running on your local system. You will also need to tunnel X traffic through your ssh connection:

local-system> ssh -Y username@andes.ccs.ornl.gov
andes-login> sview",4.15082034220471
"How can you view the list of nodes assigned to a job?
","bjobs and jobstat help to identify what’s currently running and scheduled to run, but sometimes it’s beneficial to know how much of the system is not currently in use or scheduled for use.",4.131447291501683
"How can I import an environment into Conda at OLCF?
","This guide introduces a user to the basic workflow of using conda environments, as well as providing an example of how to create a conda environment that uses a different version of Python than the base environment uses on Summit. Although Summit is being used in this guide, all of the concepts still apply to other OLCF systems. If using Frontier, this assumes you already have installed your own version of conda (e.g., by https://docs.olcf.ornl.gov/software/python/miniconda.html).

OLCF Systems this guide applies to:

Summit

Andes

Frontier (if using conda)",4.433194964945608
"How can I import an environment into Conda at OLCF?
","This page describes how a user can successfully install specific quantum software tools to run on select OLCF HPC systems. This allows a user to utilize our systems to run a simulator, or even as a ""frontend"" to a vendor's quantum machine (""backend"").

For most of these instructions, we use conda environments. For more detailed information on how to use Python modules and conda environments on OLCF systems, see our :doc:`Python on OLCF Systems page</software/python/index>`, and our https://docs.olcf.ornl.gov/software/python/conda_basics.html.",4.325508600974604
"How can I import an environment into Conda at OLCF?
","By default this should create the cloned environment in /ccs/home/${USER}/.conda/envs/cloned_env (unless you changed it, as outlined in our https://docs.olcf.ornl.gov/software/python/index.html page).

To activate the new environment you should still load the module first. This will ensure that all of the conda settings remain the same.

$ module load open-ce
(open-ce-1.2.0-py38-X) $ conda activate cloned_env
(cloned_env) $",4.262685697041515
"How can I charge my job to project ABC123 on Summit?
","The requesting project's allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Summit would be equivalent to using 16,000 Summit node-hours of a project's allocation.



As is the case with many other queuing systems, it is possible to place dependencies on jobs to prevent them from running until other jobs have started/completed/etc. Several possible dependency settings are described in the table below:",4.137775387511329
"How can I charge my job to project ABC123 on Summit?
",For Summit:,4.127143657553603
"How can I charge my job to project ABC123 on Summit?
","$ export PROJECT=""ABC123""

To run an example ""Hello world"" program with Swift/T on Summit, create a file called hello.swift with the following contents:

trace(""Hello world!"");

Now, run the program from a shell or script:

$ swift-t -m lsf hello.swift

The output should look something like the following:",4.095284536242756
"Once the build completes, what should we have?
","What happens is that oc pulls in the provided repository, in this example Django, and automatically configures everything needed to build the image. You should now be able to go to the Openshift web GUI and under the builds tab see your newly built build.

Now, since everything has been configured, you can click the Start Build button in the upper right hand side of the Web GUI anytime that you need to make another build. You can also start a another build from the command line with either:

oc start-build <buildconfig_name>

Or, if you would like to receive logs from the build:",4.012389632053078
"Once the build completes, what should we have?
","When all of the above steps are completed, your user account will be created and you will be notified by email. Now that you have a user account and it has been associated with a project, you're ready to get to work.",3.996437559298822
"Once the build completes, what should we have?
","Openshift will automatically pull and build your source. If you make a change and need an updated build simply navigate to the build option on the menu to the left and select your project and click Start Build in the upper right.

The first step to creating the the build is to create a build config. This is done in one of three ways depending on how your code is structured. If you have a git repository already configured and it is public then the command

oc new-build .

will create your build config from that repository.",3.984115002103541
"Can I use unsecured routes over HTTP in Slate?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.298105164119256
"Can I use unsecured routes over HTTP in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.273237559744157
"Can I use unsecured routes over HTTP in Slate?
","By default, a route will only expose your https://docs.olcf.ornl.gov/systems/route.html#slate_services to NCCS networks. If you need your service exposed to the world outside ORNL, you will first need to get your project approved for external routes. To do this, submit a systems ticket. In the description, give us your project name and a brief reasoning for why exposing externally is needed.

We will let you know once your project is able to set up external routes.",4.142181956675857
"What is the purpose of the revisionHistoryLimit field in a Deployment?
","Blue-green deployment requires that your application can handle both old and new versions running at the same time. Be sure to think about your application and if it can handle this. For example, if the new version of the software changes how a certain field in a database is read and written, then the old version of the software won't be able to read the database changes, and your production instance could break. This is known as ""N-1 compatibility"" or ""backward compatibility"".",3.9618371064278417
"What is the purpose of the revisionHistoryLimit field in a Deployment?
","A/B Deployments are a popular way to try a new version of an application with a small subset of users in the production environment. With this strategy, you can specify that the older version gets most of the user requests while a limited fraction of users get sent to the new version. Since you can control the amount of users which get sent to the new version, you can gradually increase the volume of requests to the new version and eventually stop using the old version. Remember that deployment configurations don't do any autoscaling of pods, so you may have to adjust the number of pod",3.916516345308482
"What is the purpose of the revisionHistoryLimit field in a Deployment?
","As with other objects in OpenShift, you can create this Deployment with oc create -f {FILE_NAME}.

Once created, you can check the status of the ongoing deployment process with the command

oc rollout status deploy/{NAME}

To get basic information about the available revisions, if you had done any updates to the deployment since, run

oc rollout history deploy/{NAME}

You can view a specific revision with

oc rollout history deploy/{NAME} --revision=1

For more detailed information about a Deployment, use

oc describe deploy {NAME}

To roll back a deployment, run",3.877197381327421
"Can I customize the project pages area in myOLCF?
","After authenticating, you are redirected to the project pages area of myOLCF.

Every individual page in the project pages area should be interpreted within the context of a single, current project, which is displayed at the top of the left navigation menu (e.g. ""ABC123""):

project pages left navigation menu

The top navigation bar has a dropdown menu that can be used to switch the current project context to any of the projects of which you are a member.

switch projects dropdown menu",4.420791352993662
"Can I customize the project pages area in myOLCF?
","Project members

The myOLCF site is provided to aid in the utilization and management of OLCF allocations. See the https://docs.olcf.ornl.gov/services_and_applications/myolcf/index.html for more information.

If you have any questions or have a request for additional data, please contact the OLCF User Assistance Center.",4.217056096875444
"Can I customize the project pages area in myOLCF?
","At any time, you can view account pages by clicking on the ""My Account"" link in the top navigation menu:

link to my account page

There is only (1) account context in myOLCF: ""you"" as the currently-authenticated user. This account context is linked to the OLCF Moderate account that you used to authenticate to myOLCF.

account page left navigation menu

The left navigation menu also includes an expandable item with links to account-centric pages.",4.206798286997969
"Are there any sample datasets available for VisIt?
","For sample data and additional examples, explore the VisIt Data Archive and various VisIt Tutorials. Supplementary test data can be found in your local installation in the data directory:

Linux: /path/to/visit/data

macOS: /path/to/VisIt.app/Contents/Resources/data

Windows: C:\path\to\LLNL\VisIt x.y.z\data

Additionally, check out our beginner friendly OLCF VisIt Tutorial which uses Andes to visualize example datasets.",4.417920259907497
"Are there any sample datasets available for VisIt?
","Past VisIt Tutorials are available on the Visit User's Wiki along with a set of Updated Tutorials available in the VisIt User Manual.

Sample data not pre-packaged with VisIt can be found in the VisIt Data Archive.

Older VisIt Versions with their release notes can be found on the old VisIt website, and Newer Versions can be found on the new VisIt website with release notes found on the VisIt Blog or VisIt Github Releases page.

Non-ORNL related bugs and issues in VisIt can be found and reported on Github.",4.333302454686327
"Are there any sample datasets available for VisIt?
","VisIt is now available on Frontier through the UMS022 module.

VisIt 3.3.3 is now available on Andes.

VisIt is an interactive, parallel analysis and visualization tool for scientific data. VisIt contains a rich set of visualization features so you can view your data in a variety of ways. It can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes. Further information regarding VisIt can be found at the links provided in the https://docs.olcf.ornl.gov/systems/visit.html#visit-resources section.",4.249286718780475
"What is the name of the Slurm command used to display information about the cluster and its current state?
","Slurm documentation for each command is available via the man utility, and on the web at https://slurm.schedmd.com/man_index.html. Additional documentation is available at https://slurm.schedmd.com/documentation.html.

Some common Slurm commands are summarized in the table below. More complete examples are given in the Monitoring and Modifying Batch Jobs section of this guide.",4.270431753516689
"What is the name of the Slurm command used to display information about the cluster and its current state?
","This section describes how to run programs on the Crusher compute nodes, including a brief overview of Slurm and also how to map processes and threads to CPU cores and GPUs.

Slurm is the workload manager used to interact with the compute nodes on Crusher. In the following subsections, the most commonly used Slurm commands for submitting, running, and monitoring jobs will be covered, but users are encouraged to visit the official documentation and man pages for more information.",4.223479008161563
"What is the name of the Slurm command used to display information about the cluster and its current state?
","Slurm provides 3 ways of submitting and launching jobs on Crusher's compute nodes: batch scripts, interactive, and single-command. The Slurm commands associated with these methods are shown in the table below and examples of their use can be found in the related subsections. Please note that regardless of the submission method used, the job will launch on compute nodes, with the first compute in the allocation serving as head-node.

| sbatch |  | | --- | --- | | salloc |  | | srun |  |",4.21192851513848
"What is the deadline for reporting project utilization?
","Principal Investigators of current OLCF projects must submit a quarterly progress report. The quarterly reports are essential as the OLCF must diligently track the use of the center's resources. In keeping with this, the OLCF (and DOE Leadership Computing Facilities in general) imposes the following penalties for late submission:

| Timeframe | Penalty | | --- | --- | | 1 Month Late | Job submissions against offending project will be suspended. | | 3 Months Late | Login privileges will be suspended for all OLCF resources for all users associated with offending project. |",4.235157032882629
"What is the deadline for reporting project utilization?
","| Date | Utilization to-Date | Forfeited Amount | | --- | --- | --- | | May 1 | < 10% | Up to 30% of remaining allocation | | < 15% | Up to 15% of remaining allocation | | September 1 | < 10% | Up to 75% of remaining allocation | | < 33% | Up to 50% of remaining allocation | | < 50% | Up to 33% of remaining allocation |

For example, a 1,000,000 core-hour INCITE project that has utilized only 50,000 core-hours (5% of the allocation) on May 1st would forfeit (0.30 * 950,000) = 285,000 core-hours from their remaining allocation.",4.201805351569928
"What is the deadline for reporting project utilization?
","Utilization is calculated daily using batch jobs which complete between 00:00 and 23:59 of the previous day. For example, if a job moves into a run state on Tuesday and completes Wednesday, the job's utilization will be recorded Thursday. Only batch jobs which write an end record are used to calculate utilization. Batch jobs which do not write end records due to system failure or other reasons are not used when calculating utilization. Jobs which fail because of run-time errors (e.g. the user's application causes a segmentation fault) are counted against the allocation.",4.193465653763346
"What are the benefits of using XQuartz (X11) for Paraview on macOS?
","If ParaView is unable to connect to our systems after trying to initiate a connection via the GUI and you see a ""The process failed to start. Either the invoked program is missing, or you may have insufficient permissions to invoke the program"" error, make sure that you have XQuartz (X11) installed.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.",4.247327819356915
"What are the benefits of using XQuartz (X11) for Paraview on macOS?
","After installing, if you see a ""Can't open display"" or a ""DISPLAY is not set"" error, try restarting your computer. Sometimes XQuartz doesn't function properly if the computer was never restarted after installing.

If ParaView crashes when using the EGL version of the ParaView module via the command line and raises errors about OpenGL drivers or features, this is most likely due to not being connected to any GPUs.

Double check that you are either running on the GPU partition on Andes (i.e., -p gpu), or that you have -g set to a value greater than zero in your jsrun command on Summit.",4.214594311059395
"What are the benefits of using XQuartz (X11) for Paraview on macOS?
","After installing, you must give ParaView the relevant server information to be able to connect to OLCF systems (comparable to VisIt's system of host profiles). The following provides an example of doing so. Although several methods may be used, the one described should work in most cases.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.

For Windows clients, it is necessary to install PuTTY to create an ssh connection in step 2.

Step 1: Save the following servers.pvsc file to your local computer",4.160524217654949
"How do I execute a workflow in FireWorks?
","FireWorks is a free, open-source tool for defining, managing, and executing workflows. Complex workflows can be defined using Python, JSON, or YAML, are stored using MongoDB, and can be monitored through a built-in web interface. Workflow execution can be automated over arbitrary computing resources, including those that have a queueing system. FireWorks has been used to run millions of workflows encompassing tens of millions of CPU-hours across diverse application areas and in long-term production projects over the span of multiple years.",4.415284540008829
"How do I execute a workflow in FireWorks?
","To learn more about FireWorks, please refer to its extensive online documentation.

Before using FireWorks itself, you will need a MongoDB service running on https://docs.olcf.ornl.gov/systems/fireworks.html#Slate<slate>. A tutorial to deploy MongoDB is available in the Slate documentation.

You will need to know the connection information for both MongoDB so that FireWorks can be configured to connect to it.

Then, to use FireWorks on Summit, load the module as shown below:

$ module load workflows
$ module load fireworks/2.0.2",4.277402530531274
"How do I execute a workflow in FireWorks?
","module load workflows
module load fireworks/2.0.2

# Edit the following line to match your own MongoDB connection string.
export MONGODB_URI=""mongodb://admin:password@apps.marble.ccs.ornl.gov:32767/test""

jsrun -n 1 python3 demo.py

Finally, submit the batch job to LSF by executing the following command from a Summit login node:

$ bsub fireworks_demo.lsf

Congratulations! Once the batch job completes, you will find new directories beginning with launcher_ and containing FW.json files that detail exactly what happened.",4.236642388619934
"What is the significance of the ""mpirun"" command in the visit_example.py script?
","The example script visit_example.py is detailed below and uses data packaged with a standard VisIt installation (tire.silo). Although the tire.silo dataset does not need a large number of MPI tasks to render quickly, users visualizing large datasets may find the syntax helpful outside of this example, however a performance boost is not guaranteed. All users are encouraged to test the effect of additional processors on their own data, as rendering speeds can widely vary depending on the amount of MPI tasks utilized. Users are highly encouraged to use this script (especially after system",4.162608988804801
"What is the significance of the ""mpirun"" command in the visit_example.py script?
","the -s flag will launch the CLI in the form of a Python shell, which can be useful for interactive batch jobs.  The -np and -nn flags represent the number of processors and nodes VisIt will use to execute the Python script, while the -l flag specifies the specific parallel method to do so (required). Execute visit -fullhelp to get a list of all command line options.",4.127755283923844
"What is the significance of the ""mpirun"" command in the visit_example.py script?
","Let's test that mpi4py is working properly first by executing the example Python script ""hello_mpi.py"":

# hello_mpi.py
from mpi4py import MPI

comm = MPI.COMM_WORLD      # Use the world communicator
mpi_rank = comm.Get_rank() # The process ID (integer 0-41 for a 42-process job)

print('Hello from MPI rank %s !' %(mpi_rank))

To do so, submit a job to the batch queue:

Summit

.. code-block:: bash

   $ bsub -L $SHELL submit_hello.lsf

Andes

.. code-block:: bash

   $ sbatch --export=NONE submit_hello.sl

Frontier

.. code-block:: bash

   $ sbatch --export=NONE submit_hello.sl",4.125810943319856
"How long does a job stay in the debug queue?
","The debug queue (and the debug-spi queue for Moderate Enhanced security enclave projects) can be used to access Summit's compute resources for short non-production debug tasks.  The queue provides a higher priority compared to jobs of the same job size bin in production queues.  Production work and job chaining in the debug queue is prohibited.  Each user is limited to one job in any state in the debug queue at any one point. Attempts to submit multiple jobs to the debug queue will be rejected upon job submission.

debug job limits:",4.3579192243679845
"How long does a job stay in the debug queue?
","Users may have only 100 jobs queued in the batch queue at any time (this includes jobs in all states). Additional jobs will be rejected at submit time.

The debug quality of service (QOS) class can be used to access Frontier's compute resources for short non-production debug tasks. The QOS provides a higher priority compare to jobs of the same job size bin in production queues. Production work and job chaining using the debug QOS is prohibited. Each user is limited to one job in any state at any one point. Attempts to submit multiple jobs to this QOS will be rejected upon job submission.",4.241555658453741
"How long does a job stay in the debug queue?
","debug job limits:

| Min Nodes | Max Nodes | Max Walltime (Hours) | Max queued any state (per user) | Aging Boost (Days) | | --- | --- | --- | --- | --- | | 1 | unlimited | 2.0 | 1 | 2 |

To submit a job to the debug queue, add the -q debug option to your bsub command or #BSUB -q debug to your job script.

Production work and job chaining in the debug queue is prohibited.",4.168594909502421
"How can I check the status of my application on the myOLCF self-service portal?
","You can check the general status of your application at any time using the myOLCF self-service portal's account status page. For more information, see the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#myOLCF self-service portal documentation<myolcf-overview>. If you need to make further inquiries about your application, you may email our Accounts Team at accounts@ccs.ornl.gov.",4.570397670282404
"How can I check the status of my application on the myOLCF self-service portal?
","Once your application is evaluated and approved, you will be notified via email of your account creation, and the quantum resource vendor will be contacted with instructions to grant you access.

You can check the general status of your application at any time using the myOLCF self-service portal's account status page. For more information, see our https://docs.olcf.ornl.gov/services_and_applications/myolcf/overview.html page. If you need to make further inquiries about your application, you may email our Accounts Team at accounts@ccs.ornl.gov.",4.457616115706131
"How can I check the status of my application on the myOLCF self-service portal?
","Detailed instructions for account application can be found in the https://docs.olcf.ornl.gov/systems/frequently_asked_questions.html#applying-for-a-user-account section of the https://docs.olcf.ornl.gov/systems/frequently_asked_questions.html#accounts-and-projects page.

Detailed instructions for checking the status of your account application can be found in the https://docs.olcf.ornl.gov/systems/frequently_asked_questions.html#checking-application-status section of the https://docs.olcf.ornl.gov/systems/frequently_asked_questions.html#accounts-and-projects page.",4.35208302781894
"How can I ensure efficient cross-socket communication in Summit's resource sets?
","jsrun will create one or more resource sets within a node. Each resource set will contain 1 or more cores and 0 or more GPUs. A resource set can span sockets, but it may not span a node. While a resource set can span sockets within a node, consideration should be given to the cost of cross-socket communication. By creating resource sets only within sockets, costly communication between sockets can be prevented.",4.136934800532284
"How can I ensure efficient cross-socket communication in Summit's resource sets?
","summit>

The following example will create 4 resource sets each with 6 tasks and 3 GPUs. Each set of 6 MPI tasks will have access to 3 GPUs. Ranks 0 - 5 will have access to GPUs 0 - 2 on the first socket of the first node ( red resource set). Ranks 6 - 11 will have access to GPUs 3 - 5 on the second socket of the first node ( green resource set). This pattern will continue until 4 resource sets have been created. The following jsrun command will request 4 resource sets (-n4). Each resource set will contain 6 MPI tasks (-a6), 3 GPUs (-g3), and 6 cores (-c6).",4.125675739861676
"How can I ensure efficient cross-socket communication in Summit's resource sets?
",For Summit:,4.093105291077218
"What is the minimum level of testing expected for installations before they are released to users?
","Project PI must ensure that installations are tested to ensure basic functionality before being released to users. These are expected to be at minimum basic function/unit tests to ensure that the build/install was successful.

The resources provided by the OLCF for UMS shall not be used for software development or for routine testing purposes beyond the installation testing as described above.

Products may be removed from UMS at the request of the Project PI by notifying the OLCF (help@olcf.ornl.gov) of their intent and cleaning up their directory space.",4.2885844356723934
"What is the minimum level of testing expected for installations before they are released to users?
","Blue-green deployment requires that your application can handle both old and new versions running at the same time. Be sure to think about your application and if it can handle this. For example, if the new version of the software changes how a certain field in a database is read and written, then the old version of the software won't be able to read the database changes, and your production instance could break. This is known as ""N-1 compatibility"" or ""backward compatibility"".",4.0667381321740885
"What is the minimum level of testing expected for installations before they are released to users?
",It is the user’s responsibility to insure the appropriate level of backup and integrity checks on critical data and programs.,4.051427355481645
"How can I install CuPy and its dependencies on Summit?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.

This guide teaches you how to build CuPy from source into a custom virtual environment. On Summit and Andes, this is done using conda, while on Frontier this is done using venv.

In this guide, you will:

Learn how to install CuPy

Learn the basics of CuPy

Compare speeds to NumPy

OLCF Systems this guide applies to:

Summit

Frontier

Andes",4.403151654961089
"How can I install CuPy and its dependencies on Summit?
","The CC and NVCC flags ensure that you are passing the correct wrappers, while the various flags for Frontier tell CuPy to build for AMD GPUs. Note that, on Summit, if you are using the instructions for installing CuPy with OpenCE below, the cuda/11.0.3 module will automatically be loaded. This installation takes, on average, 10-20 minutes to complete (due to building everything from scratch), so don't panic if it looks like the install timed-out. Eventually you should see output similar to this (versions will vary):

Successfully installed cupy-12.2.0 fastrlock-0.8.1",4.305609768457829
"How can I install CuPy and its dependencies on Summit?
","module load ums
module load ums-gen119
module load nvidia-rapids/21.08

Due different dependecies, cuCIM is available on Summit as a separate module using the next commands:

module load ums
module load ums-gen119
module load nvidia-rapids/cucim_21.08

The RAPIDS and cuCIM modules loads gcc/9.3.0 and cuda/11.0.3 modules. For a complete list of available packages, use conda list command.  After Summit's OS upgrade on August 7th, 2021. Older RAPIDS modules were deprecated.

As an example, the following LSF script will run a single-GPU RAPIDS script in one Summit node:",4.302082938088933
"How can I specify the interpreter for my batch script?
","by a shell name. For example, to request an interactive batch job (with bash as the shell) equivalent to the sample batch script above, you would use the command: bsub -W 3:00 -nnodes 2048 -P ABC123 -Is /bin/bash",4.245101648109626
"How can I specify the interpreter for my batch script?
","Interpreter Line

The first line of a script can be used to specify the script’s interpreter; this line is optional. If not used, the submitter’s default shell will be used. The line uses the hash-bang syntax, i.e., #!/path/to/shell.

Slurm Submission Options",4.237188096047167
"How can I specify the interpreter for my batch script?
","The first line of a script can be used to specify the script’s interpreter; this line is optional. If not used, the submitter’s default shell will be used. The line uses the hash-bang syntax, i.e., #!/path/to/shell.

The Slurm submission options are preceded by the string #SBATCH, making them appear as comments to a shell. Slurm will look for #SBATCH options in a batch script from the script’s first line through the first non-comment line. A comment line begins with #. #SBATCH options entered after the first non-comment line will not be read by Slurm.",4.200194387208053
"How do project members access their individual Member Archive directories?
","As with the three project work areas, the difference between these three areas lies in the accessibility of data to project members and to researchers outside of the project. Member Archive directories are accessible only by an individual project member by default, Project Archive directories are accessible by all project members, and World Archive directories are readable by any user on the system.

<string>:194: (INFO/1) Duplicate implicit target name: ""permissions"".",4.478591273064609
"How do project members access their individual Member Archive directories?
","World Archive Directory: 775

For example, if you have data that must be restricted only to yourself, keep them in your Member Archive directory for that project (and leave the default permissions unchanged). If you have data that you intend to share with researchers within your project, keep them in the project’s Project Archive directory. If you have data that you intend to share with researchers outside of a project, keep them in the project’s World Archive directory.",4.438266162066516
"How do project members access their individual Member Archive directories?
","UNIX Permissions on each project-centric archive storage area differ according to the area’s intended collaborative use. Under this setup, the process of sharing data with other researchers amounts to simply ensuring that the data resides in the proper archive directory.

Member Archive Directory: 700

Project Archive Directory: 770

World Archive Directory: 775",4.380194907888841
"How often must Principal Investigators of OLCF projects submit progress reports?
","Principal Investigators of current OLCF projects must submit a quarterly progress report. The quarterly reports are essential as the OLCF must diligently track the use of the center's resources. In keeping with this, the OLCF (and DOE Leadership Computing Facilities in general) imposes the following penalties for late submission:

| Timeframe | Penalty | | --- | --- | | 1 Month Late | Job submissions against offending project will be suspended. | | 3 Months Late | Login privileges will be suspended for all OLCF resources for all users associated with offending project. |",4.535864297177087
"How often must Principal Investigators of OLCF projects submit progress reports?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  Title: Project Reporting Policy Version: 12.10",4.390071772265898
"How often must Principal Investigators of OLCF projects submit progress reports?
","Access to OLCF resources is limited to approved projects and their users. The type of project (INCITE, ALCC, or Director's Discretion) will determine the application and review procedures. *Quarterly reports are required from industrial Director's Discretion projects only.",4.363503303717868
"How do I create a FireWorks in the FireWorks demo on Summit?
","To learn more about FireWorks, please refer to its extensive online documentation.

Before using FireWorks itself, you will need a MongoDB service running on https://docs.olcf.ornl.gov/systems/fireworks.html#Slate<slate>. A tutorial to deploy MongoDB is available in the Slate documentation.

You will need to know the connection information for both MongoDB so that FireWorks can be configured to connect to it.

Then, to use FireWorks on Summit, load the module as shown below:

$ module load workflows
$ module load fireworks/2.0.2",4.313448887958651
"How do I create a FireWorks in the FireWorks demo on Summit?
","Run the following command to verify that FireWorks is available:

$ rlaunch -v
rlaunch v2.0.2

To run this FireWorks demo on Summit, you will create a Python file and then submit it as a batch job to LSF from a Summit node.

The contents for demo.py follow:

import os

from fireworks import Firework, Workflow, LaunchPad, ScriptTask
from fireworks.core.rocket_launcher import rapidfire

# Set up and reset the LaunchPad using MongoDB URI string.
launchpad = LaunchPad(host = os.getenv(""MONGODB_URI""), uri_mode = True)
launchpad.reset('', require_password=False)",4.264612064978728
"How do I create a FireWorks in the FireWorks demo on Summit?
","module load workflows
module load fireworks/2.0.2

# Edit the following line to match your own MongoDB connection string.
export MONGODB_URI=""mongodb://admin:password@apps.marble.ccs.ornl.gov:32767/test""

jsrun -n 1 python3 demo.py

Finally, submit the batch job to LSF by executing the following command from a Summit login node:

$ bsub fireworks_demo.lsf

Congratulations! Once the batch job completes, you will find new directories beginning with launcher_ and containing FW.json files that detail exactly what happened.",4.148117637208023
"What is the benefit of continuously observing actual system state and attempting to apply the desired state?
","Versioned and Immutable: Desired state is stored in a way that enforces immutability, versioning and retains a complete version history.

Pulled Automatically: Software agents automatically pull the desired declarations from the source.

Continuously Reconciled: Software agents continuously observe actual system state and attempt to apply the desired state.

On Slate, Red Hat OpenShift GitOps is based on the open source ArgoCD project. For more information as well as how to install and use ArgoCD on Slate, see: https://docs.olcf.ornl.gov/systems/overview.html#slate_openshift_gitops.",4.0717796970946365
"What is the benefit of continuously observing actual system state and attempting to apply the desired state?
","bjobs and jobstat help to identify what’s currently running and scheduled to run, but sometimes it’s beneficial to know how much of the system is not currently in use or scheduled for use.",4.046555742938585
"What is the benefit of continuously observing actual system state and attempting to apply the desired state?
","Most users will find batch jobs an easy way to use the system, as they allow you to ""hand off"" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.",3.9419334990043247
"What is the purpose of the module load commands in the guide?
",| Command | Description | | --- | --- | | module -t list | Shows a terse list of the currently loaded modules | | module avail | Shows a table of the currently available modules | | module help <modulename> | Shows help information about <modulename> | | module show <modulename> | Shows the environment changes made by the <modulename> modulefile | | module spider <string> | Searches all possible modules according to <string> | | module load <modulename> [...] | Loads the given <modulename>(s) into the current environment | | module use <path> | Adds <path> to the modulefile search cache and,4.155292585052242
"What is the purpose of the module load commands in the guide?
",| Command | Description | | --- | --- | | module -t list | Shows a terse list of the currently loaded modules | | module avail | Shows a table of the currently available modules | | module help <modulename> | Shows help information about <modulename> | | module show <modulename> | Shows the environment changes made by the <modulename> modulefile | | module spider <string> | Searches all possible modules according to <string> | | module load <modulename> [...] | Loads the given <modulename>(s) into the current environment | | module use <path> | Adds <path> to the modulefile search cache and,4.155292585052242
"What is the purpose of the module load commands in the guide?
",| Command | Description | | --- | --- | | module -t list | Shows a terse list of the currently loaded modules | | module avail | Shows a table of the currently available modules | | module help <modulename> | Shows help information about <modulename> | | module show <modulename> | Shows the environment changes made by the <modulename> modulefile | | module spider <string> | Searches all possible modules according to <string> | | module load <modulename> [...] | Loads the given <modulename>(s) into the current environment | | module use <path> | Adds <path> to the modulefile search cache and,4.155292585052242
"How do I attend an OLCF GPU Hackathon?
","If you would like more information about how you can volunteer to mentor a team at an upcoming Open/GPU hackathon, please visit our Become a Mentor page.

<p style=""font-size:20px""><b>Who can I contact with questions?</b></p>

If you have any questions about the OLCF GPU Hackathon Series, please contact OLCF Help at (help@olcf.ornl.gov).",4.528130779087797
"How do I attend an OLCF GPU Hackathon?
","Please visit openhackathons.org/s/events-overview to see the current list of events (new ones added throughout the year) and their proposal deadlines. To submit a proposal, click on the event you'd like to attend and submit the form.

The OLCF-supported events are a subset of a larger number of Open hackathons organized around the world. Look for hackathons with the OLCF logo to find events supported by OLCF.

<p style=""font-size:20px""><b>Want to be a mentor?</b></p>",4.399124482051355
"How do I attend an OLCF GPU Hackathon?
","If you want/need to get your code running (or optimized) on a GPU-accelerated system, these hackathons offer a unique opportunity to set aside the time, surround yourself with experts in the field, and push toward your development goals. By the end of the event, each team should have part of their code running (or more optimized) on GPUs, or at least have a clear roadmap of how to get there.

<p style=""font-size:20px""><b>Target audience</b></p>",4.397604036808713
"How do I add storage to my application?
","To add the PVC to a pod using the web GUI first select Workloads and then Deployments in the hamburger menu on the left had side.

Application Deployments

Next, select the deployment that contains the pod you wish to add the storage to.

Select Actions in the upper left and then and then Add Storage.

Edit YAML

Fill out your Mount point and other options if you need them to be non-default values. Otherwise, hit the Add button at the bottom.

Add Storage Menu",4.229670706182553
"How do I add storage to my application?
","From the main screen of a project go to the Storage option in the hamburger menu on the left hand side of the screen, then click Persistent Volume Claims

Persistent Volume Claim Menu

In the upper right click the Create Persistent Volume Claim button.

Create Storage

This will take you to a screen that allows you to select what you need for your PVC. Give your claim a name and request an amount of storage and click Create. You can also click Edit YAML to edit the PVC object directly.

PV Settings",4.167219806706766
"How do I add storage to my application?
","Add Storage Menu

You should see a green popup appear in the upper right saying that the storage was added. This should additionally trigger a new deployment. To make sure a new deployment happened look at the Created time of the top most deployment.

A PersistentVolume is backed up by creating a VolumeSnapshot object. The VolumeSnapshot will create a point in time backup of your PersistentVolume and is something that you would likely do before an upgrade.

A Volume Snapshot will bind to a PersistentVolumeClaim. An example PersistentVolumeClaim to bind a VolumeSnapshot to follows:",4.130080180090994
"What is the purpose of `cp.cuda.Stream.null.synchronize()` in the provided code?
",https://vimeo.com/582093007 | | 2021-07-16 | CUDA Multithreading with Streams | Robert Searles (NVIDIA) | CUDA Training Series https://www.olcf.ornl.gov/calendar/cuda-multithreading/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/05/10-Multithreading-and-CUDA-Concurrency.pdf https://vimeo.com/575930839 | | 2021-05-26 | ROCgdb and HIP Math Libraries | Justin Chang (AMD) | HIP Training Workshop https://www.olcf.ornl.gov/calendar/2021hip/ | (slides | exercises | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/04/rocgdb_hipmath_ornl_2021_v2.pdf,3.907068097191779
"What is the purpose of `cp.cuda.Stream.null.synchronize()` in the provided code?
","Within a warp, a mask is used to specify which threads are currently active when divergent branches of code are encountered. The (active) threads within each branch execute their statements serially before threads in the next branch execute theirs. This means that programs on pre-Volta GPUs should avoid sub-warp synchronization; a sync point in the branches could cause a deadlock if all threads in a warp do not reach the synchronization point.",3.843814383635293
"What is the purpose of `cp.cuda.Stream.null.synchronize()` in the provided code?
","CUDA-aware MPI allows GPU buffers (e.g., GPU memory allocated with cudaMalloc) to be used directly in MPI calls rather than requiring data to be manually transferred to/from a CPU buffer (e.g., using cudaMemcpy) before/after passing data in MPI calls. By itself, CUDA-aware MPI does not specify whether data is staged through CPU memory or, for example, transferred directly between GPUs when passing GPU buffers to MPI calls. That is where GPUDirect comes in.",3.8404619649415466
"How can I ensure that each MPI rank has access to only 1 GPU?
","In the following examples, each MPI rank (and its OpenMP threads) will be mapped to a single GPU.

Example 0: 1 MPI rank with 1 OpenMP thread and 1 GPU (single-node)

Somewhat counterintuitively, this common test case is currently among the most difficult. Slurm ignores GPU bindings for nodes with only a single task, so we do not use --gpu-bind here. We must allocate only a single GPU to ensure that only one GPU is available to the task, and since we get the first GPU available we should bind the task to the CPU closest to the allocated GPU.",4.399759432182052
"How can I ensure that each MPI rank has access to only 1 GPU?
","In the following examples, each MPI rank (and its OpenMP threads) will be mapped to a single GPU.

Example 1: 4 MPI ranks - each with 2 OpenMP threads and 1 GPU (single-node)

This example launches 4 MPI ranks (-n4), each with 2 physical CPU cores (-c2) to launch 2 OpenMP threads (OMP_NUM_THREADS=2) on. In addition, each MPI rank (and its 2 OpenMP threads) should have access to only 1 GPU. To accomplish the GPU mapping, two new srun options will be used:

--gpus-per-task specifies the number of GPUs required for the job on each task",4.385657014909847
"How can I ensure that each MPI rank has access to only 1 GPU?
","As mentioned previously, all GPUs are accessible by all MPI ranks by default, so it is possible to programatically map any combination of MPI ranks to GPUs. However, there is currently no way to use Slurm to map multiple GPUs to a single MPI rank. If this functionality is needed for an application, please submit a ticket by emailing help@olcf.ornl.gov.

There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_jobstep program and test whether or not processes and threads are running where intended.",4.37111487124535
"How do I know which encryption type to use for my route in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.22551855701245
"How do I know which encryption type to use for my route in Slate?
","Be sure to check the Secure Route checkbox. See the CLI sections to determine which encryption type is best for you. If you don't know, it's probably edge encryption. You can leave the certificate fields blank when using edge encryption on a wildcard *.apps.<cluster>.ccs.ornl.gov",4.199749010955891
"How do I know which encryption type to use for my route in Slate?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.173290255106715
"Can you provide an example of a kustomization.yaml file?
","kustomize Examples

The kustomization.yaml file declares what resource files kustomize should use when generating kubernetes resources. Additionally, the kustomization.yaml file will specify how resources should be modified, if needed. A kustomization.yaml file will contain information that falls typically into four categories:

resources: what existing resource files should be used.

generators: what new resources should be created dynamically.

transformers: what resources should be changed and how to change them.

meta: fields that may influence all of the above.",4.391483959796233
"Can you provide an example of a kustomization.yaml file?
","Starting with the Hello World example in the prior section, the kustomization.yaml file located in the base directory would look similar to:

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
metadata:
  name: arbitrary
commonLabels:
  app: hello
resources:
- deployment.yaml
- service.yaml
- configMap.yaml",4.363955547840628
"Can you provide an example of a kustomization.yaml file?
","The structure of the kustomization.yaml file starts off similar to the structure of a kubernetes object: apiVersion, kind, and metadata.name. From there, the file contains resource information and meta information. The resource information is specified in the resources block and lists files that should be included for use by kustomize. In this case, three files are specified: deployment.yaml, service.yaml, and configMap.yaml. Each of these files define a kubernetes resource of the type indicated by the filename. Resource file names are arbitrary, but they must match the name of the file in",4.304189491217504
"What should I do if I encounter issues during the uninstall process?
","When clicking the uninstall, it may appear that the UI hangs and nothing is happening. It may take some time to remove all of the resources.

Once the installation is complete, the UI will refresh and the deployment will no longer be listed.

Verify that the runner has been unregistered from the GitLab project (GitLab->Settings->CI/CD->Runners). One could also check to ensure that all the pods were deleted by changing over to the ""Administrator"" perspective and selecting Workloads -> Pods from the navigation.",4.044277729130278
"What should I do if I encounter issues during the uninstall process?
","The information below the dashes which we omitted can be occasionally helpful for debugging, say if there is some kind of hardware problem..",3.913170951803969
"What should I do if I encounter issues during the uninstall process?
",section for instructions.,3.909822740046303
"Can I backup my data stored in my project's archive directory on HPSS?
","Member Work, Project Work, and World Work directories are not backed up. Project members are responsible for backing up these files, either to Project Archive areas (HPSS) or to an off-site location.

Moderate projects without export control restrictions are also allocated project-specific archival space on the High Performance Storage System (HPSS). The default quota is shown on the table below. If a higher quota is needed, contact the User Assistance Center.

There is no HPSS storage for Moderate Enhanced Projects, Moderate Projects subject to export control, or Open projects.",4.424503334904425
"Can I backup my data stored in my project's archive directory on HPSS?
","Project Archive directories may only be accessed via utilities called HSI and HTAR. For more information on using HSI or HTAR, see the https://docs.olcf.ornl.gov/systems/project_centric.html#data-hpss section.",4.386191069257276
"Can I backup my data stored in my project's archive directory on HPSS?
","Project members get an individual Member Archive directory for each associated project; these reside on the High Performance Storage System (HPSS), OLCF's tape-archive storage system. Member Archive areas are not shared with other users of the system and are intended for project data that the user does not want to make available to other users.  HPSS is intended for data that do not require day-to-day access. Users should not store data unrelated to OLCF projects on HPSS. Users should periodically review files and remove unneeded ones. See the section",4.385709829194805
"What kind of support is available for researchers using the SPI?
","The OLCF's Scalable Protected Infrastructure (SPI) provides resources and protocols that enable researchers to process protected data at scale.  The SPI is built around a framework of security protocols that allows researchers to process large datasets containing private information.  Using this framework researchers can use the center's large HPC resources to compute data containing protected health information (PHI), personally identifiable information (PII), data protected under International Traffic in Arms Regulations, and other types of data that require privacy.",4.219822971330195
"What kind of support is available for researchers using the SPI?
","The OLCF's Scalable Protected Infrastructure (SPI) provides resources and protocols that enable researchers to process protected data at scale.  The SPI is built around a framework of security protocals that allows researchers to process large datasets containing private information.  Using this framework researchers can use the center's large HPC resources to compute data containing protected health information (PHI), personally identifiable information (PII), data protected under International Traffic in Arms Regulations, and other types of data that require privacy.",4.213412825929661
"What kind of support is available for researchers using the SPI?
","The OLCF’s Scalable Protected Infrastructure (SPI) provides resources and protocols that enable researchers to process protected data at scale. The SPI is built around a framework of security protocols that allows researchers to process large datasets containing private information. Using this framework researchers can use the center’s large HPC resources to compute data containing protected health information (PHI), personally identifiable information (PII), data protected under International Traffic in Arms Regulations, and other types of data that require privacy.",4.201058975611121
"What is the purpose of the route settings in ArgoCD instance creation?
","Next Click the ""Create ArgoCD"" button. You will be presented with a form view similar to:

Image of the form view for ArgoCD instance creation.

Starting with the form view of the process, make the following changes allowing for access to the ArgoCD web UI via a route:

Server -> Insecure -> true

Server -> Route -> Enabled -> true

Server -> Route -> Tls -> Termination -> edge

Image of the form view for ArgoCD instance creation with the route settings configured.",4.396197733692037
"What is the purpose of the route settings in ArgoCD instance creation?
","This enables access to the ArgoCD instance once deployed via the web browser more easily. In the above images, notice that the instance name is argocd. By default, the route name to the web UI will be <<instanceName>>-server-<<projectName>>.apps.<<clusterName>>.ccs.ornl.gov. If a different host name is desired to access the instance, enter the desired name in the Host parameter while maintaining the pattern new-name.apps.<cluster>.ccs.ornl.gov. For example,

Image of the form view with a custom host name set.",4.281489585441044
"What is the purpose of the route settings in ArgoCD instance creation?
","NAME                                     HOST/PORT                                PATH   SERVICES        PORT   TERMINATION   WILDCARD
route.route.openshift.io/argocd-server   argocd-stf042.apps.marble.ccs.ornl.gov          argocd-server   http   edge          None

When one navigates to the route in a web browser, the ArgoCD login screen will be presented:

Image  of the ArgoCD login screen.

For ArgoCD authentication, the default user is admin with the password stored in the <<instanceName>>-cluster secret in the project. Following login, the instance is ready for configuration:",4.281384529127582
"Can I increase my home directory quota on Slate?
","$ quota -Qs
Disk quotas for user usrid (uid 12345):
     Filesystem  blocks   quota   limit   grace   files   quota   limit   grace
nccsfiler1a.ccs.ornl.gov:/vol/home
                  4858M   5000M   5000M           29379   4295m   4295m

Moderate enhanced projects have home directores located in GPFS. There is no enforced quota, but it is recommended that users not exceed 50 TB. These home directories are subject to the 90 day purge",4.232580363863116
"Can I increase my home directory quota on Slate?
","You can check your home directory quota with the quota command. If it is over quota, you need to bring usage under the quota and then your jobs should run without encountering the Disk quota exceeded error.

Footnotes

1

This entry is for legacy User Archive directories which contained user data on January 14, 2020.

2

User Archive directories that were created (or had no user data) after January 14, 2020. Settings other than permissions are not applicable because directories are root-owned and contain no user files.

3",4.199706668441233
"Can I increase my home directory quota on Slate?
","Once your Slate Project Allocation Request is approved, you can create your own namespaces and move your allocation around those namespaces via the quota dashboard located at https://quota.marble.ccs.ornl.gov and https://quota.onyx.ccs.ornl.gov. The terms ""namespace"" and ""project"" may get used interchangeably when referring to your project's usable space within the requested resource boundaries (CPU/Memory/Storage).



The OC tool provides CLI access to the OpenShift cluster. It needs to be installed on your machine.",4.15754476951714
"How long does a job in the ""debug"" queue receive a priority aging boost for scheduling?
","The debug queue (and the debug-spi queue for Moderate Enhanced security enclave projects) can be used to access Summit's compute resources for short non-production debug tasks.  The queue provides a higher priority compared to jobs of the same job size bin in production queues.  Production work and job chaining in the debug queue is prohibited.  Each user is limited to one job in any state in the debug queue at any one point. Attempts to submit multiple jobs to the debug queue will be rejected upon job submission.

debug job limits:",4.350098984731993
"How long does a job in the ""debug"" queue receive a priority aging boost for scheduling?
","The basic priority-setting mechanism for jobs waiting in the queue is the time a job has been waiting relative to other jobs in the queue.

If your jobs require resources outside these queue policies such as higher priority or longer walltimes, please contact help@olcf.ornl.gov.

Jobs are aged according to the job's requested processor count (older age equals higher queue priority). Each job's requested processor count places it into a specific bin. Each bin has a different aging parameter, which all jobs in the bin receive.",4.313620114127597
"How long does a job in the ""debug"" queue receive a priority aging boost for scheduling?
","The basic priority mechanism for jobs waiting in the queue is the time the job has been waiting in the queue. If your jobs require resources outside these policies such as higher priority or longer walltimes, please contact help@olcf.ornl.gov

Jobs are aged according to the job's requested node count (older age equals higher queue priority). Each job's requested node count places it into a specific bin. Each bin has a different aging parameter, which all jobs in the bin receive.",4.282548003540449
"How do I ensure the security of my SecurID token?
","You will also be sent a request to complete identity verification. When your account is approved, your RSA SecurID token will also be enabled. Please refer to our https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#system-user-guides for more information on host access. DO NOT share your PIN or RSA SecurID token with anyone. Sharing of accounts will result in termination. If your SecurID token is stolen or misplaced, contact the OLCF immediately and report the missing token. Upon termination of your account access, return the token to the OLCF in person or via mail.",4.303219983679582
"How do I ensure the security of my SecurID token?
","If you are processing sensitive or proprietary data, additional paperwork is required and will be sent to you.

If you need an RSA SecurID token from our facility, the token and additional paperwork will be sent to you via email to complete identity proofing.",4.287884901839193
"How do I ensure the security of my SecurID token?
","Access to systems is provided via Secure Shell version 2 (sshv2). You will need to ensure that your ssh client supports keyboard-interactive authentication. The method of setting up this authentication varies from client to client, so you may need to contact your local administrator for assistance. Most new implementations support this authentication type, and many ssh clients are available on the web. Login sessions will be automatically terminated after a period of inactivity. When you apply for an account, you will be mailed an RSA SecurID token. You will also be sent a request to complete",4.145160481195094
"Can I use the HPSS utilities to move files off of Slate?
",Please note that the HPSS is not mounted directly onto Frontier nodes. There are two main methods for accessing and moving data to/from the HPSS. The first is to use the command line utilities hsi and htar. The second is to use the Globus data transfer service. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-hpss for more information on both of these methods.,4.236079090565619
"Can I use the HPSS utilities to move files off of Slate?
","Copying data to the HPSS archive system

The hsi and htar utilities can be used to to transfer data from the Orion filesystem to the HPSS. The tools can also be used to transfer data from the HPSS to the Orion filesystem.

Globus is also available to transfer data directly to the HPSS

Please do not use the HPSS as a method to migrate data

Due to the large amounts of data on the Alpine scratch filesystem and the limited available space on the HPSS archive system, we strongly recommend not using the HPSS to transfer data between Alpine and Orion.",4.232955852376065
"Can I use the HPSS utilities to move files off of Slate?
","hsi ls

hsi commands are similar to ftp commands. For example, hsi get and hsi put are used to retrieve and store individual files, and hsi mget and hsi mput can be used to retrieve multiple files. To send a file to HPSS, you might use:

hsi put a.out : /hpss/prod/[projid]/users/[userid]/a.out

To retrieve one, you might use:

hsi get /hpss/prod/[projid]/proj-shared/a.out

Here is a list of commonly used hsi commands.",4.175871005609726
"How can I suspend a job on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.264689451343902
"How can I suspend a job on Summit?
","If you submit a job to a ""normal"" Summit queue while on Citadel, such as -q batch, your job will be unable to launch.",4.258947507454705
"How can I suspend a job on Summit?
",For Summit:,4.233211272640938
"What are the different user roles available in OLCF?
","New to the Oak Ridge Leadership Computing Facility?

Welcome! The information below introduces how we structure user accounts, projects, and system allocations. It's all you need to know about getting to work. In general, OLCF resources are granted to projects in allocations, and are made available to the users associated with each project.",4.387622288302749
"What are the different user roles available in OLCF?
","Collaborators involved with an approved and activated OLCF project can apply for a user account associated with it. There are several steps in receiving a user account, and we're here to help you through them.

Project PIs do not receive a user account with project creation, and must also apply.",4.383708983228596
"What are the different user roles available in OLCF?
","When all of the above steps are completed, your user account will be created and you will be notified by email. Now that you have a user account and it has been associated with a project, you're ready to get to work. This website provides extensive documentation for OLCF systems, and can help you efficiently use your project's allocation. We recommend reading the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#System User Guides<system-user-guides> for the machines you will be using often.",4.35157415073112
"How can I use wide formatting for output?
",and CUDA Fortran. The image below demonstrates the general pattern for WMMA usage.,3.8992200892758566
"How can I use wide formatting for output?
","-s | Show suspended jobs, including the reason(s) they're suspended | | bjobs -r | Show running jobs | | bjobs -p | Show pending jobs | | bjobs -w | Use ""wide"" formatting for output |",3.8872355849286544
"How can I use wide formatting for output?
","$ module load imagemagick # for convert utility
$ export WALLTIME=00:10:00
$ export PROJECT=STF019
$ export TURBINE_OUTPUT=/gpfs/alpine/scratch/ketan2/stf019/swift-work/cross-facility/data
$ swift-t -O0 -m lsf workflow.swift

If all goes well, and when the job starts running, the output will be produced in the data directory output.txt file.",3.8839218594152007
"What is the default value for the number of resource sets per host in Summit?
","summit>

The following example will create 4 resource sets each with 6 tasks and 3 GPUs. Each set of 6 MPI tasks will have access to 3 GPUs. Ranks 0 - 5 will have access to GPUs 0 - 2 on the first socket of the first node ( red resource set). Ranks 6 - 11 will have access to GPUs 3 - 5 on the second socket of the first node ( green resource set). This pattern will continue until 4 resource sets have been created. The following jsrun command will request 4 resource sets (-n4). Each resource set will contain 6 MPI tasks (-a6), 3 GPUs (-g3), and 6 cores (-c6).",4.214839569215556
"What is the default value for the number of resource sets per host in Summit?
","Because the login nodes are resources shared by all Summit users, we utilize cgroups to help better ensure resource availability for all users of the shared nodes. By default each user is limited to 16 hardware-threads, 16GB of memory, and 1 GPU.  Please note that limits are set per user and not individual login sessions. All user processes on a node are contained within a single cgroup and share the cgroup's limits.",4.125116270153267
"What is the default value for the number of resource sets per host in Summit?
","The following example will create 12 resource sets each with 1 task, 4 threads, and 1 GPU. Each MPI task will start 4 threads and have access to 1 GPU. Rank 0 will have access to GPU 0 and start 4 threads on the first socket of the first node ( red resource set). Rank 2 will have access to GPU 1 and start 4 threads on the second socket of the first node ( green resource set). This pattern will continue until 12 resource sets have been created. The following jsrun command will create 12 resource sets (-n12). Each resource set will contain 1 MPI task (-a1), 1 GPU (-g1), and 4 cores (-c4).",4.113558547270594
"How can I use Score-P with Autotools?
","For CMake and Autotools based builds it is recommended to configure in the following way(s):

#Example for CMake

$ SCOREP_WRAPPER=off cmake .. \
     -DCMAKE_C_COMPILER=scorep-gcc \
     -DCMAKE_CXX_COMPILER=scorep-g++ \
     -DCMAKE_Fortran_COMPILER=scorep-ftn

#Example for autotools

$ SCOREP_WRAPPER=off  ../configure \
     CC=scorep-gcc \
     CXX=scorep-g++ \
     FC=scorep--ftn \
     --disable-dependency-tracking",4.380914270565565
"How can I use Score-P with Autotools?
","$ scorep --user gcc -c test.c
$ scorep --user gcc -o test test.o

Now you can manually instrument Score-P to the source code as seen below:

C,C++

.. code::

   #include <scorep/SCOREP_User.h>

   void foo() {
      SCOREP_USER_REGION_DEFINE(my_region)
      SCOREP_USER_REGION_BEGIN(my_region, ""foo"", SCOREP_USER_REGION_TYPE_COMMON)
      // do something
      SCOREP_USER_REGION_END(my_region)
   }

Fortran

.. code::

   #include <scorep/SCOREP_User.inc>",4.233420896666657
"How can I use Score-P with Autotools?
",".PHONY: clean

clean:
   rm -f test *.o

For CMake and Autotools based build systems, it is recommended to use the scorep-wrapper script instances. The intended usage of the wrapper instances is to replace the application's compiler and linker with the corresponding wrapper at configuration time so that they will be used at build time. As the Score-P instrumentation during the CMake or configure steps is likely to fail, the wrapper script allows for disabling the instrumentation by setting the variable SCOREP_WRAPPER=off.",4.223229964810923
"How do I ensure that my MPI program uses the correct number of physical CPU cores on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.429370898465665
"How do I ensure that my MPI program uses the correct number of physical CPU cores on Frontier?
","Because a Frontier compute node has two hardware threads available (2 logical cores per physical core), this enables the possibility of multithreading your application (e.g., with OpenMP threads). Although the additional hardware threads can be assigned to additional MPI tasks, this is not recommended. It is highly recommended to only use 1 MPI task per physical core and to use OpenMP threads instead on any additional logical cores gained when using both hardware threads.",4.424511415548042
"How do I ensure that my MPI program uses the correct number of physical CPU cores on Frontier?
","As has been pointed out previously in the Frontier documentation, notice that GPUs are NOT mapped to MPI ranks in sequential order (e.g., MPI rank 0 is mapped to physical CPU cores 1-7 and GPU 4, MPI rank 1 is mapped to physical CPU cores 9-15 and GPU 5), but this IS expected behavior. It is simply a consequence of the Frontier node architectures as shown in the Frontier Node Diagram and subsequent https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Note on NUMA domains <numa-note>.

Example 2: 1 MPI rank with 7 CPU cores and 1 GPU (single-node)",4.349497802877379
"How can I use the new environment with CuPy on a new login or in a job?
","Before setting up your environment, you must exit and log back in so that you have a fresh login shell. This is to ensure that no previously activated environments exist in your $PATH environment variable. Additionally, you should execute module reset.

Building CuPy from source is highly sensitive to the current environment variables set in your profile. Because of this, it is extremely important that all the modules and environments you plan to load are done in the correct order, so that all the environment variables are set correctly.",4.361529414403328
"How can I use the new environment with CuPy on a new login or in a job?
","If you wish to use CuPy within a clone of the OpenCE environment, the installation process is very similar to what we do in the regular CuPy installation we saw above.

The open-ce/1.2.0-pyXY-0 (which is the current default) will not support this. So make sure you are using open-ce/1.5.0-pyXY-0 or higher.

The contents of the open-ce module cannot be modified so you need to make your own clone of the open-ce environment.",4.239263088475978
"How can I use the new environment with CuPy on a new login or in a job?
","$ module purge
$ module load DefApps
$ module unload xl
$ module load open-ce/1.5.2-py39-0
$ conda create --clone open-ce-1.5.2-py39-0 -p /ccs/proj/<project_id>/<user_id>/envs/summit/opence_cupy_summit
$ conda activate /ccs/proj/<project_id>/<user_id>/envs/summit/opence_cupy_summit

Next, install CuPy the way you did before. This installation will use the system GCC /usr/bin/gcc which is currently 8.3.1.

$ CC=gcc NVCC=nvcc pip install --no-binary=cupy cupy

Now, everytime you want to use this environment with CuPy on a new login or in a job, you will have to do the sequence of the following",4.228901767625136
"How much memory space is required for the trace of the matvec_sub function on each process?
","In addition to the trace, Score-P requires some additional memory to maintain internal data structures. Thus, it provides also an estimation for the total amount of required memory on each process. The memory size per process that Score-P reserves is set via the environment variable SCOREP_TOTAL_MEMORY. In the given example the per process memory is about 10GB. When defining a filter, it is recommended to exclude short, frequently called functions from measurement since they require a lot of buffer space (represented by a high value under max_tbc) but incur a high measurement overhead. MPI",4.267907516538067
"How much memory space is required for the trace of the matvec_sub function on each process?
","The first line of the output gives an estimation of the total size of the trace, aggregated over all processes. This information is useful for estimating the space required on disk. In the given example, the estimated total size of the event trace is 40GB. The second line prints an estimation of the memory space required by a single process for the trace. Since flushes heavily disturb measurements, the memory space that Score-P reserves on each process at application start must be large enough to hold the process’ trace in memory in order to avoid flushes during runtime.",4.25831445742808
"How much memory space is required for the trace of the matvec_sub function on each process?
","USR   3,421,305,420    522,844,416   144.46     13.4            0.28  matmul_sub
     USR   3,421,305,420    522,844,416   102.40      9.5            0.20  matvec_sub",4.164110334628077
"How can I ensure that my batch job runs on a specific node in Andes?
","Batch Queues on Andes

The compute nodes on Andes are separated into two partitions the ""batch partition"" and the ""GPU partition"" as described in the https://docs.olcf.ornl.gov/systems/your_file.html#andes-compute-nodes section. The scheduling policies for the individual partitions are as follows:

Batch Partition Policy (default)

Jobs that do not specify a partition will run in the 704 node batch partition:",4.37820763278915
"How can I ensure that my batch job runs on a specific node in Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.341593795400314
"How can I ensure that my batch job runs on a specific node in Andes?
","The compute nodes on Andes are separated into two partitions the ""batch partition"" and the ""GPU partition"" as described in the https://docs.olcf.ornl.gov/systems/andes_user_guide.html#andes-compute-nodes section. The scheduling policies for the individual partitions are as follows:

Jobs that do not specify a partition will run in the 704 node batch partition:",4.3275327205581045
"How can I save account credentials for a specific provider using Qiskit?
","# Load default account credentials
    provider = IBMProvider()

    # Print instances (different hub/group/project options)
    print( provider.instances() )

    # Load a specific hub/group/project.
    #provider = IBMProvider(instance=""ibm-q-ornl/ornl/csc431"")

    # Print available backends
    print( provider.backends() )

    ######################################

    backend = QasmSimulator() #works with backend.run()

    circuit = QuantumCircuit(2, 2)
    circuit.h(0)
    circuit.cx(0, 1)
    circuit.measure([0,1], [0,1])
    compiled_circuit = transpile(circuit, backend)",4.134803148552912
"How can I save account credentials for a specific provider using Qiskit?
","#QiskitRuntimeService.save_account(channel=""ibm_quantum"", token=""API TOKEN GOES HERE"", overwrite=True)
    service = QiskitRuntimeService(channel=""ibm_quantum"", instance=""ibm-q-ornl/ornl/csc431"")

    backend = service.backend(""ibmq_qasm_simulator"", instance=""ibm-q-ornl/ornl/csc431"") #does not work with backend.run()

    circuit = QuantumCircuit(2, 2)
    circuit.h(0)
    circuit.cx(0, 1)
    circuit.measure([0,1], [0,1])
    compiled_circuit = transpile(circuit, backend)

    sampl = Sampler(backend)
    job = sampl.run(compiled_circuit,shots=1000)",4.103185540382766
"How can I save account credentials for a specific provider using Qiskit?
","Below is a simple code example to test if things installed properly.  Note that methods for either using Qiskit Runtime or IBMProvider are provided.

Your IBMQ API Token is listed on your IBM dashboard at https://quantum-computing.ibm.com/ .

IBMProvider

.. code-block:: python

    import numpy as np
    from qiskit import QuantumCircuit, transpile
    from qiskit.providers.aer import QasmSimulator
    from qiskit_ibm_provider import IBMProvider

    #### IF YOU HAVE AN IBMQ ACCOUNT (using an actual backend) #####

    # Save account credentials
    #IBMProvider.save_account(TOKEN)",4.102353007985747
"How do I update a module in Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.154104832151689
"How do I update a module in Andes?
",For Andes:,4.101564464030575
"How do I update a module in Andes?
","Changing compilers

If a different compiler is required, it is important to use the correct environment for each compiler. To aid users in pairing the correct compiler and environment, the module system on andes automatically pulls in libraries compiled with a given compiler when changing compilers. The compiler modules will load the correct pairing of compiler version, message passing libraries, and other items required to build and run code. To change the default loaded intel environment to the gcc environment for example, use:

$ module load gcc",4.089191403781253
"Can I run VisIt's GUI client from an OLCF machine?
","VisIt uses a client-server architecture. You will obtain the best performance by running the VisIt client on your local computer and running the server on OLCF resources. VisIt for your local computer can be obtained here: VisIt Installation.

Recommended VisIt versions on our systems:

Summit: VisIt 3.1.4

Andes: VisIt 3.3.3

Frontier: VisIt 3.3.3

Using a different version than what is listed above is not guaranteed to work properly.",4.461592732283183
"Can I run VisIt's GUI client from an OLCF machine?
","Using VisIt via the command line should always result in a batch job, and should always be executed on a compute node -- never the login or launch nodes.

Although most users find better performance following the approach outlined in https://docs.olcf.ornl.gov/systems/visit.html#visit-remote-gui, some users that don't require a GUI may find better performance using VisIt's CLI in a batch job. An example for doing this on OLCF systems is provided below.

For Summit (module):",4.39173754447159
"Can I run VisIt's GUI client from an OLCF machine?
","Click ""Apply""

At the top menu click on ""Options""→""Save Settings""



Once you have VisIt installed and set up on your local computer:

Open VisIt on your local computer.

Go to: ""File→Open file"" or click the ""Open"" button on the GUI.

Click the ""Host"" dropdown menu on the ""File open"" window that popped up and choose ""ORNL_Andes"".

This will prompt you for your OLCF password, and connect you to Andes.

Navigate to the appropriate file.",4.350748999160531
"How do I concretize and install a Spack environment?
","Spack - package management tool

Spack 101 tutorial - Spack tutorial",4.329328344755553
"How do I concretize and install a Spack environment?
","Definitions  Spack environment - A set of Spack specs for the purpose of building, rebuilding and deploying in a coherent fashion.  External Packages - An externally-installed package used by Spack, rather than building its own package.

Clone the OLCF User Environment repo and the Spack repo, start a new Spack instance, and create and activate a new Spack environment:

## Using Summit as the example system

$ git clone https://github.com/olcf/spack-environments.git
$ cd spack-environments

$ git clone https://github.com/spack/spack.git
$ source spack/share/spack/setup-env.sh",4.273503097451702
"How do I concretize and install a Spack environment?
","Traditionally, the user environment is modified by module files.  For example, a user would add use  module load cmake/3.18.2 to load CMake version 3.18.2 into their environment.  Using a Spack environment, a user can add an OLCF provided package and build against it using Spack without having to load the module file separately.

The information presented here is a subset of what can be found at the Spack documentation site.",4.2594638933037405
"How do I install CuPy?
","If you wish to use CuPy within a clone of the OpenCE environment, the installation process is very similar to what we do in the regular CuPy installation we saw above.

The open-ce/1.2.0-pyXY-0 (which is the current default) will not support this. So make sure you are using open-ce/1.5.0-pyXY-0 or higher.

The contents of the open-ce module cannot be modified so you need to make your own clone of the open-ce environment.",4.379685506416839
"How do I install CuPy?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.

This guide teaches you how to build CuPy from source into a custom virtual environment. On Summit and Andes, this is done using conda, while on Frontier this is done using venv.

In this guide, you will:

Learn how to install CuPy

Learn the basics of CuPy

Compare speeds to NumPy

OLCF Systems this guide applies to:

Summit

Frontier

Andes",4.375947799934725
"How do I install CuPy?
","Compute nodes equipped with NVIDIA GPUs will be able to take full advantage of CuPy's capabilities on the system, providing significant speedups over NumPy-written code. CuPy with AMD GPUs is still being explored, and the same performance is not guaranteed (especially with larger data sizes). Instructions for Frontier are available in this guide, but users must note that the CuPy developers have labeled this method as experimental and has limitations.



<string>:60: (INFO/1) Duplicate implicit target name: ""installing cupy"".",4.319060833147576
"How do I get started with the Rigetti Quantum Computing environment?
","Rigetti provides system access via a cloud-based JupyterLab development environment: https://docs.rigetti.com/qcs/getting-started/jupyterlab-ide.  From there, users can access a JupyterLab server loaded with Rigetti's PyQuil programming framework, Rigetti's Forest Software Development Kit, and associated program examples and tutorials.  This is the method that allows access to Rigetti's QPU's directly, as opposed to simulators.",4.432583808366461
"How do I get started with the Rigetti Quantum Computing environment?
","Users are able to install Rigetti software locally for the purpose of development using a provided Quantum Virtual Machine, or QVM, an implementation of a quantum computer simulator that can run Rigetti's Quil programs.  This can be done via two methods:

Installing manually: https://docs.rigetti.com/qcs/getting-started/installing-locally

Docker: https://hub.docker.com/r/rigetti/forest",4.420737824888637
"How do I get started with the Rigetti Quantum Computing environment?
","accessing the hybrid infrastructure of available quantum processors and classical computational framework via the cloud. From the QCS, users can view system status and availability, initiate and manage quantum infrastructure reservations (either executing programs manually or adding them to the queue). Information on using this resource is available on the Rigetti's Documentation or our https://docs.olcf.ornl.gov/quantum/quantum_systems/rigetti.html.",4.393267308862947
"What are the requirements for being a mentor?
","If you would like more information about how you can volunteer to mentor a team at an upcoming Open/GPU hackathon, please visit our Become a Mentor page.

<p style=""font-size:20px""><b>Who can I contact with questions?</b></p>

If you have any questions about the OLCF GPU Hackathon Series, please contact OLCF Help at (help@olcf.ornl.gov).",4.116621805743707
"What are the requirements for being a mentor?
","Typically, these hackathons are in-person events, where each team (app developers + mentors) sits at their own round table in a single large conference room. This structure allows teams to hack away on their own codes, but also to interact (ask questions, give advice, etc.) with members/mentors from other teams when needed.",3.9740901072730455
"What are the requirements for being a mentor?
","First, you must decide which event you'd like to attend (use link below to find a hackathon whose dates make sense for your team), and then submit a short proposal form describing your application and team. The organizing committee will then review all proposals after the call for that event closes and select the teams they believe are best suited for the event.",3.972837626284437
"How can I get more detailed information about a job on Summit?
",For Summit:,4.26516853542417
"How can I get more detailed information about a job on Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.256686048500763
"How can I get more detailed information about a job on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.221548658396138
"How can I run a Dask scheduler in the background?
","SCHEDULER_FILE=$SCHEDULER_DIR/my-scheduler.json

echo 'Running scheduler'
jsrun --nrs 1 --tasks_per_rs 1 --cpu_per_rs 2 --smpiargs=""-disable_gpu_hooks"" \
      dask-scheduler --interface ib0 --scheduler-file $SCHEDULER_FILE \
                     --no-dashboard --no-show &

#Wait for the dask-scheduler to start
sleep 10",4.228313793387393
"How can I run a Dask scheduler in the background?
","SCHEDULER_FILE=$SCHEDULER_DIR/my-scheduler.json

echo 'Running scheduler'
jsrun --nrs 1 --tasks_per_rs 1 --cpu_per_rs 1 --smpiargs=""-disable_gpu_hooks"" \
      dask-scheduler --interface ib0 \
                     --scheduler-file $SCHEDULER_FILE \
                     --no-dashboard --no-show &

#Wait for the dask-scheduler to start
sleep 10",4.223961439078511
"How can I run a Dask scheduler in the background?
","#Wait for the dask-scheduler to start
sleep 10

jsrun --rs_per_host 6 --tasks_per_rs 1 --cpu_per_rs 2 --gpu_per_rs 1 --smpiargs=""-disable_gpu_hooks"" \
      dask-cuda-worker --nthreads 1 --memory-limit 82GB --device-memory-limit 16GB --rmm-pool-size=15GB \
                       --interface ib0 --scheduler-file $SCHEDULER_FILE --local-directory $WORKER_DIR \
                       --no-dashboard &

#Wait for WORKERS
sleep 10

WORKERS=12

python -u $CONDA_PREFIX/examples/dask-cuda/verify_dask_cuda_cluster.py $SCHEDULER_FILE $WORKERS

wait

#clean DASK files
rm -fr $SCHEDULER_DIR

echo ""Done!""",4.177596994661284
"How can I run the Singularity build step interactively?
","Users can build container images with Podman, convert it to a SIF file, and run the container using the Singularity runtime. This page is intended for users with some familiarity with building and running containers.

Users will make use of two applications on Summit - Podman and Singularity - in their container build and run workflow. Both are available without needing to load any modules.",4.200640463822874
"How can I run the Singularity build step interactively?
","You can now create a Singularity sif file with singularity build --disable-cache --docker-login simple.sif docker://docker.io/<username>/simple.

This will ask you to enter your Docker username and password again for Singularity to download the image from Dockerhub and convert it to a sif file.",4.152270620514543
"How can I run the Singularity build step interactively?
","h41n08
h41n08

Here, Jsrun starts 2 separate Singularity container runtimes since we pass the -n2 flag to start two processes. Each Singularity container runtime then loads the container image simple.sif and executes the hostname command from that container. If we had requested 2 nodes in the batch script and had run jsrun -n2 -r1 singularity exec ./simple.sif hostname, Jsrun would've started a Singularity runtime on each node and the output would look something like

h41n08
h41n09



Creating Singularity containers that run MPI programs require a few additional steps.",4.143575109984196
"How does the Volta architecture improve performance?
","Volta GPUs improve MPS with new capabilities. For instance, each Volta MPS client (MPI rank) is assigned a ""subcontext"" that has its own GPU address space, instead of sharing the address space with other clients. This isolation helps protect MPI ranks from out-of-range reads/writes performed by other ranks within CUDA kernels. Because each subcontext manages its own GPU resources, it can submit work directly to the GPU without the need to first pass through the MPS server. In addition, Volta GPUs support up to 48 MPS clients (up from 16 MPS clients on Pascal).",4.231028821330931
"How does the Volta architecture improve performance?
","For more information on the NVIDIA Volta architecture, please visit the following (outside) links.

NVIDIA Volta Architecture White Paper

NVIDIA PARALLEL FORALL blog article",4.2184298082101614
"How does the Volta architecture improve performance?
","The V100 supports independent thread scheduling, which allows threads to synchronize and cooperate at sub-warp scales. Pre-Volta GPUs implemented warps (groups of 32 threads which execute instructions in single-instruction, multiple thread - SIMT - mode) with a single call stack and program counter for a warp as a whole.",4.212166870563514
"Who is the intended audience for the Director's Discretion Review Form?
","Request a Director's Discretionary (DD) Project Use this form to request a Director's Discretionary (DD) Project. Select ""OLCF Director's Discretionary Program"" from the drop down menu.

Principal Investigator Agreement The Oak Ridge Leadership Computing Facility must have a signed copy of this form on file from the project's principal investigator(s) before any accounts for the project will be processed.",4.103372600915652
"Who is the intended audience for the Director's Discretion Review Form?
","DD – Director’s Discretion (DD) projects are dedicated to leadership computing preparation, INCITE and ALCC scaling, and application performance to maximize scientific application efficiency and productivity on leadership computing platforms. The OLCF Resource Utilization Council, as well as independent referees, review and approve all DD requests. Applications are accepted year-round via the OLCF Director's Discretion Project Application. Select ""OLCF Director's Discretionary Project"" from the drop down menu to begin.",4.100327867014593
"Who is the intended audience for the Director's Discretion Review Form?
","Increased disk quota

Purge exemption for User/Group/World Work areas

Software requests



Closeout Report Template Use this template if you have been asked to submit a closeout report for your project.  Note this form does not apply to INCITE projects.  If you have been provided a template via email, only that template applies.

Industry Quarterly Report Template Use this template if you have an industry project to submit a quarterly report.



Director's Discretion Review Form For internal use only.",4.089507728588143
"Is the project shared directory read-only?
","UNIX Permissions on each project-centric work storage area differ according to the area’s intended collaborative use. Under this setup, the process of sharing data with other researchers amounts to simply ensuring that the data resides in the proper work directory.

Member Work Directory: 700

Project Work Directory: 770

World Work Directory: 775",4.175164601494818
"Is the project shared directory read-only?
","To check your project’s current usage, run df -h /ccs/proj/abc123 (where abc123 is your project ID). Quotas are enforced on project home directories. The current limit is shown in the table above.

The default permissions for project home directories are 0770 (full access to the user and group). The directory is owned by root and the group includes the project’s group members. All members of a project should also be members of that group-specific project. For example, all members of project “ABC123” should be members of the “abc123” UNIX group.",4.153624165075501
"Is the project shared directory read-only?
",The difference between the three lies in the accessibility of the data to project members and to researchers outside of the project. Member Work directories are accessible only by an individual project member by default. Project Work directories are accessible by all project members. World Work directories are readable by any user on the system.,4.149061586476685
"What should I do after deleting a host profile in VisIt?
","Another solution is to delete all copies of a host profile (including the original) and remake them. This can be achieved with the ""Delete Host"" button in the Host Profiles window. Make sure to save your settings after deleting the profiles, exit and restart VisIt, and then proceed with remaking your profiles.

If none of the above solutions work for you, the final option would be to delete the duplicate host profile entirely and just modify the settings of the original when needed.",4.550290703884635
"What should I do after deleting a host profile in VisIt?
","One solution is to change the host nickname of the duplicate host profile to start with ""Copy of"".  For example, if my original host profile was named ""ORNL Andes"", a proper duplicate should be named ""Copy of ORNL Andes"" (this is the same nickname that would be generated when clicking the ""Copy Host"" button in VisIt). After renaming, make sure to save your settings via ""Options/Save Settings"" then close and restart VisIt.",4.284785875869196
"What should I do after deleting a host profile in VisIt?
","If the above does not apply to you, double check that you set up your host profile exactly as how it is outlined in the https://docs.olcf.ornl.gov/systems/visit.html#visit-host-profiles section. It may be helpful to delete and remake your host profile, but just remember to always save your settings via ""Options/Save Settings"".

If VisIt keeps asking for your ""Password"" in the dialog box below, and you are entering your correct PIN + RSA token code, you might need to select ""Change username"" and then enter your OLCF username when prompted.",4.267983157475058
"What is the default deployment strategy on Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.329152746171009
"What is the default deployment strategy on Slate?
","Edit Deployment Config



A deployment strategy is a method for upgrading an application. The goal of deployment strategies is to make an update with no downtime to the end users.

The two most common values here will be RollingUpdate and Recreate. The default is RollingUpdate.

Since the end user usually will be accessing an application with a route, the deployment strategy can focus on deployment configuration features. Here are a few examples of the deployment configuration based strategies.",4.226647448000643
"What is the default deployment strategy on Slate?
","Blue-green deployments are defined as running two versions of an application at the same time, then moving traffic from the old production version (the green version) to the new production version (the blue version). You could use a Rolling Deployment Strategy for this, but for the sake of showing how route-based deployments work, we'll use a route.",4.107453132348978
"How can I run Dask workers with Nvidia Rapids?
","Running RAPIDS multi-gpu/multi-node workloads requires a dask-cuda cluster. Setting up a dask-cuda cluster on Summit requires two components:

dask-scheduler.

dask-cuda-workers.

Once the dask-cluster is running, the RAPIDS script should perform four main tasks. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

Reference of multi-gpu/multi-node operation with cuDF, cuML, cuGraph is available in the next links:

10 Minutes to cuDF and Dask-cuDF.

cuML's Multi-Node, Multi-GPU Algorithms.",4.3818891395064
"How can I run Dask workers with Nvidia Rapids?
","Large workloads.

Long runtimes on Summit's high memory nodes.

Your Python script has support for multi-gpu/multi-node execution via dask-cuda.

Your Python script is single GPU but requires simultaneous job steps.

RAPIDS is provided in Jupyter following  these instructions.

Note that Python scripts prepared on Jupyter can be deployed on Summit if they use the same RAPIDS version. Use !jupyter nbconvert --to script my_notebook.ipynb to convert notebook files to Python scripts.

RAPIDS is provided on Summit through the module load command:",4.253687976911799
"How can I run Dask workers with Nvidia Rapids?
","As mentioned earlier, the RAPIDS code should perform four main tasks as shown in the following script. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

import sys
from dask.distributed import Client

def disconnect(client, workers_list):
    client.retire_workers(workers_list, close_workers=True)
    client.shutdown()

if __name__ == '__main__':

    sched_file = str(sys.argv[1]) #scheduler file
    num_workers = int(sys.argv[2]) # number of workers to wait for",4.232196707268388
"Can I use the ""oc port-forward"" command to access a service or deployment in a different namespace in Slate?
","Additionally, oc port-forward doesn't have to be given a pod name. This tool is aware of services and deployments as well. If you had a service called nginx-svc and a deployment called nginx, for example, the following commands would achieve the same result:

oc port-forward deployment/nginx 7777:8080
oc port-forward svc/nginx-svc 7777:8080

You will be forwarded to any of the pods matched by the service or deployment.

Furthermore, this doesn't only work for http traffic. You could also access other exposed services such as databases.",4.391988679427666
"Can I use the ""oc port-forward"" command to access a service or deployment in a different namespace in Slate?
","Both the web UI and the API endpoint for the oc client are exposed outside of ORNL. However, you must log in with NCCS USERNAME AND PASSWORD rather than NCCS Single Sign On on the Web UI.

For production workloads, it is recommended to learn about https://docs.olcf.ornl.gov/systems/port_forwarding.html#services <slate_services> and https://docs.olcf.ornl.gov/systems/port_forwarding.html#routes <slate_routes> in order to gain access to your internal resources.

However, for testing and development, oc port-forward can be a powerful tool for quick access to internal cluster resources.",4.384070534252341
"Can I use the ""oc port-forward"" command to access a service or deployment in a different namespace in Slate?
","For example, let's look at service that was created in the https://docs.olcf.ornl.gov/systems/nodeport.html#slate_services document. This document assumes that it was deployed to the my-project project.

If you run oc get services, you should see your service in the list.

$ oc get services
NAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)     AGE
my-service            ClusterIP   172.25.170.246   <none>        8080/TCP    8s

Then, get some information about the service with oc describe service my-service.",4.328704569312541
"How can I specify the job name for my batch job?
","by a shell name. For example, to request an interactive batch job (with bash as the shell) equivalent to the sample batch script above, you would use the command: bsub -W 3:00 -nnodes 2048 -P ABC123 -Is /bin/bash",4.254715748837181
"How can I specify the job name for my batch job?
","Most users will find batch jobs an easy way to use the system, as they allow you to ""hand off"" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.",4.226382617330033
"How can I specify the job name for my batch job?
","Most users will find batch jobs to be the easiest way to interact with the system, since they permit you to hand off a job to the scheduler and then work on other tasks; however, it is sometimes preferable to run interactively on the system. This is especially true when developing, modifying, or debugging a code.",4.20950697396209
"How can I format the tick labels on the colorbar in Paraview?
","# Set Colorbar Properties
display.SetScalarBarVisibility(curr_view,True) # Show bar
scalarBar = GetScalarBar(cmap, curr_view)      # Get bar's properties
scalarBar.WindowLocation = 'Any Location'       # Allows free movement
scalarBar.Orientation = 'Horizontal'           # Switch from Vertical to Horizontal
scalarBar.Position = [0.15,0.80]               # Bar Position in [x,y]
scalarBar.LabelFormat = '%.0f'                 # Format of tick labels
scalarBar.RangeLabelFormat = '%.0f'            # Format of min/max tick labels
scalarBar.ScalarBarLength = 0.7                # Set length of bar",4.017078235753968
"How can I format the tick labels on the colorbar in Paraview?
","After setting up and installing ParaView, you can connect to OLCF systems remotely to visualize your data interactively through ParaView's GUI. To do so, go to File→Connect and select either ORNL Andes or ORNL Summit (provided they were successfully imported -- as outlined in https://docs.olcf.ornl.gov/systems/paraview.html#paraview-install-setup). Next, click on Connect and change the values in the Connection Options box.",3.970465153147986
"How can I format the tick labels on the colorbar in Paraview?
","ParaView is an open-source, multi-platform data analysis and visualization application. ParaView users can quickly build visualizations to analyze their data using qualitative and quantitative techniques. The data exploration can be done interactively in 3D or programmatically using ParaView’s batch processing capabilities. Further information regarding ParaView can be found at the links provided in the https://docs.olcf.ornl.gov/systems/paraview.html#paraview-resources section.",3.946354023985851
"What types of events can I measure with Score-P on Summit?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi-processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations.

For detailed information about using Score-P on Summit and the builds available, please see the Score-P Software Page.",4.3758430009572615
"What types of events can I measure with Score-P on Summit?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations. It works in combination with Periscope, Scalasca, Vampir, and Tau.

Steps in a typical Score-P workflow to run on https://docs.olcf.ornl.gov/systems/Scorep.html#summit-user-guide:",4.364462130229277
"What types of events can I measure with Score-P on Summit?
","The regions ""sum"" and ""my_calculations"" in the above examples would then be included in the profiling and tracing runs and can be analysed with Vampir. For more details, refer to the Advanced Score-P training in the https://docs.olcf.ornl.gov/systems/Scorep.html#training-archive.

Please see the provided video below to watch a brief demo of using Score-P provided by TU-Dresden and presented by Ronny Brendel.",4.209997352020071
"Where can I find the Summit User Guide?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.614908126833345
"Where can I find the Summit User Guide?
",For Summit:,4.319509558570732
"Where can I find the Summit User Guide?
",This section of the Summit User Guide is intended to show current OLCF users how to start preparing their applications to run on the upcoming Frontier system. We will continue to add more topics to this section in the coming months. Please see the topics below to get started.,4.2472876144236125
"How can I access the Summit User Guide for more information on programming environment, compiling, and running batch jobs?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.442553295451804
"How can I access the Summit User Guide for more information on programming environment, compiling, and running batch jobs?
","More information on the Programming Environment, Compiling, and Running Batch Jobs can be found in the https://docs.olcf.ornl.gov/systems/citadel_user_guide.html#Summit User Guide<summit-documentation-resources>.

For notable differences between Citadel and Summit, please see the https://docs.olcf.ornl.gov/systems/citadel_user_guide.html#SPI<spi-compute-citadel> documentation.

The login node used by Citadel mirrors the Summit login nodes in hardare and software.  The login node also provides access to the same compute resources as are accessible from Summit.",4.374782354285692
"How can I access the Summit User Guide for more information on programming environment, compiling, and running batch jobs?
","Summit has a node hierarchy that can be very confusing for the uninitiated. The Summit User Guide explains this in depth. This has some consequences that may be unusual for R programmers. A few important ones to note are:

You must have a script that you can run in batch, e.g. with Rscript or R CMD BATCH

All data that needs to be visible to the R process (including the script and your packages) must be on gpfs (not your home directory!).

You must launch your script from the launch nodes with jsrun.

We'll start with something very simple. The script is:

""hello world""",4.327583547027011
"Can I use the same compiler wrappers on Frontier as I did on Summit?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers. Please see the following https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more detailed inforation on how to compile using these modules.",4.2539251822921065
"Can I use the same compiler wrappers on Frontier as I did on Summit?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers.

It is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) whenever possible. See the next section for more details.",4.239133658337216
"Can I use the same compiler wrappers on Frontier as I did on Summit?
",Programming Environment  Frontier utilizes the Cray Programming Environment.  Many aspects including LMOD are similar to Summit's environment.  But Cray's compiler wrappers and the Cray and AMD compilers are worth noting.  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more information.  AMD GPUs  Each frontier node has 4 AMD MI250X accelerators with two Graphic Compute Dies (GCDs) in each accelerator. The system identifies each GCD as an independent device (so for simplicity we use the term GPU when we talk about a GCD) for a total of 8,4.193252942005893
"Can I use the deep learning module on Summit for machine learning tasks other than deep learning?
",| | 2020-02-10 | NCCL on Summit | Sylvain Jeaugey (NVIDIA) | Scaling Up Deep Learning Applications on Summit https://www.olcf.ornl.gov/calendar/scaling-up-deep-learning-applications-on-summit/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/12/Summit-NCCL.pdf https://vimeo.com/391520479 | | 2020-02-10 | Introduction to Watson Machine Learning CE | Brad Nemanich & Bryant Nelson (IBM) | Scaling Up Deep Learning Applications on Summit https://www.olcf.ornl.gov/calendar/scaling-up-deep-learning-applications-on-summit/ | (slides | recording),4.257334773812369
"Can I use the deep learning module on Summit for machine learning tasks other than deep learning?
","MLflow is an open source platform to manage the Machine Learning (ML) lifecycle, including experimentation, reproducibility, deployment, and a central model registry. To learn more about MLflow, please refer to its documentation.

In order to use MLflow on Summit, load the module as shown below:

$ module load workflows
$ module load mlflow/1.22.0

Run the following command to verify that MLflow is available:

$ mlflow --version
mlflow, version 1.22.0

To run this MLflow demo on Summit, you will create a directory with two files and then submit a batch job to LSF from a Summit login node.",4.239293200638387
"Can I use the deep learning module on Summit for machine learning tasks other than deep learning?
",| (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_20190211_summit_workshop_python.pdf https://vimeo.com/346452419 | | 2019-02-11 | Practical Tips for Running on Summit | David Appelhans (IBM) | Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_GettingStartedExamples_169ratio.pdf https://vimeo.com/346452176 | | 2018-12-06 | ML/DL Frameworks on Summit | Junqi Yin (OLCF) | Summit Training Workshop,4.181508893505538
"What is the purpose of the ""noCache"" field in the Docker strategy?
","The reason we include the --disable-cache flag is because Singularity's caching can fill up your home directory without you realizing it. And if the home directory is full, Singularity builds will fail. If you wish to make use of the cache, you can set the environment variable SINGULARITY_CACHEDIR=/tmp/containers/<user>/singularitycache or something like that so that the NVMe storage is used as the cache.

As a simple example, we will run hostname with the Singularity container.

Create a file submit.lsf with the contents below.",3.9336707855702535
"What is the purpose of the ""noCache"" field in the Docker strategy?
","Edit Deployment Config



A deployment strategy is a method for upgrading an application. The goal of deployment strategies is to make an update with no downtime to the end users.

The two most common values here will be RollingUpdate and Recreate. The default is RollingUpdate.

Since the end user usually will be accessing an application with a route, the deployment strategy can focus on deployment configuration features. Here are a few examples of the deployment configuration based strategies.",3.849848099435412
"What is the purpose of the ""noCache"" field in the Docker strategy?
","The --restart=Never tells oc run to generate only a Pod spec. By default it would generate something more advanced we will talk about later.

The openshift/hello-openshift is just a simple Docker image

List current running pods:

oc get pods

Get more information on our pod:

oc describe pod hello-world-pod

Once we see Status: Running (near the top of the output, not the bottom) we can interact with the container by first setting up port forwarding:

oc port-forward hello-world-pod 8080:8080",3.8484752731142255
"What is the total number of MPI tasks that will be created across all resource sets?
","The following example will create 12 resource sets each with 1 task, 4 threads, and 1 GPU. Each MPI task will start 4 threads and have access to 1 GPU. Rank 0 will have access to GPU 0 and start 4 threads on the first socket of the first node ( red resource set). Rank 2 will have access to GPU 1 and start 4 threads on the second socket of the first node ( green resource set). This pattern will continue until 12 resource sets have been created. The following jsrun command will create 12 resource sets (-n12). Each resource set will contain 1 MPI task (-a1), 1 GPU (-g1), and 4 cores (-c4).",4.433868992891588
"What is the total number of MPI tasks that will be created across all resource sets?
","summit>

The following example will create 4 resource sets each with 6 tasks and 3 GPUs. Each set of 6 MPI tasks will have access to 3 GPUs. Ranks 0 - 5 will have access to GPUs 0 - 2 on the first socket of the first node ( red resource set). Ranks 6 - 11 will have access to GPUs 3 - 5 on the second socket of the first node ( green resource set). This pattern will continue until 4 resource sets have been created. The following jsrun command will request 4 resource sets (-n4). Each resource set will contain 6 MPI tasks (-a6), 3 GPUs (-g3), and 6 cores (-c6).",4.407579638887087
"What is the total number of MPI tasks that will be created across all resource sets?
","The following jsrun command will request 12 resource sets (-n12). Each resource set will contain 2 MPI tasks (-a2), 1 GPU (-g1), and 2 cores (-c2). 2 MPI tasks will have access to a single GPU. Ranks 0 - 1 will have access to GPU 0 on the first node ( red resource set). Ranks 2 - 3 will have access to GPU 1 on the first node ( green resource set). This pattern will continue until 12 resource sets have been created.",4.378288040947262
"How much storage capacity does Summit have access to through its connection to the IBM Spectrum Scale™ filesystem?
","Summit is connected to an IBM Spectrum Scale™ filesystem providing 250PB of storage capacity with a peak write speed of 2.5 TB/s. Summit also has access to the center-wide NFS-based filesystem (which provides user and project home areas) and has access to the center’s High Performance Storage System (HPSS) for user and project archival storage.

Summit is running Red Hat Enterprise Linux (RHEL) version 8.2.",4.665886895425574
"How much storage capacity does Summit have access to through its connection to the IBM Spectrum Scale™ filesystem?
","Summit mounts a POSIX-based IBM Spectrum Scale parallel filesystem called Alpine. Alpine's maximum capacity is 250 PB. It is consisted of 77 IBM Elastic Storage Server (ESS) GL4 nodes running IBM Spectrum Scale 5.x which are called Network Shared Disk (NSD) servers. Each IBM ESS GL4 node, is a scalable storage unit (SSU), constituted by two dual-socket IBM POWER9 storage servers, and a 4X EDR InfiniBand network for up to 100Gbit/sec of networking bandwidth.  The maximum performance of the final production system will be about 2.5 TB/s for sequential I/O and 2.2 TB/s for random I/O under FPP",4.546305828033317
"How much storage capacity does Summit have access to through its connection to the IBM Spectrum Scale™ filesystem?
",Spock is connected to an IBM Spectrum Scale™ filesystem providing 250 PB of storage capacity with a peak write speed of 2.5 TB/s. Spock also has access to the center-wide NFS-based filesystem (which provides user and project home areas). While Spock does not have direct access to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the https://docs.olcf.ornl.gov/systems/spock_quick_start_guide.html#dtn-user-guide to move data to/from HPSS.,4.407688142583786
"How can I include all libraries when using SBCAST?
","Notice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable. Once libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.

As mentioned above, you can alternatively use --exclude=NONE on sbcast to send all libraries along with the binary. Using --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time. A job script for the previous example, modified for sending all libraries is shown below.",4.2695117501691415
"How can I include all libraries when using SBCAST?
","Notice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable. Once libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.

As mentioned above, you can use --exclude=NONE on sbcast to send all libraries along with the binary. Using --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time. A job script for the previous example, modified for sending all libraries is shown below.",4.265448935409453
"How can I include all libraries when using SBCAST?
","# If you SBCAST **all** your libraries (ie, `--exclude=NONE`), you may use the following line:
#export LD_LIBRARY_PATH=""/mnt/bb/$USER/${exe}_libs:$(pkg-config --variable=libdir libfabric)""
# Use with caution -- certain libraries may use ``dlopen`` at runtime, and that is NOT covered by sbcast
# If you use this option, we recommend you contact OLCF Help Desk for the latest list of additional steps required",4.208737368676902
"What is the purpose of the -t flag in the VisIt command?
","the -s flag will launch the CLI in the form of a Python shell, which can be useful for interactive batch jobs.  The -np and -nn flags represent the number of processors and nodes VisIt will use to execute the Python script, while the -l flag specifies the specific parallel method to do so (required). Execute visit -fullhelp to get a list of all command line options.",4.242313292323152
"What is the purpose of the -t flag in the VisIt command?
","For Summit (module):

$ module load visit
$ visit -nowin -cli -v 3.1.4 -l bsub/jsrun -p batch -b XXXYYY -t 00:05 -np 42 -nn 1 -s visit_example.py

Due to the nature of the custom VisIt launcher for Summit, users are unable to solely specify -l jsrun for VisIt to work properly. Instead of manually creating a batch script, as seen in the Andes method outlined below, VisIt submits its own through -l bsub/jsrun. The -t flag sets the time limit, -b specifies the project to be charged, and -p designates the queue the job will submit to.",4.130993832743584
"What is the purpose of the -t flag in the VisIt command?
","VisIt is now available on Frontier through the UMS022 module.

VisIt 3.3.3 is now available on Andes.

VisIt is an interactive, parallel analysis and visualization tool for scientific data. VisIt contains a rich set of visualization features so you can view your data in a variety of ways. It can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes. Further information regarding VisIt can be found at the links provided in the https://docs.olcf.ornl.gov/systems/visit.html#visit-resources section.",4.062895511096104
"Can I use the tracing library with other debugging tools on Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.136940049084681
"Can I use the tracing library with other debugging tools on Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.130385983534251
"Can I use the tracing library with other debugging tools on Summit?
","Small trace files can be viewed locally on your machine if you have the Vampir client downloaded, otherwise they can be viewed locally on Summit. For large trace files, it is strongly recommended to run vampirserver reverse-connected to a local copy of the Vampir client. See the https://docs.olcf.ornl.gov/systems/Scorep.html#vamptunnel section for more details.

In addition to automatically profiling and tracing functions, there is also a way to manually instrument a specific region in the source code. To do this, you will need to add the --user flag to the scorep command when compiling:",4.11345351729997
"How can you optimize the performance of your application in Frontier?
",This section of the Summit User Guide is intended to show current OLCF users how to start preparing their applications to run on the upcoming Frontier system. We will continue to add more topics to this section in the coming months. Please see the topics below to get started.,4.262423408610861
"How can you optimize the performance of your application in Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.218830942882138
"How can you optimize the performance of your application in Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.1919736887161125
"What is the cause of the ""Scalable Render Request Failed (VisItException)"" error message?
","Some users have encountered their compute engine exiting abnormally on Andes after VisIt reaches 100% when drawing a plot, resulting in a ""Scalable Render Request Failed (VisItException)"" error message. This message has also been reported when users try to save plots, if VisIt was successfully able to draw. The error seems to more commonly occur for users that are trying to visualize large datasets.",4.485237099672984
"What is the cause of the ""Scalable Render Request Failed (VisItException)"" error message?
","VisIt developers have been notified, and at this time the current workaround is to disable Scalable Rendering from being used. To do this, go to Options→Rendering→Advanced and set the ""Use Scalable Rendering"" option to ""Never"".

However, this workaround has been reported to affect VisIt's ability to save images, as scalable rendering is utilized to save plots as image files (which can result in another compute engine crash). To avoid this, screen capture must be enabled. Go to File→""Set save options"" and check the box labeled ""Screen capture"".",4.254156407718001
"What is the cause of the ""Scalable Render Request Failed (VisItException)"" error message?
","Using VisIt on Summit is also an option, as the scalable rendering problem is currently not an issue on Summit (as of Sept. 2021).

As of February 2022, this issue on Andes has been fixed (must use VisIt 3.2.2 or higher).",4.114473514018905
"What are the system requirements for running the GitLab Runner Kubernetes Executor on Slate?
","GitLab CI/CD jobs may be accomplished using the GitLab Runner application. An open-source application written in Go, a GitLab Runner may be installed with a variety of executors. This documentation focuses on the installation and configuration for use of the GitLab Runner Kubernetes Executor on Slate.

https://docs.gitlab.com/runner/

https://docs.gitlab.com/runner/executors/kubernetes.html

https://docs.gitlab.com/runner/install/kubernetes.html",4.46766974866034
"What are the system requirements for running the GitLab Runner Kubernetes Executor on Slate?
","Jenkins

OpenShift Pipelines

GitLab Runners

Deploying a GitLab Runner into a Slate project may be found in the https://docs.olcf.ornl.gov/systems/overview.html#slate_gitlab_runners document which leverages a localized Slate Helm Chart. The GitLab Pipelines documentation as well as GitLab CI/CD Examples provide more details on GitLab Runner capabilities and usage.

Jenkins

For Jenkins deployment, a Slate Helm Chart is planned to be released. Documentation concerning Jenkins Pipelines as well as Jenkins Pipeline Tutorials are available.

OpenShift Pipelines",4.368002108366476
"What are the system requirements for running the GitLab Runner Kubernetes Executor on Slate?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",4.302781555096852
"What is the average MPI-IO Write Bandwidth (MB/s) for the profile?
","604    1.6E+05    1.6E+05    1.6E+05          0  MPI-IO Bytes Written : int main(int, char **) => void output(double *, double) => MPI_File_write_at_all()
      1058       9412     0.1818       3311       3816  MPI-IO Write Bandwidth (MB/s)
       454      1.856     0.1818     0.5083     0.1904  MPI-IO Write Bandwidth (MB/s) : int main(int, char **) => void output(double *, double) => MPI_File_write_at()
       604       9412      2.034       5799       3329  MPI-IO Write Bandwidth (MB/s) : int main(int, char **) => void output(double *, double) => MPI_File_write_at_all()",4.338837596316044
"What is the average MPI-IO Write Bandwidth (MB/s) for the profile?
","USER EVENTS Profile :NODE 0, CONTEXT 0, THREAD 0
---------------------------------------------------------------------------------------
NumSamples   MaxValue   MinValue  MeanValue  Std. Dev.  Event Name
---------------------------------------------------------------------------------------
      1058    1.6E+05          4  9.134E+04  7.919E+04  MPI-IO Bytes Written
       454        284          4      5.947       13.2  MPI-IO Bytes Written : int main(int, char **) => void output(double *, double) => MPI_File_write_at()",4.198538769688087
"What is the average MPI-IO Write Bandwidth (MB/s) for the profile?
","I/O and 2.2 TB/s for random I/O under FPP mode, which means each process, writes its own file. Metada operations are improved with around to minimum 50,000 file access per sec and aggregated up to 2.6 million accesses of 32KB small files.",4.175162896825886
"How do I navigate to the ""swift-work"" directory in the terminal?
","The suggested directory structure is to have a outer directory say swift-work that has the swift source and shell scripts. Inside of swift-work create a new directory called data.

Additionally, we will need two terminals open. In the first terminal window, navigate to the swift-work directory and invoke the data generation script like so:

$ ./gendata.sh

In the second terminal, we will run the swift workflow as follows (make sure to change the project name per your allocation):",4.46223894829359
"How do I navigate to the ""swift-work"" directory in the terminal?
","$ module load imagemagick # for convert utility
$ export WALLTIME=00:10:00
$ export PROJECT=STF019
$ export TURBINE_OUTPUT=/gpfs/alpine/scratch/ketan2/stf019/swift-work/cross-facility/data
$ swift-t -O0 -m lsf workflow.swift

If all goes well, and when the job starts running, the output will be produced in the data directory output.txt file.",4.178827102830155
"How do I navigate to the ""swift-work"" directory in the terminal?
","$ export PROJECT=""ABC123""

To run an example ""Hello world"" program with Swift/T on Summit, create a file called hello.swift with the following contents:

trace(""Hello world!"");

Now, run the program from a shell or script:

$ swift-t -m lsf hello.swift

The output should look something like the following:",4.148371057986492
"What is the format of the directory created by TAU?
","$ module show tau
---------------------------------------------------------------
   /sw/summit/modulefiles/core/tau/2.28.1:
---------------------------------------------------------------
whatis(""TAU 2.28.1 github "")
setenv(""TAU_DIR"",""/sw/summit/tau/tau2/ibm64linux"")
prepend_path(""PATH"",""/sw/summit/tau/tau2/ibm64linux/bin"")
help([[https://www.olcf.ornl.gov/software_package/tau
]])

The available Makefiles are named per-compiler and are located in:",4.134402806107562
"What is the format of the directory created by TAU?
","Below, we'll look at using TAU to profile each case.

Edit the cmake_summit_pgi.sh and replace mpic++ with tau_cxx.sh. This applies only for the non-GPU versions.

TAU works with special TAU makefiles to declare what programming models are expected from the application:

The available makefiles are located inside TAU installation:",4.065633129674056
"What is the format of the directory created by TAU?
","A similar approach for other metrics, not all of them can be used. TAU provides a tool called tau_cupti_avail, where we can see the list of available metrics, then we have to figure out which CUPTI metrics use these ones.

Activate tracing and declare the data format to OTF2. OTF2 format is supported only by MPI and OpenSHMEM applications.

$ export TAU_TRACE=1
$ export TAU_TRACE_FORMAT=otf2

Use Vampir for visualization.

For example, do not instrument routine sort*(int *)

Create a file select.tau

BEGIN_EXCLUDE_LIST
void sort_#(int *)
END_EXCLUDE_LIST

Declare the TAU_OPTIONS variable",4.065314053118637
"What is the name of the shared object that provides the drm_amdgpu functionality on Frontier?
","The Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.174851452379947
"What is the name of the shared object that provides the drm_amdgpu functionality on Frontier?
","module load craype-accel-amd-gfx90a

export MPICH_GPU_SUPPORT_ENABLED=1

If using PrgEnv-cray:

module load craype-accel-amd-gfx90a
module load amd-mixed

export MPICH_GPU_SUPPORT_ENABLED=1

There are extra steps needed to enable GPU-aware MPI on Frontier, which depend on the compiler that is used (see 1. and 2. below).

When using GPU-aware Cray MPICH with the Cray compiler wrappers, most of the needed libraries are automatically linked through the environment variables.

Though, the following header files and libraries must be included explicitly:",4.135489963302884
"What is the name of the shared object that provides the drm_amdgpu functionality on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.095772574436926
"How long are the GPU hackathons?
","A GPU hackathon is a multi-day coding event in which teams of developers port their own applications to run on GPUs, or optimize their applications that already run on GPUs. Each team consists of 3 or more developers who are intimately familiar with (some part of) their application, and they work alongside 1 or more mentors with GPU programming expertise. Our mentors come from universities, national laboratories, supercomputing centers, and industry partners.",4.524483902388491
"How long are the GPU hackathons?
","If you want/need to get your code running (or optimized) on a GPU-accelerated system, these hackathons offer a unique opportunity to set aside the time, surround yourself with experts in the field, and push toward your development goals. By the end of the event, each team should have part of their code running (or more optimized) on GPUs, or at least have a clear roadmap of how to get there.

<p style=""font-size:20px""><b>Target audience</b></p>",4.489163513211976
"How long are the GPU hackathons?
","We are looking for teams of 3-6 developers with a scalable** application to port (or optimize on) GPUs. Collectively, the team should know the application intimately.

** We say scalable here because we're typically looking for codes intended to run on multiple nodes (e.g. MPI-enabled), although porting/optimizing such codes on a single node during the events is encouraged whenever possible.

<p style=""font-size:20px""><b>Virtual hackathon format</b></p>",4.336536884341486
"How can I format the min/max tick labels on the colorbar in Paraview?
","# Set Colorbar Properties
display.SetScalarBarVisibility(curr_view,True) # Show bar
scalarBar = GetScalarBar(cmap, curr_view)      # Get bar's properties
scalarBar.WindowLocation = 'Any Location'       # Allows free movement
scalarBar.Orientation = 'Horizontal'           # Switch from Vertical to Horizontal
scalarBar.Position = [0.15,0.80]               # Bar Position in [x,y]
scalarBar.LabelFormat = '%.0f'                 # Format of tick labels
scalarBar.RangeLabelFormat = '%.0f'            # Format of min/max tick labels
scalarBar.ScalarBarLength = 0.7                # Set length of bar",3.995014827424684
"How can I format the min/max tick labels on the colorbar in Paraview?
","After setting up and installing ParaView, you can connect to OLCF systems remotely to visualize your data interactively through ParaView's GUI. To do so, go to File→Connect and select either ORNL Andes or ORNL Summit (provided they were successfully imported -- as outlined in https://docs.olcf.ornl.gov/systems/paraview.html#paraview-install-setup). Next, click on Connect and change the values in the Connection Options box.",3.908675293828532
"How can I format the min/max tick labels on the colorbar in Paraview?
","The second window that is automatically loaded shows the TIME metric for each process (they are called ""nodes"") where each color is a different call. Each horizontal line is a process or Std.Dev./mean/max/min. The length of each color is related to the metric, if it is TIME, it is duration.



Select Options -> Uncheck Stack Bars Together

It is easier to check the load imbalance across the processes



If you click on any color, then a new window opens with information about the specific routine.",3.904706001244573
"Can I use CuPy with Python scripts on Summit?
","Summit

Frontier

Andes

GPU computing has become a big part of the data science landscape, as array operations with NVIDIA GPUs can provide considerable speedups over CPU computing. Although GPU computing on Summit is often utilized in codes that are written in Fortran and C, GPU-related Python packages are quickly becoming popular in the data science community. One of these packages is CuPy, a NumPy/SciPy-compatible array library accelerated with NVIDIA CUDA.",4.360976231081433
"Can I use CuPy with Python scripts on Summit?
",of how to run a Python script using PvBatch on Andes and Summit.,4.325349934955954
"Can I use CuPy with Python scripts on Summit?
","$ export CUPY_CACHE_DIR=""/gpfs/alpine/scratch/<YOUR_USER_ID>/<YOUR_PROJECT_ID>/.cupy/kernel_cache""

Before you start testing CuPy with Python scripts, let's go over some of the basics. The developers provide a great introduction to using CuPy in their user guide under the CuPy Basics section. We will be following this walkthrough on Summit. The syntax below assumes being in a Python shell with access to 4 GPUs (through a jsrun -g4 ... command).

On Frontier, running in an interactive job will return 8 GPUs available to CuPy.",4.313753276066966
"What is the purpose of the X-Forwarded-Proto header?
","Host: nginx-echo-headers-stf002platform.bedrock-dev.ccs.ornl.gov
X-Remote-User: kincljc
X-Forwarded-Host: nginx-echo-headers-stf002platform.bedrock-dev.ccs.ornl.gov
X-Forwarded-Port: 443
X-Forwarded-Proto: https
Forwarded: for=160.91.195.36;host=nginx-echo-headers-stf002platform.bedrock-dev.ccs.ornl.gov;proto=https;proto-version=
X-Forwarded-For: 160.91.195.36

Routes are secured by adding the annotation ccs.ornl.gov/requireAuth = ""true"" to the route.

If you have an application that should not require authentication reach out to NCCS Support.",4.053996143108875
"What is the purpose of the X-Forwarded-Proto header?
","The authenticated user must be on the project in order to use the application running in OpenShift

The authentication will be handled by the cluster load balancers so that nothing is required by a user application. If a user application needs to authenticate a user we set the X-Remote-User header which is the NCCS username of the authenticated user.

An example list of headers that are set by the loadbalancer:",3.880845265830336
"What is the purpose of the X-Forwarded-Proto header?
","3. Prior to your project being initialized, we must have source IP addresses for devices in your organization that are authorized to transfer sensitive/controlled information to/from your organization, or for use in accessing that information. Encryption is necessary for transferring sensitive and/or controlled information to and from the OLCF.",3.867674226412732
"How does Frontier handle memory fragmentation?
","The AMD MI250X and operating system on Frontier supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Frontier. In the following section we'll discuss how the heterogenous memory space on a Frontier node is surfaced within your application.",4.270731433915295
"How does Frontier handle memory fragmentation?
","Most applications that use ""managed"" or ""unified"" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Frontier. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.",4.249826255596098
"How does Frontier handle memory fragmentation?
","The Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.23846844489878
"How do I specify the job name in a batch script?
","by a shell name. For example, to request an interactive batch job (with bash as the shell) equivalent to the sample batch script above, you would use the command: bsub -W 3:00 -nnodes 2048 -P ABC123 -Is /bin/bash",4.334619369579425
"How do I specify the job name in a batch script?
","The most common way to interact with the batch system is via batch scripts. A batch script is simply a shell script with added directives to request various resoruces from or provide certain information to the scheduling system.  Aside from these directives, the batch script is simply the series of commands needed to set up and run your job.

To submit a batch script, use the command sbatch myjob.sl

Consider the following batch script:

#!/bin/bash
#SBATCH -A ABC123
#SBATCH -J RunSim123
#SBATCH -o %x-%j.out
#SBATCH -t 1:00:00
#SBATCH -p batch
#SBATCH -N 1024",4.269191606871843
"How do I specify the job name in a batch script?
","to request an interactive batch job with the same resources that the batch script above requests, you would use salloc -A ABC123 -J RunSim123 -t 1:00:00 -p batch -N 1024. Note there is no option for an output file...you are running interactively, so standard output and standard error will be displayed to the terminal.",4.256213267906972
"How do I push a Slate build to a registry?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",4.106255522955479
"How do I push a Slate build to a registry?
","Then you can push and pull from the integrated registry. In the following example we will pull busybox:latest from Docker Hub and push it to our namespace in the integrate registry.

$ docker pull busybox:latest
latest: Pulling from library/busybox
ee153a04d683: Pull complete
Digest: sha256:9f1003c480699be56815db0f8146ad2e22efea85129b5b5983d0e0fb52d9ab70
Status: Downloaded newer image for busybox:latest
docker.io/library/busybox:latest

$ docker tag busybox:latest registry.apps.marble.ccs.ornl.gov/stf002platform/busybox:latest",4.0775724770064254
"How do I push a Slate build to a registry?
","Now, find the repository and tag information of the local image you want to add to the registry and tag it accordingly.

$ docker images
REPOSITORY                                TAG                 IMAGE ID            CREATED             SIZE
example:5000/streams                      v3.1.4              fd7673fdbe30        3 weeks ago         1.95GB

The command to tag your image is:

docker tag example:5000/streams:v3.1.4 registry.apps.<cluster>.ccs.ornl.gov/<namespace>/<image>:<tag>

Lastly, the image needs to be pushed to the registry.",4.0553070433871055
"How can I update my Docker image in the registry?
","Now, find the repository and tag information of the local image you want to add to the registry and tag it accordingly.

$ docker images
REPOSITORY                                TAG                 IMAGE ID            CREATED             SIZE
example:5000/streams                      v3.1.4              fd7673fdbe30        3 weeks ago         1.95GB

The command to tag your image is:

docker tag example:5000/streams:v3.1.4 registry.apps.<cluster>.ccs.ornl.gov/<namespace>/<image>:<tag>

Lastly, the image needs to be pushed to the registry.",4.351458319925058
"How can I update my Docker image in the registry?
","Then you can push and pull from the integrated registry. In the following example we will pull busybox:latest from Docker Hub and push it to our namespace in the integrate registry.

$ docker pull busybox:latest
latest: Pulling from library/busybox
ee153a04d683: Pull complete
Digest: sha256:9f1003c480699be56815db0f8146ad2e22efea85129b5b5983d0e0fb52d9ab70
Status: Downloaded newer image for busybox:latest
docker.io/library/busybox:latest

$ docker tag busybox:latest registry.apps.marble.ccs.ornl.gov/stf002platform/busybox:latest",4.253369728932158
"How can I update my Docker image in the registry?
","docker push registry.apps.<cluster>.ccs.ornl.gov/<namespace>/<image>:<tag>

OpenShift has an integrated container registry that can be accessed from outside the cluster to push and pull images as well as run containers.

This assumes that you have Docker installed locally. Installing Docker is outside of the scope of this documentation.

First you have to log into OpenShift

oc login https://api.<cluster>.ccs.ornl.gov

Next you can use your token to log into the integrated registry.

docker login -u user -p $(oc whoami -t) registry.apps.<cluster>.ccs.ornl.gov",4.231537441464809
"Can users combine the -munsafe-fp-atomics flag with other flags or allocators to further optimize performance?
","In ROCm-5.1 and earlier versions, the flag -munsafe-fp-atomics is interpreted as a suggestion by the compiler, whereas from ROCm-5.2 the flag will always enforce the use of fast hardware-based FP atomics.

The following tables summarize the result granularity of various combinations of allocators, flags and arguments.

For hipHostMalloc(), the following table shows the nature of the memory returned based on the flag passed as argument.",4.520652991238002
"Can users combine the -munsafe-fp-atomics flag with other flags or allocators to further optimize performance?
","In ROCm-5.1 and earlier versions, the flag -munsafe-fp-atomics is interpreted as a suggestion by the compiler, whereas from ROCm-5.2 the flag will always enforce the use of fast hardware-based FP atomics.

The following tables summarize the result granularity of various combinations of allocators, flags and arguments.

For hipHostMalloc(), the following table shows the nature of the memory returned based on the flag passed as argument.",4.520652991238002
"Can users combine the -munsafe-fp-atomics flag with other flags or allocators to further optimize performance?
","Users applying floating point atomic operations (e.g., atomicAdd) on memory regions allocated via regular hipMalloc() can safely apply the -munsafe-fp-atomics flags to their codes to get the best possible performance and leverage hardware supported floating point atomics. Atomic operations supported in hardware on non-FP datatypes  (e.g., INT32) will work correctly regardless of the nature of the memory region used.",4.513148853960364
"Is there a way to view more usage details with showusage?
","Each user may view usage for projects on which they are members from the command line tool showusage and the myOLCF site.

The showusage utility can be used to view your usage from January 01 through midnight of the previous day. For example:

$ showusage
  Usage:
                           Project Totals
  Project             Allocation      Usage      Remaining     Usage
  _________________|______________|___________|____________|______________
  abc123           |  20000       |   126.3   |  19873.7   |   1560.80

The -h option will list more usage details.",4.376026430712679
"Is there a way to view more usage details with showusage?
","Each user may view usage for projects on which they are members from the command line tool showusage and the myOLCF site.

The showusage utility can be used to view your usage from January 01 through midnight of the previous day. For example:

$ showusage
  Usage:
                           Project Totals
  Project             Allocation      Usage      Remaining     Usage
  _________________|______________|___________|____________|______________
  abc123           |  20000       |   126.3   |  19873.7   |   1560.80

The -h option will list more usage details.",4.376026430712679
"Is there a way to view more usage details with showusage?
","Each user may view usage for projects on which they are members from the command line tool showusage and the myOLCF site.

On the Command Line via showusage

The showusage utility can be used to view your usage from January 01 through midnight of the previous day. For example:

$ showusage
  Usage:
                           Project Totals
  Project             Allocation      Usage      Remaining     Usage
  _________________|______________|___________|____________|______________
  abc123           |  20000       |   126.3   |  19873.7   |   1560.80",4.335001851393146
"Is it possible to purge files stored in User Home, Moderate Enhanced Projects?
","Open and Moderate Projects are provided with a Project Home storage area in the NFS-mounted filesystem. This area is intended for storage of data, code, and other files that are of interest to all members of a project. Since Project Home is an NFS-mounted filesystem, its performance will not be as high as other filesystems.

Moderate Enhanced projects are not provided with Project Home spaces, just Project Work spaces.

Project Home area is accessible at /ccs/proj/abc123 (where abc123 is your project ID).",4.269901934209945
"Is it possible to purge files stored in User Home, Moderate Enhanced Projects?
","and files are automatically purged on a regular basis. Files should not be retained in this file system for long, but rather should be migrated to Project Home or Project Archive space as soon as the files are not actively being used. If a file system associated with your Member Work directory is nearing capacity, the OLCF may contact you to request that you reduce the size of your Member Work directory. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for more details on applicable quotas, backups, purge, and retention timeframes.",4.240271334839261
"Is it possible to purge files stored in User Home, Moderate Enhanced Projects?
","footnotes

1

This entry is for legacy User Archive directories which contained user data on January 14, 2020. There is also a quota/limit of 2,000 files on this directory.

2

User Archive directories that were created (or had no user data) after January 14, 2020. Settings other than permissions are not applicable because directories are root-owned and contain no user files.

Moderate Enhanced projects do not have HPSS storage",4.208692708367918
"Can I view the logs for a pod that has been deleted in Slate?
","You should now be on the 'Pod' screen with the 'Overview' tab selected From here you can get a quick idea of the amount of resources (memory, CPU etc) that your  Pod is using.

Click on the 'Logs' tab to get the logs from your pod. This will display ""Hello World!"" in our example because of our echo command. There will be a dropdown here that for our example will contain only one item named 'hello-openshift'. This is the name of the container that you are viewing the logs for inside your pod.",4.178798264983209
"Can I view the logs for a pod that has been deleted in Slate?
","oc delete pod hello-world-pod

Deleting the pod will remove our ability to inspect the log output from oc logs so if you are debugging an issue you will want to keep the pod until the issue is resolved.

To create a single pod using the web console we will create from YAML

First, in the upper right-hand corner, click the + symbol. This can be used to add any YAML object from the web UI.

Add to Project

Make sure the project in the upper left-hand dropdown is set to the project in which you wish to deploy. Then paste this YAML into the box.",4.131835632259524
"Can I view the logs for a pod that has been deleted in Slate?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.07877464997856
"What is the difference between the number of threads and the number of cores in Summit?
","Rank: 11; RankCore: 120; Thread: 0; ThreadCore: 120; Hostname: a33n05; OMP_NUM_PLACES: {120},{124},{128},{132}
Rank: 11; RankCore: 120; Thread: 1; ThreadCore: 124; Hostname: a33n05; OMP_NUM_PLACES: {120},{124},{128},{132}
Rank: 11; RankCore: 120; Thread: 2; ThreadCore: 128; Hostname: a33n05; OMP_NUM_PLACES: {120},{124},{128},{132}
Rank: 11; RankCore: 120; Thread: 3; ThreadCore: 132; Hostname: a33n05; OMP_NUM_PLACES: {120},{124},{128},{132}

summit>",4.226933833378942
"What is the difference between the number of threads and the number of cores in Summit?
","Hardware threads are a feature of the POWER9 processor through which individual physical cores can support multiple execution streams, essentially looking like one or more virtual cores (similar to hyperthreading on some Intel      microprocessors). This feature is often called Simultaneous Multithreading or SMT. The POWER9 processor on Summit supports SMT levels of 1, 2, or 4, meaning (respectively) each physical core looks like 1, 2, or 4 virtual cores. The SMT level is controlled by the -alloc_flags option to bsub. For example, to set the SMT level to 2, add the line #BSUB –alloc_flags",4.211250151486002
"What is the difference between the number of threads and the number of cores in Summit?
","This section provides some of the most commonly used LSF commands as well as some of the most useful options to those commands and information on jsrun, Summit's job launch command. Many commands have much more information than can be easily presented here. More information about these commands is available via the online manual (i.e. man jsrun). Additional LSF information can be found on IBM’s website.

Each physical core on Summit contains 4 hardware threads. The SMT level can be set using LSF flags (the default is smt4):

SMT1",4.183274262142124
"Can users use OLCF computing resources for personal gain or profit?
",The Oak Ridge Leadership Computing Facility (OLCF) computing resources are provided to users for research purposes. All users must agree to abide by all security measures described in this document. Failure to comply with security procedures will result in termination of access to OLCF computing resources and possible legal actions.,4.487939722031213
"Can users use OLCF computing resources for personal gain or profit?
","Computers, software, and communications systems provided by the OLCF are to be used for work associated with and within the scope of the approved project. The use of OLCF resources for personal or non-work-related activities is prohibited. All computers, networks, E-mail, and storage systems are property of the United States Government. Any misuse or unauthorized access is prohibited, and is subject to criminal and civil penalties. OLCF systems are provided to our users without any warranty. OLCF will not be held liable in the event of any system failure or data loss or corruption for any",4.463974553531708
"Can users use OLCF computing resources for personal gain or profit?
","The OLCF provides a comprehensive suite of hardware and software resources for the creation, manipulation, and retention of scientific data. This document comprises guidelines for acceptable use of those resources. It is an official policy of the OLCF, and as such, must be agreed to by relevant parties as a condition of access to and use of OLCF computational resources.",4.43463833462811
"How do I optimize my WMMA kernel for the Tesla V100 Tensor Cores?
",section provides information for using the V100 Tensor Cores.,4.279422025549379
"How do I optimize my WMMA kernel for the Tesla V100 Tensor Cores?
","For more information, please see the following section of NVIDIA's CUDA Programming Guide: http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#independent-thread-scheduling-7-x

The Tesla V100 contains 640 tensor cores (8 per SM) intended to enable faster training of large neural networks. Each tensor core performs a D = AB + C operation on 4x4 matrices. A and B are FP16 matrices, while C and D can be either FP16 or FP32:",4.247047879616344
"How do I optimize my WMMA kernel for the Tesla V100 Tensor Cores?
","The Warp Matrix Multiply and Accumulate (WMMA) API was introduced in CUDA 9 explicitly for programming the Tesla V100 Tensor Cores. This is a low-level API that supports loading matrix data into fragments within the threads of a warp, applying a Tensor Core multiplication on that data, and then restoring it to the main GPU memory. This API is called within CUDA kernels and all WMMA operations are warp-synchronous, meaning the threads in a warp will leave the operation synchronously. Examples are available for using the WMMA instructions in C++ and CUDA Fortran. The image below demonstrates",4.238480174078503
"What is the name of the queue that the job is submitted to?
","There are special queue names when submitting jobs to citadel.ccs.ornl.gov (the Moderate Enhanced version of Summit). These queues are: batch-spi, batch-hm-spi, and debug-spi.  For example, to submit a job to the batch-spi queue on Citadel, you would need -q batch-spi when using the bsub command or #BSUB -q batch-spi when using a job script.

Except for the enhanced security policies for jobs in these queues, all other queue properties are the same as the respective Summit queues described above, such as maximum walltime and number of eligible running jobs.",4.219259726777377
"What is the name of the queue that the job is submitted to?
","Users may have only 100 jobs queued in the batch queue at any time (this includes jobs in all states). Additional jobs will be rejected at submit time.

The debug quality of service (QOS) class can be used to access Frontier's compute resources for short non-production debug tasks. The QOS provides a higher priority compare to jobs of the same job size bin in production queues. Production work and job chaining using the debug QOS is prohibited. Each user is limited to one job in any state at any one point. Attempts to submit multiple jobs to this QOS will be rejected upon job submission.",4.183092516250137
"What is the name of the queue that the job is submitted to?
","User can submit jobs to IBM Quantum backends both via a fair-sharing queue system as well as via priority reservation system.  As discussed below, the dynamic fair-sharing queue system determines the queuing order of jobs so as to fairly balance system time between access providers, of which the OLCF QCUP is only one.  Because of this, the order of when a user's job in the fair-share queue will run varies dynamically, and can't be predicted. In light of this, for time-critical applications or iterative algorithms, IBM Quantum recommends users consider making a priority reservation.",4.137817326404749
"How do I ensure that my code is compatible with the ROCm Toolchain on Frontier?
","hipcc requires the ROCm Toolclain, See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#exposing-the-rocm-toolchain-to-your-programming-environment





Computational work on Frontier is performed by jobs. Jobs typically consist of several componenets:

A batch submission script

A binary executable

A set of input files for the executable

A set of output files created by the executable

In general, the process for running a job is to:

Prepare executables and input files.

Write a batch script.

Submit the batch script to the batch scheduler.",4.364895343841793
"How do I ensure that my code is compatible with the ROCm Toolchain on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.290801501647209
"How do I ensure that my code is compatible with the ROCm Toolchain on Frontier?
","The compatibility table below was determined by linker testing with all current combinations of cray-mpich and ROCm-related modules on Frontier.

| cray-mpich | ROCm | | --- | --- | | 8.1.17 | 5.4.0, 5.3.0, 5.2.0, 5.1.0 | | 8.1.23 | 5.4.0, 5.3.0, 5.2.0, 5.1.0 |

This section shows how to compile with OpenMP using the different compilers covered above.

| Vendor | Module | Language | Compiler | OpenMP flag (CPU thread) | | --- | --- | --- | --- | --- | | Cray | cce | C, C++ |  | -fopenmp | | Fortran | ftn (wraps crayftn) |  | | AMD | amd |  |  | -fopenmp | | GCC | gcc |  |  | -fopenmp |",4.2480940164756005
"Where are the secret values picked up from?
","These values are picked up as environment variables from the templates/minio-standalone-deployment.yaml file.

It is recommended to keep the secret-token.yaml file safe, locally, and not in a repository if unencrypted.

At this point we are ready to install our minio-standalone chart in our Marble project namespace.

To list your available project spaces run this command:

$ oc projects

Check list:

You have the OC CLI Tool

You have Helm version 3

You are logged into Marble, with the OC CLI Tool, and in the correct Marble project.

You have configured your values.yaml file.",3.965700131154976
"Where are the secret values picked up from?
","The regions ""sum"" and ""my_calculations"" in the above examples would then be included in the profiling and tracing runs and can be analysed with Vampir. For more details, refer to the Advanced Score-P training in the https://docs.olcf.ornl.gov/systems/Scorep.html#training-archive.

Please see the provided video below to watch a brief demo of using Score-P provided by TU-Dresden and presented by Ronny Brendel.",3.9309031929145095
"Where are the secret values picked up from?
","If you click on the label (node 0, node 1, max, etc.), you can see the value across each routine in your application.



Right click on the label (node 0, node 1, max, etc.), and then select ""Show Context Event Window"" (with callpath activated). We can then see various calls from where they were executed, how many times, and other various information.



Select Options -> Show Derived Metric Panel, choose the metrics and then the operator that you want, then click Apply. Uncheck the Show Derived Metric.",3.8818476168344858
"Can you provide more information on the available options for the MPICH_OFI_NIC_POLICY environment variable in Frontier?
","If a user attempts to map a process to a set of cores that span more than 1 NUMA domain using the default NIC mapping, they will see an error such as MPICH ERROR: Unable to use a NIC_POLICY of 'NUMA'. Rank 0 is not confined to a single NUMA node.. This is expected behavior for the default NIC policy.

The default behavior can be changed by using the MPICH_OFI_NIC_POLICY environment variable (see man mpi for available options).",4.161752017109379
"Can you provide more information on the available options for the MPICH_OFI_NIC_POLICY environment variable in Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.040384059866168
"Can you provide more information on the available options for the MPICH_OFI_NIC_POLICY environment variable in Frontier?
","For more detailed information about center-wide file systems and data archiving available on Frontier, please refer to the pages on https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers. The subsections below give a quick overview of NFS, Lustre,and HPSS storage spaces as well as the on node NVMe ""Burst Buffers"" (SSDs).",4.025135109941989
"What is the recommended way to use the BurstBuffers on Summit?
","Each compute node on Summit has a 1.6TB Non-Volatile Memory (NVMe) storage device (high-memory nodes have a 6.4TB NVMe storage device), colloquially known as a ""Burst Buffer"" with theoretical performance peak of 2.1 GB/s for writing and 5.5 GB/s for reading. The NVMes could be used to reduce the time that applications wait for I/O.  More information can be found later in the Burst Buffer section.



<string>:208: (INFO/1) Duplicate implicit target name: ""software"".",4.191668170570497
"What is the recommended way to use the BurstBuffers on Summit?
",NVMes will need to be copied back to the parallel filesystem before the job ends. This largely manual mode of usage will not be the recommended way to use the burst buffer for most applications because tools are actively being developed to automate and improve the NVMe transfer and data management process. Here are the basic steps for using the BurstBuffers in their current limited mode of usage:,4.175445619920616
"What is the recommended way to use the BurstBuffers on Summit?
","The following example illustrates how to use the burst buffers (NVMes) by default on Summit. This example uses a submission script, check_nvme.lsf. It is assumed that the files are saved in the user's GPFS scratch area, /gpfs/alpine/scratch/$USER/projid, and that the user is operating from there as well. Do not forget that for all the commands on NVMe, it is required to use jsrun. This will submit a job to run on one node.

Job submssion script: check_nvme.lsf.

#!/bin/bash
#BSUB -P project123
#BSUB -J name_test
#BSUB -o nvme_test.o%J
#BSUB -W 2
#BSUB -nnodes 1
#BSUB -alloc_flags NVME",4.12124933150607
"How long does a build typically take to complete in Slate?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",4.052013665819871
"How long does a build typically take to complete in Slate?
","This section will describe the project allocation process for Slate. This is applicable to both the Onyx and Marble OLCF OpenShift clusters.

In addition, we will go over the process to log into Onyx and Marble, and how namespaces work within the OpenShift context.

Please fill out the Slate Request Form in order to use Slate resources. This must be done in order to proceed.

If you have any question please contact User Assistance, via a Help Ticket Submission or by emailing help@olcf.ornl.gov.

Required information:

- Existing OLCF Project ID

- Project PI",4.043932095768906
"How long does a build typically take to complete in Slate?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",4.009607234985135
"How can I forward a local port to a pod in the Deployment using the oc command?
","This tool will forward a local port on your system to a pod inside the cluster.

For example, if you have an nginx deployment running on port 8080 inside the container, you can view this nginx instance locally by running:

oc port-forward ${pod_name} 7777:8080

The first port is the local port you want forwarded, and the second port is the port exposed by the pod. After running this command, you can go into your browser (or use curl in a second terminal) and connect to http://localhost:7777.",4.630028981407291
"How can I forward a local port to a pod in the Deployment using the oc command?
","Additionally, oc port-forward doesn't have to be given a pod name. This tool is aware of services and deployments as well. If you had a service called nginx-svc and a deployment called nginx, for example, the following commands would achieve the same result:

oc port-forward deployment/nginx 7777:8080
oc port-forward svc/nginx-svc 7777:8080

You will be forwarded to any of the pods matched by the service or deployment.

Furthermore, this doesn't only work for http traffic. You could also access other exposed services such as databases.",4.493935393223874
"How can I forward a local port to a pod in the Deployment using the oc command?
","oc port-forward hello-world-pod 8080:8080

Then, since oc port-forward stays in the foreground, we run curl http://localhost:8080 in a second terminal.

The initial port in the port pair references a non-allocated port on our local system similar to how SSH port forwarding works.

Pods also have logs. And we can see the logs for the pod: (whatever was printed to stdout from within the container, not kubernetes).

oc logs hello-world-pod

Now lets delete our pod:

oc delete pod hello-world-pod",4.46761911012509
"What is the maximum number of processes that can run on a single compute node in the CAAR and ECP ""batch"" partition?
","Spock's compute nodes are separated into 2 Slurm partitions (queues): 1 for CAAR projects and 1 for ECP projects. Please see the tables below for details.

If CAAR or ECP teams require a temporary exception to this policy, please email help@olcf.ornl.gov with your request and it will be given to the OLCF Resource Utilization Council (RUC) for review.

The CAAR partition consists of 24 total compute nodes. On a per-project basis, each user can have 1 running and 1 eligible job at a time, with no limit on the number of jobs submitted.",4.423148545749227
"What is the maximum number of processes that can run on a single compute node in the CAAR and ECP ""batch"" partition?
","| sinfo |  | | --- | --- | | squeue |  | | sacct |  | | scancel |  | | scontrol |  |



Crusher's compute nodes are contained within a single Slurm partition (queue) for both CAAR and ECP projects. Please see the table below for details.

The CAAR and ECP ""batch"" partition consists of 192 total compute nodes. On a per-project basis, each user can have 2 running and 2 eligible jobs at a time, with up to 20 jobs submitted.

| Number of Nodes | Max Walltime | | --- | --- | | 1 - 8 | 8 hours | | 9 - 64 | 4 hours | | 65 - 160 | 2 hours |",4.414135223031417
"What is the maximum number of processes that can run on a single compute node in the CAAR and ECP ""batch"" partition?
","| Number of Nodes | Max Walltime | | --- | --- | | 1 - 4 | 3 hours | | 5 - 16 | 1 hour |

The ECP partition consists of 12 total compute nodes. On a per-project basis, each user can have 1 running and 1 eligible job at a time, with up to 5 jobs submitted.

| Number of Nodes | Max Walltime | | --- | --- | | 1 - 4 | 3 hours |",4.358255998062526
"How do I apply the secret-token.yaml file to my MinIO deployment?
","These are the root credentials referenced here.

To establish these credentials in our Marble project, allowing our MinIO deployment to use them, we need to create a secret-token.yaml file and apply it to our project.

Create this example secret-token.yaml file locally:",4.431552762745753
"How do I apply the secret-token.yaml file to my MinIO deployment?
","Replace <name-of-your-app> with the name value you put in your values.yaml file.

Replace <your-choice> with strings of your choice (the access-key length should be at least 3, and the secret-key must be at least 8 characters). These will be the SECRET_TOKEN values.

Once your secret-token.yaml file is set, you can apply it to your Marble project/namespace with this command (assumes you are logged into Marble's CLI):

$ oc apply -f secret-token.yaml

You should get output similar to this:

secret ""rprout-test-minio-access-key"" created
secret ""rprout-test-minio-secret-key"" created",4.381523955400681
"How do I apply the secret-token.yaml file to my MinIO deployment?
","These values are picked up as environment variables from the templates/minio-standalone-deployment.yaml file.

It is recommended to keep the secret-token.yaml file safe, locally, and not in a repository if unencrypted.

At this point we are ready to install our minio-standalone chart in our Marble project namespace.

To list your available project spaces run this command:

$ oc projects

Check list:

You have the OC CLI Tool

You have Helm version 3

You are logged into Marble, with the OC CLI Tool, and in the correct Marble project.

You have configured your values.yaml file.",4.326975977937833
"Can you provide an example of when the -l flag would be useful for Summit?
",For Summit:,4.183783140311105
"Can you provide an example of when the -l flag would be useful for Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.145016589549425
"Can you provide an example of when the -l flag would be useful for Summit?
","The https://docs.olcf.ornl.gov/systems/summit_user_guide.html#OLCF Training Archive<training-archive> provides a list of previous training events, including multi-day Summit Workshops. Some examples of topics addressed during these workshops include using Summit's NVME burst buffers, CUDA-aware MPI, advanced networking and MPI, and multiple ways of programming multiple GPUs per node. You can also find simple tutorials and code examples for some common programming and running tasks in our Github tutorial page .",4.142608488278265
"How can I use nvprof to profile multiple GPUs on Summit?
","summit> module load cuda

A simple ""Hello, World!"" run using nvprof can be done by adding ""nvprof"" to the jsrun (see: https://docs.olcf.ornl.gov/systems/summit_user_guide.html#job-launcher-jsrun) line in your batch script (see https://docs.olcf.ornl.gov/systems/summit_user_guide.html#batch-scripts).

...
jsrun -n1 -a1 -g1 nvprof ./hello_world_gpu
...

Although nvprof doesn't provide aggregated MPI data, the %h and %p output file modifiers can be used to create separate output files for each host and process.

...
jsrun -n1 -a1 -g1 nvprof -o output.%h.%p ./hello_world_gpu
...",4.409977775774588
"How can I use nvprof to profile multiple GPUs on Summit?
","Prior to Nsight Systems and Nsight Compute, the NVIDIA command line profiling tool was nvprof, which provides both tracing and kernel profiling capabilities. Like with Nsight Systems and Nsight Compute, the profiler data output can be saved and imported into the NVIDIA Visual Profiler for additional graphical analysis. nvprof is in maintenance mode now: it still works on Summit and significant bugs will be fixed, but no new feature development is occurring on this tool.

To use nvprof, the cuda module must be loaded.

summit> module load cuda",4.389004224119767
"How can I use nvprof to profile multiple GPUs on Summit?
","There are several tools that can be used to profile the performance of a deep learning job. Below are links to several tools that are available as part of the open-ce module.

The open-ce module contains the nvprof profiling tool. It can be used to profile work that is running on GPUs. It will give information about when different CUDA kernels are being launched and how long they take to complete. For more information on using the NVIDA profiling tools on Summit, please see these slides.",4.38555444073435
"How does one ensure that sensitive information is transmitted securely on OLCF systems?
","Never allow access to your sensitive data to anyone outside of your group.

Transfer of sensitive data must be through the use encrypted methods (scp, sftp, etc).

All sensitive data must be removed from all OLCF resources when your project has concluded.",4.4705300339474014
"How does one ensure that sensitive information is transmitted securely on OLCF systems?
","3. Prior to your project being initialized, we must have source IP addresses for devices in your organization that are authorized to transfer sensitive/controlled information to/from your organization, or for use in accessing that information. Encryption is necessary for transferring sensitive and/or controlled information to and from the OLCF.",4.416309460943064
"How does one ensure that sensitive information is transmitted securely on OLCF systems?
","The OLCF systems provide protections to maintain the confidentiality, integrity, and availability of user data. Measures include: the availability of file permissions, archival systems with access control lists, and parity/CRC checks on data paths/files. It is the user’s responsibility to set access controls appropriately for data. In the event of system failure or malicious actions, the OLCF makes no guarantee against loss of data nor makes a guarantee that a user’s data could not be potentially accessed, changed, or deleted by another individual. It is the user’s responsibility to insure",4.404382381148443
"How do I list all projects/namespaces available to me in Slate?
","Once your Slate Project Allocation Request is approved, you can create your own namespaces and move your allocation around those namespaces via the quota dashboard located at https://quota.marble.ccs.ornl.gov and https://quota.onyx.ccs.ornl.gov. The terms ""namespace"" and ""project"" may get used interchangeably when referring to your project's usable space within the requested resource boundaries (CPU/Memory/Storage).



The OC tool provides CLI access to the OpenShift cluster. It needs to be installed on your machine.",4.223528490506238
"How do I list all projects/namespaces available to me in Slate?
","This section will describe the project allocation process for Slate. This is applicable to both the Onyx and Marble OLCF OpenShift clusters.

In addition, we will go over the process to log into Onyx and Marble, and how namespaces work within the OpenShift context.

Please fill out the Slate Request Form in order to use Slate resources. This must be done in order to proceed.

If you have any question please contact User Assistance, via a Help Ticket Submission or by emailing help@olcf.ornl.gov.

Required information:

- Existing OLCF Project ID

- Project PI",4.202632650519317
"How do I list all projects/namespaces available to me in Slate?
","Once you have navigated to https://console-openshift-console.apps.<cluster>.ccs.ornl.gov/k8s/cluster/projects you should see a list of Projects or Namespaces, that you have access to. Scroll down or use the filter box in the upper right to select your project; your project will have the same name as your allocation in RATS. Once there your screen should look similar to the picture below:



The name of the pictured project is of the form RATS_ALLOCATION-CUSTOM_NAME. Your project will be only RATS_ALLOCATION.",4.186787513640291
"How does the OLCF Policy ensure that projects that are over their allocation do not have an unfair advantage over projects that have not?
","Allocation Overuse Policy

Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.36453324055956
"How does the OLCF Policy ensure that projects that are over their allocation do not have an unfair advantage over projects that have not?
","Access to OLCF resources is limited to approved projects and their users. The type of project (INCITE, ALCC, or Director's Discretion) will determine the application and review procedures. *Quarterly reports are required from industrial Director's Discretion projects only.",4.327801822520781
"How does the OLCF Policy ensure that projects that are over their allocation do not have an unfair advantage over projects that have not?
",The OLCF has a pull-back policy for under-utilization of INCITE allocations. Under-utilized INCITE project allocations will have core-hours removed from their outstanding core-hour project balance at specific times during the INCITE calendar year. The following table summarizes the current under-utilization policy:,4.295694134033813
"How can I run a parallel h5py job on Frontier?
","Frontier

.. code-block:: bash

   #!/bin/bash
   #SBATCH -A <PROJECT_ID>
   #SBATCH -J h5py
   #SBATCH -N 1
   #SBATCH -p batch
   #SBATCH -t 0:05:00

   unset SLURM_EXPORT_ENV

   cd $SLURM_SUBMIT_DIR
   date

   module load PrgEnv-gnu
   module load hdf5
   export PATH=""/path/to/your/miniconda/bin:$PATH""

   source activate /ccs/proj/<project_id>/<user_id>/envs/frontier/h5pympi-frontier

   srun -n42 python3 hdf5_parallel.py",4.435022131355905
"How can I run a parallel h5py job on Frontier?
","This guide has been adapted for Frontier only for a conda workflow. Using the default cray-python module on Frontier does not work with parallel h5py (because Python 3.9 is incompatible). Thus, this guide assumes that you are using a personal https://docs.olcf.ornl.gov/software/python/miniconda.html.

For venv users only interested in installing mpi4py, the pip command in this guide is still accurate.

This guide has been adapted from a challenge in OLCF's Hands-On with Summit GitHub repository (Python: Parallel HDF5).",4.352530361206052
"How can I run a parallel h5py job on Frontier?
","source activate /ccs/proj/<project_id>/<user_id>/envs/summit/h5pympi-summit

   jsrun -n1 -r1 -a42 -c42 python3 hdf5_parallel.py

Andes

.. code-block:: bash

   #!/bin/bash
   #SBATCH -A <PROJECT_ID>
   #SBATCH -J h5py
   #SBATCH -N 1
   #SBATCH -p gpu
   #SBATCH -t 0:05:00

   unset SLURM_EXPORT_ENV

   cd $SLURM_SUBMIT_DIR
   date

   module load gcc
   module load hdf5
   module load python

   source activate /ccs/proj/<project_id>/<user_id>/envs/andes/h5pympi-andes

   srun -n42 python3 hdf5_parallel.py

Frontier

.. code-block:: bash",4.261436534960868
"How can I avoid my job bouncing between running and eligible states indefinitely on Summit?
",For Summit:,4.094114124652075
"How can I avoid my job bouncing between running and eligible states indefinitely on Summit?
","If you submit a job to a ""normal"" Summit queue while on Citadel, such as -q batch, your job will be unable to launch.",4.084123534419217
"How can I avoid my job bouncing between running and eligible states indefinitely on Summit?
","Timeline

Proposals accepted beginning September 19

Proposals will undergo review and the OLCF will notify awardees in mid-to-late November.

Projects are anticipated to start in mid-to-late January 2024.



Your project may continue to submit jobs on Summit through your current project's end date (which varies by allocation program) or December 18th (whichever comes first).  The last day batch jobs from current projects will run on Summit is December 18, 2023.",4.0813834473417785
"What are the default Resource Request settings for Slate?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",4.154187332545663
"What are the default Resource Request settings for Slate?
","- Existing OLCF Project ID

- Project PI

- Enclave (i.e. Open or Moderate Enclave - Onyx or Marble respectively)

NOTE: Summit is in Moderate

- Description (i.e. How you will use Slate)

- Resource Request (i.e. CPU/Memory/Storage requirements - Default is

8CPU/16GB/50GB respectively)

The web UI for OpenShift is available from all of ORNL (you should be able to reach it from your laptop on ORNL WiFi as well as the VPN).",4.12547949178553
"What are the default Resource Request settings for Slate?
","This section will describe the project allocation process for Slate. This is applicable to both the Onyx and Marble OLCF OpenShift clusters.

In addition, we will go over the process to log into Onyx and Marble, and how namespaces work within the OpenShift context.

Please fill out the Slate Request Form in order to use Slate resources. This must be done in order to proceed.

If you have any question please contact User Assistance, via a Help Ticket Submission or by emailing help@olcf.ornl.gov.

Required information:

- Existing OLCF Project ID

- Project PI",4.090619609802224
"How can I ensure that my data is secure on OLCF resources?
","Never allow access to your sensitive data to anyone outside of your group.

Transfer of sensitive data must be through the use encrypted methods (scp, sftp, etc).

All sensitive data must be removed from all OLCF resources when your project has concluded.",4.478935466503034
"How can I ensure that my data is secure on OLCF resources?
","The OLCF systems provide protections to maintain the confidentiality, integrity, and availability of user data. Measures include: the availability of file permissions, archival systems with access control lists, and parity/CRC checks on data paths/files. It is the user’s responsibility to set access controls appropriately for data. In the event of system failure or malicious actions, the OLCF makes no guarantee against loss of data nor makes a guarantee that a user’s data could not be potentially accessed, changed, or deleted by another individual. It is the user’s responsibility to insure",4.4413900396414165
"How can I ensure that my data is secure on OLCF resources?
","The OLCF systems provide protections to maintain the confidentiality, integrity, and availability of user data. Measures include the availability of file permissions, archival systems with access control lists, and parity and CRC checks on data paths and files. It is the user’s responsibility to set access controls appropriately for the data. In the event of system failure or malicious actions, the OLCF makes no guarantee against loss of data or that a user’s data can be accessed, changed, or deleted by another individual. It is the user’s responsibility to insure the appropriate level of",4.426486727731619
"How many INT32 cores does each SM on the V100 contain?
","Each SM on the V100 contains 32 FP64 (double-precision) cores, 64 FP32 (single-precision) cores, 64 INT32 cores, and 8 tensor cores. A 128-KB combined memory block for shared memory and L1 cache can be configured to allow up to 96 KB of shared memory. In addition, each SM has 4 texture units which use the (configured size of the) L1 cache.",4.520493241780558
"How many INT32 cores does each SM on the V100 contain?
","For more information, please see the following section of NVIDIA's CUDA Programming Guide: http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#independent-thread-scheduling-7-x

The Tesla V100 contains 640 tensor cores (8 per SM) intended to enable faster training of large neural networks. Each tensor core performs a D = AB + C operation on 4x4 matrices. A and B are FP16 matrices, while C and D can be either FP16 or FP32:",4.246068336939091
"How many INT32 cores does each SM on the V100 contain?
","The POWER9 processor is built around IBM’s SIMD Multi-Core (SMC). The processor provides 22 SMCs with separate 32kB L1 data and instruction caches. Pairs of SMCs share a 512kB L2 cache and a 10MB L3 cache. SMCs support Simultaneous Multi-Threading (SMT) up to a level of 4, meaning each physical core supports up to 4 https://docs.olcf.ornl.gov/systems/summit_user_guide.html#hardware-threads.",4.230547219104814
"How can I see all queued jobs for all users?
","The most straightforward monitoring is with the bjobs command. This command will show the current queue, including both pending and running jobs. Running bjobs -l will provide much more detail about a job (or group of jobs). For detailed output of a single job, specify the job id after the -l. For example, for detailed output of job 12345, you can run bjobs -l 12345 . Other options to bjobs are shown below. In general, if the command is specified with -u all it will show information for all users/all jobs. Without that option, it only shows your jobs. Note that this is not an exhaustive list.",4.313655341058437
"How can I see all queued jobs for all users?
","The squeue command is used to show the batch queue. You can filter the level of detail through several command-line options. For example:

| squeue -l | Show all jobs currently in the queue | | --- | --- | |  | Show all of your jobs currently in the queue |

The sacct command gives detailed information about jobs currently in the queue and recently-completed jobs. You can also use it to see the various steps within a batch jobs.",4.175319610181984
"How can I see all queued jobs for all users?
","Most users will find batch jobs an easy way to use the system, as they allow you to ""hand off"" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.",4.158091966873337
"How can I check the version of Python installed in the active environment using Conda at OLCF?
","While default Python modules on OLCF systems are already set to Python 3, we recommend all users follow PEP394 by explicitly invoking either ‘python2’ or ‘python3’ instead of simply ‘python’. Python 2 Conda Environments and user installations of Python 2 will remain as options for using Python 2 on OLCF systems.

Official documentation for porting from Python 2 to Python3 can be found at: https://docs.python.org/3/howto/pyporting.html

General information and a list of open source packages dropping support for Python 2 can be found at: https://python3statement.org/",4.338024803693277
"How can I check the version of Python installed in the active environment using Conda at OLCF?
","This guide introduces a user to the basic workflow of using conda environments, as well as providing an example of how to create a conda environment that uses a different version of Python than the base environment uses on Summit. Although Summit is being used in this guide, all of the concepts still apply to other OLCF systems. If using Frontier, this assumes you already have installed your own version of conda (e.g., by https://docs.olcf.ornl.gov/software/python/miniconda.html).

OLCF Systems this guide applies to:

Summit

Andes

Frontier (if using conda)",4.306671300331994
"How can I check the version of Python installed in the active environment using Conda at OLCF?
","You can find the version of Python that exists in this base environment by executing:

$ python --version

Python 3.8.3

For this guide, you are going to install a different version of Python.

To do so, create a new environment using the conda create command:

$ conda create -p /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/py3711-summit python=3.7.11",4.292095603239647
"Can I use the `roc-obj-ls` tool to view the code objects in a binary for a specific shared library?
","The AMD tool `roc-obj-ls` will let you see what code objects are in a binary.

.. code::
    $ hipcc --amdgpu-target=gfx90a:xnack+ square.hipref.cpp -o xnack_plus.exe
    $ roc-obj-ls -v xnack_plus.exe
    Bundle# Entry ID:                                                              URI:
    1       host-x86_64-unknown-linux                                           file://xnack_plus.exe#offset=8192&size=0
    1       hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+                              file://xnack_plus.exe#offset=8192&size=9752",4.392472914092743
"Can I use the `roc-obj-ls` tool to view the code objects in a binary for a specific shared library?
","The AMD tool `roc-obj-ls` will let you see what code objects are in a binary.

.. code::
    $ hipcc --amdgpu-target=gfx90a:xnack+ square.hipref.cpp -o xnack_plus.exe
    $ roc-obj-ls -v xnack_plus.exe
    Bundle# Entry ID:                                                              URI:
    1       host-x86_64-unknown-linux                                           file://xnack_plus.exe#offset=8192&size=0
    1       hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+                              file://xnack_plus.exe#offset=8192&size=9752",4.392472914092743
"Can I use the `roc-obj-ls` tool to view the code objects in a binary for a specific shared library?
","OLCF systems provide many software packages and scientific libraries pre-installed at the system-level for users to take advantage of. In order to link these libraries into an application, users must direct the compiler to their location. The module show command can be used to determine the location of a particular library. For example",4.082337980567435
"How can I check the version of the GitLab runner application deployed on Slate?
","To deploy a GitLab Runner from the Developer Perspective, navigate to ""Topology"" -> ""Helm Chart"" and select ""GitLab Runner vX.X.X Provided by Slate Helm Charts"". From the new window, select ""Install Helm Chart"".

On the resulting screen, one can customize the installation of the GitLab Runner helm chart. The chart has been forked from upstream to allow for customized deployment to Slate. From the Form View, expand the ""GitLab Runner"" fields. Using the registration token retrieved in the prior section, enter token for adding the runner to the GitLab server in the empty field.",4.309611935058369
"How can I check the version of the GitLab runner application deployed on Slate?
","Jenkins

OpenShift Pipelines

GitLab Runners

Deploying a GitLab Runner into a Slate project may be found in the https://docs.olcf.ornl.gov/systems/overview.html#slate_gitlab_runners document which leverages a localized Slate Helm Chart. The GitLab Pipelines documentation as well as GitLab CI/CD Examples provide more details on GitLab Runner capabilities and usage.

Jenkins

For Jenkins deployment, a Slate Helm Chart is planned to be released. Documentation concerning Jenkins Pipelines as well as Jenkins Pipeline Tutorials are available.

OpenShift Pipelines",4.284706141865939
"How can I check the version of the GitLab runner application deployed on Slate?
","GitLab CI/CD jobs may be accomplished using the GitLab Runner application. An open-source application written in Go, a GitLab Runner may be installed with a variety of executors. This documentation focuses on the installation and configuration for use of the GitLab Runner Kubernetes Executor on Slate.

https://docs.gitlab.com/runner/

https://docs.gitlab.com/runner/executors/kubernetes.html

https://docs.gitlab.com/runner/install/kubernetes.html",4.266782796166698
"Can I use the oc rsh command to access a container that is not the first one in the pod?
","Finally, to get a remote shell in the pod we run the oc rsh <POD_NAME> command. This will default to using /bin/sh in the pod. If a different shell is required, we can provide the optional --shell=/path/to/shell flag. For example, if we wanted to open a bash shell in the pod we would run the following command:

oc rsh --shell='/bin/bash' <POD_NAME>

If you have multiple containers in your pod, the oc rsh <POD_NAME> command will default to the first container. If you would like to start a remote shell in one of the other containers, you can use the optional -c <CONTAINER_NAME> flag.",4.54317848440616
"Can I use the oc rsh command to access a container that is not the first one in the pod?
","If we have a pod that is crash looping then it is exiting too quickly to spawn a shell inside the container. We can use oc debug to start a pod with that same image but instead of the image entrypoint we use /bin/sh instead.

$ oc debug misbehaving-pod-1
Defaulting container name to bad.
Use 'oc describe pod/misbehaving-pod-1' to see all of the containers in this pod.

Debugging with pod/misbehaving-pod-1, original command: <image entrypoint>
Waiting for pod to start ...
If you don't see a command prompt, try pressing enter.
/ $

What if we want to get a shell inside of the container to debug?",4.319693327074102
"Can I use the oc rsh command to access a container that is not the first one in the pod?
","oc port-forward hello-world-pod 8080:8080

Then, since oc port-forward stays in the foreground, we run curl http://localhost:8080 in a second terminal.

The initial port in the port pair references a non-allocated port on our local system similar to how SSH port forwarding works.

Pods also have logs. And we can see the logs for the pod: (whatever was printed to stdout from within the container, not kubernetes).

oc logs hello-world-pod

Now lets delete our pod:

oc delete pod hello-world-pod",4.196697878734401
"How can I specify the configuration file for Paraview?
","Click ""Apply""

At the top menu click on ""Options""→""Save Settings""



Once you have VisIt installed and set up on your local computer:

Open VisIt on your local computer.

Go to: ""File→Open file"" or click the ""Open"" button on the GUI.

Click the ""Host"" dropdown menu on the ""File open"" window that popped up and choose ""ORNL_Andes"".

This will prompt you for your OLCF password, and connect you to Andes.

Navigate to the appropriate file.",4.176240470444579
"How can I specify the configuration file for Paraview?
","Although they can be separate files, both Andes and Summit server configurations can be combined and saved into one file following the hierarchy <Servers><Server name= >...<\Server><Server name= >...<\Server><\Servers>.

Step 2: Launch ParaView on your Desktop and Click on File -> Connect

Start ParaView and then select File/Connect to begin.



Step 3: Import Servers

Click Load Servers button and find the servers.pvsc file",4.16356804385231
"How can I specify the configuration file for Paraview?
","After setting up and installing ParaView, you can connect to OLCF systems remotely to visualize your data interactively through ParaView's GUI. To do so, go to File→Connect and select either ORNL Andes or ORNL Summit (provided they were successfully imported -- as outlined in https://docs.olcf.ornl.gov/systems/paraview.html#paraview-install-setup). Next, click on Connect and change the values in the Connection Options box.",4.149229102719082
"What is the purpose of the -Xopenmp-target flag?
","If invoking amdclang, amdclang++, or amdflang directly for openmp offload, or using hipcc you will need to add:  -fopenmp -target x86_64-pc-linux-gnu -fopenmp-targets=amdgcn-amd-amdhsa -Xopenmp-target=amdgcn-amd-amdhsa -march=gfx90a.",4.301931474492363
"What is the purpose of the -Xopenmp-target flag?
","For gnu, add ""-fopenmp"" to the build line.

$ mpicc -fopenmp test.c -o test.x
$ export OMP_NUM_THREADS=2

For intel, add ""-qopenmp"" to the build line.

$ mpicc -qopenmp test.c -o test.x
$ export OMP_NUM_THREADS=2

For information on running threaded codes, please see the https://docs.olcf.ornl.gov/systems/andes_user_guide.html#andes-thread-layout subsection of the https://docs.olcf.ornl.gov/systems/andes_user_guide.html#andes-running-jobs section in this user guide.",4.294486534697116
"What is the purpose of the -Xopenmp-target flag?
","| Vendor | Module | Language | Compiler | OpenMP flag (GPU) | | --- | --- | --- | --- | --- | | Cray | cce | C C++ |  | -fopenmp | | Fortran | ftn (wraps crayftn) |  | | AMD | rocm |  |  | -fopenmp |

If invoking amdclang, amdclang++, or amdflang directly, or using hipcc you will need to add: -fopenmp -target x86_64-pc-linux-gnu -fopenmp-targets=amdgcn-amd-amdhsa -Xopenmp-target=amdgcn-amd-amdhsa -march=gfx90a.

This section shows how to compile HIP codes using the Cray compiler wrappers and hipcc compiler driver.",4.27990672233678
"Can you list the possible states a job can go through?
","A job will transition through several states during its lifetime. Common ones include:

| State Code | State | Description | | --- | --- | --- | | CA | Canceled | The job was canceled (could've been by the user or an administrator) | | CD | Completed | The job completed successfully (exit code 0) | | CG | Completing | The job is in the process of completing (some processes may still be running) | | PD | Pending | The job is waiting for resources to be allocated | | R | Running | The job is currently running |",4.413115371128252
"Can you list the possible states a job can go through?
","Jobs may end up in the PSUSP state for a number of reasons. Two common reasons for PSUSP jobs include jobs that have been held by the user or jobs with unresolved dependencies.  Another common reason that jobs end up in a PSUSP state is a job that the system is unable to start. You may notice a job alternating between PEND and RUN states a few times and ultimately ends up as PSUSP. In this case, the system attempted to start the job but failed for some reason. This can be due to a system issue, but we have also seen this casued by improper settings on user ~/.ssh/config files. (The batch",4.234077111660744
"Can you list the possible states a job can go through?
","In addition to state codes, jobs that are pending will have a ""reason code"" to explain why the job is pending. Completed jobs will have a reason describing how the job ended. Some codes you might see include:",4.192533734009435
"What is the maximum number of GPUs that a single MPI task can access in the first resource set?
","The following example will create 12 resource sets each with 1 task, 4 threads, and 1 GPU. Each MPI task will start 4 threads and have access to 1 GPU. Rank 0 will have access to GPU 0 and start 4 threads on the first socket of the first node ( red resource set). Rank 2 will have access to GPU 1 and start 4 threads on the second socket of the first node ( green resource set). This pattern will continue until 12 resource sets have been created. The following jsrun command will create 12 resource sets (-n12). Each resource set will contain 1 MPI task (-a1), 1 GPU (-g1), and 4 cores (-c4).",4.4502925029301785
"What is the maximum number of GPUs that a single MPI task can access in the first resource set?
","summit>

The following example will create 4 resource sets each with 6 tasks and 3 GPUs. Each set of 6 MPI tasks will have access to 3 GPUs. Ranks 0 - 5 will have access to GPUs 0 - 2 on the first socket of the first node ( red resource set). Ranks 6 - 11 will have access to GPUs 3 - 5 on the second socket of the first node ( green resource set). This pattern will continue until 4 resource sets have been created. The following jsrun command will request 4 resource sets (-n4). Each resource set will contain 6 MPI tasks (-a6), 3 GPUs (-g3), and 6 cores (-c6).",4.445685278057764
"What is the maximum number of GPUs that a single MPI task can access in the first resource set?
","Rank 0 will have access to GPU 0 on the first node ( red resource set). Rank 1 will have access to GPU 1 on the first node ( green resource set). This pattern will continue until 12 resources sets have been created.

The following jsrun command will request 12 resource sets (-n12) 6 per node (-r6). Each resource set will contain 1 MPI task (-a1), 1 GPU (-g1), and 1 core (-c1).",4.43044858602817
"What is the name of the signal handling module being used in the code?
","below), you can choose between the rendering options by loading its corresponding module on the system you're connected to. For example, to see these modules on Summit:",3.945063824940396
"What is the name of the signal handling module being used in the code?
",| Software Name | Loaded Version | Module Name | | --- | --- | --- | | adios2 | 2.7.1 | adios2/2.7.1 | | arborx | 1.0 | arborx/1.0 | | cabana | 0.3.0 | cabana/0.3.0 | | caliper | 2.5.0 | caliper/2.5.0 | | conduit | 0.7.2 | conduit/0.7.2 | | faodel | 1.1906.1 | faodel/1.1906.1 | | flecsi | 1.4 | flecsi/1.4 | | globalarrays | 5.8 | globalarrays/5.8 | | hdf5 | 1.10.7 | hdf5/1.10.7 | | heffte | 2.0.0 | heffte/2.0.0 | | hypre | 2.20.0 | hypre/2.20.0 | | libquo | 1.3.1 | libquo/1.3.1 | | mfem | 4.2.0 | mfem/4.2.0 | | omega-h | 9.32.5 | omega-h/9.32.5 | | openpmd-api | 0.13.4 | openpmd-api/0.13.4 |,3.888688665030711
"What is the name of the signal handling module being used in the code?
","Modules are changed recursively. Some commands, such as module swap, are available to maintain compatibility with scripts using Tcl Environment Modules, but are not necessary since Lmod recursively processes loaded modules and automatically resolves conflicts.",3.8683920116597954
"How can I change the namespace for an application on ArgoCD?
","From the ArgoCD Applications screen, click the Create Application button. In the General application settings, give the application deployment a name for ArgoCD to refer to in the display. For Project usually the ArgoCD default project created during the ArgoCD instance installation is sufficient. However, your workload may benefit from a different logical grouping by using multiple ArgoCD projects. If it is desired to have ArgoCD automatically deploy resources to an OpenShift namespace, change the Sync Policy to Automatic. Check the Prune Resources to automatically remove objects when they",4.40718269982404
"How can I change the namespace for an application on ArgoCD?
","Image of ArgoCD namespace error message.

The Namespace XXXX for XXXX is not managed. indicates that the namespace has not yet been setup for ArgoCD to deploy resources. Please contact the Platforms Group for assistance in changing the configuration of the OpenShift namespace.

In this case, the namespace to deploy resources to was incorrect. The application was editted to change the namespace to deploy resources, and the application tile was reviewed:

Image of ArgoCD application tile with corrected namespace.",4.393663409667504
"How can I change the namespace for an application on ArgoCD?
","ArgoCD has successfully accessed the namespace, realized that the state of the resources in the namespace are OutOfSync with the resource requirements of the code repository, and started the Syncing process to create and/or modify resources in the namespace to match the desired configuration. Clicking on the application tile will reveal more detailed information on the process:

Image of ArgoCD application tile detailed information.",4.364034511009497
"What is the input field name for the simulator function?
","sim_specs = {
    ""sim_f"": sim_find_sine,  # Our simulator function
    ""in"": [""x""],  # Input field names. 'x' from gen_f output
    ""out"": [(""y"", float)],  # sim_f output. 'y' = sine('x')
}

persis_info = add_unique_random_streams({}, 5)  # Initialize manager/workers random streams

exit_criteria = {""sim_max"": 80}  # Stop libEnsemble after 80 simulations

H, persis_info, flag = libE(sim_specs, gen_specs, exit_criteria, persis_info, libE_specs=libE_specs)",3.8517356549634174
"What is the input field name for the simulator function?
","Information on submitting jobs to Quantinuum systems, system availability, checking job status, and tracking usage can be found in the Quantinuum Systems User Guide under the Examples tab on the Quantinuum User Portal.

Users have access to the API validator to check program syntax, and to the Quantinuum System Model H1 emulator, which returns actual results back as if users submitted code to the real quantum hardware.",3.843374911373804
"What is the input field name for the simulator function?
","Simulator backends currently available: https://quantum-computing.ibm.com/services?services=simulators

IBM's Documentation

IBM Quantum Insider",3.835494983551519
"How much memory is requested for the MinIO deployment?
","MinIO is a high-performance software-defined object storage suite that enables the ability to easily deploy cloud-native data infrastructure for various data workloads. In this example we are deploying a simple, standalone implementation of MinIO on our cloud-native platform, Slate (https://docs.olcf.ornl.gov/systems/minio.html#slate_overview).

This service will only be accessible from inside of ORNL's network.

If your project requires an externally facing service available to the Internet, please contact User Assistance by submitting a help ticket. There is a process to get such approval.",4.2014277308225925
"How much memory is requested for the MinIO deployment?
","We hope this provides a starting point for more robust implementations of MinIO, if your workload/project benefits from that. In addition, it gives insight into some of the core building blocks for establishing your own, different, applications on Slate.

This example deployment of MinIO enables two possible configurations (which we configure in the following sections):

MinIO running on a NCCS filesystem (GPFS or NFS - exposing filesystem project space through the MinIO GUI).",4.158340825034694
"How much memory is requested for the MinIO deployment?
","MinIO running on a dedicated volume, allocated automatically from the NetApp storage server, isolated to the MinIO server.

It is important to note that we are also launching MinIO in standalone mode, which is a single MinIO server instance. MinIO also supports distributed mode for more robust implementations, but we are not setting that up in this example.

<string>:5: (INFO/1) Duplicate explicit target name: ""user assistance"".",4.1320854518277255
"Can I use VisIt on Andes without encountering the metadata server error?
","Using VisIt on Summit is also an option, as the scalable rendering problem is currently not an issue on Summit (as of Sept. 2021).

As of February 2022, this issue on Andes has been fixed (must use VisIt 3.2.2 or higher).",4.363199017112136
"Can I use VisIt on Andes without encountering the metadata server error?
","Some users have encountered their compute engine exiting abnormally on Andes after VisIt reaches 100% when drawing a plot, resulting in a ""Scalable Render Request Failed (VisItException)"" error message. This message has also been reported when users try to save plots, if VisIt was successfully able to draw. The error seems to more commonly occur for users that are trying to visualize large datasets.",4.262016740956634
"Can I use VisIt on Andes without encountering the metadata server error?
","VisIt uses a client-server architecture. You will obtain the best performance by running the VisIt client on your local computer and running the server on OLCF resources. VisIt for your local computer can be obtained here: VisIt Installation.

Recommended VisIt versions on our systems:

Summit: VisIt 3.1.4

Andes: VisIt 3.3.3

Frontier: VisIt 3.3.3

Using a different version than what is listed above is not guaranteed to work properly.",4.251124393942587
"What is the name of the shared object that provides the drm functionality on Frontier?
","The Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.123216317549194
"What is the name of the shared object that provides the drm functionality on Frontier?
","<p style=""font-size:20px""><b>Frontier: Darshan Runtime 3.4.0 (May 10, 2023)</b></p>

The Darshan Runtime modulefile darshan-runtime/3.4.0 on Frontier is now loaded by default. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>.

Unloading darshan-runtime modulefile is recommended for users profiling their applications with other profilers to prevent conflicts.",4.114566772548986
"What is the name of the shared object that provides the drm functionality on Frontier?
","Frontier connects to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#dtn-user-guide to move data to/from HPSS.

Frontier is running Cray OS 2.4 based on SUSE Linux Enterprise Server (SLES) version 15.4.",4.0628058109839
"How can I create a new Deployment in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.23369006769229
"How can I create a new Deployment in Slate?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.172784284048886
"How can I create a new Deployment in Slate?
","To deploy a GitLab Runner from the Developer Perspective, navigate to ""Topology"" -> ""Helm Chart"" and select ""GitLab Runner vX.X.X Provided by Slate Helm Charts"". From the new window, select ""Install Helm Chart"".

On the resulting screen, one can customize the installation of the GitLab Runner helm chart. The chart has been forked from upstream to allow for customized deployment to Slate. From the Form View, expand the ""GitLab Runner"" fields. Using the registration token retrieved in the prior section, enter token for adding the runner to the GitLab server in the empty field.",4.171748115095663
"Are there any limitations for jobs using 29-32 qubits on Quantinuum?
","When jobs are submitted on IBM Quantum backends, the jobs enter into the ""fair-share"" queuing system, in which jobs run in a dynamically calculated order so as to provide fair sharing among all users of the device, to prevent individual projects or users from monopolizing a given backend.

All OLCF users have access to the ""premium"" (>=20 qubits) and ""open"" (<20 qubit) devices.  Since most of the open devices are shared with the public, queue times will often be longer than the queues for the larger devices.",4.272722333193084
"Are there any limitations for jobs using 29-32 qubits on Quantinuum?
","Due to hardware emulation complexity, jobs using 29-32 qubits are likely to experience significantly slowed execution times.

The TKET framework is a software platform for the development and execution of gate-level quantum computation, providing state-of-the-art performance in circuit compilation. It was created and is maintained by Quantinuum. The toolset is designed to extract the most out of the available NISQ devices of today and is platform-agnostic.",4.246491885608226
"Are there any limitations for jobs using 29-32 qubits on Quantinuum?
","IBM Quantum Services provides access to more than 20 currently available quantum systems (known as backends).  IBM's quantum processors are made up of superconducting transmon qubits, and users can utilize these systems via the universal, gate-based, circuit model of quantum computation.  Additionally, users have access to 5 different types of simulators, simulating from 32 up to 5000 qubits to represent different aspects of the quantum backends.",4.170465337859711
"How can I load the PGI compiler and parallel-netcdf module on Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.542152580528162
"How can I load the PGI compiler and parallel-netcdf module on Summit?
","The following compilers are available on Summit:

XL: IBM XL Compilers (loaded by default)

LLVM: LLVM compiler infrastructure

PGI: Portland Group compiler suite

NVHPC: Nvidia HPC SDK compiler suite

GNU: GNU Compiler Collection

NVCC: CUDA C compiler

PGI was bought out by Nvidia and have rebranded their compilers, incorporating them into the NVHPC compiler suite. There will be no more new releases of the PGI compilers.",4.370132360683121
"How can I load the PGI compiler and parallel-netcdf module on Summit?
","First, load the gnu compiler module (most Python packages assume GCC), relevant GPU module (necessary for CuPy), and the python module (allows you to create a new environment):

Summit

.. code-block:: bash

   $ module load gcc/7.5.0 # might work with other GCC versions
   $ module load cuda/11.0.2
   $ module load python

Frontier

.. code-block:: bash

   $ module load PrgEnv-gnu
   $ module load amd-mixed/5.3.0
   $ module load craype-accel-amd-gfx90a
   $ module load cray-python # only if not using Miniconda on Frontier",4.363429569284724
"How does the OLCF Policy handle projects that are at 125% of their allocated time?
","Allocation Overuse Policy

Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.41102573475828
"How does the OLCF Policy handle projects that are at 125% of their allocated time?
","Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.366853950909882
"How does the OLCF Policy handle projects that are at 125% of their allocated time?
",The OLCF has a pull-back policy for under-utilization of INCITE allocations. Under-utilized INCITE project allocations will have core-hours removed from their outstanding core-hour project balance at specific times during the INCITE calendar year. The following table summarizes the current under-utilization policy:,4.341283652725967
"How do I know which namespace to use when pushing my image to the registry?
","When tagging an image, you must use the format registry.apps.<cluster>.ccs.ornl.gov/<namespace>/<image> where:

Cluster is the name of the OpenShift cluster

Namespace is the name of the Kubernetes namespace you are using (Use oc status to see what OpenShift Project/Kubernetes Namespace you are currently in)

Image is the name of the image you want to push

Once you push the image into the registry, a OpenShift ImageStream will be automatically created",4.352354881307104
"How do I know which namespace to use when pushing my image to the registry?
","Then you can push and pull from the integrated registry. In the following example we will pull busybox:latest from Docker Hub and push it to our namespace in the integrate registry.

$ docker pull busybox:latest
latest: Pulling from library/busybox
ee153a04d683: Pull complete
Digest: sha256:9f1003c480699be56815db0f8146ad2e22efea85129b5b5983d0e0fb52d9ab70
Status: Downloaded newer image for busybox:latest
docker.io/library/busybox:latest

$ docker tag busybox:latest registry.apps.marble.ccs.ornl.gov/stf002platform/busybox:latest",4.270069960142344
"How do I know which namespace to use when pushing my image to the registry?
","Now, find the repository and tag information of the local image you want to add to the registry and tag it accordingly.

$ docker images
REPOSITORY                                TAG                 IMAGE ID            CREATED             SIZE
example:5000/streams                      v3.1.4              fd7673fdbe30        3 weeks ago         1.95GB

The command to tag your image is:

docker tag example:5000/streams:v3.1.4 registry.apps.<cluster>.ccs.ornl.gov/<namespace>/<image>:<tag>

Lastly, the image needs to be pushed to the registry.",4.267062144154003
"How can I find out which path is correct for my home directory?
","The environment variable $HOME will always point to your current home directory. It is recommended, where possible, that you use this variable to reference your home directory. In cases in which using $HOME is not feasible, it is recommended that you use /ccs/home/$USER.

Users should note that since this is an NFS-mounted filesystem, its performance will not be as high as other filesystems.

Quotas are enforced on user home directories. To request an increased quota, contact the OLCF User Assistance Center. To view your current quota and usage, use the quota command:",4.089837926022623
"How can I find out which path is correct for my home directory?
","If a shell is an interactive login shell (i.e. an ssh to the system) or a non-interactive shell started with the --login option (say, a batch script with #!/bin/bash --login as the first line), it will source /etc/profile and will then search your home directory for ~/.bash_profile, ~/.bash_login, and ~/.profile. It will source the first of those that it finds (once it sources one, it stops looking for the others).

If a shell is an interactive, non-login shell (say, if you run 'bash' in your login session to start a subshell), it will source ~/.bashrc",4.036811891895608
"How can I find out which path is correct for my home directory?
","You can check your home directory quota with the quota command. If it is over quota, you need to bring usage under the quota and then your jobs should run without encountering the Disk quota exceeded error.

Footnotes

1

This entry is for legacy User Archive directories which contained user data on January 14, 2020.

2

User Archive directories that were created (or had no user data) after January 14, 2020. Settings other than permissions are not applicable because directories are root-owned and contain no user files.

3",4.028345414031888
"Can you provide an example of a scenario where disabling XNACK would be beneficial?
","Most applications that use ""managed"" or ""unified"" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Frontier. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.",4.198028762734204
"Can you provide an example of a scenario where disabling XNACK would be beneficial?
","Most applications that use ""managed"" or ""unified"" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Crusher. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.",4.1818536064029965
"Can you provide an example of a scenario where disabling XNACK would be beneficial?
",will result in fatal unhandled page faults. The table below shows how common allocators behave with XNACK disabled.,4.131557357372413
"How does the memory hierarchy in Frontier differ from traditional CPU-GPU architectures?
","The Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.371840601292631
"How does the memory hierarchy in Frontier differ from traditional CPU-GPU architectures?
","The AMD MI250X and operating system on Frontier supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Frontier. In the following section we'll discuss how the heterogenous memory space on a Frontier node is surfaced within your application.",4.349438278533688
"How does the memory hierarchy in Frontier differ from traditional CPU-GPU architectures?
","The page migration behavior summarized by the following tables represents the current, observable behavior. Said behavior will likely change in the near future.  CPU accesses to migratable memory may behave differently than other platforms you're used to. On Frontier, pages will not migrate from GPU HBM to CPU DDR4 based on access patterns alone. Once a page has migrated to GPU HBM it will remain there even if the CPU accesses it, and all accesses which do not resolve in the CPU cache will occur over the Infinity Fabric between the AMD ""Optimized 3rd Gen EPYC"" CPU and AMD MI250X GPU. Pages",4.267234539489858
"What is the difference between HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS and HSA_AMD_AGENT_INFO_SVM_DIRECT_DEVICE_ACCESS?
",":1:rocdevice.cpp            :1573: 43966600550 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966601109 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966601673 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966602248 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:hip_code_object.cpp      :460 : 43966602806 us: hipErrorNoBinaryForGpu: Unable to find code object for all current devices!",4.168415554107796
"What is the difference between HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS and HSA_AMD_AGENT_INFO_SVM_DIRECT_DEVICE_ACCESS?
",":1:rocdevice.cpp            :1573: 43966600550 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966601109 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966601673 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966602248 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:hip_code_object.cpp      :460 : 43966602806 us: hipErrorNoBinaryForGpu: Unable to find code object for all current devices!",4.168415554107796
"What is the difference between HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS and HSA_AMD_AGENT_INFO_SVM_DIRECT_DEVICE_ACCESS?
","$ AMD_LOG_LEVEL=1 HSA_XNACK=0 srun -n 1 -N 1 -t 1 ./xnack_plus.exe
:1:rocdevice.cpp            :1573: 43966598070 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966598762 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966599392 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.
:1:rocdevice.cpp            :1573: 43966599970 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.",4.106298333444988
"What is the name of the hostname for Summit?
",For Summit:,4.341110323052645
"What is the name of the hostname for Summit?
","Summit

**For Summit:**

- **Host nickname**: ``Summit`` (this is arbitrary)
- **Remote hostname**: ``summit.olcf.ornl.gov`` (required)
- **Host name aliases**: ``login#`` (required)
- **Maximum Nodes**: Unchecked
- **Maximum processors**: Unchecked (arbitrary)
- **Path to VisIt Installation**: ``/sw/summit/visit`` (required)
- **Username**: Your OLCF Username (required)
- **Tunnel data connections through SSH**: Checked (required)

Under the “Launch Profiles” tab create a launch profile. Most of these values
are arbitrary",4.285064785469287
"What is the name of the hostname for Summit?
",Figure 1. An example of the NDS servers on Summit,4.236595604495938
"What is the benefit of using a distributed storage system like GPFS for executing applications?
","application as it is not always necessary, we can execute the binary on the GPFS and write/read the data from NVMe if it is supported by the application.",4.318101112186535
"What is the benefit of using a distributed storage system like GPFS for executing applications?
","The following table shows the differences of executing an application on GPFS, NVMe, and NVMe with Spectral. This example is using one compute node. We copy the executable and input file for the NVMe cases but this is not always necessary. Depending on the application, you could execute the binary from the GPFS and save the output files on NVMe. Adjust your parameters to copy, if necessary, the executable and input files onto all the NVMe devices.",4.26122040719266
"What is the benefit of using a distributed storage system like GPFS for executing applications?
","On Summit, there is no concept of striping from the user point of view, the user uses the Alpine storage without the need to declare the striping for files/directories. The GPFS will handle the workload, the file system was tuned during the installation.",4.1837318637680045
"Is the /sw directory excluded from SBCAST's default exclusion list?
","# SBCAST executable from Orion to NVMe -- NOTE: ``-C nvme`` is needed in SBATCH headers to use the NVMe drive
# NOTE: dlopen'd files will NOT be picked up by sbcast
# SBCAST automatically excludes several directories: /lib,/usr/lib,/lib64,/usr/lib64,/opt
#   - These directories are node-local and are very fast to read from, so SBCASTing them isn't critical
#   - see ``$ scontrol show config | grep BcastExclude`` for current list
#   - OLCF-provided libraries in ``/sw`` are not on the exclusion list. ``/sw`` is an NFS shared file system
#   - To override, add ``--exclude=NONE`` to arguments",4.076307872763877
"Is the /sw directory excluded from SBCAST's default exclusion list?
","# SBCAST executable from Orion to NVMe -- NOTE: ``-C nvme`` is needed in SBATCH headers to use the NVMe drive
# NOTE: dlopen'd files will NOT be picked up by sbcast
# SBCAST automatically excludes several directories: /lib,/usr/lib,/lib64,/usr/lib64,/opt
#   - These directories are node-local and are very fast to read from, so SBCASTing them isn't critical
#   - see ``$ scontrol show config | grep BcastExclude`` for current list
#   - OLCF-provided libraries in ``/sw`` are not on the exclusion list. ``/sw`` is an NFS shared file system
#   - To override, add ``--exclude=NONE`` to arguments",4.076307872763877
"Is the /sw directory excluded from SBCAST's default exclusion list?
","Notice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable. Once libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.

As mentioned above, you can alternatively use --exclude=NONE on sbcast to send all libraries along with the binary. Using --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time. A job script for the previous example, modified for sending all libraries is shown below.",4.062681539038282
"Where can I find additional information on MI250X reduced precision?
","If you encounter significant differences when running using reduced precision, explore replacing non-converging models in FP16 with BF16, because of the greater dynamic range in BF16. We recommend using BF16 for ML models in general. If you have further questions or encounter issues, contact help@olcf.ornl.gov.

Additional information on MI250X reduced precision can be found at:

The MI250X ISA specification details the flush to zero denorm behavior at: https://developer.amd.com/wp-content/resources/CDNA2_Shader_ISA_18November2021.pdf (See page 41 and 46)",4.229509575193119
"Where can I find additional information on MI250X reduced precision?
","If you encounter significant differences when running using reduced precision, explore replacing non-converging models in FP16 with BF16, because of the greater dynamic range in BF16. We recommend using BF16 for ML models in general. If you have further questions or encounter issues, contact help@olcf.ornl.gov.

Additional information on MI250X reduced precision can be found at:

The MI250X ISA specification details the flush to zero denorm behavior at: https://developer.amd.com/wp-content/resources/CDNA2_Shader_ISA_18November2021.pdf (See page 41 and 46)",4.229509575193119
"Where can I find additional information on MI250X reduced precision?
","The MI250X has different denormal handling for FP16 and BF16 datatypes, which is relevant for ML training. Prefer using the BF16 over the FP16 datatype for ML models as you are more likely to encounter denormal values with FP16 (which get flushed to zero, causing failure in convergence for some ML models). See more in https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#using-reduced-precision.",4.095500504555408
"What is the achieved performance of Frontier?
","The 2 GCDs on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The OLCF Director's Discretionary (DD) program allocates approximately 10% of the available Frontier hours in a calendar year. Frontier is allocated in *node* hours, and a typical DD project is awarded between 15,000 - 20,000 *node* hours. For more information about Frontier, please visit the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#frontier-user-guide.",4.230512599096736
"What is the achieved performance of Frontier?
","Frontier is a HPE Cray EX supercomputer located at the Oak Ridge Leadership Computing Facility. With a theoretical peak double-precision performance of approximately 2 exaflops (2 quintillion calculations per second), it is the fastest system in the world for a wide range of traditional computational science applications. The system has 74 Olympus rack HPE cabinets, each with 128 AMD compute nodes, and a total of 9,408 AMD compute nodes.",4.223354713946598
"What is the achieved performance of Frontier?
",The Frontier nodes are connected with [4x] HPE Slingshot 200 Gbps (25 GB/s) NICs providing a node-injection bandwidth of 800 Gbps (100 GB/s).,4.20424868951333
"How can I specify multiple job IDs as dependencies in Slurm?
","Oftentimes, a job will need data from some other job in the queue, but it's nonetheless convenient to submit the second job before the first finishes. Slurm allows you to submit a job with constraints that will keep it from running until these dependencies are met. These are specified with the -d option to Slurm. Common dependency flags are summarized below. In each of these examples, only a single jobid is shown but you can specify multiple job IDs as a colon-delimited list (i.e. #SBATCH -d afterok:12345:12346:12346). For the after dependency, you can optionally specify a +time value for",4.45595883414305
"How can I specify multiple job IDs as dependencies in Slurm?
","Slurm reads a number of environment variables, many of which can provide the same information as the job options noted above. We recommend using the job options rather than environment variables to specify job options, as it allows you to have everything self-contained within the job submission script (rather than having to remember what options you set for a given job).

Slurm also provides a number of environment variables within your running job. The following table summarizes those that may be particularly useful within your job (e.g. for naming output log files):",4.228707846844944
"How can I specify multiple job IDs as dependencies in Slurm?
","However, there is currently no way to use Slurm to map multiple GPUs to a single MPI rank. If this functionality is needed for an application, please submit a ticket by emailing help@olcf.ornl.gov.

There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_jobstep program and test whether or not processes and threads are running where intended.",4.2090313414046285
"Can I use GDB to debug programs written in languages other than C and C++?
","GDB, the GNU Project Debugger, is a command-line debugger useful for traditional debugging and investigating code crashes. GDB lets you debug programs written in Ada, C, C++, Objective-C, Pascal (and many other languages).

GDB is available on Summit under all compiler families:

module load gdb

To use GDB to debug your application run:

gdb ./path_to_executable

Additional information about GDB usage can befound on the GDB Documentation Page.",4.513775832460048
"Can I use GDB to debug programs written in languages other than C and C++?
","GDB, the GNU Project Debugger, is a command-line debugger useful for traditional debugging and investigating code crashes. GDB lets you debug programs written in Ada, C, C++, Objective-C, Pascal (and many other languages).

GDB is availableon Summit under all compiler families:

module load gdb

To use GDB to debug your application run:

gdb ./path_to_executable

Additional information about GDB usage can befound on the GDB Documentation Page.",4.508990960824892
"Can I use GDB to debug programs written in languages other than C and C++?
","GDB, the GNU Project Debugger, is a command-line debugger useful for traditional debugging and investigating code crashes. GDB lets you debug programs written in Ada, C, C++, Objective-C, Pascal (and many other languages).

GDB is available on andes via the gdb module:

module load gdb

To use GDB to debug your application run:

gdb ./path_to_executable

Additional information about GDB usage can befound on the GDB Documentation Page.",4.507230679488785
"Can I use Lmod to set environment variables that are not related to software packages?
","Environment management with lmod

The modules software package allows you to dynamically modify your user environment by using pre-written modulefiles. Environment modules are provided through Lmod, a Lua-based module system for dynamically altering shell environments.  By managing changes to the shell’s environment variables (such as path, ld_library_path, and pkg_config_path), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.",4.4603294523221
"Can I use Lmod to set environment variables that are not related to software packages?
","The modules software package allows you to dynamically modify your user environment by using pre-written modulefiles. Environment modules are provided through Lmod, a Lua-based module system for dynamically altering shell environments.  By managing changes to the shell’s environment variables (such as path, ld_library_path, and pkg_config_path), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.",4.43738456485889
"Can I use Lmod to set environment variables that are not related to software packages?
","Environment modules are provided through Lmod, a Lua-based module system for dynamically altering shell environments. By managing changes to the shell’s environment variables (such as PATH, LD_LIBRARY_PATH, and PKG_CONFIG_PATH), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.

The interface to Lmod is provided by the module command:",4.420644672579822
"How many nodes are being used in the Frontier cluster?
","Frontier is connected to Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Data will not be automatically transferred from Alpine to Orion. Frontier also has access to the center-wide NFS-based filesystem (which provides user and project home areas). Each compute node has two 1.92TB Non-Volatile Memory storage devices. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-data-storage for more",4.33921725629223
"How many nodes are being used in the Frontier cluster?
",The Frontier nodes are connected with [4x] HPE Slingshot 200 Gbps (25 GB/s) NICs providing a node-injection bandwidth of 800 Gbps (100 GB/s).,4.309813963052368
"How many nodes are being used in the Frontier cluster?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.294873843071151
"How do I access the OLCF quantum computing resources?
","After submitting the OLCF quantum account application and receiving approval, proceed to https://quantum-computing.ibm.com/ and click on ""Create an IBMid account"". Your IBM Quantum Hub account email will be the email associated with your OLCF account. If sign-in fails, contact help@olcf.ornl.gov. Once logged in, users will have access to the IBM Quantum Hub, IBM’s online platform for QPU access, forums for quantum computing discussion, etc. From the IBM Quantum Hub Dashboard, users can manage system reservations, view system (backend) statuses, and view the results of your past jobs. More",4.511997200919377
"How do I access the OLCF quantum computing resources?
","Access to the IBM Quantum Computing queues, reservations, and simulators can be obtained via multiple methods -- either through the https://docs.olcf.ornl.gov/systems/ibm_quantum.html#cloud <ibm-cloud> or https://docs.olcf.ornl.gov/systems/ibm_quantum.html#locally <ibm-local>.",4.506018419418788
"How do I access the OLCF quantum computing resources?
","with the project will need to apply for a https://docs.olcf.ornl.gov/systems/quantum_access.html#User Account <quantum-user>. After your user account is approved, you can then move on to accessing the quantum resources offered by our vendors (see https://docs.olcf.ornl.gov/systems/quantum_access.html#quantum-vendors).",4.501165378911552
"Can a project that is overallocated still submit new jobs?
","Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.425529839330128
"Can a project that is overallocated still submit new jobs?
","Allocation Overuse Policy

Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.418905897389513
"Can a project that is overallocated still submit new jobs?
","For example, consider that job1 is submitted at the same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. In addition, projects that are at 125% of their allocated time will be limited to only one running job at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.414971771483299
"What is the maximum number of GPUs available for a single rank in Summit?
","Each Summit Compute node has 6 NVIDIA V100 GPUs.  The NVIDIA Tesla V100 accelerator has a peak performance of 7.8 TFLOP/s (double-precision) and contributes to a majority of the computational work performed on Summit. Each V100 contains 80 streaming multiprocessors (SMs), 16 GB (32 GB on high-memory nodes) of high-bandwidth memory (HBM2), and a 6 MB L2 cache that is available to the SMs. The GigaThread Engine is responsible for distributing work among the SMs and (8) 512-bit memory controllers control access to the 16 GB (32 GB on high-memory nodes) of HBM2 memory. The V100 uses NVIDIA's",4.375409681014896
"What is the maximum number of GPUs available for a single rank in Summit?
","summit>

The following example will create 4 resource sets each with 6 tasks and 3 GPUs. Each set of 6 MPI tasks will have access to 3 GPUs. Ranks 0 - 5 will have access to GPUs 0 - 2 on the first socket of the first node ( red resource set). Ranks 6 - 11 will have access to GPUs 3 - 5 on the second socket of the first node ( green resource set). This pattern will continue until 4 resource sets have been created. The following jsrun command will request 4 resource sets (-n4). Each resource set will contain 6 MPI tasks (-a6), 3 GPUs (-g3), and 6 cores (-c6).",4.286519847522921
"What is the maximum number of GPUs available for a single rank in Summit?
","Rank:   20; NumRanks: 24; RankCore:  96; Hostname: a01n01; GPU: 4
Rank:   21; NumRanks: 24; RankCore: 100; Hostname: a01n01; GPU: 4

Rank:   22; NumRanks: 24; RankCore: 104; Hostname: a01n01; GPU: 5
Rank:   23; NumRanks: 24; RankCore: 108; Hostname: a01n01; GPU: 5

summit>",4.258348724460776
"Can subprojects be used to control access to resources in OLCF?
","primary project users must be associated with a subproject(s). If you have any questions, or would like to request a subproject, please contact the OLCF Accounts Team at accounts@ccs.ornl.gov.",4.413966710282016
"Can subprojects be used to control access to resources in OLCF?
","Users should acknowledge the OLCF in all publications and presentations that speak to work performed on OLCF resources:

This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725.

Many OLCF projects make use of optional subprojects. Subprojects provide a useful means for

dividing allocations among different applications, groups, or individuals

controlling priority

monitoring progress",4.356944361437418
"Can subprojects be used to control access to resources in OLCF?
","Access to OLCF resources is limited to approved projects and their users. The type of project (INCITE, ALCC, or Director's Discretion) will determine the application and review procedures. *Quarterly reports are required from industrial Director's Discretion projects only.",4.353746761050678
"How can I track conda environments in a subdirectory called summit in Project Home?
","This will create a .condarc file in your $HOME directory if you do not have one already, which will now contain this new envs_dirs location. This will now enable you to use the --name env_name flag when using conda commands for environments stored in the summit directory, instead of having to use the -p /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/env_name flag and specifying the full path to the environment. For example, you can do source activate py3711-summit instead of source activate /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/py3711-summit.",4.340657236404288
"How can I track conda environments in a subdirectory called summit in Project Home?
","Due to the specific nature of conda on Summit, you must use source activate and source deactivate instead of conda activate and conda deactivate. Let's activate the new environment:

$ source activate /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/py3711-summit

The path to the environment should now be displayed in ""( )"" at the beginning of your terminal lines, which indicate that you are currently using that specific conda environment. And if you check with conda env list again, you should see that the * marker has moved to your newly activated environment:

$ conda env list",4.306112866225293
"How can I track conda environments in a subdirectory called summit in Project Home?
","$ conda create -p /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/baseclone-summit --clone base
$ source activate /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/baseclone-summit

Adding known environment locations:

For a conda environment to be callable by a ""name"", it must be installed in one of the envs_dirs directories. The list of known directories can be seen by executing:

$ conda config --show envs_dirs",4.29804156993306
"What are the permissions for the Project Work area on Lustre Orion?
","Though the NFS filesystem's User Home and Project Home areas are read/write from Frontier's compute nodes, we strongly recommend that users launch and run jobs from the Lustre Orion parallel filesystem instead due to its larger storage capacity and superior performance. Please see below for Lustre Orion filesystem storage areas and paths.",4.346014314774018
"What are the permissions for the Project Work area on Lustre Orion?
",| Area | Path | Type | Permissions | Quota | Backups | Purged | Retention | On Compute Nodes | | --- | --- | --- | --- | --- | --- | --- | --- | --- | | Member Work | /lustre/orion/[projid]/scratch/[userid] | Lustre HPE ClusterStor | 700 | 50 TB | No | 90 days | N/A | Yes | | Project Work | /lustre/orion/[projid]/proj-shared | Lustre HPE ClusterStor | 770 | 50 TB | No | 90 days | N/A | Yes | | World Work | /lustre/orion/[projid]/world-shared | Lustre HPE ClusterStor | 775 | 50 TB | No | 90 days | N/A | Yes |,4.315585318954896
"What are the permissions for the Project Work area on Lustre Orion?
",| Area | Path | Type | Permissions | Quota | Backups | Purged | Retention | On Compute Nodes | | --- | --- | --- | --- | --- | --- | --- | --- | --- | | Member Work | /lustre/orion/[projid]/scratch/[userid] | Lustre HPE ClusterStor | 700 | 50 TB | No | 90 days | N/A | Yes | | Project Work | /lustre/orion/[[projid]/proj-shared | Lustre HPE ClusterStor | 770 | 50 TB | No | 90 days | N/A | Yes | | World Work | /lustre/orion/[[projid]/world-shared | Lustre HPE ClusterStor | 775 | 50 TB | No | 90 days | N/A | Yes |,4.312788600887726
"Can I use Nsight Compute with a different GPU vendor other than NVIDIA?
","The profiler will print several sections including information about the CUDA API calls made by the application, as well as any GPU kernels that were launched. Nsight Systems can be used for CUDA C++, CUDA Fortran, OpenACC, OpenMP offload, and other programming models that target NVIDIA GPUs, because under the hood they all ultimately take the same path for generating the binary code that runs on the GPU.",4.275644910830337
"Can I use Nsight Compute with a different GPU vendor other than NVIDIA?
","that data motion. But if that bottleneck is a GPU kernel, then Nsight Compute can be used to collect performance counters to understand whether the kernel is running efficiently and if there's anything you can do to improve.",4.202962660213897
"Can I use Nsight Compute with a different GPU vendor other than NVIDIA?
",| Nsight Compute | Felix Schmitt (NVIDIA) | NVIDIA Profiling Tools - Nsight Compute https://www.olcf.ornl.gov/calendar/nvidia-profiling-tools-nsight-compute/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/OLCF-Webinar-Nsight-Compute.pdf https://vimeo.com/398929189 | | 2020-03-09 | Nsight Systems | Holly Wilper (NVIDIA) | NVIDIA Profiling Tools - Nsight Systems https://www.olcf.ornl.gov/calendar/nvidia-profiling-tools-nsight-systems/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/Summit-Nsight-Systems-Introduction.pdf,4.192103230148704
"What is the purpose of the X-Remote-User header?
","The authenticated user must be on the project in order to use the application running in OpenShift

The authentication will be handled by the cluster load balancers so that nothing is required by a user application. If a user application needs to authenticate a user we set the X-Remote-User header which is the NCCS username of the authenticated user.

An example list of headers that are set by the loadbalancer:",4.193077072600456
"What is the purpose of the X-Remote-User header?
","Host: nginx-echo-headers-stf002platform.bedrock-dev.ccs.ornl.gov
X-Remote-User: kincljc
X-Forwarded-Host: nginx-echo-headers-stf002platform.bedrock-dev.ccs.ornl.gov
X-Forwarded-Port: 443
X-Forwarded-Proto: https
Forwarded: for=160.91.195.36;host=nginx-echo-headers-stf002platform.bedrock-dev.ccs.ornl.gov;proto=https;proto-version=
X-Forwarded-For: 160.91.195.36

Routes are secured by adding the annotation ccs.ornl.gov/requireAuth = ""true"" to the route.

If you have an application that should not require authentication reach out to NCCS Support.",4.016350361021327
"What is the purpose of the X-Remote-User header?
",One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Frontier from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Frontier. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.,3.887074018611372
"How can we ensure that the Swift/T workflow is fault-tolerant and can recover from failures gracefully?
","This example demonstrates a continuously running cross-facility workflow. The idea is that there is a science facility (eg. SNS at ORNL) that produces scientific data to be processed by the remote compute facility (eg. OLCF at ORNL). The data is continuously arriving in a designated directory at the compute facility from science facility. The workflow picks data from that directory and does the processing to the data to produce some output. The Swift source file workflow.swift looks as follows:

import files;
import io;",4.177565420669449
"How can we ensure that the Swift/T workflow is fault-tolerant and can recover from failures gracefully?
","Swift/T is a completely new implementation of the Swift language for high-performance computing which translates Swift scripts into MPI programs that use the Turbine (hence, /T) and ADLB runtime libraries. This tutorial shows how to get up and running with Swift/T on Summit specifically. For more information about Swift/T, please refer to its documentation.

Swift/T is available as a module on Summit, and it can be loaded as follows:

$ module load workflows
$ module load swift/1.5.0

You will also need to set the PROJECT environment variable:

$ export PROJECT=""ABC123""",4.109001195873812
"How can we ensure that the Swift/T workflow is fault-tolerant and can recover from failures gracefully?
","The suggested directory structure is to have a outer directory say swift-work that has the swift source and shell scripts. Inside of swift-work create a new directory called data.

Additionally, we will need two terminals open. In the first terminal window, navigate to the swift-work directory and invoke the data generation script like so:

$ ./gendata.sh

In the second terminal, we will run the swift workflow as follows (make sure to change the project name per your allocation):",4.0631581806597
"How does Slurm interpret the execution section of a batch script?
","The execution section of a script will be interpreted by a shell and can contain multiple lines of executables, shell commands, and comments.  when the job's queue wait time is finished, commands within this section will be executed on the primary compute node of the job's allocated resources. Under normal circumstances, the batch job will exit the queue after the last line of the script is executed.

Example Batch Script

#!/bin/bash
#SBATCH -A XXXYYY
#SBATCH -J test
#SBATCH -N 2
#SBATCH -t 1:00:00

cd $SLURM_SUBMIT_DIR
date
srun -n 8 ./a.out",4.447955230216179
"How does Slurm interpret the execution section of a batch script?
","The execution section of a script will be interpreted by a shell and can contain multiple lines of executables, shell commands, and comments.  when the job's queue wait time is finished, commands within this section will be executed on the primary compute node of the job's allocated resources. Under normal circumstances, the batch job will exit the queue after the last line of the script is executed.

#!/bin/bash
#SBATCH -A XXXYYY
#SBATCH -J test
#SBATCH -N 2
#SBATCH -t 1:00:00

cd $SLURM_SUBMIT_DIR
date
srun -n 8 ./a.out",4.434007937049431
"How does Slurm interpret the execution section of a batch script?
","cd $SLURM_SUBMIT_DIR
date
srun -n 8 ./a.out

This batch script shows examples of the three sections outlined above:

Interpreter Line

<string>:509: (INFO/1) Duplicate implicit target name: ""interpreter line"".

1: This line is optional and can be used to specify a shell to interpret the script. In this example, the bash shell will be used.

Slurm Options

2: The job will be charged to the “XXXYYY” project.

3: The job will be named test.

4: The job will request (2) nodes.

5: The job will request (1) hour walltime.

Shell Commands",4.390813635958237
"What is the responsibility of the Principal Investigator (PI) regarding sensitive controls outlined in the OLCF Policy?
","5. The Principal Investigator (PI) has the responsibility to make sure that other project members follow the sensitive controls outlined in this policy and protect sensitive/controlled information. It is also the responsibility of the PI to alert us of any personnel changes on the project.

If you have security-related questions, contact us via email at: security-admins@ccs.ornl.gov. Other questions can be sent to help@olcf.ornl.gov.

<string>:3: (INFO/1) Duplicate implicit target name: ""software"".

More information about the UMS program can be found in the Software section.",4.523123443122801
"What is the responsibility of the Principal Investigator (PI) regarding sensitive controls outlined in the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  Title: Project Reporting Policy Version: 12.10",4.4063768283101705
"What is the responsibility of the Principal Investigator (PI) regarding sensitive controls outlined in the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to or use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title:Security Policy Version: 12.10",4.368734491976553
"How will I be notified when my user account is created and associated with a project?
","When all of the above steps are completed, your user account will be created and you will be notified by email. Now that you have a user account and it has been associated with a project, you're ready to get to work.",4.5798075823827045
"How will I be notified when my user account is created and associated with a project?
","Upon completion of the above steps, the PI will be notified that the project has been created, and provided with the project ID and system allocation details. At this time, project participants may apply for user accounts.",4.396875717766641
"How will I be notified when my user account is created and associated with a project?
","When all of the above steps are completed, your user account will be created and you will be notified by email. Now that you have a user account and it has been associated with a project, you're ready to get to work. This website provides extensive documentation for OLCF systems, and can help you efficiently use your project's allocation. We recommend reading the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#System User Guides<system-user-guides> for the machines you will be using often.",4.305503440439959
"How long are files kept on Lustre Orion before they are purged?
","To keep the Spectrum Scale file system exceptionally performant, files that have not been accessed in the project and user areas are purged at the intervals shown in the table above. Please make sure that valuable data is moved off of these systems regularly. See https://docs.olcf.ornl.gov/systems/policies.html#data-hpss. for information about using the HSI and HTAR utilities to archive data on HPSS. Just to note that when you read a file, then the 90 days counter restarts.",4.378447729588505
"How long are files kept on Lustre Orion before they are purged?
","and files are automatically purged on a regular basis. Files should not be retained in this file system for long, but rather should be migrated to Project Home or Project Archive space as soon as the files are not actively being used. If a file system associated with your Member Work directory is nearing capacity, the OLCF may contact you to request that you reduce the size of your Member Work directory. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for more details on applicable quotas, backups, purge, and retention timeframes.",4.314730526715039
"How long are files kept on Lustre Orion before they are purged?
","Data purge mechanisms are enabled on some OLCF file system directories in order to maintain sufficient disk space availability for job execution. Files in these scratch areas are automatically purged on a regular purge timeframe. If a file system with an active purge policy is nearing capacity, the OLCF may contact you to request that you reduce the size of a directory within that file system, even if the purge timeframe has not been exceeded. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for details on purge timeframes for each storage area, if",4.290655262070779
"What is the difference between the ""premium"" and ""open"" devices on IBM Quantum Services?
","When jobs are submitted on IBM Quantum backends, the jobs enter into the ""fair-share"" queuing system, in which jobs run in a dynamically calculated order so as to provide fair sharing among all users of the device, to prevent individual projects or users from monopolizing a given backend.

All OLCF users have access to the ""premium"" (>=20 qubits) and ""open"" (<20 qubit) devices.  Since most of the open devices are shared with the public, queue times will often be longer than the queues for the larger devices.",4.331987760508279
"What is the difference between the ""premium"" and ""open"" devices on IBM Quantum Services?
","IBM Quantum Services provides access to more than 20 currently available quantum systems (known as backends).  IBM's quantum processors are made up of superconducting transmon qubits, and users can utilize these systems via the universal, gate-based, circuit model of quantum computation.  Additionally, users have access to 5 different types of simulators, simulating from 32 up to 5000 qubits to represent different aspects of the quantum backends.",4.214786329119421
"What is the difference between the ""premium"" and ""open"" devices on IBM Quantum Services?
","Current status listing, scheduled maintenance, and system capabilities for IBM Quantum's quantum resources can be found here: https://quantum-computing.ibm.com/services?services=systems



Qiskit documentation is available at https://qiskit.org/documentation/

Qiskit Terra is the foundational module set upon which the rest of Qiskit's features are built; for more information, see: https://qiskit.org/documentation/apidoc/terra.html

Qiskit Aer is IBM Quantum's package for simulating quantum circuits, with different backends for specific types of simulation",4.139188391899403
"What is rsync and how does it differ from scp?
","rsync -avz --dry-run mydir/ $USER@dtn.ccs.ornl.gov:/path/

See the manual pages for more information:

$ man scp
$ man rsync

Differences:

scp cannot continue if it is interrupted. rsync can.

rsync is optimized for performance.

By default, rsync checks if the transfer of the data was successful.

Standard file transfer protocol (FTP) and remote copy (RCP) should not be used to transfer files to the NCCS high-performance computing (HPC) systems due to security concerns.",4.295620125772517
"What is rsync and how does it differ from scp?
","Standard tools such as rsync and scp can also be used through the DTN, but may be slower and require more manual intervention than Globus

Copying data directly from Alpine (GPFS) to Orion (Lustre)

Globus is the suggested tool to transfer needed data from Alpine to Orion.

Globus should be used when transfer large amounts of data.

Standard tools such as rsync and cp can also be used. The DTN mounts both filesystems and should be used when transferring with rsync and cp tools. These methods should not be used to transfer large amounts of data.

Copying data to the HPSS archive system",4.127584961357734
"What is rsync and how does it differ from scp?
","This page has been deprecated, and relevant information is now available on https://docs.olcf.ornl.gov/systems/transferring.html#data-storage-and-transfers. Please update any bookmarks to use that page.

In general, when transferring data into or out of the OLCF from the command line, it's best to initiate the transfer from outside the OLCF. If moving many small files, it can be beneficial to compress them into a single archive file, then transfer just the one archive file.

scp and rsync are available for remote transfers.

scp - secure copy (remote file copy program)

Sending a file to OLCF",4.100954077066535
"What is the purpose of the --launch_distribution option on Frontier?
","| Slurm Option | Description | | --- | --- | | --distribution=<value>[:<value>][:<value>] | Specifies the distribution of MPI ranks across compute nodes, sockets (L3 cache regions on Frontier), and cores, respectively. The default values are block:cyclic:cyclic, which is where the cyclic assignment comes from in the previous examples. |",4.132662645388029
"What is the purpose of the --launch_distribution option on Frontier?
","Unlike Summit and Titan, there are no launch/batch nodes on Frontier. This means your batch script runs on a node allocated to you rather than a shared node. You still must use the job launcher (srun) to run parallel jobs across all of your nodes, but serial tasks need not be launched with srun.



To easily visualize job examples (see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-mapping further below), the compute node diagram has been simplified to the picture shown below.

Simplified Frontier node architecture diagram",4.09035202813393
"What is the purpose of the --launch_distribution option on Frontier?
","To override this default layout (not recommended), set -S 0 at job allocation.



Frontier uses SchedMD's Slurm Workload Manager for scheduling and managing jobs. Slurm maintains similar functionality to other schedulers such as IBM's LSF, but provides unique control of Frontier's resources through custom commands and options specific to Slurm. A few important commands can be found in the conversion table below, but please visit SchedMD's Rosetta Stone of Workload Managers for a more complete conversion reference.",4.083609424874362
"What is the purpose of the #BSUB command in the batch script?
","by a shell name. For example, to request an interactive batch job (with bash as the shell) equivalent to the sample batch script above, you would use the command: bsub -W 3:00 -nnodes 2048 -P ABC123 -Is /bin/bash",4.272497623180485
"What is the purpose of the #BSUB command in the batch script?
",".. code-block:: bash
     script.bash

    #BSUB -P <PROJECT>
    #BSUB -W 0:10
    #BSUB -nnodes 2
    #BSUB -q batch
    #BSUB -J mldl_test_job
    #BSUB -o /ccs/home/<user>/job%J.out
    #BSUB -e /ccs/home/<user>/job%J.out

    module load open-ce

    jsrun -bpacked:7 -g6 -a6 -c42 -r1 python $CONDA_PREFIX/horovod/examples/tensorflow2_synthetic_benchmark.py


bsub is used to launch the script as follows:

bsub script.bash

For more information on bsub and job submission please see: https://docs.olcf.ornl.gov/systems/ibm-wml-ce.html#running-jobs.",4.263845472224891
"What is the purpose of the #BSUB command in the batch script?
","The table below summarizes options for submitted jobs. Unless otherwise noted, they can be used for either batch scripts or interactive batch jobs. For scripts, they can be added on the sbatch command line or as a #BSUB directive in the batch script. (If they're specified in both places, the command line takes precedence.) This is only a subset of all available options. Check the Slurm Man Pages for a more complete list.",4.261600302151559
"How do I calculate the number of FLOPS on Crusher?
","Crusher contains a total of 768 AMD MI250X. The AMD MI250X has a peak performance of 53 TFLOPS in double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 26.5 TFLOPS (double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in both directions simultaneously).



To connect to Crusher, ssh to crusher.olcf.ornl.gov. For example:

$ ssh <username>@crusher.olcf.ornl.gov",4.297375432046126
"How do I calculate the number of FLOPS on Crusher?
","For ROCm/5.2.0 and earlier, there is a known issue with the timings provided by --timestamp on. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#crusher-known-issues.

The above formula can be adapted to compute the total FLOPS across all floating-point precisions (INT excluded).",4.290909866961691
"How do I calculate the number of FLOPS on Crusher?
","For ROCm/5.2.0 and earlier, there is a known issue with the timings provided by --timestamp on. See https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#crusher-known-issues.

The above formula can be adapted to compute the total FLOPS across all floating-point precisions (INT excluded).",4.276175960385599
"How can I prefetch data from the host DDR4 memory into the GPU HBM in Frontier?
","The Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.233619116004545
"How can I prefetch data from the host DDR4 memory into the GPU HBM in Frontier?
","The AMD MI250X and operating system on Frontier supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Frontier. In the following section we'll discuss how the heterogenous memory space on a Frontier node is surfaced within your application.",4.193780990596575
"How can I prefetch data from the host DDR4 memory into the GPU HBM in Frontier?
","Each Frontier compute node consists of [1x] 64-core AMD ""Optimized 3rd Gen EPYC"" CPU (with 2 hardware threads per physical core) with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X, each with 2 Graphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can think of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth memory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU, allowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of 36+36 GB/s. The 2 GCDs on the same MI250X are connected with Infinity",4.190271001780557
"How do I know which kubernetes cluster ArgoCD is installed in?
",kubernetes cluster to deploy. This will likely be https://kubernetes.default.svc- the same cluster the ArgoCD instance is installed. The Namespace setting should be the OpenShift namespace that ArgoCD will deploy resources. This may or may not be the same namespace that ArgoCD is installed (see prior discussion on multiple namespace management in this document).,4.376766456538763
"How do I know which kubernetes cluster ArgoCD is installed in?
","The Red Hat OpenShift GitOps operator is already installed onto each of the Slate clusters. However, to use ArgoCD an instance will need to be added into a project namespace. If one is to use ArgoCD to manage resources in a single project, then the ArgoCD instance could be located in the same project as the resources being managed. However, if the application being managed is resource (CPU and memory) sensitive or if resources in multiple projects are to be managed, it may be better to have the ArgoCD instance deployed into a separate project to allow for better control of resources allocated",4.295778791306804
"How do I know which kubernetes cluster ArgoCD is installed in?
","With a git repository defined in the ArgoCD Repositories settings that has kustomize environments ready for use, one can start using ArgoCD to deploy and manage kubernetes resources.",4.216115804783256
"How can I activate a new environment created using Conda at OLCF?
","This guide introduces a user to the basic workflow of using conda environments, as well as providing an example of how to create a conda environment that uses a different version of Python than the base environment uses on Summit. Although Summit is being used in this guide, all of the concepts still apply to other OLCF systems. If using Frontier, this assumes you already have installed your own version of conda (e.g., by https://docs.olcf.ornl.gov/software/python/miniconda.html).

OLCF Systems this guide applies to:

Summit

Andes

Frontier (if using conda)",4.440786961804663
"How can I activate a new environment created using Conda at OLCF?
","By default this should create the cloned environment in /ccs/home/${USER}/.conda/envs/cloned_env (unless you changed it, as outlined in our https://docs.olcf.ornl.gov/software/python/index.html page).

To activate the new environment you should still load the module first. This will ensure that all of the conda settings remain the same.

$ module load open-ce
(open-ce-1.2.0-py38-X) $ conda activate cloned_env
(cloned_env) $",4.374243452739836
"How can I activate a new environment created using Conda at OLCF?
","This page describes how a user can successfully install specific quantum software tools to run on select OLCF HPC systems. This allows a user to utilize our systems to run a simulator, or even as a ""frontend"" to a vendor's quantum machine (""backend"").

For most of these instructions, we use conda environments. For more detailed information on how to use Python modules and conda environments on OLCF systems, see our :doc:`Python on OLCF Systems page</software/python/index>`, and our https://docs.olcf.ornl.gov/software/python/conda_basics.html.",4.326488190114225
"How can I get the output of bjobs without formatting?
","The most straightforward monitoring is with the bjobs command. This command will show the current queue, including both pending and running jobs. Running bjobs -l will provide much more detail about a job (or group of jobs). For detailed output of a single job, specify the job id after the -l. For example, for detailed output of job 12345, you can run bjobs -l 12345 . Other options to bjobs are shown below. In general, if the command is specified with -u all it will show information for all users/all jobs. Without that option, it only shows your jobs. Note that this is not an exhaustive list.",4.316532525233416
"How can I get the output of bjobs without formatting?
","| Command | Description | | --- | --- | | bjobs | Show your current jobs in the queue | | bjobs -u all | Show currently queued jobs for all users | | bjobs -P ABC123 | Shows currently-queued jobs for project ABC123 | | bjobs -UF | Don't format output (might be useful if you're using the output in a script) | | bjobs -a | Show jobs in all states, including recently finished jobs | | bjobs -l | Show long/detailed output | | bjobs -l 12345 | Show long/detailed output for jobs 12345 | | bjobs -d | Show details for recently completed jobs | | bjobs -s | Show suspended jobs, including the reason(s)",4.230111900062731
"How can I get the output of bjobs without formatting?
","-s | Show suspended jobs, including the reason(s) they're suspended | | bjobs -r | Show running jobs | | bjobs -p | Show pending jobs | | bjobs -w | Use ""wide"" formatting for output |",4.208513215348461
"What is the advantage of using the -l hip_device_ enumeration option in my MPI+OpenMP+HIP program on Frontier?
","A simple MPI+OpenMP ""Hello, World"" program (hello_mpi_omp) will be used to clarify the mappings.

For the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-gpu-map section:

An MPI+OpenMP+HIP ""Hello, World"" program (hello_jobstep) will be used to clarify the GPU mappings.

Additionally, it may be helpful to cross reference the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#simplified Frontier node diagram <frontier-simple> -- specifically the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>.",4.37057124071497
"What is the advantage of using the -l hip_device_ enumeration option in my MPI+OpenMP+HIP program on Frontier?
","In this sub-section, an MPI+OpenMP+HIP ""Hello, World"" program (hello_jobstep) will be used to show how to make only specific GPUs available to processes - which we will refer to as ""GPU mapping"". Again, Slurm's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive method was used to request an allocation of 2 compute nodes for these examples: salloc -A <project_id> -t 30 -p <parition> -N 2. The CPU mapping part of this example is very similar to the example used above in the Multithreading sub-section, so the focus here will be on the GPU mapping part.",4.24177145860312
"What is the advantage of using the -l hip_device_ enumeration option in my MPI+OpenMP+HIP program on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.203660020898418
"What are the storage areas and paths available on Lustre Orion?
","Though the NFS filesystem's User Home and Project Home areas are read/write from Frontier's compute nodes, we strongly recommend that users launch and run jobs from the Lustre Orion parallel filesystem instead due to its larger storage capacity and superior performance. Please see below for Lustre Orion filesystem storage areas and paths.",4.4146343230086735
"What are the storage areas and paths available on Lustre Orion?
","For more detailed information about center-wide file systems and data archiving available on Frontier, please refer to the pages on https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers. The subsections below give a quick overview of NFS, Lustre,and HPSS storage spaces as well as the on node NVMe ""Burst Buffers"" (SSDs).",4.359658642547998
"What are the storage areas and paths available on Lustre Orion?
","Frontier mounts Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Frontier will not mount Alpine when Orion is in production.",4.328467578129821
"What are the available metrics that can be collected by TAU for my CUDA applications?
","A similar approach for other metrics, not all of them can be used. TAU provides a tool called tau_cupti_avail, where we can see the list of available metrics, then we have to figure out which CUPTI metrics use these ones.

Activate tracing and declare the data format to OTF2. OTF2 format is supported only by MPI and OpenSHMEM applications.

$ export TAU_TRACE=1
$ export TAU_TRACE_FORMAT=otf2

Use Vampir for visualization.

For example, do not instrument routine sort*(int *)

Create a file select.tau

BEGIN_EXCLUDE_LIST
void sort_#(int *)
END_EXCLUDE_LIST

Declare the TAU_OPTIONS variable",4.381183886113221
"What are the available metrics that can be collected by TAU for my CUDA applications?
","From the main window right click one label and select “Show User Event Statistics Window”. Then, we can see the data transfered to the devices



The CUDA Profiling Tools Interface (CUPTI) is used by profiling and tracing tools that target CUDA applications.



Matrix multiplication with MPI+OpenMP:

$ export TAU_METRICS=TIME,achieved_occupancy
$ jsrun -n 2 -r 2 -g 1  tau_exec -T mpi,pdt,papi,cupti,openmp -ompt -cupti  ./add

We choose to use tau_exec with MPI, PDT, PAPI, CUPTI, and OpenMP.

Output directories:",4.333068013402369
"What are the available metrics that can be collected by TAU for my CUDA applications?
","Add the following in your submission file:

export TAU_METRICS=TIME
export TAU_PROFILE=1
export TAU_TRACK_MESSAGE=1
export TAU_COMM_MATRIX=1
jsrun -n 6 -r 6 --smpiargs=""-gpu"" -g 1  tau_exec -T mpi,pgi,pdt -openacc ./miniWeather_mpi_openacc

We declare to TAU to profile the MPI with PDT support through the -T parameters, as well as using the pgi tag for the TAU makefile and OpenACC.

CUPTI metrics for OpenACC are not yet supported for TAU.

When the execution of the instrumented application finishes, there is one directory for each TAU_METRICS declaration with the format MULTI__",4.31960260056433
"How can I prevent my job from starting until job 12345 has failed?
","cannot start until job 12345 exits with an exit code of 0. See the Job Dependency section for more information | | -C | #SBATCH -C nvme | Request the burst buffer/NVMe on each node be made available for your job. See the Burst Buffers section for more information on using them. | | -J | #SBATCH -J MyJob123 | Specify the job name (this will show up in queue listings) | | -o | #SBATCH -o jobout.%j | File where job STDOUT will be directed (%j will be replaced with the job ID). If no -e option is specified, job STDERR will be placed in this file, too. | | -e | #SBATCH -e joberr.%j | File where",4.117951376601228
"How can I prevent my job from starting until job 12345 has failed?
",of EXIT (i.e. completed abnormally) | | #BSUB -w ended(12345) | The job will not start until job 12345 has a state of EXIT or DONE |,4.093737771334996
"How can I prevent my job from starting until job 12345 has failed?
","Sometimes you may need to place a hold on a job to keep it from starting. For example, you may have submitted it assuming some needed data was in place but later realized that data is not yet available. This can be done with the scontrol hold command. Later, when the data is ready, you can release the job (i.e. tell the system that it's now OK to run the job) with the scontrol release command. For example:

| scontrol hold 12345 | Place job 12345 on hold | | --- | --- | | scontrol release 12345 | Release job 12345 (i.e. tell the system it's OK to run it) |",4.073013376365603
"What is the difference between a job and a task in HPC?
","In High Performance Computing (HPC), computational work is performed by jobs. Individual jobs produce data that lend relevant insight into grand challenges in science and engineering. As such, the timely, efficient execution of jobs is the primary concern in the operation of any HPC system.

A job on a commodity cluster typically comprises a few different components:

A batch submission script.

A binary executable.

A set of input files for the executable.

A set of output files created by the executable.

And the process for running a job, in general, is to:",4.403243460350591
"What is the difference between a job and a task in HPC?
","Running Jobs

In High Performance Computing (HPC), computational work is performed by jobs. Individual jobs produce data that lend relevant insight into grand challenges in science and engineering. As such, the timely, efficient execution of jobs is the primary concern in the operation of any HPC system.

A job on a commodity cluster typically comprises a few different components:

A batch submission script.

A binary executable.

A set of input files for the executable.

A set of output files created by the executable.

And the process for running a job, in general, is to:",4.395748034168904
"What is the difference between a job and a task in HPC?
","Job Execution

Once resources have been allocated through the batch system, users have the option of running commands on the allocated resources' primary compute node (a serial job) and/or running an MPI/OpenMP executable across all the resources in the allocated resource pool simultaneously (a parallel job).

Serial Job Execution

The executable portion of batch scripts is interpreted by the shell specified on the first line of the script. If a shell is not specified, the submitting user’s default shell will be used.",4.159110749964374
"How do I ensure that my ParaView client and server are using the same version?
","You will obtain the best performance by running the ParaView client on your local computer and running the server on OLCF resources with the same version of ParaView. It is highly recommended to check the available ParaView versions using module avail paraview on the system you plan to connect ParaView to. Precompiled ParaView binaries for Windows, macOS, and Linux can be downloaded from Kitware.

Recommended ParaView versions on our systems:

Summit: ParaView 5.9.1, 5.10.0, 5.11.0

Andes: ParaView 5.9.1, 5.10.0, 5.11.0",4.392420189935037
"How do I ensure that my ParaView client and server are using the same version?
","Although in a single machine setup both the ParaView client and server run on the same host, this need not be the case. It is possible to run a local ParaView client to display and interact with your data while the ParaView server runs in an Andes or Summit batch job, allowing interactive analysis of very large data sets.",4.300184426305022
"How do I ensure that my ParaView client and server are using the same version?
","After installing, you must give ParaView the relevant server information to be able to connect to OLCF systems (comparable to VisIt's system of host profiles). The following provides an example of doing so. Although several methods may be used, the one described should work in most cases.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.

For Windows clients, it is necessary to install PuTTY to create an ssh connection in step 2.

Step 1: Save the following servers.pvsc file to your local computer",4.2820846727290265
"How do I customize the provided Spack environment files to fit my needs?
","Traditionally, the user environment is modified by module files.  For example, a user would add use  module load cmake/3.18.2 to load CMake version 3.18.2 into their environment.  Using a Spack environment, a user can add an OLCF provided package and build against it using Spack without having to load the module file separately.

The information presented here is a subset of what can be found at the Spack documentation site.",4.377863489579415
"How do I customize the provided Spack environment files to fit my needs?
","This not intended as a guide for a new Spack user.  Please see the Spack 101 tutorial if you need assistance starting out with Spack.

The provided Spack environment files are intended to assist OLCF users in setup their development environment at the OLCF.  The base environment file includes the compilers and packages that are installed at the system level.",4.346137423127472
"How do I customize the provided Spack environment files to fit my needs?
","## Make changes to the template environment module before continuing!!

$ spack env create my_env linux-rhel8-ppc64le/summit/spack.yaml
$ spack env activate my_env

The template file contains usable, but not advisable, settings for configuration items.  Options marked with FIXME are specifically recommended to be changed, like the installation root directory.  Items marked OPTIONAL indicate points that are not required, but are useful as noted.

Now a user can add and install their dependencies with Spack and proceed with developing their application.",4.323943434657946
"Can you explain the concept of superposition in quantum computing?
","Conventional/classical computing utilizes information storage based on digital devices storing “bits”, which are in either of two distinct states at a given time, i.e. 0 or 1. Quantum computers utilize properties of quantum mechanics, such as superposition and entanglement, in order to exceed certain capabilities of classical computers. Superposition means that the units of information storage can be in multiple states at the same time, and entanglement means the states can depend on each other.  In quantum computing systems, information is stored not using “bits”, but instead using “qubits”.",4.376851972900846
"Can you explain the concept of superposition in quantum computing?
","IBM Quantum Services provides access to more than 20 currently available quantum systems (known as backends).  IBM's quantum processors are made up of superconducting transmon qubits, and users can utilize these systems via the universal, gate-based, circuit model of quantum computation.  Additionally, users have access to 5 different types of simulators, simulating from 32 up to 5000 qubits to represent different aspects of the quantum backends.",4.146581636656324
"Can you explain the concept of superposition in quantum computing?
","A qubit (pronounced “cue-bit”, a portmanteau of “quantum bit”) is the physical unit of quantum information in quantum computing. It is the quantum version of a bit (itself a portmanteau of “binary digit”), consisting of a two-state quantum mechanical system that can (like a classical bit) exist in one state, |0⟩, or the other, |1⟩, but unlike the classical bit counterpart, a qubit can also be in a quantum superposition of both states.

Applications for both Quantum Computing projects and quantum user accounts can be found on the :doc:`/quantum/quantum_access` page.",4.138904802915036
"How do I create a Project in Slate?
","This section will describe the project allocation process for Slate. This is applicable to both the Onyx and Marble OLCF OpenShift clusters.

In addition, we will go over the process to log into Onyx and Marble, and how namespaces work within the OpenShift context.

Please fill out the Slate Request Form in order to use Slate resources. This must be done in order to proceed.

If you have any question please contact User Assistance, via a Help Ticket Submission or by emailing help@olcf.ornl.gov.

Required information:

- Existing OLCF Project ID

- Project PI",4.225734238524251
"How do I create a Project in Slate?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",4.218735903169928
"How do I create a Project in Slate?
","When all of the above steps are completed, your user account will be created and you will be notified by email. Now that you have a user account and it has been associated with a project, you're ready to get to work.",4.192842226611117
"How can we ensure that the MPI ranks are distributed evenly across the L3 cache regions?
","Because the distribution across L3 cache regions has been changed to a ""packed"" (block) configuration, caution must be taken to ensure MPI ranks end up in the L3 cache regions where the GPUs they intend to be mapped to are located. To accomplish this, the number of physical CPU cores assigned to an MPI rank was increased - in this case to 4. Doing so ensures that only 2 MPI ranks can fit into a single L3 cache region. If the value of -c was left at 1, all 8 MPI ranks would be ""packed"" into the first L3 region, where the ""closest"" GPU would be GPU 4 - the only GPU in that L3 region.",4.533376912093858
"How can we ensure that the MPI ranks are distributed evenly across the L3 cache regions?
","Because the distribution across L3 cache regions has been changed to a ""packed"" (block) configuration, caution must be taken to ensure MPI ranks end up in the L3 cache regions where the GPUs they intend to be mapped to are located. To accomplish this, the number of physical CPU cores assigned to an MPI rank was increased - in this case to 4. Doing so ensures that only 2 MPI ranks can fit into a single L3 cache region. If the value of -c was left at 1, all 8 MPI ranks would be ""packed"" into the first L3 region, where the ""closest"" GPU would be GPU 4 - the only GPU in that L3 region.  Notice",4.521392311692076
"How can we ensure that the MPI ranks are distributed evenly across the L3 cache regions?
","There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.

Assigning MPI ranks in a ""round-robin"" (cyclic) manner across L3 cache regions (sockets) is the default behavior on Frontier. This mode will assign consecutive MPI tasks to different sockets before it tries to ""fill up"" a socket.",4.411904961088661
"How does the processdata.sh script display the progress of the image processing?
","Next, we have the data processing script called processdata.sh that looks as follows:

#!/bin/bash
set -eu

TASK=convert
DATA=$1
echo ""\nProcessing ${DATA}\n""
${TASK} ${DATA} -fuzz 10% -fill white -opaque white -fill black +opaque white -format ""%[fx:100*mean]"" info:
sleep 5

The above script computes the cloud cover percentage by looking at the amount of white pixels in the image. Note that it uses ImageMagick's convert utility.",4.385355267589065
"How does the processdata.sh script display the progress of the image processing?
","In order to demonstrate the data generation, we have a script that downloads image data from the NOAA website periodically. The image is a geographical image showing current cloud cover over south-east US. The code gendata.sh looks like so:

#!/bin/bash
set -eu

function cleanup() {
  \rm -f ./data/earth*.jpg
}

while true
do
  uid=$(uuidgen | awk -F- '{print $1}')
  wget -q https://cdn.star.nesdis.noaa.gov/GOES16/ABI/SECTOR/se/GEOCOLOR/1200x1200.jpg -O ./data/earth${uid}.jpg
  sleep 5
  trap cleanup EXIT
done",4.129627859185626
"How does the processdata.sh script display the progress of the image processing?
","import files;
import io;

app (void v) processdata(file f)
{
 // change path per your location
 ""/gpfs/alpine/scratch/ketan2/stf019/swift-work/cross-facility/processdata.sh"" f ;
}

for (boolean b = true; b; b=c)
{
  boolean c;
  // You can change the number of data files while the workflow is running
  file data[] = glob(""*.jpg"");
  void V[];
  foreach f, i in data
  {
    V[i] = processdata(f);
  }
  printf(""processed %i files."", size(V)) => c = true;
}",4.0850088429145
"What is the default XNACK setting for the MI250X GPU on Crusher?
","XNACK (pronounced X-knack) refers to the AMD GPU's ability to retry memory accesses that fail due to a page fault. The XNACK mode of an MI250X can be changed by setting the environment variable HSA_XNACK before starting a process that uses the GPU. Valid values are 0 (disabled) and 1 (enabled), and all processes connected to a GPU must use the same XNACK setting. The default MI250X on Crusher is HSA_XNACK=0.",4.510491060943299
"What is the default XNACK setting for the MI250X GPU on Crusher?
","XNACK (pronounced X-knack) refers to the AMD GPU's ability to retry memory accesses that fail due to a page fault. The XNACK mode of an MI250X can be changed by setting the environment variable HSA_XNACK before starting a process that uses the GPU. Valid values are 0 (disabled) and 1 (enabled), and all processes connected to a GPU must use the same XNACK setting. The default MI250X on Frontier is HSA_XNACK=0.",4.437526491337335
"What is the default XNACK setting for the MI250X GPU on Crusher?
","Although XNACK is a capability of the MI250X GPU, it does require that kernels be able to recover from page faults. Both the ROCm and CCE HIP compilers will default to generating code that runs correctly with both XNACK enabled and disabled. Some applications may benefit from using the following compilation options to target specific XNACK modes.

hipcc --amdgpu-target=gfx90a or CC --offload-arch=gfx90a -x hip

Kernels are compiled to a single ""xnack any"" binary, which will run correctly with both XNACK enabled and XNACK disabled.",4.2912781168587575
"Can I use the --ntasks-per-gpu option to launch 16 MPI ranks on a single GPU?
","This example launches 16 MPI ranks (-n16), each with 4 physical CPU cores (-c4) to launch 1 OpenMP thread (OMP_NUM_THREADS=1) on. The MPI ranks will be assigned to GPUs in a packed fashion so that each of the 8 GPUs on the node are shared by 2 MPI ranks. Similar to Example 4, -ntasks-per-gpu=2 will be used, but a new srun flag will be used to change the default round-robin (cyclic) distribution of MPI ranks across NUMA domains:",4.4922725991744015
"Can I use the --ntasks-per-gpu option to launch 16 MPI ranks on a single GPU?
","This example launches 16 MPI ranks (-n16), each with 4 physical CPU cores (-c4) to launch 1 OpenMP thread (OMP_NUM_THREADS=1) on. Because it is using 4 physical CPU cores per task, core specialization needs to be disabled (-S 0). The MPI ranks will be assigned to GPUs in a packed fashion so that each of the 8 GPUs on the node are shared by 2 MPI ranks. Similar to Example 5, -ntasks-per-gpu=2 will be used, but a new srun flag will be used to change the default round-robin (cyclic) distribution of MPI ranks across NUMA domains:",4.479090406875599
"Can I use the --ntasks-per-gpu option to launch 16 MPI ranks on a single GPU?
","| Slurm Option | Description | | --- | --- | | --ntasks-per-gpu | Specifies the number of MPI ranks that will share access to a GPU. |

On AMD's MI250X, multi-process service (MPS) is not needed since multiple MPI ranks per GPU is supported natively.

Example 4: 16 MPI ranks - where 2 ranks share a GPU (round-robin, single-node)

This example launches 16 MPI ranks (-n16), each with 1 physical CPU core (-c1) to launch 1 OpenMP thread (OMP_NUM_THREADS=1) on. The MPI ranks will be assigned to GPUs in a round-robin fashion so that each of the 8 GPUs on the node are shared by 2 MPI ranks.",4.468395066752044
"What is the purpose of the ""add<<<N, 1>>>(da, db);"" line in the code?
","#include <stdio.h>
#define N 1000

__global__
void add(int *a, int *b) {
    int i = blockIdx.x;
    if (i<N) {
        b[i] = 2*a[i];
    }
}

int main() {
    int ha[N], hb[N];

    int *da, *db;
    cudaMalloc((void **)&da, N*sizeof(int));
    cudaMalloc((void **)&db, N*sizeof(int));

    for (int i = 0; i<N; ++i) {
        ha[i] = i;
    }
cudaMemcpy(da, ha, N*sizeof(int), cudaMemcpyHostToDevice);

add<<<N, 1>>>(da, db);

cudaMemcpy(hb, db, N*sizeof(int), cudaMemcpyDeviceToHost);",4.095474484715073
"What is the purpose of the ""add<<<N, 1>>>(da, db);"" line in the code?
","for (int i = 0; i<N; ++i) {
    if(i+i != hb[i]) {
        printf(""Something went wrong in the GPU calculation\n"");
    }
}
printf(""COMPLETE!"");
     cudaFree(da);
     cudaFree(db);

     return 0;
}

Create a file named gpuexample.dockerfile with the following contents

FROM code.ornl.gov:4567/olcfcontainers/olcfbaseimages/mpiimage-centos-cuda:latest
RUN mkdir /app
COPY cudaexample.cu /app
RUN cd /app && nvcc -o cudaexample cudaexample.cu

Run the following commands to build the container image with Podman and convert it to Singularity",3.954285021872232
"What is the purpose of the ""add<<<N, 1>>>(da, db);"" line in the code?
","In cases when contention is very low, a FP32 CAS loop implementing an atomicAdd() operation could be faster than an hardware FP32 LDS atomicAdd(). Applications using single precision FP atomicAdd() are encouraged to experiment with the use of double precision to evaluate the trade-off between high atomicAdd() performance vs. potential lower occupancy due to higher LDS usage.",3.860555775513646
"What is the difference between the ""merged"" and ""snapshot"" settings in TAU?
","matrix | | TAU_THROTTLE | 1 | Setting to 0 turns off throttling, by default removes overhead | | TAU_THROTTLE_NUMCALLS | 100000 | Number of calls before testing throttling | | TAU_THROTTLE_PERCALL | 10 | If a routine is called more than 100000 times and it takes less than 10 usec of inclusive time, throttle it | | TAU_COMPENSATE | 10 | Setting to 1 enables runtime compensation of instrumentation overhead | | TAU_PROFILE_FORMAT | Profile | Setting to ""merged"" generates a single file, ""snapshot"" generates a snapshot per thread | | TAU_METRICS | TIME | Setting to a comma separated list",4.040438078979802
"What is the difference between the ""merged"" and ""snapshot"" settings in TAU?
","Below, we'll look at using TAU to profile each case.

Edit the cmake_summit_pgi.sh and replace mpic++ with tau_cxx.sh. This applies only for the non-GPU versions.

TAU works with special TAU makefiles to declare what programming models are expected from the application:

The available makefiles are located inside TAU installation:",4.035358293798498
"What is the difference between the ""merged"" and ""snapshot"" settings in TAU?
",| Variable | Default | Description | | --- | --- | --- | | TAU_TRACE | 0 | Setting to 1 turns on tracing | | TAU_CALLPATH | 0 | Setting to 1 turns on callpath profiling | | TAU_TRACK_MEMORY_LEAKS | 0 | Setting to 1 turns on leak detection | | TAU_TRACK_HEAP | 0 | Setting to 1 turns on heap memory routine entry/exit | | TAU_CALLPATH_DEPTH | 2 | Specifies depth of callpath | | TAU_TRACK_IO_PARAMS | 0 | Setting 1 with -optTrackIO | | TAU_SAMPLING | 1 | Generates sample based profiles | | TAU_COMM_MATRIX | 0 | Setting to 1 generates communication matrix | | TAU_THROTTLE | 1 | Setting to 0 turns,3.960675703927892
"How can I enable tracing in TAU?
",| Variable | Default | Description | | --- | --- | --- | | TAU_TRACE | 0 | Setting to 1 turns on tracing | | TAU_CALLPATH | 0 | Setting to 1 turns on callpath profiling | | TAU_TRACK_MEMORY_LEAKS | 0 | Setting to 1 turns on leak detection | | TAU_TRACK_HEAP | 0 | Setting to 1 turns on heap memory routine entry/exit | | TAU_CALLPATH_DEPTH | 2 | Specifies depth of callpath | | TAU_TRACK_IO_PARAMS | 0 | Setting 1 with -optTrackIO | | TAU_SAMPLING | 1 | Generates sample based profiles | | TAU_COMM_MATRIX | 0 | Setting to 1 generates communication matrix | | TAU_THROTTLE | 1 | Setting to 0 turns,4.3125440579673535
"How can I enable tracing in TAU?
","A similar approach for other metrics, not all of them can be used. TAU provides a tool called tau_cupti_avail, where we can see the list of available metrics, then we have to figure out which CUPTI metrics use these ones.

Activate tracing and declare the data format to OTF2. OTF2 format is supported only by MPI and OpenSHMEM applications.

$ export TAU_TRACE=1
$ export TAU_TRACE_FORMAT=otf2

Use Vampir for visualization.

For example, do not instrument routine sort*(int *)

Create a file select.tau

BEGIN_EXCLUDE_LIST
void sort_#(int *)
END_EXCLUDE_LIST

Declare the TAU_OPTIONS variable",4.299705935645431
"How can I enable tracing in TAU?
","TAU is a portable profiling and tracing toolkit that supports many programming languages. The instrumentation can be done by inserting in the source code using an automatic tool based on the Program Database Toolkit (PDT), on the compiler instrumentation, or manually using the instrumentation API.

Webpage: https://www.cs.uoregon.edu/research/tau/home.php",4.290168924205952
"Can you explain the difference between memory allocated through standard system allocators such as malloc and memory allocated through HSA::Memory::Allocator in Crusher with XNACK disabled?
","Most applications that use ""managed"" or ""unified"" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Crusher. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.",4.392719685787827
"Can you explain the difference between memory allocated through standard system allocators such as malloc and memory allocated through HSA::Memory::Allocator in Crusher with XNACK disabled?
","Most applications that use ""managed"" or ""unified"" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Frontier. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.",4.290775720742867
"Can you explain the difference between memory allocated through standard system allocators such as malloc and memory allocated through HSA::Memory::Allocator in Crusher with XNACK disabled?
","HSA_XNACK=0 Automatic Page Migration Disabled

| Allocator | Initial Physical Location | Default Behavior for CPU Access | Default Behavior for GPU Access | | --- | --- | --- | --- | | System Allocator (malloc,new,allocate, etc) | CPU DDR4 | Local read/write | Fatal Unhandled Page Fault | | hipMallocManaged | CPU DDR4 | Local read/write | Zero copy read/write over Infinity Fabric | | hipHostMalloc | CPU DDR4 | Local read/write | Zero copy read/write over Infinity Fabric | | hipMalloc | GPU HBM | Zero copy read/write over Inifinity Fabric | Local read/write |",4.244506397508157
"How does the MI250X GPU's denormal handling compare to other GPUs in Summit?
","Users leveraging BF16 and FP16 datatypes for applications such as ML/AI training and low-precision matrix multiplication should be aware that the AMD MI250X GPU has different denormal handling than the V100 GPUs on Summit. On the MI250X, the V_DOT2 and the matrix instructions for FP16 and BF16 flush input and output denormal values to zero. FP32 and FP64 MFMA instructions do not flush input and output denormal values to zero.",4.3840221790108655
"How does the MI250X GPU's denormal handling compare to other GPUs in Summit?
","Users leveraging BF16 and FP16 datatypes for applications such as ML/AI training and low-precision matrix multiplication should be aware that the AMD MI250X GPU has different denormal handling than the V100 GPUs on Summit. On the MI250X, the V_DOT2 and the matrix instructions for FP16 and BF16 flush input and output denormal values to zero. FP32 and FP64 MFMA instructions do not flush input and output denormal values to zero.",4.3840221790108655
"How does the MI250X GPU's denormal handling compare to other GPUs in Summit?
","The 110 CUs in each GPU deliver peak performance of 26.5 TFLOPS in double precision. Also, each GPU contains 64 GB of high-bandwidth memory (HBM2) accessible at a peak bandwidth of 1.6 TB/s. The 2 GPUs in an MI250X are connected with [4x] GPU-to-GPU Infinity Fabric links providing 200+200 GB/s of bandwidth. (Consult the diagram in the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes section for information on how the accelerators are connected to each other, to the CPU, and to the network.",4.327544112049678
"Can you provide a tutorial on how to use pbdR on Summit?
","If you want to do GPU computing on Summit with R, we would love to collaborate with you (see contact details at the bottom of this document).

For more information about using R and pbdR effectively in an HPC environment such as Summit, please see the R and pbdR articles. These are long-form articles that introduce HPC concepts like MPI programming in much more detail than we do here.

Also, if you want to use R and/or pbdR on Summit, please feel free to contact us directly:

Mike Matheson - mathesonma AT ornl DOT gov

George Ostrouchov - ostrouchovg AT ornl DOT gov",4.394804600072054
"Can you provide a tutorial on how to use pbdR on Summit?
","George Ostrouchov - ostrouchovg AT ornl DOT gov

Drew Schmidt - schmidtda AT ornl DOT gov

We are happy to provide support and collaboration for R and pbdR users on Summit.",4.27369230122417
"Can you provide a tutorial on how to use pbdR on Summit?
",of how to run a Python script using PvBatch on Andes and Summit.,4.220905201200155
"How do I instrument a C++ program with Score-P?
","To instrument your code, you need to compile the code using the Score-P instrumentation command (scorep), which is added as a prefix to your compile statement. In most cases the Score-P instrumentor is able to automatically detect the programming paradigm from the set of compile and link options given to the compiler. Some cases will, however, require some additional link options within the compile statement e.g. CUDA instrumentation.

Below are some basic examples of the different instrumentation scenarios:",4.566909251431853
"How do I instrument a C++ program with Score-P?
","$ scorep --user gcc -c test.c
$ scorep --user gcc -o test test.o

Now you can manually instrument Score-P to the source code as seen below:

C,C++

.. code::

   #include <scorep/SCOREP_User.h>

   void foo() {
      SCOREP_USER_REGION_DEFINE(my_region)
      SCOREP_USER_REGION_BEGIN(my_region, ""foo"", SCOREP_USER_REGION_TYPE_COMMON)
      // do something
      SCOREP_USER_REGION_END(my_region)
   }

Fortran

.. code::

   #include <scorep/SCOREP_User.inc>",4.398448248690186
"How do I instrument a C++ program with Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi-processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations.

For detailed information about using Score-P on Summit and the builds available, please see the Score-P Software Page.",4.350379876515957
"Who is responsible for ensuring compliance with export control regulations?
","Export Control: The project request will be reviewed by ORNL Export Control to determine whether sensitive or proprietary data will be generated or used. The results of this review will be forwarded to the PI. If the project request is deemed sensitive and/or proprietary, the OLCF Security Team will schedule a conference call with the PI to discuss the data protection needs.",4.140495210775772
"Who is responsible for ensuring compliance with export control regulations?
","these prohibited data types or information that falls under Export Control. For questions, contact help@olcf.ornl.gov.",4.081904267603115
"Who is responsible for ensuring compliance with export control regulations?
","these prohibited data types or information that falls under Export Control. For questions, contact help@nccs.gov.",4.0792188212299605
"Can I modify my reservation on Summit?
",For Summit:,4.280434486840016
"Can I modify my reservation on Summit?
","The requesting project's allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Summit would be equivalent to using 16,000 Summit node-hours of a project's allocation.



As is the case with many other queuing systems, it is possible to place dependencies on jobs to prevent them from running until other jobs have started/completed/etc. Several possible dependency settings are described in the table below:",4.027251539850238
"Can I modify my reservation on Summit?
","Due to the large amount of data on the filesystems, we strongly urge you to start transferring your data now, and do not wait until later in the year.

Jan 01, all remaining Alpine data will be PERMANENTLY DELETED.  Do not wait to move needed data.



Summit will be returned to service early 2024.

Projects awarded a 2024 Summit allocation will be able to log into Summit and submit batch jobs once the system has been made available.",4.0096897335612605
"How can I roll out a new deployment in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.286919761701956
"How can I roll out a new deployment in Slate?
","Jenkins

OpenShift Pipelines

GitLab Runners

Deploying a GitLab Runner into a Slate project may be found in the https://docs.olcf.ornl.gov/systems/overview.html#slate_gitlab_runners document which leverages a localized Slate Helm Chart. The GitLab Pipelines documentation as well as GitLab CI/CD Examples provide more details on GitLab Runner capabilities and usage.

Jenkins

For Jenkins deployment, a Slate Helm Chart is planned to be released. Documentation concerning Jenkins Pipelines as well as Jenkins Pipeline Tutorials are available.

OpenShift Pipelines",4.202887051060286
"How can I roll out a new deployment in Slate?
","The following items need to be established before deploying applications on Slate systems (Marble or Onyx OpenShift clusters).

Follow https://docs.olcf.ornl.gov/systems/helm_prerequisite.html#slate_getting_started if you do not already have a project/namespace established on Onyx or Marble.

It is recommended to use Helm version 3.

Helm enables application deployment via Helm Charts. The high level Helm Architecture docs are also a great reference for understanding Helm.

Like oc, Helm is a single binary executable.

This can be installed on macOS with Homebrew :

$ brew install helm",4.17328046741069
"How can I verify that my MPI application is using the correct number of GPUs?
","As mentioned previously, all GPUs are accessible by all MPI ranks by default, so it is possible to programatically map any combination of MPI ranks to GPUs. However, there is currently no way to use Slurm to map multiple GPUs to a single MPI rank. If this functionality is needed for an application, please submit a ticket by emailing help@olcf.ornl.gov.

There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_jobstep program and test whether or not processes and threads are running where intended.",4.2632362343145385
"How can I verify that my MPI application is using the correct number of GPUs?
","However, there is currently no way to use Slurm to map multiple GPUs to a single MPI rank. If this functionality is needed for an application, please submit a ticket by emailing help@olcf.ornl.gov.

There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_jobstep program and test whether or not processes and threads are running where intended.",4.236255808769448
"How can I verify that my MPI application is using the correct number of GPUs?
","So the job step (i.e., srun command) used above gave the desired output. Each MPI rank spawned 2 OpenMP threads and had access to a unique GPU. The --gpus-per-task=1 allocated 1 GPU for each MPI rank and the --gpu-bind=closest ensured that the closest GPU to each rank was the one used.

Example 2: 8 MPI ranks - each with 2 OpenMP threads and 1 GPU (multi-node)

This example will extend Example 1 to run on 2 nodes. As the output shows, it is a very straightforward exercise of changing the number of nodes to 2 (-N2) and the number of MPI ranks to 8 (-n8).",4.23141700550522
"How can I access the Summit/Frontier login nodes from the Citadel framework?
","The login nodes listed above mirrors the Summit and Frontier login nodes in hardware and software.  The login node also provides access to the same compute resources as are accessible from Summit and Frontier's non-SPI workflows.

The Citadel login nodes cannot access the external network and are only accessible from whitelisted IP addresses.",4.5579190499979445
"How can I access the Summit/Frontier login nodes from the Citadel framework?
",project on Summit/Frontier.  The partner project would provide login access to the non-SPI Summit/Frontier login nodes and a build location that is writable from the non-SPI Summit/Frontier and read-only from within the Citadel framework.  For example the partner project would provide the ability to build on Summit/Frontier in /sw/summit/mde/abc123_mde where abc123_mde is replaced by your Citadel project. This location is writable from Summit/Frontier but only readable from within the Citadel framework.,4.480299848411081
"How can I access the Summit/Frontier login nodes from the Citadel framework?
","The Citadel framework allows use of the Summit/Frontier compute resources but adds additional layers of security to ensure data protection.  To ensure proper configuration and protection access to the compute resources, the following batch queue(s) must be used from the https://docs.olcf.ornl.gov/systems/index.html#Citadel login nodes<citadel-login-nodes>:

batch-spi",4.415557891148447
"How can I ensure that my MPI program uses a round-robin scheduling algorithm?
","There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.

Assigning MPI ranks in a ""round-robin"" (cyclic) manner across L3 cache regions (sockets) is the default behavior on Frontier. This mode will assign consecutive MPI tasks to different sockets before it tries to ""fill up"" a socket.",4.336199740679555
"How can I ensure that my MPI program uses a round-robin scheduling algorithm?
","The intent with both of the following examples is to launch 8 MPI ranks across the node where each rank is assigned its own logical (and, in this case, physical) core.  Using the -m distribution flag, we will cover two common approaches to assign the MPI ranks -- in a ""round-robin"" (cyclic) configuration and in a ""packed"" (block) configuration. Slurm's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive method was used to request an allocation of 1 compute node for these examples: salloc -A <project_id> -t 30 -p <parition> -N 1",4.294825579640272
"How can I ensure that my MPI program uses a round-robin scheduling algorithm?
","Instead, you can assign MPI ranks so that the L3 regions are filled in a ""packed"" (block) manner.  This mode will assign consecutive MPI tasks to the same L3 region (socket) until it is ""filled up"" or ""packed"" before assigning a task to a different socket.

Recall that the -m flag behaves like: -m <node distribution>:<socket distribution>.  Hence, the key setting to achieving the round-robin nature is the -m block:block flag, specifically the block setting provided for the ""socket distribution"". This ensures that the MPI tasks will be distributed in a packed manner.",4.285572055627068
"Can I use the same command to mount my home directory in all Summit nodes?
","Summit has a node hierarchy that can be very confusing for the uninitiated. The Summit User Guide explains this in depth. This has some consequences that may be unusual for R programmers. A few important ones to note are:

You must have a script that you can run in batch, e.g. with Rscript or R CMD BATCH

All data that needs to be visible to the R process (including the script and your packages) must be on gpfs (not your home directory!).

You must launch your script from the launch nodes with jsrun.

We'll start with something very simple. The script is:

""hello world""",4.184020504453577
"Can I use the same command to mount my home directory in all Summit nodes?
","It is highly recommended to create new environments in the ""Project Home"" directory (on Summit, this is /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>). This space avoids purges, allows for potential collaboration within your project, and works better with the compute nodes. It is also recommended, for convenience, that you use environment names that indicate the hostname, as virtual environments created on one system will not necessarily work on others.",4.164689088704242
"Can I use the same command to mount my home directory in all Summit nodes?
","Users will be building and running containers on Summit without root permissions i.e. containers on Summit are rootless.  This means users can get the benefits of containers without needing additional privileges. This is necessary for a shared system like Summit. And this is part of the reason why Docker doesn't work on Summit. Podman and Singularity provides rootless support but to different extents hence why users need to use a combination of both.

Users will need to set up a file in their home directory /ccs/home/<username>/.config/containers/storage.conf with the following content:",4.151215684904376
"How can you use the nvprof profiler to measure Tensor Core utilization for a program running on a Summit node?
","When using NVIDIA’s nvprof profiler, one should add the -m tensor_precision_fu_utilization option to measure Tensor Core utilization. Below is the output from measuring this metric on one of the example programs.

$ nvprof -m tensor_precision_fu_utilization ./simpleCUBLAS
==43727== NVPROF is profiling process 43727, command: ./simpleCUBLAS
GPU Device 0: ""Tesla V100-SXM2-16GB"" with compute capability 7.0",4.446492281107032
"How can you use the nvprof profiler to measure Tensor Core utilization for a program running on a Summit node?
","When attempting to use Tensor Cores it is useful to measure and confirm that the Tensor Cores are being used within your code. For implicit use via a library like cuBLAS, the Tensor Cores will only be used above a certain threshold, so Tensor Core use should not be assumed. The NVIDIA Tools provide a performance metric to measure Tensor Core utilization on a scale from 0 (Idle) to 10 (Max) utilization.",4.357473561946546
"How can you use the nvprof profiler to measure Tensor Core utilization for a program running on a Summit node?
","NVIDIA’s Nsight Compute may also be used to measure tensor core utilization via the sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_active metric, as follows:

$ nv-nsight-cu-cli --metrics sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_active ./cudaTensorCoreGemm",4.320537842651645
"How do I know which parameters are available for use with ArgoCD?
",allow for better control of resources allocated to ArgoCD.,4.292516436893427
"How do I know which parameters are available for use with ArgoCD?
","The initial resources set by OpenShift GitOps should be sufficient to start working with ArgoCD. If needed, these may be increased should performance issues occur. On the right hand side is the schema for the ArgoCD custom resource listing all of the available parameters that could be used. At this point, no other parameters are needed to create an instance. However, if there are questions over a parameter or capability, please contact the Platforms Group.",4.24165860993729
"How do I know which parameters are available for use with ArgoCD?
",Image of ArgoCD new application general settings.,4.196728122786596
"How can I verify that my route has been created successfully in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.156377249779184
"How can I verify that my route has been created successfully in Slate?
","By default, a route will only expose your https://docs.olcf.ornl.gov/systems/route.html#slate_services to NCCS networks. If you need your service exposed to the world outside ORNL, you will first need to get your project approved for external routes. To do this, submit a systems ticket. In the description, give us your project name and a brief reasoning for why exposing externally is needed.

We will let you know once your project is able to set up external routes.",4.139711890227598
"How can I verify that my route has been created successfully in Slate?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.13300060684332
"How can I list all running pods in my Slate cluster?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.209326018651552
"How can I list all running pods in my Slate cluster?
","For example, let's look at service that was created in the https://docs.olcf.ornl.gov/systems/nodeport.html#slate_services document. This document assumes that it was deployed to the my-project project.

If you run oc get services, you should see your service in the list.

$ oc get services
NAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)     AGE
my-service            ClusterIP   172.25.170.246   <none>        8080/TCP    8s

Then, get some information about the service with oc describe service my-service.",4.181537879893566
"How can I list all running pods in my Slate cluster?
","You should now be on the 'Pod' screen with the 'Overview' tab selected From here you can get a quick idea of the amount of resources (memory, CPU etc) that your  Pod is using.

Click on the 'Logs' tab to get the logs from your pod. This will display ""Hello World!"" in our example because of our echo command. There will be a dropdown here that for our example will contain only one item named 'hello-openshift'. This is the name of the container that you are viewing the logs for inside your pod.",4.179027320666288
"How do I enable parallel processing in VisIt?
","Navigate to the appropriate file.

Once you choose a file, you will be prompted for the number of nodes and processors you would like to use (remember that each node of Andes contains 32 processors, or 28 if using the high-memory GPU partition) and the Project ID, which VisIt calls a ""Bank"" as shown below.



Once specified, the server side of VisIt will be launched, and you can interact with your data.",4.287120220211362
"How do I enable parallel processing in VisIt?
","Click ""Apply""

At the top menu click on ""Options""→""Save Settings""



Once you have VisIt installed and set up on your local computer:

Open VisIt on your local computer.

Go to: ""File→Open file"" or click the ""Open"" button on the GUI.

Click the ""Host"" dropdown menu on the ""File open"" window that popped up and choose ""ORNL_Andes"".

This will prompt you for your OLCF password, and connect you to Andes.

Navigate to the appropriate file.",4.1877455618112265
"How do I enable parallel processing in VisIt?
","the -s flag will launch the CLI in the form of a Python shell, which can be useful for interactive batch jobs.  The -np and -nn flags represent the number of processors and nodes VisIt will use to execute the Python script, while the -l flag specifies the specific parallel method to do so (required). Execute visit -fullhelp to get a list of all command line options.",4.185387292481714
"Why is it recommended to remove unnecessary paths from LD_LIBRARY_PATH in Frontier?
","Some libraries still resolved to paths outside of /mnt/bb, and the reason for that is that the executable may have several paths in RPATH.



Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-user-guide.",4.1468849906634935
"Why is it recommended to remove unnecessary paths from LD_LIBRARY_PATH in Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.100892158952497
"Why is it recommended to remove unnecessary paths from LD_LIBRARY_PATH in Frontier?
","# At minimum: prepend the node-local path to LD_LIBRARY_PATH to pick up the SBCAST libraries
# It is also recommended that you **remove** any paths that you don't need, like those that contain the libraries that you just SBCAST'd
# Failure to remove may result in unnecessary calls to stat shared file systems
export LD_LIBRARY_PATH=""/mnt/bb/$USER/${exe}_libs:${LD_LIBRARY_PATH}""",4.034603748350624
"Can I use the `xnack` flag with the `srun` command to specify the XNACK mode for Crusher?
","XNACK (pronounced X-knack) refers to the AMD GPU's ability to retry memory accesses that fail due to a page fault. The XNACK mode of an MI250X can be changed by setting the environment variable HSA_XNACK before starting a process that uses the GPU. Valid values are 0 (disabled) and 1 (enabled), and all processes connected to a GPU must use the same XNACK setting. The default MI250X on Crusher is HSA_XNACK=0.",4.20268919723293
"Can I use the `xnack` flag with the `srun` command to specify the XNACK mode for Crusher?
","Most applications that use ""managed"" or ""unified"" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Crusher. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.",4.167611396562173
"Can I use the `xnack` flag with the `srun` command to specify the XNACK mode for Crusher?
","If the HIP runtime cannot find a kernel image that matches the XNACK mode of the device, it will fail with hipErrorNoBinaryForGpu.

$ HSA_XNACK=0 srun -n 1 -N 1 -t 1 ./xnack_plus.exe
""hipErrorNoBinaryForGpu: Unable to find code object for all current devices!""
srun: error: crusher002: task 0: Aborted
srun: launch/slurm: _step_signal: Terminating StepId=74100.0

NOTE: This works in my shell because I used cpan to install the URI::Encode perl modules.
This won't work generically unless those get installed, so commenting out this block now.",4.1661636607561405
"How do Network Policies in Slate affect the performance of a project?
","Network policies are specifications of how groups of pods are allowed to communicate with each other and other network endpoints. A pod is selected in a Network Policy based on a label so the Network Policy can define rules to specify traffic to that pod.

When you create a namespace in Slate with the Quota Dashboard some Network Policies are added to your newly created namespace. This means that pods inside your project are isolated from connections not defined in these Network Policies.",4.298072254934906
"How do Network Policies in Slate affect the performance of a project?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.116268417836895
"How do Network Policies in Slate affect the performance of a project?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",4.106608095656604
"How can I access a service exposed by a pod in Slate?
","For example, let's look at service that was created in the https://docs.olcf.ornl.gov/systems/nodeport.html#slate_services document. This document assumes that it was deployed to the my-project project.

If you run oc get services, you should see your service in the list.

$ oc get services
NAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)     AGE
my-service            ClusterIP   172.25.170.246   <none>        8080/TCP    8s

Then, get some information about the service with oc describe service my-service.",4.315675871596484
"How can I access a service exposed by a pod in Slate?
","In general, using FQDNs to access a service is more convenient. If your service communicates over HTTP or HTTPS, you can set up a https://docs.olcf.ornl.gov/systems/services.html#slate_routes to achieve this. If your service uses another protocol, you can use https://docs.olcf.ornl.gov/systems/services.html#slate_nodeports.

NodePort is a type of Service that reserves a port across the cluster that can route traffic to your pods. This is more flexible than a Route and can handle any TCP or UDP traffic.",4.296073151282478
"How can I access a service exposed by a pod in Slate?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.217213796321227
"How can I search for a specific version of a module using the spider sub-command?
",| Command | Description | | --- | --- | | module spider | Shows the entire possible graph of modules | | module spider <modulename> | Searches for modules named <modulename> in the graph of possible modules | | module spider <modulename>/<version> | Searches for a specific version of <modulename> in the graph of possible modules | | module spider <string> | Searches for modulefiles containing <string> |,4.393132468110615
"How can I search for a specific version of a module using the spider sub-command?
",| Command | Description | | --- | --- | | module spider | Shows the entire possible graph of modules | | module spider <modulename> | Searches for modules named <modulename> in the graph of possible modules | | module spider <modulename>/<version> | Searches for a specific version of <modulename> in the graph of possible modules | | module spider <string> | Searches for modulefiles containing <string> |,4.393132468110615
"How can I search for a specific version of a module using the spider sub-command?
",| Command | Description | | --- | --- | | module spider | Shows the entire possible graph of modules | | module spider <modulename> | Searches for modules named <modulename> in the graph of possible modules | | module spider <modulename>/<version> | Searches for a specific version of <modulename> in the graph of possible modules | | module spider <string> | Searches for modulefiles containing <string> |,4.393132468110615
"What are the benefits of using Red Hat OpenShift GitOps on Slate?
","Versioned and Immutable: Desired state is stored in a way that enforces immutability, versioning and retains a complete version history.

Pulled Automatically: Software agents automatically pull the desired declarations from the source.

Continuously Reconciled: Software agents continuously observe actual system state and attempt to apply the desired state.

On Slate, Red Hat OpenShift GitOps is based on the open source ArgoCD project. For more information as well as how to install and use ArgoCD on Slate, see: https://docs.olcf.ornl.gov/systems/overview.html#slate_openshift_gitops.",4.440896057977438
"What are the benefits of using Red Hat OpenShift GitOps on Slate?
","From the release notes:

Red Hat OpenShift GitOps is a declarative way to implement continuous deployment for cloud native applications. Red Hat OpenShift GitOps ensures consistency in applications when you deploy them to different clusters in different environments, such as: development, staging, and production.",4.434759698775041
"What are the benefits of using Red Hat OpenShift GitOps on Slate?
","OpenShift Pipelines

A more recent offering on Slate, Red Hat OpenShift Pipelines is based on the open source Tekton project and provides a cloud native test, build and deployment framework fully integrated into the OpenShift Console. For details, see https://docs.olcf.ornl.gov/systems/overview.html#slate_openshift_pipelines.",4.431342531965296
"What is the number of GPUs available on each Crusher node?
","Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.",4.409979670605585
"What is the number of GPUs available on each Crusher node?
","Each Crusher compute node consists of [1x] 64-core AMD EPYC 7A53 ""Optimized 3rd Gen EPYC"" CPU (with 2 hardware threads per physical core) with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X, each with 2 Graphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can think of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth memory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU, allowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of 36+36 GB/s. The 2 GCDs on the same MI250X are connected with",4.34888217450067
"What is the number of GPUs available on each Crusher node?
","TERMINOLOGY:  The 8 GCDs contained in the 4 MI250X will show as 8 separate GPUs according to Slurm, ROCR_VISIBLE_DEVICES, and the ROCr runtime, so from this point forward in the quick-start guide, we will simply refer to the GCDs as GPUs.

Crusher node architecture diagram",4.326128154713194
"What is the purpose of specifying the version of Python in the conda create command?
","You can find the version of Python that exists in this base environment by executing:

$ python --version

Python 3.8.3

For this guide, you are going to install a different version of Python.

To do so, create a new environment using the conda create command:

$ conda create -p /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/py3711-summit python=3.7.11",4.29094565102025
"What is the purpose of specifying the version of Python in the conda create command?
","The OS-provided Python will no longer be accessible as python (including variations like /usr/bin/python or /usr/bin/env python); rather, you must specify it as python2 or python3. If you are using python from one of the modulefiles rather than the version in /usr/bin, this change should not affect how you invoke python in your scripts, although we encourage specifying python2 or python3 as a best practice.",4.185178185025297
"What is the purpose of specifying the version of Python in the conda create command?
","While default Python modules on OLCF systems are already set to Python 3, we recommend all users follow PEP394 by explicitly invoking either ‘python2’ or ‘python3’ instead of simply ‘python’. Python 2 Conda Environments and user installations of Python 2 will remain as options for using Python 2 on OLCF systems.

Official documentation for porting from Python 2 to Python3 can be found at: https://docs.python.org/3/howto/pyporting.html

General information and a list of open source packages dropping support for Python 2 can be found at: https://python3statement.org/",4.174682445193875
"What is the purpose of a management Web UI for viewing queries?
","Upon login to the web UI for Marble or Onyx, you will be on a projects page. In the navigation bar in the upper left corner, you should see either the ""Developer"" or ""Administrator"" perspective indicated. These two perspectives present two different ways of viewing resources deployed to the cluster. The Developer Perspective focuses on a Topology overview for the resources and will be used for the deployment of a GitLab Runner.",4.095410047837663
"What is the purpose of a management Web UI for viewing queries?
","When your job reaches the top of the queue, the main window will be returned to your control. At this point you are connected and can open files that reside there and visualize them interactively.",3.9911410735989294
"What is the purpose of a management Web UI for viewing queries?
","Spark also provides a web UI to monitor cluster, and you can access it on your local machine by port forwarding the master node to local machine.

For example, if master node is running on andes338, you can run the following code on your local machine terminal.

ssh -N <USERNAME>@andes-login1.olcf.ornl.gov -L 8080:andes338.olcf.ornl.gov:8080

Then access the Spark dashboard using address http://localhost:8080/ on a web browser on your local machine.

The spark documentation is very useful tool, go through it to find the Spark capabilities.",3.9710742729928254
"How do I move data to and from the NVMe storage devices on Frontier?
","Each compute node on Frontier has [2x] 1.92TB Non-Volatile Memory (NVMe) storage devices (SSDs), colloquially known as a ""Burst Buffer"" with a peak sequential performance of 5500 MB/s (read) and 2000 MB/s (write). The purpose of the Burst Buffer system is to bring improved I/O performance to appropriate workloads. Users are not required to use the NVMes. Data can also be written directly to the parallel filesystem.



The NVMes on Frontier are local to each node.",4.328762896562328
"How do I move data to and from the NVMe storage devices on Frontier?
","The NVMes on Frontier are local to each node.

To use the NVMe, users must request access during job allocation using the -C nvme option to sbatch, salloc, or srun. Once the devices have been granted to a job, users can access them at /mnt/bb/<userid>. Users are responsible for moving data to/from the NVMe before/after their jobs. Here is a simple example script:

#!/bin/bash
#SBATCH -A <projid>
#SBATCH -J nvme_test
#SBATCH -o %x-%j.out
#SBATCH -t 00:05:00
#SBATCH -p batch
#SBATCH -N 1
#SBATCH -C nvme

date",4.248925214935586
"How do I move data to and from the NVMe storage devices on Frontier?
","For more detailed information about center-wide file systems and data archiving available on Frontier, please refer to the pages on https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers. The subsections below give a quick overview of NFS, Lustre,and HPSS storage spaces as well as the on node NVMe ""Burst Buffers"" (SSDs).",4.23028146517181
"How many CPUs are allocated per node when running a job on Summit with the -r 2 option?
","This example simply extends Example 1 to run on 2 nodes, which simply requires changing the number of nodes to 2 (-N2) and the number of MPI ranks to 16 (-n16).

$ OMP_NUM_THREADS=7 srun -N2 -n16 -c7 --gpus-per-task=1 --gpu-bind=closest ./hello_jobstep | sort

In the following examples, 2 MPI ranks will be mapped to 1 GPU. For brevity, OMP_NUM_THREADS will be set to 1, so -c1 will be used unless otherwise specified. A new srun option will also be introduced to accomplish the new mapping:",4.278993320715014
"How many CPUs are allocated per node when running a job on Summit with the -r 2 option?
","So the job step (i.e., srun command) used above gave the desired output. Each MPI rank spawned 2 OpenMP threads and had access to a unique GPU. The --gpus-per-task=1 allocated 1 GPU for each MPI rank and the --gpu-bind=closest ensured that the closest GPU to each rank was the one used.

Example 2: 8 MPI ranks - each with 2 OpenMP threads and 1 GPU (multi-node)

This example will extend Example 1 to run on 2 nodes. As the output shows, it is a very straightforward exercise of changing the number of nodes to 2 (-N2) and the number of MPI ranks to 8 (-n8).",4.270875679079465
"How many CPUs are allocated per node when running a job on Summit with the -r 2 option?
","This example is an extension of Example 7 to use 2 compute nodes. Again, because it is using 4 physical CPU cores per task, core specialization needs to be disabled (-S 0). With the appropriate changes put in place in Example 7, it is a straightforward exercise to change to using 2 nodes (-N2) and 32 MPI ranks (-n32).

$ export OMP_NUM_THREADS=1
$ srun -N2 -n32 -c4 --ntasks-per-gpu=2 --gpu-bind=closest --distribution=*:block ./hello_jobstep | sort

Example 9: 4 independent and simultaneous job steps in a single allocation",4.264140845876731
"How do I authenticate to the Vampir server?
","Launch the Vampir GUI on your local machine

Similar to how we have connected Vampir to the VampirServer in the https://docs.olcf.ornl.gov/systems/Vampir.html#previous section, <vampauth> you will follow the same steps except you will use localhost for the server name and your local machine port number you selected. Press 'Connect' and this should open the authentication window where you will enter your UserID and the https://docs.olcf.ornl.gov/systems/Vampir.html#VampirServer password <vampserpw> printed after a successful connection.",4.487207562619648
"How do I authenticate to the Vampir server?
","When the server authentication window pops up, you will need to enter your USERID & the https://docs.olcf.ornl.gov/systems/Vampir.html#VampirServer password <vampserpw> that was printed on the terminal screen. Once authenticated, you will be able to navigate through the filesystem to your .otf2 files















































This connection method is more complex than the other 2 methods, however it also can provide a more optimal experience for very large trace files.",4.381478028323559
"How do I authenticate to the Vampir server?
","$ vampir &



Once the GUI has opened, you will need to connect to the https://docs.olcf.ornl.gov/systems/Vampir.html#VampirServer <vamps> using the Remote File option as shown below. If there is a 'recent files' window open, select 'open other'. Enter the node ID and the port number and press 'Connect'. Also, you will need to select Encrypted password from the Authentication dropdown option.",4.3460241597567775
"What is the name of the modulefile that contains the latest version of HPE/Cray Programming Environments (PE) on Crusher?
","The operating system was upgraded to Cray OS 2.4 based on SLES 15.4.

HPE/Cray Programming Environment (PE):

PE 22.12 was installed and is now default. This PE includes the following components:

Cray MPICH 8.1.23

Cray Libsci 22.12.1.1

CCE 15.0.0

PE 23.03 is now also available and includes:

Cray MPICH 8.1.25

Cray Libsci 23.02.1.1

CCE 15.0.1

ROCm:

ROCm 5.3.0 is now default.

ROCm 5.4.0 and 5.4.3 are available.

File Systems:

The Orion Lustre parallel file system is now available on Crusher.",4.378671876943748
"What is the name of the modulefile that contains the latest version of HPE/Cray Programming Environments (PE) on Crusher?
","The system was upgraded to Cray OS 2.5, Slingshot Host Software 2.0.2-112, and the AMD GPU 6.0.5 device driver (ROCm 5.5.1 release).

ROCm 5.5.1 is now available via the rocm/5.5.1 modulefile.

HPE/Cray Programming Environments (PE) 23.05 is now available via the cpe/23.05 modulefile.

HPE/Cray PE 23.05 introduces support for ROCm 5.5.1. However, due to issues identified during testing, ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.

On Wednesday, April 5, 2023, the Crusher TDS was upgraded to a new software stack.  A summary of the changes is included below.",4.35789302612308
"What is the name of the modulefile that contains the latest version of HPE/Cray Programming Environments (PE) on Crusher?
","On Tuesday, September 19, 2023, Crusher's system software was upgraded. The following changes took place:

The system was upgraded to Slingshot Host Software 2.1.0.

ROCm 5.6.0 and 5.7.0 are now available via the rocm/5.6.0 and rocm/5.7.0 modulefiles, respectively.

HPE/Cray Programming Environments (PE) 23.09 is now available via the cpe/23.09 modulefile.

ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.

On Tuesday, July 18, 2023, the Crusher TDS was upgraded to a new version of the system software stack. During the upgrade, the following changes took place:",4.319928326527121
"How can I ensure that my deployment on Slate is reverted if it fails?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.139819697368426
"How can I ensure that my deployment on Slate is reverted if it fails?
","It is assumed you have already gone through https://docs.olcf.ornl.gov/systems/minio.html#slate_getting_started and established the https://docs.olcf.ornl.gov/systems/minio.html#helm_prerequisite. Please do that before attempting to deploy anything on Slate's clusters.

This example uses Helm version 3 to deploy a MinIO standalone Helm chart on Slate's Marble Cluster. This is the cluster in OLCF's Moderate enclave, the same enclave as Summit.

To start, clone the slate helm examples repository , containing the MinIO standalone Helm chart, and navigate into the charts directory:",4.081315169284742
"How can I ensure that my deployment on Slate is reverted if it fails?
","Versioned and Immutable: Desired state is stored in a way that enforces immutability, versioning and retains a complete version history.

Pulled Automatically: Software agents automatically pull the desired declarations from the source.

Continuously Reconciled: Software agents continuously observe actual system state and attempt to apply the desired state.

On Slate, Red Hat OpenShift GitOps is based on the open source ArgoCD project. For more information as well as how to install and use ArgoCD on Slate, see: https://docs.olcf.ornl.gov/systems/overview.html#slate_openshift_gitops.",4.072810806773847
"How does the Allocation Overuse Policy affect the apparent submit time of a job?
","Allocation Overuse Policy

Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.485349992635105
"How does the Allocation Overuse Policy affect the apparent submit time of a job?
","number of processors requested above, this is an adjustment to the

apparent submit time of the job. However, this adjustment has the effect

of making jobs appear much younger than jobs submitted under projects

that have not exceeded their allocation. In addition to the priority

change, these jobs are also limited in the amount of wall time that can

be used. For example, consider that ``job1`` is submitted at the same

time as ``job2``. The project associated with ``job1`` is over its

allocation, while the project for ``job2`` is not. The batch system will",4.432893112546501
"How does the Allocation Overuse Policy affect the apparent submit time of a job?
","Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.4175386787440525
"How do I ensure that my h5py installation has MPI support?
","The HDF5_MPI flag is the key to telling pip to build h5py with parallel support, while the CC flag makes sure that you are using the correct C wrapper for MPI. This installation will take much longer than both the mpi4py and NumPy installations (5+ minutes if the system is slow). When the installation finishes, you will see a ""Successfully installed h5py"" message.

Test your build by trying to write an HDF5 file in parallel using 42 MPI tasks.

First, change directories to your GPFS scratch area:

$ cd $MEMBERWORK/<YOUR_PROJECT_ID>
$ mkdir h5py_test
$ cd h5py_test",4.435272048635735
"How do I ensure that my h5py installation has MPI support?
","Both HDF5 and h5py can be compiled with MPI support, which allows you to optimize your HDF5 I/O in parallel. MPI support in Python is accomplished through the mpi4py package, which provides complete Python bindings for MPI. Building h5py against mpi4py allows you to write to an HDF5 file using multiple parallel processes, which can be helpful for users handling large datasets in Python. h5Py is available after loading the default Python module on either Summit or Andes, but it has not been built with parallel support.",4.425404427611089
"How do I ensure that my h5py installation has MPI support?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.

This guide teaches you how to build a personal, parallel-enabled version of h5py and how to write an HDF5 file in parallel using mpi4py and h5py.

In this guide, you will:

Learn how to install mpi4py

Learn how to install parallel h5py

Test your build with Python scripts

OLCF Systems this guide applies to:

Summit

Andes

Frontier",4.3209920868576095
"Do I need to specify a machine file for VisIt to run on?
","Navigate to the appropriate file.

Once you choose a file, you will be prompted for the number of nodes and processors you would like to use (remember that each node of Andes contains 32 processors, or 28 if using the high-memory GPU partition) and the Project ID, which VisIt calls a ""Bank"" as shown below.



Once specified, the server side of VisIt will be launched, and you can interact with your data.",4.21139452008527
"Do I need to specify a machine file for VisIt to run on?
","Click ""Apply""

At the top menu click on ""Options""→""Save Settings""



Once you have VisIt installed and set up on your local computer:

Open VisIt on your local computer.

Go to: ""File→Open file"" or click the ""Open"" button on the GUI.

Click the ""Host"" dropdown menu on the ""File open"" window that popped up and choose ""ORNL_Andes"".

This will prompt you for your OLCF password, and connect you to Andes.

Navigate to the appropriate file.",4.1616441205424906
"Do I need to specify a machine file for VisIt to run on?
","VisIt uses a client-server architecture. You will obtain the best performance by running the VisIt client on your local computer and running the server on OLCF resources. VisIt for your local computer can be obtained here: VisIt Installation.

Recommended VisIt versions on our systems:

Summit: VisIt 3.1.4

Andes: VisIt 3.3.3

Frontier: VisIt 3.3.3

Using a different version than what is listed above is not guaranteed to work properly.",4.125181526109665
"How can I ensure that my Spack environment is using the correct version of a package?
","Alternatively, a user may install a package and its dependencies manually by:

$ spack install <my_app_dependencies@version%compiler>

## This may or may not add the spec to the spack.yaml depending on the Spack version being used.

For more information regarding Spack and its usage, please see the Spack documentation.

<string>:3: (INFO/1) Duplicate explicit target name: ""the spack 101 tutorial"".

For an extensive tutorial concerning Spack, go to the Spack 101 tutorial.

For more information concerning external packages, please see here.

Spack - package management tool",4.269603625962927
"How can I ensure that my Spack environment is using the correct version of a package?
","Traditionally, the user environment is modified by module files.  For example, a user would add use  module load cmake/3.18.2 to load CMake version 3.18.2 into their environment.  Using a Spack environment, a user can add an OLCF provided package and build against it using Spack without having to load the module file separately.

The information presented here is a subset of what can be found at the Spack documentation site.",4.253834538058499
"How can I ensure that my Spack environment is using the correct version of a package?
","Spack - package management tool

Spack 101 tutorial - Spack tutorial",4.239471727102926
"How many qubits are available in the Quantinuum System Model H1 and Model H2?
","Running a job on the System Model H1 family and System Model H2 hardware requires Quantinuum Credits. Additional information on credit usage can be found in the Quantinuum Systems User Guide under the Examples tab on the Quantinuum User Portal. Due to increased demand and to make the most efficient use of credits, the following allocating policy will go into effect starting October 1st 2022:

Any request for credits must be submitted by the project Principle Investigator (PI) to help@olcf.ornl.gov",4.320005854345255
"How many qubits are available in the Quantinuum System Model H1 and Model H2?
","Allocations will be granted on a monthly basis to maximize the availability of the H1 family and H2 machines. Please note that allocations do not carry over to the next month and must be consumed in the month granted.

Allocation requests requiring 20 qubits and under will be considered for H1 family machines, and allocation requests requiring 21-32 qubits will be considered for H2.

Allocation requests for the following month must be submitted no later than the 25th of the preceding month.  The uptime schedule is available on the Calendar tab of the Quantinuum User Portal.",4.30622416469497
"How many qubits are available in the Quantinuum System Model H1 and Model H2?
","Information on submitting jobs to Quantinuum systems, system availability, checking job status, and tracking usage can be found in the Quantinuum Systems User Guide under the Examples tab on the Quantinuum User Portal.

Users have access to the API validator to check program syntax, and to the Quantinuum System Model H1 emulator, which returns actual results back as if users submitted code to the real quantum hardware.",4.3025615751770445
"What are the criteria for selecting proposals for SummitPLUS?
","SummitPLUS is one of the new allocation programs that will be used to allocate a significant portion of the system for 2024. The program is open to researchers from academia, government laboratories, federal agencies, and industry. We welcome proposals for computationally ready projects from investigators who are new to Summit, as well as from previous INCITE, ALCC, DD, ECP awardees and projects. We encourage proposals on emerging paradigms for computational campaigns including data-intensive science and AI/ML.",4.351698917563717
"What are the criteria for selecting proposals for SummitPLUS?
","Individuals or teams interested in SummitPLUS must submit a proposal through the https://my.olcf.ornl.gov/ portal from September 19 to October 30.  Once on the portal, go to “New Accounts” across the top and then “Project Application”. Choose the SummitPLUS form from the dropdown list.

Summit will continue to be allocated in node hours, and a typical SummitPLUS award will be between 100,000 – 250,000 node hours. The proposals will undergo review and the OLCF will notify awardees in mid-to-late November. Projects are anticipated to start in mid-to-late January 2024.

Timeline",4.346518860669958
"What are the criteria for selecting proposals for SummitPLUS?
",For Summit:,4.1900112513884125
"Load the cray-python module and activate the virtual environment.
",".. note::
   If you are using a `https://docs.olcf.ornl.gov/software/python/miniconda.html`, the above ``module load cray-python`` should not be loaded.

Andes

.. code-block:: bash

   $ module load gcc/9.3.0 # works with older GCC versions if using cuda/10.2.89
   $ module load cuda/11.0.2
   $ module load python

Loading a python module puts you in a ""base"" environment, but you need to create a new environment using the conda create command (Summit and Andes) or the venv command (Frontier):

Summit

.. code-block:: bash",4.309584455642782
"Load the cray-python module and activate the virtual environment.
","Andes

.. code-block:: bash

    $ module load python
    $ source activate base
    $ conda create -n ENV_NAME python=3.9 # pyQuil requires Python version 3.7, 3.8, or 3.9
    $ conda activate ENV_NAME
    $ pip install pyquil --no-cache-dir

Frontier

.. code-block:: bash

    $ module load cray-python
    $ python3 -m venv ENV_NAME
    $ source ENV_NAME/bin/activate
    $ python3 -m pip install pyquil --no-cache-dir

Now that everything is installed properly, the rest of the instructions follow Rigetti's Documentation .",4.270149617608848
"Load the cray-python module and activate the virtual environment.
","Users select or supply generator and simulator functions to express their ensembles; the generator typically steers the ensemble based on prior simulator results. Such functions can also launch and monitor external executables at any scale.

Begin by loading the python module:

$ module load cray-python

libEnsemble is available on PyPI, conda-forge, the xSDK, and E4S. Most users install libEnsemble via pip:

$ pip install libensemble",4.197332570733659
"How can I achieve maximum bandwidth for my MPI application?
","The intent with both of the following examples is to launch 8 MPI ranks across the node where each rank is assigned its own logical (and, in this case, physical) core.  Using the -m distribution flag, we will cover two common approaches to assign the MPI ranks -- in a ""round-robin"" (cyclic) configuration and in a ""packed"" (block) configuration. Slurm's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive method was used to request an allocation of 1 compute node for these examples: salloc -A <project_id> -t 30 -p <parition> -N 1",4.207461056889881
"How can I achieve maximum bandwidth for my MPI application?
","As mentioned previously, all GPUs are accessible by all MPI ranks by default, so it is possible to programatically map any combination of MPI ranks to GPUs. However, there is currently no way to use Slurm to map multiple GPUs to a single MPI rank. If this functionality is needed for an application, please submit a ticket by emailing help@olcf.ornl.gov.

There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_jobstep program and test whether or not processes and threads are running where intended.",4.191226009479367
"How can I achieve maximum bandwidth for my MPI application?
","Because a Frontier compute node has two hardware threads available (2 logical cores per physical core), this enables the possibility of multithreading your application (e.g., with OpenMP threads). Although the additional hardware threads can be assigned to additional MPI tasks, this is not recommended. It is highly recommended to only use 1 MPI task per physical core and to use OpenMP threads instead on any additional logical cores gained when using both hardware threads.",4.189449759763688
"Can I use the Burst Buffers on Summit for data archiving?
","Each compute node on Summit has a 1.6TB Non-Volatile Memory (NVMe) storage device (high-memory nodes have a 6.4TB NVMe storage device), colloquially known as a ""Burst Buffer"" with theoretical performance peak of 2.1 GB/s for writing and 5.5 GB/s for reading. The NVMes could be used to reduce the time that applications wait for I/O.  More information can be found later in the Burst Buffer section.



<string>:208: (INFO/1) Duplicate implicit target name: ""software"".",4.232023725052464
"Can I use the Burst Buffers on Summit for data archiving?
","The https://docs.olcf.ornl.gov/systems/summit_user_guide.html#OLCF Training Archive<training-archive> provides a list of previous training events, including multi-day Summit Workshops. Some examples of topics addressed during these workshops include using Summit's NVME burst buffers, CUDA-aware MPI, advanced networking and MPI, and multiple ways of programming multiple GPUs per node. You can also find simple tutorials and code examples for some common programming and running tasks in our Github tutorial page .",4.170800706858017
"Can I use the Burst Buffers on Summit for data archiving?
",| (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/06/20200603_summit_workshop_python.pdf https://vimeo.com/427794043 | | 2020-06-03 | NVMe - Burst Buffers (Part2) | George Markomanolis (OLCF) | 2020 OLCF User Meeting (Summit New User Training) https://www.olcf.ornl.gov/calendar/2020-olcf-user-meeting/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2020/02/Burst_Buffer_summit_june_2020.pdf https://vimeo.com/427792243 | | 2020-06-03 | NVMe - Burst Buffers (Part1) | Chris Zimmer (OLCF) | 2020 OLCF User Meeting (Summit New User Training),4.155085081553246
"What are the SPI filesystems?
","https://docs.olcf.ornl.gov/systems/index.html#SPI resources mount SPI filesystems<spi-file-systems>.  The SPI resources do not mount the non-SPI's scratch filesystems, home areas, or mass storage.

https://docs.olcf.ornl.gov/systems/index.html#SPI compute resources cannot access external resources<spi-data-transfer>.  Needed data must be transferred to the SPI resources through the SPI's DTN.

https://docs.olcf.ornl.gov/systems/index.html#The Citadel login nodes<spi-compute-citadel> and batch queues must be used to access Summit and Frontier for SPI workflows.",4.373590119976197
"What are the SPI filesystems?
","https://docs.olcf.ornl.gov/systems/index.html#Transfer needed data<spi-data-transfer> to the SPI filesystems.  The SPI resources mount filesystems unique to the SPI.  Needed data, code, and libraries must be transferred into the SPI using the SPI's Data Transfer Nodes.",4.361399297510358
"What are the SPI filesystems?
","Available filesystems:

| Name | Location | Purpose | | --- | --- | --- | | Home | /gpfs/arx/<proj>/home/<userid> | Your login/home directory.  Used to store small scripts and source. | | Project Shared | /gpfs/arx/<proj>/proj-shared | Location to share data with others in your project. | | Scratch | /gpfs/arx/<proj>/scratch/<userid> | Location to store compute job I/O. |

SPI resources do not mount filesystems accessible from non-SPI resources.  SPI resources only mount the GPFS Arx filesytem.



Globus is the best option to transfer data into and out of the SPI resources.",4.316145310748167
"How do I compile with OpenMP Offload using the GCC compiler?
","This section shows how to compile with OpenMP Offload using the different compilers covered above.

Make sure the craype-accel-amd-gfx90a module is loaded when using OpenMP offload.

| Vendor | Module | Language | Compiler | OpenMP flag (GPU) | | --- | --- | --- | --- | --- | | Cray | cce | C C++ |  | -fopenmp | | Fortran | ftn (wraps crayftn) |  | | AMD | amd |  |  | -fopenmp |",4.42943460307541
"How do I compile with OpenMP Offload using the GCC compiler?
","-I${MPICH_DIR}/include
-L${MPICH_DIR}/lib -lmpi -L${CRAY_MPICH_ROOTDIR}/gtl/lib -lmpi_gtl_hsa

This section shows how to compile with OpenMP using the different compilers covered above.

| Vendor | Module | Language | Compiler | OpenMP flag (CPU thread) | | --- | --- | --- | --- | --- | | Cray | cce | C, C++ |  | -fopenmp | | Fortran | ftn |  | | AMD | rocm |  |  | -fopenmp | | GCC | gcc |  |  | -fopenmp |

This section shows how to compile with OpenMP Offload using the different compilers covered above.

Make sure the craype-accel-amd-gfx908 module is loaded when using OpenMP offload.",4.419780031678913
"How do I compile with OpenMP Offload using the GCC compiler?
","| Vendor | Module | Language | Compiler | OpenMP flag (CPU thread) | | --- | --- | --- | --- | --- | | Cray | cce | C, C++ |  | -fopenmp | | Fortran | ftn (wraps crayftn) |  | | AMD | rocm |  |  | -fopenmp | | GCC | gcc |  |  | -fopenmp |

This section shows how to compile with OpenMP Offload using the different compilers covered above.

Make sure the craype-accel-amd-gfx90a module is loaded when using OpenMP offload.",4.40408297541525
"Who should I contact to request a purge exemption?
","on purge timeframes for each storage area, if applicable.",4.091024768950008
"Who should I contact to request a purge exemption?
","Increased disk quota

Purge exemption for User/Group/World Work areas

Software requests



Closeout Report Template Use this template if you have been asked to submit a closeout report for your project.  Note this form does not apply to INCITE projects.  If you have been provided a template via email, only that template applies.

Industry Quarterly Report Template Use this template if you have an industry project to submit a quarterly report.



Director's Discretion Review Form For internal use only.",4.076549742529644
"Who should I contact to request a purge exemption?
","If you need an exception to the limits listed in the table above, such as a higher quota in your User/Project Home or a purge exemption in a Member/Project/World Work area, contact help@olcf.ornl.gov with a summary of the exception that you need.

By default, the OLCF does not guarantee lifetime data retention on any OLCF resources. Following a user account deactivation or project end, user and project data in non-purged areas will be retained for 90 days. After this timeframe, the OLCF retains the right to delete data. Data in purged areas remains subject to normal purge policies.",4.0457004348455055
"How do I request storage for my application?
","From the main screen of a project go to the Storage option in the hamburger menu on the left hand side of the screen, then click Persistent Volume Claims

Persistent Volume Claim Menu

In the upper right click the Create Persistent Volume Claim button.

Create Storage

This will take you to a screen that allows you to select what you need for your PVC. Give your claim a name and request an amount of storage and click Create. You can also click Edit YAML to edit the PVC object directly.

PV Settings",4.188279614112573
"How do I request storage for my application?
","Although there are no hard quota limits for the project storage, an upper storage limit should be reported in the project request. The available space of a project can be modified upon request.",4.177526180304989
"How do I request storage for my application?
","Users interested in sharing files publicly via the World Wide Web can request a user website directory be created for their account. User website directories (~/www) have a 5GB storage quota and allow access to files at http://users.nccs.gov/~user (where user is your userid). If you are interested in having a user website directory created, please contact the User Assistance Center at help@olcf.ornl.gov.",4.114649250462087
"Can you explain the expected behavior of jsrun for Summit when the -l flag is given a capitalized value?
","Expected behavior:

When capitalized, jsrun should not compromise on the resource layout, and will wait to begin the job step until the ideal resources are available. When given a lowercase value, jsrun will not wait, but initiate the job step with the most ideal layout as is available at the time. This also means that when there's no resource contention, such as running a single job step at a time, capitalization should not matter, as they should both yield the same resources.

Actual behavior:",4.281034072946412
"Can you explain the expected behavior of jsrun for Summit when the -l flag is given a capitalized value?
","This section describes tools that users might find helpful to better understand the jsrun job launcher.

hello_jsrun is a ""Hello World""-type program that users can run on Summit nodes to better understand how MPI ranks and OpenMP threads are mapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A screencast showing how to use Hello_jsrun is also available: https://vimeo.com/261038849

Job Step Viewer provides a graphical view of an application's runtime layout on Summit. It allows users to preview and quickly iterate with multiple jsrun options to understand and optimize job launch.",4.153240208567567
"Can you explain the expected behavior of jsrun for Summit when the -l flag is given a capitalized value?
","Summit has a node hierarchy that can be very confusing for the uninitiated. The Summit User Guide explains this in depth. This has some consequences that may be unusual for R programmers. A few important ones to note are:

You must have a script that you can run in batch, e.g. with Rscript or R CMD BATCH

All data that needs to be visible to the R process (including the script and your packages) must be on gpfs (not your home directory!).

You must launch your script from the launch nodes with jsrun.

We'll start with something very simple. The script is:

""hello world""",4.152109470042132
"How do I submit a job to Frontier's Slurm scheduler?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.57267739137021
"How do I submit a job to Frontier's Slurm scheduler?
","To override this default layout (not recommended), set -S 0 at job allocation.



Frontier uses SchedMD's Slurm Workload Manager for scheduling and managing jobs. Slurm maintains similar functionality to other schedulers such as IBM's LSF, but provides unique control of Frontier's resources through custom commands and options specific to Slurm. A few important commands can be found in the conversion table below, but please visit SchedMD's Rosetta Stone of Workload Managers for a more complete conversion reference.",4.403808950611837
"How do I submit a job to Frontier's Slurm scheduler?
","Slurm provides 3 ways of submitting and launching jobs on Crusher's compute nodes: batch scripts, interactive, and single-command. The Slurm commands associated with these methods are shown in the table below and examples of their use can be found in the related subsections. Please note that regardless of the submission method used, the job will launch on compute nodes, with the first compute in the allocation serving as head-node.

| sbatch |  | | --- | --- | | salloc |  | | srun |  |",4.342451234490162
"What is the advantage of using pbdR for high performance computing?
","For parallelism, you should use pbdR packages, Rmpi directly, or an interface which can use Rmpi as a backend. We address GPUs specifically next.

There are some R packages which can use GPUs, such as xgboost. There is also the gpuR series of packages. Several pbdR packages support GPU computing. It is also possible to offload some linear algebra computations (specifically matrix-matrix products, and methods which are computationally dominated by them) to the GPU using NVIDIA’s NVBLAS.",4.252876331904483
"What is the advantage of using pbdR for high performance computing?
","If you want to do GPU computing on Summit with R, we would love to collaborate with you (see contact details at the bottom of this document).

For more information about using R and pbdR effectively in an HPC environment such as Summit, please see the R and pbdR articles. These are long-form articles that introduce HPC concepts like MPI programming in much more detail than we do here.

Also, if you want to use R and/or pbdR on Summit, please feel free to contact us directly:

Mike Matheson - mathesonma AT ornl DOT gov

George Ostrouchov - ostrouchovg AT ornl DOT gov",4.220314797159402
"What is the advantage of using pbdR for high performance computing?
",to using the HBM since the data is in the compute unit itself.,4.085083213403571
"How can I set up my environment to use Qiskit and PennyLane?
","PennyLane is a cross-platform Python library for programming quantum computers.  Its differentiable programming paradigm enables the execution and training of quantum programs on various backends.

General information of how to install and use PennyLane can be found here:

https://docs.pennylane.ai/en/stable/introduction/pennylane.html

https://pennylane.ai/qml/demos_getting-started.html

https://pennylane.ai/install.html

On our systems, the install method is relatively simple:

Andes

.. code-block:: bash",4.310025688149043
"How can I set up my environment to use Qiskit and PennyLane?
","Currently, the install steps listed below only work for our x86_64 based systems (Andes, Frontier, etc.). The steps can be explored on Summit, but -- due to Summit's Power architecture -- is not recommended or guaranteed to work.

Both Qiskit and pyQuil can live in the same Python environment if desired. However, as this may not always be the case, it is highly recommened to use separate environments if possible or test if packages still function after modifying your environment.",4.266526963660974
"How can I set up my environment to use Qiskit and PennyLane?
","As opposed to using IBM's JupyterLab server (described in https://docs.olcf.ornl.gov/systems/ibm_quantum.html#Cloud Access <ibm-cloud> above), users are able to install IBM Quantum Qiskit locally via two methods:

Installing manually: https://qiskit.org/documentation/stable/0.24/install.html. This option allows for building locally and executing jobs via a python virtual environment.

Docker: https://www.ibm.com/cloud/learn/docker or https://hub.docker.com/u/ibmq",4.207884789297283
"How can I optimize memory usage for my application on Frontier?
","Most applications that use ""managed"" or ""unified"" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Frontier. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.",4.23049190083136
"How can I optimize memory usage for my application on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.193662369060624
"How can I optimize memory usage for my application on Frontier?
","The AMD MI250X and operating system on Frontier supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Frontier. In the following section we'll discuss how the heterogenous memory space on a Frontier node is surfaced within your application.",4.184109668203693
"How can I see all of my jobs since 1 PM on July 1, 2022 using a particular output format?
","sleep 1
    done

wait

Output:

Fri 02 Jun 2023 03:33:45 PM EDT
    Fri 02 Jun 2023 03:33:46 PM EDT
    Fri 02 Jun 2023 03:33:47 PM EDT
    Fri 02 Jun 2023 03:33:48 PM EDT
    Fri 02 Jun 2023 03:33:49 PM EDT
    Fri 02 Jun 2023 03:33:50 PM EDT
    Fri 02 Jun 2023 03:33:51 PM EDT
    Fri 02 Jun 2023 03:33:52 PM EDT",4.13681452491721
"How can I see all of my jobs since 1 PM on July 1, 2022 using a particular output format?
","| sacct -a -X | Show all jobs (-a) in the queue, but summarize the whole allocation instead of showing individual steps (-X) | | --- | --- | | sacct -u $USER | Show all of your jobs, and show the individual steps (since there was no -X option) | | sacct -j 12345 | Show all job steps that are part of job 12345 | | sacct -u $USER -S 2022-07-01T13:00:00 -o ""jobid%5,jobname%25,nodelist%20"" -X | Show all of your jobs since 1 PM on July 1, 2022 using a particular output format |",4.132104624778287
"How can I see all of my jobs since 1 PM on July 1, 2022 using a particular output format?
","+========================+======================+==========================+==================+

| < 100%                 | 0 days               | 4 jobs                   | unlimited jobs   |

+------------------------+----------------------+--------------------------+------------------+

| 100% to 125%           | 30 days              | 4 jobs                   | unlimited jobs   |

+------------------------+----------------------+--------------------------+------------------+

| > 125%                 | 365 days             | 4 jobs                   | 1 job            |",4.084203191240813
"What is the date and time of the example given in the information provided?
",for more information.,4.003386552402855
"What is the date and time of the example given in the information provided?
","sleep 1
    done

wait

Output:

Fri 02 Jun 2023 03:33:45 PM EDT
    Fri 02 Jun 2023 03:33:46 PM EDT
    Fri 02 Jun 2023 03:33:47 PM EDT
    Fri 02 Jun 2023 03:33:48 PM EDT
    Fri 02 Jun 2023 03:33:49 PM EDT
    Fri 02 Jun 2023 03:33:50 PM EDT
    Fri 02 Jun 2023 03:33:51 PM EDT
    Fri 02 Jun 2023 03:33:52 PM EDT",3.967679521376231
"What is the date and time of the example given in the information provided?
","The output shows that each independent process ran on its own CPU core and GPU on the same single node. To show that the ranks ran simultaneously, date was called before each job step and a 20 second sleep was added to the end of the hello_jobstep program. So the output also shows that the first job step was submitted at :45 and the subsequent job steps were all submitted between :46 and :52. But because each hello_jobstep sleeps for 20 seconds, the subsequent job steps must have all been running while the first job step was still sleeping (and holding up its resources). And the same argument",3.958068290784846
"Can you give an example of an API call that can be used to request migration of a memory region on Crusher?
","Most applications that use ""managed"" or ""unified"" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Crusher. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.",4.195375427993427
"Can you give an example of an API call that can be used to request migration of a memory region on Crusher?
","The AMD MI250X and operating system on Crusher supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Crusher. In the following section we'll discuss how the heterogenous memory space on a Crusher node is surfaced within your application.",4.103112275475802
"Can you give an example of an API call that can be used to request migration of a memory region on Crusher?
","The page migration behavior summarized by the following tables represents the current, observable behavior. Said behavior will likely change in the near future.  CPU accesses to migratable memory may behave differently than other platforms you're used to. On Crusher, pages will not migrate from GPU HBM to CPU DDR4 based on access patterns alone. Once a page has migrated to GPU HBM it will remain there even if the CPU accesses it, and all accesses which do not resolve in the CPU cache will occur over the Infinity Fabric between the AMD ""Optimized 3rd Gen EPYC"" CPU and AMD MI250X GPU. Pages",4.084606967180494
"How do I obtain the clacc compiler on Frontier?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers. Please see the following https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more detailed inforation on how to compile using these modules.",4.370155187316413
"How do I obtain the clacc compiler on Frontier?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers.

It is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) whenever possible. See the next section for more details.",4.300677316208701
"How do I obtain the clacc compiler on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.269499565606481
"Is there a limit to the number of jobs that can run at the same time for a project that is not over its allocation?
","same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. Additionally, projects that are at 125% of their allocated time will be limited to only 3 running jobs at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.416741702705852
"Is there a limit to the number of jobs that can run at the same time for a project that is not over its allocation?
","same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. Additionally, projects that are at 125% of their allocated time will be limited to only 3 running jobs at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.416741702705852
"Is there a limit to the number of jobs that can run at the same time for a project that is not over its allocation?
","For example, consider that job1 is submitted at the same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. In addition, projects that are at 125% of their allocated time will be limited to only one running job at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.371048157402412
"How do I access Job Step Viewer?
","This section describes tools that users might find helpful to better understand the jsrun job launcher.

hello_jsrun is a ""Hello World""-type program that users can run on Summit nodes to better understand how MPI ranks and OpenMP threads are mapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A screencast showing how to use Hello_jsrun is also available: https://vimeo.com/261038849

Job Step Viewer provides a graphical view of an application's runtime layout on Summit. It allows users to preview and quickly iterate with multiple jsrun options to understand and optimize job launch.",4.124173942417409
"How do I access Job Step Viewer?
","For bug reports or suggestions, please email help@olcf.ornl.gov.

Request a Summit allocation

bsub -W 10 -nnodes 2 -P $OLCF_PROJECT_ID -Is $SHELL

Load the job-step-viewer module

module load job-step-viewer

Test out a jsrun line by itself, or provide an executable as normal

jsrun -n12 -r6 -c7 -g1 -a1 -EOMP_NUM_THREADS=7 -brs

Visit the provided URL

https://jobstepviewer.olcf.ornl.gov/summit/871957-1",4.089277041432197
"How do I access Job Step Viewer?
","Provides additional details of given job.

The sview tool provide a graphical queue monitoring tool. To use, you will need an X server running on your local system. You will also need to tunnel X traffic through your ssh connection:

local-system> ssh -Y username@andes.ccs.ornl.gov
andes-login> sview",4.083653128746364
"What is the latest version of Horovod available in the open-ce/1.5.2 environment?
",| PyTorch 1.10.2 https://github.com/open-ce/pytorch-feedstock | | Horovod 0.21.0 (NCCL Backend) https://github.com/horovod/horovod | Horovod 0.22.1 (NCCL Backend) https://github.com/horovod/horovod | Horovod 0.23.0 (NCCL Backend) https://github.com/horovod/horovod | Horovod 0.23.0 (NCCL Backend) https://github.com/horovod/horovod | | Complete List | 1.2.0 Software Packages https://github.com/open-ce/open-ce/releases/tag/open-ce-v1.2.0 | 1.4.0 Software Packages https://github.com/open-ce/open-ce/releases/tag/open-ce-v1.4.0 | 1.5.0 Software Packages,4.257835002874265
"What is the latest version of Horovod available in the open-ce/1.5.2 environment?
","On Tuesday, July 18, 2023, Frontier was upgraded to a new version of the system software stack. During the upgrade, the following changes took place:

The system was upgraded to Cray OS 2.5, Slingshot Host Software 2.0.2-112, and the AMD GPU 6.0.5 device driver (ROCm 5.5.1 release).

ROCm 5.5.1 is now available via the rocm/5.5.1 modulefile.

HPE/Cray Programming Environments (PE) 23.05 is now available via the cpe/23.05 modulefile.

HPE/Cray PE 23.05 introduces support for ROCm 5.5.1. However, due to issues identified during testing, ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.",4.1013682525919934
"What is the latest version of Horovod available in the open-ce/1.5.2 environment?
","Horovod comes with a tool called Timeline which can help analyze the performance of Horovod. This is particularly useful when trying to scale a deep learning job to many nodes. The Timeline tool can help pick various options that can improve the performance of distributed deep learning jobs that are using Horovod. For more information, please see Horovod's documentation.",4.097866313844996
"How can I transfer files between the local machine and the Andes cluster?
","When the installation has finished, click on the Globus icon and select Web: Transfer Files as below



Globus will ask you to login. If your institution does not have an organizational login, you may choose to either Sign in with Google or Sign in with ORCiD iD.



In the main Globus web page, select the two-panel view, then set the source and destination endpoints. (Left/Right order does not matter)



Next, navigate to the appropriate source and destination paths to select the files you want to transfer. Click the ""Start"" button to begin the transfer.",4.3164556842766775
"How can I transfer files between the local machine and the Andes cluster?
","Please see the https://docs.olcf.ornl.gov/systems/your_file.html#batch-queues-on-andes section to learn about the queuing policies for these two partitions. Both compute partitions are accessible through the same batch queue from Andes’s https://docs.olcf.ornl.gov/systems/your_file.html#andes-login-nodes.

Andes features a S8500 Series HDR Infiniband interconnect, with a maximum theoretical transfer rate of 200 Gb/s.



Login nodes",4.2019579392660935
"How can I transfer files between the local machine and the Andes cluster?
","Navigate to the appropriate file.

Once you choose a file, you will be prompted for the number of nodes and processors you would like to use (remember that each node of Andes contains 32 processors, or 28 if using the high-memory GPU partition) and the Project ID, which VisIt calls a ""Bank"" as shown below.



Once specified, the server side of VisIt will be launched, and you can interact with your data.",4.185877640355929
"What constitutes suspected abuse/misuse of computational resources on OLCF systems?
","authorized individual or investigative agency is in effect during the period of your access to information on a DOE computer and for a period of three years thereafter. OLCF personnel and users are required to address, safeguard against, and report misuse, abuse and criminal activities. Misuse of OLCF resources can lead to temporary or permanent disabling of accounts, loss of DOE allocations, and administrative or legal actions. Users who have not accessed a OLCF computing resource in at least 6 months will be disabled. They will need to reapply to regain access to their account. All users",4.425756306917198
"What constitutes suspected abuse/misuse of computational resources on OLCF systems?
","The OLCF provides a comprehensive suite of hardware and software resources for the creation, manipulation, and retention of scientific data. This document comprises guidelines for acceptable use of those resources. It is an official policy of the OLCF, and as such, must be agreed to by relevant parties as a condition of access to and use of OLCF computational resources.",4.381452838159046
"What constitutes suspected abuse/misuse of computational resources on OLCF systems?
",The Oak Ridge Leadership Computing Facility (OLCF) computing resources are provided to users for research purposes. All users must agree to abide by all security measures described in this document. Failure to comply with security procedures will result in termination of access to OLCF computing resources and possible legal actions.,4.374144097878048
"Can we use other utilities in addition to ImageMagick's convert utility to process the image data?
","Next, we have the data processing script called processdata.sh that looks as follows:

#!/bin/bash
set -eu

TASK=convert
DATA=$1
echo ""\nProcessing ${DATA}\n""
${TASK} ${DATA} -fuzz 10% -fill white -opaque white -fill black +opaque white -format ""%[fx:100*mean]"" info:
sleep 5

The above script computes the cloud cover percentage by looking at the amount of white pixels in the image. Note that it uses ImageMagick's convert utility.",4.13414615536251
"Can we use other utilities in addition to ImageMagick's convert utility to process the image data?
","$ module load imagemagick # for convert utility
$ export WALLTIME=00:10:00
$ export PROJECT=STF019
$ export TURBINE_OUTPUT=/gpfs/alpine/scratch/ketan2/stf019/swift-work/cross-facility/data
$ swift-t -O0 -m lsf workflow.swift

If all goes well, and when the job starts running, the output will be produced in the data directory output.txt file.",3.954458636003032
"Can we use other utilities in addition to ImageMagick's convert utility to process the image data?
","Users can build container images with Podman, convert it to a SIF file, and run the container using the Singularity runtime. This page is intended for users with some familiarity with building and running containers.

Users will make use of two applications on Summit - Podman and Singularity - in their container build and run workflow. Both are available without needing to load any modules.",3.921134582614868
"What programming languages are supported by Rigetti's QCS?
","Rigetti currently offers access to their systems via their Quantum Cloud Services (QCS).  With QCS, Rigetti's quantum processors (QPUs) are tightly integrated with classical computing infrastructure and made available to you over the cloud. Rigetti also provides users with quantum computing example algorithms for optimization, quantum system profiling, and other applications.

A list of available Rigetti systems/QPUs, along with their performance statistics, can be found on the Rigetti Systems Page.",4.374217704707762
"What programming languages are supported by Rigetti's QCS?
","accessing the hybrid infrastructure of available quantum processors and classical computational framework via the cloud. From the QCS, users can view system status and availability, initiate and manage quantum infrastructure reservations (either executing programs manually or adding them to the queue). Information on using this resource is available on the Rigetti's Documentation or our https://docs.olcf.ornl.gov/quantum/quantum_systems/rigetti.html.",4.344328373716225
"What programming languages are supported by Rigetti's QCS?
","Rigetti provides system access via a cloud-based JupyterLab development environment: https://docs.rigetti.com/qcs/getting-started/jupyterlab-ide.  From there, users can access a JupyterLab server loaded with Rigetti's PyQuil programming framework, Rigetti's Forest Software Development Kit, and associated program examples and tutorials.  This is the method that allows access to Rigetti's QPU's directly, as opposed to simulators.",4.331951591339241
"How can I use Globus to transfer data to and from SPI resources?
","The SPI provides separate https://docs.olcf.ornl.gov/systems/index.html#Data Transfer Nodes<spi-data-transfer> configured specifically for SPI workflows.  The nodes are not directly accessible for login but are accessible through the Globus tool.  The SPI DTNs mount the same Arx filesystem available on the SPI compute resources.  Globus is the preferred method to transfer data into and out of the SPI resources.

Please see the https://docs.olcf.ornl.gov/systems/index.html#Data Transfer Nodes<spi-data-transfer> section for more details.",4.416196435047341
"How can I use Globus to transfer data to and from SPI resources?
","The SPI Data Transfer Nodes are not directly accessible, but can be used through Globus to transfer data.

A simple example using the CLI:

myproxy-logon -T -b -l usera_prj123_mde
globus-url-copy -cred /gpfs/arx/prj123_mde/home/usera_prj123_mde/dataA -dcpriv -list",4.401079666569332
"How can I use Globus to transfer data to and from SPI resources?
","When the installation has finished, click on the Globus icon and select Web: Transfer Files as below



Globus will ask you to login. If your institution does not have an organizational login, you may choose to either Sign in with Google or Sign in with ORCiD iD.



In the main Globus web page, select the two-panel view, then set the source and destination endpoints. (Left/Right order does not matter)



Next, navigate to the appropriate source and destination paths to select the files you want to transfer. Click the ""Start"" button to begin the transfer.",4.353592209551043
"How can I ensure that my application returns accurate results when using hardware-based FP atomics on a MI250X processor?
","The fast hardware-based Floating point (FP) atomic operations available on MI250X are assumed to be working on coarse grained memory regions; when these instructions are applied to a fine-grained memory region, they will silently produce a no-op. To avoid returning incorrect results, the compiler never emits hardware-based FP atomics instructions by default, even when applied to coarse grained memory regions. Currently, users can use the -munsafe-fp-atomics flag to force the compiler to emit hardware-based FP atomics. Using hardware-based FP atomics translates in a substantial performance",4.484729288326739
"How can I ensure that my application returns accurate results when using hardware-based FP atomics on a MI250X processor?
","The fast hardware-based Floating point (FP) atomic operations available on MI250X are assumed to be working on coarse grained memory regions; when these instructions are applied to a fine-grained memory region, they will silently produce a no-op. To avoid returning incorrect results, the compiler never emits hardware-based FP atomics instructions by default, even when applied to coarse grained memory regions. Currently, users can use the -munsafe-fp-atomics flag to force the compiler to emit hardware-based FP atomics. Using hardware-based FP atomics translates in a substantial performance",4.484729288326739
"How can I ensure that my application returns accurate results when using hardware-based FP atomics on a MI250X processor?
","Users applying floating point atomic operations (e.g., atomicAdd) on memory regions allocated via regular hipMalloc() can safely apply the -munsafe-fp-atomics flags to their codes to get the best possible performance and leverage hardware supported floating point atomics. Atomic operations supported in hardware on non-FP datatypes  (e.g., INT32) will work correctly regardless of the nature of the memory region used.",4.2758423111057
"How can I see detailed information about jobs currently in the queue and recently-completed jobs?
","The most straightforward monitoring is with the bjobs command. This command will show the current queue, including both pending and running jobs. Running bjobs -l will provide much more detail about a job (or group of jobs). For detailed output of a single job, specify the job id after the -l. For example, for detailed output of job 12345, you can run bjobs -l 12345 . Other options to bjobs are shown below. In general, if the command is specified with -u all it will show information for all users/all jobs. Without that option, it only shows your jobs. Note that this is not an exhaustive list.",4.314324667181842
"How can I see detailed information about jobs currently in the queue and recently-completed jobs?
","The squeue command is used to show the batch queue. You can filter the level of detail through several command-line options. For example:

| squeue -l | Show all jobs currently in the queue | | --- | --- | |  | Show all of your jobs currently in the queue |

The sacct command gives detailed information about jobs currently in the queue and recently-completed jobs. You can also use it to see the various steps within a batch jobs.",4.258914636449984
"How can I see detailed information about jobs currently in the queue and recently-completed jobs?
","bjobs and jobstat help to identify what’s currently running and scheduled to run, but sometimes it’s beneficial to know how much of the system is not currently in use or scheduled for use.",4.221241445541409
"What is the purpose of the ""-L"" option in the ""BSUB"" command in parallel h5py?
","The table below shows shows LSF (bsub) command-line/batch script options and the PBS/Torque/Moab (qsub) options that provide similar functionality.

| LSF Option | PBS/Torque/Moab Option | Description | | --- | --- | --- | | #BSUB -W 60 | #PBS -l walltime=1:00:00 | Request a walltime of 1 hour | | #BSUB -nnodes 1024 | #PBS -l nodes=1024 | Request 1024 nodes | | #BSUB -P ABC123 | #PBS -A ABC123 | Charge the job to project ABC123 | | #BSUB -alloc_flags gpumps | No equivalent (set via environment variable) | Enable multiple MPI tasks to simultaneously access a GPU |",4.242096540974885
"What is the purpose of the ""-L"" option in the ""BSUB"" command in parallel h5py?
","Time to execute ""hdf5_parallel.py"" by submitting ""submit_h5py"" to the batch queue:

Summit

.. code-block:: bash

   $ bsub -L $SHELL submit_h5py.lsf

Andes

.. code-block:: bash

   $ sbatch --export=NONE submit_h5py.sl

Frontier

.. code-block:: bash

   $ sbatch --export=NONE submit_h5py.sl

Example ""submit_h5py"" batch script:

Summit

.. code-block:: bash

   #!/bin/bash
   #BSUB -P <PROJECT_ID>
   #BSUB -W 00:05
   #BSUB -nnodes 1
   #BSUB -J h5py
   #BSUB -o h5py.%J.out
   #BSUB -e h5py.%J.err

   cd $LSB_OUTDIR
   date

   module load gcc
   module load hdf5
   module load python",4.19141635864978
"What is the purpose of the ""-L"" option in the ""BSUB"" command in parallel h5py?
","The table below summarizes options for submitted jobs. Unless otherwise noted, these can be used from batch scripts or interactive jobs. For interactive jobs, the options are simply added to the bsub command line. For batch scripts, they can either be added on the bsub command line or they can appear as a #BSUB directive in the batch script. If conflicting options are specified (i.e. different walltime specified on the command line versus in the script), the option on the command line takes precedence. Note that LSF has numerous options; only the most common ones are described here. For more",4.164830575345728
"What is the purpose of the --ntasks-per-node and --gpus-per-node options in srun?
","srun accepts the following common options:

| -N | Minimum number of nodes | | --- | --- | | -n | Total number of MPI tasks | | --cpu-bind=no | Allow code to control thread affinity | | -c | Cores per MPI task | | --cpu-bind=cores | Bind to cores |

If you do not specify the number of MPI tasks to srun via -n, the system will default to using only one task per node.

Each compute node on Andes contains two sockets each with 16 cores.  Depending on your job, it may be useful to control task layout within and across nodes.",4.351153280447953
"What is the purpose of the --ntasks-per-node and --gpus-per-node options in srun?
","It's recommended to explicitly specify jsrun options and not rely on the default values. This most often includes --nrs,--cpu_per_rs, --gpu_per_rs, --tasks_per_rs, --bind, and --launch_distribution.

The below examples were launched in the following 2 node interactive batch job:

summit> bsub -nnodes 2 -Pprj123 -W02:00 -Is $SHELL

The following example will create 12 resource sets each with 1 MPI task and 1 GPU. Each MPI task will have access to a single GPU.",4.341842871218128
"What is the purpose of the --ntasks-per-node and --gpus-per-node options in srun?
","| Option | jsrun (Summit) | srun  (Frontier) | | --- | --- | --- | | Number of nodes | -nnodes | -N, --nnodes | | Number of tasks | defined with resource set | -n, --ntasks | | Number of tasks per node | defined with resource set | --ntasks-per-node | | Number of CPUs per task | defined with resource set | -c, --cpus-per-task | | Number of resource sets | -n, --nrs | N/A | | Number of resource sets per host | -r, --rs_per_host | N/A | | Number of tasks per resource set | -a, --tasks_per_rs | N/A | | Number of CPUs per resource set | -c, --cpus_per_rs | N/A | | Number of GPUs per resource set",4.338710279385773
"What is the name of the pod that contains the container?
","A pod is the smallest unit in Kubernetes, it is a grouping of containers that will be scheduled together onto a node in the cluster. usually it will just be one container but it could be a group of processes that make up an application.

Pods have a lifecycle: they are defined, scheduled onto a node, and then they run until their containers exit or the pod is removed from the node for some reason. Pods are immutable and changes to a pod are not persisted between restarts.

A pod does not:

have state (data should be stored in persistent volumes)

move nodes once scheduled onto a node",4.287747469239981
"What is the name of the pod that contains the container?
","You should now be on the 'Pod' screen with the 'Overview' tab selected From here you can get a quick idea of the amount of resources (memory, CPU etc) that your  Pod is using.

Click on the 'Logs' tab to get the logs from your pod. This will display ""Hello World!"" in our example because of our echo command. There will be a dropdown here that for our example will contain only one item named 'hello-openshift'. This is the name of the container that you are viewing the logs for inside your pod.",4.240588310180412
"What is the name of the pod that contains the container?
","name: the-container
        ports:
        - containerPort: 8080",4.228534870685636
"Can you explain the concept of ""pull-back"" in the context of the OLCF Policy?
",The OLCF has a pull-back policy for under-utilization of INCITE allocations. Under-utilized INCITE project allocations will have core-hours removed from their outstanding core-hour project balance at specific times during the INCITE calendar year. The following table summarizes the current under-utilization policy:,4.165695732480362
"Can you explain the concept of ""pull-back"" in the context of the OLCF Policy?
","The project data retention policy exists to reclaim storage space after a project ends. By default, the OLCF will retain data in project-centric storage areas only for a designated amount of time after the project end date. During this time, a project member can request a temporary user account extension for data access. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for details on purge and retention timeframes for each project-centric storage area.",4.083978422009837
"Can you explain the concept of ""pull-back"" in the context of the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to or use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title:Security Policy Version: 12.10",4.067290066772845
"What is the difference between ClusterIP and NodePort?
","A NodePort reserves a port across all nodes of the cluster. This port routes traffic to a service, which points to the pods that match the service's label selector.

NodePorts are given in the 30000-32767 range. These are ports you can use from outside the cluster to access resources inside of OpenShift.

For the Openshift clusters you will additionally need to create a https://docs.olcf.ornl.gov/systems/nodeport.html#network policy <slate_network_policies> file to allow external traffic into your namespace.",4.297594021147903
"What is the difference between ClusterIP and NodePort?
","In this example, looking at Endpoints, we have 3 pods running with the my-app selector. This means that from inside the cluster if an application accesses the ClusterIP on port 8080 the traffic will be directed to one of the three pods.

A service of type ClusterIP will only ever be accessible from inside the cluster. If you need access to your service from outside of the cluster there are a few different options.",4.273481016711719
"What is the difference between ClusterIP and NodePort?
","In general, using FQDNs to access a service is more convenient. If your service communicates over HTTP or HTTPS, you can set up a https://docs.olcf.ornl.gov/systems/services.html#slate_routes to achieve this. If your service uses another protocol, you can use https://docs.olcf.ornl.gov/systems/services.html#slate_nodeports.

NodePort is a type of Service that reserves a port across the cluster that can route traffic to your pods. This is more flexible than a Route and can handle any TCP or UDP traffic.",4.261056102466197
"What is the purpose of the script?
",to use this script (especially after system upgrades) for testing purposes.,4.142146199567194
"What is the purpose of the script?
","Batch scripts, or job submission scripts, are the mechanism by which a user configures and submits a job for execution. A batch script is simply a shell script that also includes commands to be interpreted by the batch scheduling software (e.g. Slurm).",4.01110110034389
"What is the purpose of the script?
","The execution section of a script will be interpreted by a shell and can contain multiple lines of executables, shell commands, and comments.  when the job's queue wait time is finished, commands within this section will be executed on the primary compute node of the job's allocated resources. Under normal circumstances, the batch job will exit the queue after the last line of the script is executed.

Example Batch Script

#!/bin/bash
#SBATCH -A XXXYYY
#SBATCH -J test
#SBATCH -N 2
#SBATCH -t 1:00:00

cd $SLURM_SUBMIT_DIR
date
srun -n 8 ./a.out",3.983829652716455
"How can I unload the Darshan runtime environment before using Score-P?
","You will need to unload the darshan-runtime module. In some instances you may need to unload the xalt and xl modules.  $ module unload darshan-runtime

Serial

..  C

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load scorep
        $ module load gcc
        $ scorep gcc -c test.c
        $ scorep gcc -o test test.o

..  C++

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load scorep
        $ module load gcc
        $ scorep g++ -c test.cpp main.cpp
        $ scorep g++ -o test test.o main.o

..  Fortran",4.375034136415966
"How can I unload the Darshan runtime environment before using Score-P?
","..  C++

    .. code-block:: bash

          $ module unload darshan-runtime
          $ module load scorep
          $ module load spectrum-mpi
          $ module load gcc
          $ scorep mpiCC -c test.cpp main.cpp
          $ scorep mpiCC -o test test.o main.o

..  Fortran

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load gcc
        $ module load Scorep
        $ scorep mpifort -c test.f90
        $ scorep mpifort -o test test.o

MPI + OpenMP

..  C

    .. code-block:: bash",4.163644052199124
"How can I unload the Darshan runtime environment before using Score-P?
","..  Fortran

    .. code-block:: bash

          $ module unload darshan-runtime
          $ module load scorep
          $ module load gcc
          $ scorep mpifort -pthread -fopenmp -c test.f90
          $ scorep mpifort -pthread -fopenmp -o test test.o

CUDA

In some cases e.g. **CUDA** applications, Score-P needs to be made aware of the programming paradigm in order to do the correct instrumentation.

.. code-block:: bash",4.132501655904302
"How does the University of Tennessee's research on Iterative Refinement impact Summit's performance?
","Summit is an IBM system located at the Oak Ridge Leadership Computing Facility. With a theoretical peak double-precision performance of approximately 200 PF, it is one of the most capable systems in the world for a wide range of traditional computational science applications. It is also one of the ""smartest"" computers in the world for deep learning applications with a mixed-precision capability in excess of 3 EF.



Summit node architecture diagram",4.150841264967862
"How does the University of Tennessee's research on Iterative Refinement impact Summit's performance?
","Iterative Refinement is a technique for performing linear algebra solvers in a reduced precision, then iterating to improve the results and return them to full precision. This technique has been used for several years to use 32-bit math operations and achieve 64-bit results, which often results in a speed-up due to single precision math often have a 2X performance advantage on modern CPUs and many GPUs. NVIDIA and the University of Tennessee have been working to extend this technique to perform operations in half-precision and obtain higher precision results. One such place where this",4.134350723228283
"How does the University of Tennessee's research on Iterative Refinement impact Summit's performance?
",For Summit:,4.129855306292522
"What is the purpose of the make install command?
","Once you have entered the virtual environment, pmake can be installed with:

$ python -m pip install git+https://code.ornl.gov/99R/pmake.git@latest

Run the following command to verify that pmake is available:

$ pmake --help

To run a pmake demo on Summit, you will create a pmake-example directory with its preferred file layout, then submit a batch job to LSF from a Summit login node.

First, create the directories,

$ mkdir -p pmake-example/simulation

Next, create pmake's two configuration files, rules.yaml and targets.yaml:

# pmake-example/targets.yaml",4.122869376440982
"What is the purpose of the make install command?
","pmake is a standard python package.  It is recommended to install it in a virtual environment.  One easy way to create a virtual environment is to load an available python module, and then put a new environment into /ccs/proj/<projid>/<systemname>.  This way, the project can share environments, and each system gets its own install location.

$ module load python/3.8-anaconda3
$ python -m venv /path/to/new-venv
$ source /path/to/new-venv/bin/activate

On subsequent logins, remember to load the same python module and run the source /path/to/new-venv/bin/activate command again.",4.0680818348298144
"What is the purpose of the make install command?
","Inside the simulation directory, you should see 3 new files, simulate.sh, which contains the shell script pmake built from the simulate rule, simulate.log, containing the log output from running simulate.sh, and run.log, the file written during rule execution.

Extending pmake using your own rules is straightforward. pmake acts like make, running rules to create output files (that do not yet exist) from input files (that must exist before the rule is run).

Unlike make, pmake does not run a rule unless its output is requested by some target.",4.034415731788489
"How can I generate a flame graph using HPCToolkit?
","Programming models supported by HPCToolkit include MPI, OpenMP, OpenACC, CUDA, OpenCL, DPC++, HIP, RAJA, Kokkos, and others.

Below is an example that generates a profile and loads the results in their GUI-based viewer.

module use /gpfs/alpine/csc322/world-shared/modulefiles/ppc64le
module load hpctoolkit

# 1. Profile and trace an application using CPU time and GPU performance counters
jsrun <jsrun_options> hpcrun -o <measurement_dir> -t -e CPUTIME -e gpu=nvidia <application>

# 2. Analyze the binary of executables and its dependent libraries
hpcstruct <measurement_dir>",4.266544915207912
"How can I generate a flame graph using HPCToolkit?
","Programming models supported by HPCToolkit include MPI, OpenMP, OpenACC, CUDA, OpenCL, DPC++, HIP, RAJA, Kokkos, and others.

Below is an example that generates a profile and loads the results in their GUI-based viewer.

module use /gpfs/alpine/csc322/world-shared/modulefiles/x86_64
module load hpctoolkit

# 1. Profile and trace an application using CPU time and GPU performance counters
srun <srun_options> hpcrun -o <measurement_dir> -t -e CPUTIME -e gpu=amd <application>

# 2. Analyze the binary of executables and its dependent libraries
hpcstruct <measurement_dir>",4.25873682738387
"How can I generate a flame graph using HPCToolkit?
","# 3. Combine measurements with program structure information and generate a database
hpcprof -o <database_dir> <measurement_dir>

# 4. Understand performance issues by analyzing profiles and traces with the GUI
hpcviewer <database_dir>

A quick summary of HPCToolkit options can be found in the HPCTookit wiki page. More detailed information on HPCToolkit can be found in the HPCToolkit User's Manual.

HPCToolkit does not require a recompile to profile the code. It is recommended to use the -g optimization flag for attribution to source lines.",4.25294229023807
"How can I reset loaded modules to system defaults in Andes?
","If a different compiler is required, it is important to use the correct environment for each compiler. To aid users in pairing the correct compiler and environment, the module system on andes automatically pulls in libraries compiled with a given compiler when changing compilers. The compiler modules will load the correct pairing of compiler version, message passing libraries, and other items required to build and run code. To change the default loaded intel environment to the gcc environment for example, use:

$ module load gcc",4.08452902234151
"How can I reset loaded modules to system defaults in Andes?
","Changing compilers

If a different compiler is required, it is important to use the correct environment for each compiler. To aid users in pairing the correct compiler and environment, the module system on andes automatically pulls in libraries compiled with a given compiler when changing compilers. The compiler modules will load the correct pairing of compiler version, message passing libraries, and other items required to build and run code. To change the default loaded intel environment to the gcc environment for example, use:

$ module load gcc",4.084212827543974
"How can I reset loaded modules to system defaults in Andes?
","Alternatively, the ParaView installations in /sw/andes/paraview (i.e., the paraview/5.9.1-egl and paraview/5.9.1-osmesa modules) can also be loaded to avoid this issue.



The ParaView at OLCF Tutorial highlights how to get started on Andes with example datasets.

The Official ParaView User's Guide and the Python API Documentation contain all information regarding the GUI and Python interfaces.

A full list of ParaView Documentation can be found on ParaView's website.

The ParaView Wiki contains extensive information about all things ParaView.",4.06199702406781
"What is the purpose of the Prune Resources feature in ArgoCD?
",allow for better control of resources allocated to ArgoCD.,4.3178772381070765
"What is the purpose of the Prune Resources feature in ArgoCD?
","From the ArgoCD Applications screen, click the Create Application button. In the General application settings, give the application deployment a name for ArgoCD to refer to in the display. For Project usually the ArgoCD default project created during the ArgoCD instance installation is sufficient. However, your workload may benefit from a different logical grouping by using multiple ArgoCD projects. If it is desired to have ArgoCD automatically deploy resources to an OpenShift namespace, change the Sync Policy to Automatic. Check the Prune Resources to automatically remove objects when they",4.183387017672128
"What is the purpose of the Prune Resources feature in ArgoCD?
","ArgoCD has successfully accessed the namespace, realized that the state of the resources in the namespace are OutOfSync with the resource requirements of the code repository, and started the Syncing process to create and/or modify resources in the namespace to match the desired configuration. Clicking on the application tile will reveal more detailed information on the process:

Image of ArgoCD application tile detailed information.",4.1211430053805405
"How do we specify the location of the index.html and nginx.conf files in the build pod?
","oc create -f <FILENAME>

The first object we wish to create is our BuildConfig. This is the object that defines how we build our NGINX image.

Before we create the BuildConfig we should give it a way to access two files before they are pulled into the build pod. The files are my index.html and my nginx.conf file. You can get them into your build pod however you wish, for simplicity I chose to add them to a public git repository and wget them. The index.html and nginx.conf file are defined respectively as:

<h1>Hello, World!</h1>",4.29851260457009
"How do we specify the location of the index.html and nginx.conf files in the build pod?
","location / {
        }

        error_page 404 /404.html;
            location = /40x.html {
        }

        error_page 500 502 503 504 /50x.html;
            location = /50x.html {
        }
    }
}

The NGINX configuration file is completely standard except I changed the listen port to be from 80 to 8080 since the server will be running as a non-root user. The Route that we will add later on will redirect traffic coming in on port 80 to our server running on port 8080.

The BuildConfig, the following should be placed inside a buildconfig.yaml file:",4.134496574314814
"How do we specify the location of the index.html and nginx.conf files in the build pod?
","One of the simplest use cases for Kubernetes is running a web server. We will walk through the steps needed to set up an NGINX web server on OpenShift that serves a static html file. This example assumes that you have an allocation on the cluster.

First make sure that you are in the correct project:

oc project <YOUR_PROJECT>

In the next part of this we will be creating a few objects needed to run NGINX. The objects will be saved into a file and then added to the cluster with the command:

oc create -f <FILENAME>",4.102734623638798
"Can I request migration of a memory location to GPU HBM on Frontier?
","If HSA_XNACK=0, page faults in GPU kernels are not handled and will terminate the kernel. Therefore all memory locations accessed by the GPU must either be resident in the GPU HBM or mapped by the HIP runtime. Memory regions may be migrated between the host DDR4 and GPU HBM using explicit HIP library functions such as hipMemAdvise and hipPrefetchAsync, but memory will not be automatically migrated based on access patterns alone.",4.293372296926953
"Can I request migration of a memory location to GPU HBM on Frontier?
","If HSA_XNACK=0, page faults in GPU kernels are not handled and will terminate the kernel. Therefore all memory locations accessed by the GPU must either be resident in the GPU HBM or mapped by the HIP runtime. Memory regions may be migrated between the host DDR4 and GPU HBM using explicit HIP library functions such as hipMemAdvise and hipPrefetchAsync, but memory will not be automatically migrated based on access patterns alone.",4.293372296926953
"Can I request migration of a memory location to GPU HBM on Frontier?
","The page migration behavior summarized by the following tables represents the current, observable behavior. Said behavior will likely change in the near future.  CPU accesses to migratable memory may behave differently than other platforms you're used to. On Frontier, pages will not migrate from GPU HBM to CPU DDR4 based on access patterns alone. Once a page has migrated to GPU HBM it will remain there even if the CPU accesses it, and all accesses which do not resolve in the CPU cache will occur over the Infinity Fabric between the AMD ""Optimized 3rd Gen EPYC"" CPU and AMD MI250X GPU. Pages",4.280303168547092
"How can I name my job on Summit?
",For Summit:,4.362152816899326
"How can I name my job on Summit?
","Summit has a node hierarchy that can be very confusing for the uninitiated. The Summit User Guide explains this in depth. This has some consequences that may be unusual for R programmers. A few important ones to note are:

You must have a script that you can run in batch, e.g. with Rscript or R CMD BATCH

All data that needs to be visible to the R process (including the script and your packages) must be on gpfs (not your home directory!).

You must launch your script from the launch nodes with jsrun.

We'll start with something very simple. The script is:

""hello world""",4.120776614722289
"How can I name my job on Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.116192912566252
"Can you explain the concept of LDS memory in Frontier and how it relates to FP atomicAdd() operations?
","Hardware FP atomic operations performed in LDS memory are usually always faster than an equivalent CAS loop, in particular when contention on LDS memory locations is high. Because of a hardware design choice, FP32 LDS atomicAdd() operations can be slower than equivalent FP64 LDS atomicAdd(), in particular when contention on memory locations is low (e.g. random access pattern). The aforementioned behavior is only true for FP atomicAdd() operations. Hardware atomic operations for CAS/Min/Max on FP32 are usually faster than the FP64 counterparts. In cases when contention is very low, a FP32 CAS",4.345595415176253
"Can you explain the concept of LDS memory in Frontier and how it relates to FP atomicAdd() operations?
","Hardware FP atomic operations performed in LDS memory are usually always faster than an equivalent CAS loop, in particular when contention on LDS memory locations is high. Because of a hardware design choice, FP32 LDS atomicAdd() operations can be slower than equivalent FP64 LDS atomicAdd(), in particular when contention on memory locations is low (e.g. random access pattern). The aforementioned behavior is only true for FP atomicAdd() operations. Hardware atomic operations for CAS/Min/Max on FP32 are usually faster than the FP64 counterparts. In cases when contention is very low, a FP32 CAS",4.345595415176253
"Can you explain the concept of LDS memory in Frontier and how it relates to FP atomicAdd() operations?
","In cases when contention is very low, a FP32 CAS loop implementing an atomicAdd() operation could be faster than an hardware FP32 LDS atomicAdd(). Applications using single precision FP atomicAdd() are encouraged to experiment with the use of double precision to evaluate the trade-off between high atomicAdd() performance vs. potential lower occupancy due to higher LDS usage.",4.28159512711731
"How do I specify the project ID for my job using srun?
","$ srun -A <project_id> -t 00:05:00 -p <partition> -N 2 -n 4 --ntasks-per-node=2 ./a.out
<output printed to terminal>

The job name and output options have been removed since stdout/stderr are typically desired in the terminal window in this usage mode.

The table below summarizes commonly-used Slurm job submission options:",4.263345600492087
"How do I specify the project ID for my job using srun?
","$ srun -A <project_id> -t 00:05:00 -p <partition> -N 2 -n 4 --ntasks-per-node=2 ./a.out
<output printed to terminal>

The job name and output options have been removed since stdout/stderr are typically desired in the terminal window in this usage mode.

The table below summarizes commonly-used Slurm job submission options:",4.263345600492087
"How do I specify the project ID for my job using srun?
","Using srun

By default, commands will be executed on the job’s primary compute node, sometimes referred to as the job’s head node. The srun command is used to execute an MPI binary on one or more compute nodes in parallel.

srun accepts the following common options:

| -N | Minimum number of nodes | | --- | --- | | -n | Total number of MPI tasks | | --cpu-bind=no | Allow code to control thread affinity | | -c | Cores per MPI task | | --cpu-bind=cores | Bind to cores |

If you do not specify the number of MPI tasks to srun via -n, the system will default to using only one task per node.",4.23951463049372
"Can you provide an example of how the OLCF Policy would affect the scheduling of jobs?
","Allocation Overuse Policy

Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.342529096671789
"Can you provide an example of how the OLCF Policy would affect the scheduling of jobs?
","How the software is obtained.

Explanation of why the software is needed. If OLCF provides equivalent software, what is different about this software? If requesting an upgrade, describe new features or bug fixes.

Who will be using this software? Approximately how many people will be using it? If possible, list individual users of this software.

To request exemption to certain system policies, contact help@olcf.ornl.gov with an overview of the exemption you need. Example requests include:

Relaxed queue limits for one or more jobs (longer walltime, higher priority)",4.32866078754382
"Can you provide an example of how the OLCF Policy would affect the scheduling of jobs?
","agreed to by the following persons as a condition of access to or use of

OLCF computational resources:



-  Principal Investigators (Non-Profit)

-  Principal Investigators (Industry)

-  All Users



**Title:** Titan Scheduling Policy **Version:** 13.02



In a simple batch queue system, jobs run in a first-in, first-out (FIFO)

order. This often does not make effective use of the system. A large job

may be next in line to run. If the system is using a strict FIFO queue,

many processors sit idle while the large job waits to run. *Backfilling*",4.31096557112565
"What is the cause of the error when attempting to run an application with CUDA-aware MPI using more than one resource set per node?
","This bug was fixed in xl/16.1.1-3

Attempting to run an application with CUDA-aware MPI using more than one resource set per node with produce the following error on each MPI rank:",4.552338861715601
"What is the cause of the error when attempting to run an application with CUDA-aware MPI using more than one resource set per node?
","Spectrum MPI relies on CUDA Inter-process Communication (CUDA IPC) to provide fast on-node between GPUs. At present this capability cannot function with more than one resource set per node.

Set the environment variable PAMI_DISABLE_IPC=1 to force Spectrum MPI to not use fast GPU Peer-to-peer communication. This option will allow your code to run with more than one resource set per host, but you may see slower GPU to GPU communication.

Run in a single resource set per host, i.e. with jsrun --gpu_per_rs 6",4.274839903283779
"What is the cause of the error when attempting to run an application with CUDA-aware MPI using more than one resource set per node?
","Serial applications, that are not MPI enabled, often face the following issue when compiled with Spectrum MPI's wrappers and run with jsrun:

CUDA Hook Library: Failed to find symbol mem_find_dreg_entries, ./a.out: undefined symbol: __PAMI_Invalidate_region

The same issue can occur if CUDA API calls that interact with the GPU (e.g. allocating memory) are called before MPI_Init() in an MPI enabled application. Depending on context, this error can either be harmless or it can be fatal.",4.267555999458843
"How do Network Policies affect pods in a project in Slate?
","Network policies are specifications of how groups of pods are allowed to communicate with each other and other network endpoints. A pod is selected in a Network Policy based on a label so the Network Policy can define rules to specify traffic to that pod.

When you create a namespace in Slate with the Quota Dashboard some Network Policies are added to your newly created namespace. This means that pods inside your project are isolated from connections not defined in these Network Policies.",4.576276865370488
"How do Network Policies affect pods in a project in Slate?
","Network Policies do not conflict, they are additive. This means that if two policies match a pod the pod will be restricted to what is allowed by the union of those policies' ingress/egress rules.

To create a Network policy using the GUI click the Networking tab followed by the Network Policy tab:

Creating Network Policies

This will place you in an editor with some boiler plate YAML. From here you can define the network policy that you need for your namespace. Below is an example Network Policy that you would use to allow all external traffic to access a nodePort in your namespace.",4.290072284342744
"How do Network Policies affect pods in a project in Slate?
","The key value pair, or label, under spec.podSelector.matchLabels will need to match exactly to the pod in your namespace that the policy is for example the above NetworkPolicy would match pods with these labels set:  apiVersion: v1 Kind: Pod metadata:   labels:     key: value ...

To view the Network policies in your namespace you can run:

oc get networkpolicy -n YOUR_NAMESPACE

to get the name of the network policy and then:

oc get networkpolicy NETWORKPOLICY_NAME -o yaml

to view object's YAML.",4.173963402217408
"How can I specify the container's command in a Pod?
","command: [""/bin/sh"",""-c""]

args: [""echo 'Hello World!'; cat""]

Finally, we need a tty. This will give us the ability to open a shell in our  Pod and get a better understanding of what is happing. To do this, add the following two lines under the command line that you just added:

tty: true

stdin: true

Your page should now look as follows:



You can now click the 'Create' button in the lower left which will take you to the screen where the   Pod is created.",4.2588857050301225
"How can I specify the container's command in a Pod?
","Secondly, the  Pod needs something to do when it starts. For an nginx server this would be running nginx, for a flask app this would be running the app.py file etc. For illustrative purposes this  Pod is going to be starting a shell with the /bin/sh command, echoing a ""Hello World!"" prompt then running a cat command as a means to keep the pod running. Without the addition of the cat at the end the echo command would end causing the /bin/sh to end causing the  Pod to go from a status of Running to Completed.  To make these changes add the following lines below the image line:",4.243004064421826
"How can I specify the container's command in a Pod?
","Finally, to get a remote shell in the pod we run the oc rsh <POD_NAME> command. This will default to using /bin/sh in the pod. If a different shell is required, we can provide the optional --shell=/path/to/shell flag. For example, if we wanted to open a bash shell in the pod we would run the following command:

oc rsh --shell='/bin/bash' <POD_NAME>

If you have multiple containers in your pod, the oc rsh <POD_NAME> command will default to the first container. If you would like to start a remote shell in one of the other containers, you can use the optional -c <CONTAINER_NAME> flag.",4.215471642798458
"What is the benefit of using multiple ArgoCD projects for a workload?
",allow for better control of resources allocated to ArgoCD.,4.3996138163626215
"What is the benefit of using multiple ArgoCD projects for a workload?
","In other words, the CD in ArgoCD is for continuous delivery of the application(s).",4.266630987856833
"What is the benefit of using multiple ArgoCD projects for a workload?
","From the ArgoCD Applications screen, click the Create Application button. In the General application settings, give the application deployment a name for ArgoCD to refer to in the display. For Project usually the ArgoCD default project created during the ArgoCD instance installation is sufficient. However, your workload may benefit from a different logical grouping by using multiple ArgoCD projects. If it is desired to have ArgoCD automatically deploy resources to an OpenShift namespace, change the Sync Policy to Automatic. Check the Prune Resources to automatically remove objects when they",4.2273382977829135
"Are there any known issues with running multiple jsrun commands on Summit?
","summit>



Jsrun provides the ability to launch multiple jsrun job launches within a single batch job allocation. This can be done within a single node, or across multiple nodes.

By default, multiple invocations of jsrun in a job script will execute serially in order. In this configuration, jobs will launch one at a time and the next one will not start until the previous is complete. The batch node allocation is equal to the largest jsrun submitted, and the total walltime must be equal to or greater then the sum of all jsruns issued.",4.338010265765691
"Are there any known issues with running multiple jsrun commands on Summit?
","This section describes tools that users might find helpful to better understand the jsrun job launcher.

hello_jsrun is a ""Hello World""-type program that users can run on Summit nodes to better understand how MPI ranks and OpenMP threads are mapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A screencast showing how to use Hello_jsrun is also available: https://vimeo.com/261038849

Job Step Viewer provides a graphical view of an application's runtime layout on Summit. It allows users to preview and quickly iterate with multiple jsrun options to understand and optimize job launch.",4.242766331054033
"Are there any known issues with running multiple jsrun commands on Summit?
","Summit has a node hierarchy that can be very confusing for the uninitiated. The Summit User Guide explains this in depth. This has some consequences that may be unusual for R programmers. A few important ones to note are:

You must have a script that you can run in batch, e.g. with Rscript or R CMD BATCH

All data that needs to be visible to the R process (including the script and your packages) must be on gpfs (not your home directory!).

You must launch your script from the launch nodes with jsrun.

We'll start with something very simple. The script is:

""hello world""",4.227736553137877
"How do I ensure that my application has access to the Persistent Volume?
","Add Storage Menu

You should see a green popup appear in the upper right saying that the storage was added. This should additionally trigger a new deployment. To make sure a new deployment happened look at the Created time of the top most deployment.

A PersistentVolume is backed up by creating a VolumeSnapshot object. The VolumeSnapshot will create a point in time backup of your PersistentVolume and is something that you would likely do before an upgrade.

A Volume Snapshot will bind to a PersistentVolumeClaim. An example PersistentVolumeClaim to bind a VolumeSnapshot to follows:",4.307618075949245
"How do I ensure that my application has access to the Persistent Volume?
","From the main screen of a project go to the Storage option in the hamburger menu on the left hand side of the screen, then click Persistent Volume Claims

Persistent Volume Claim Menu

In the upper right click the Create Persistent Volume Claim button.

Create Storage

This will take you to a screen that allows you to select what you need for your PVC. Give your claim a name and request an amount of storage and click Create. You can also click Edit YAML to edit the PVC object directly.

PV Settings",4.283881293308902
"How do I ensure that my application has access to the Persistent Volume?
","PV Settings

This will redirect you to the status page of your new PVC. You should see Bound in the Status field.

PV Status

Once the claim has been granted to the project a pod within that project can access that storage. To do this, edit the YAML of the pod and under the volume portion of the file add a name and the name of the claim. It will look similar to the following.

volumes:
  name: pvol
  persistentVolumeClaim:
    claimName: storage-1

Then all that is left to do is mount the storage into a container:

volumeMounts:
  - mountPath: /tmp/
    name: pvol",4.28073308573041
"Can I use the -EOMP_NUM_THREADS flag with other flags in the jsrun command?
","In the below example, you could also do export OMP_NUM_THREADS=16 in your job script instead of passing it as a -E flag to jsrun. The below example starts 1 resource set with 2 tasks and 8 cores, 4 cores bound to each task, 16 threads for each task. We can set 16 threads since there are 4 cores per task and the default is smt4 for each core (4 * 4 = 16 threads).

jsrun -n1 -a2 -c8 -g1 -bpacked:4 -dpacked -EOMP_NUM_THREADS=16 csh -c 'echo $OMP_NUM_THREADS $OMP_PLACES'

16 0:4,4:4,8:4,12:4
16 16:4,20:4,24:4,28:4",4.419278697473646
"Can I use the -EOMP_NUM_THREADS flag with other flags in the jsrun command?
","In addition to specifying the SMT level, you can also control the number of threads per MPI task by exporting the OMP_NUM_THREADS environment variable. If you don't export it yourself, Jsrun will automatically set the number of threads based on the number of cores requested (-c) and the binding (-b) option. It is better to be explicit and set the OMP_NUM_THREADS value yourself rather than relying on Jsrun constructing it for you. Especially when you are using Job Step Viewer which relies on the presence of that environment variable to give you visual thread assignment information.",4.320982564462841
"Can I use the -EOMP_NUM_THREADS flag with other flags in the jsrun command?
","Because jsrun sees 8 cores and the -brs flag, it assigns all 8 cores to each of the 2 tasks in the resource set. Jsrun will set up OMP_NUM_THREADS as 32 (8 cores with 4 threads per core) which will apply to all the tasks in the resource set. This means that each task sees that it can have 32 threads (which means 64 threads for the 2 tasks combined) which will oversubscribe the cores and may decrease efficiency as a result.",4.296044833592027
"Can I use the ticket system to ask for help with a issue that is not related to OLCF?
","Please do not respond to previous tickets with new, unrelated issues. This can slow down response time and make finding relevant information harder thereby slowing down time to resolution.

Let us know if you've solved the issue yourself (and let us know what worked!)

MOST IMPORTANT: do not hesitate to contact us (help@olcf.ornl.gov); we will work through the details with you.",4.265938085131259
"Can I use the ticket system to ask for help with a issue that is not related to OLCF?
","If your project is still active and you require continued access to OLCF, you'll need to request a replacement fob. To do so, contact either the User Assistance Team (help@olcf.ornl.gov) or the Accounts Team (accounts@olcf.ornl.gov). You do not need to return the broken/expired RSA token to OLCF. Disposal and recycling information can be found in the vendor's disposal statement.



When submitting a ticket to help@olcf.ornl.gov requesting help, you will likely get faster resolution by supporting a few best practices:",4.179351261587679
"Can I use the ticket system to ask for help with a issue that is not related to OLCF?
","If you have problems or need helping running on Crusher, please submit a ticket by emailing help@olcf.ornl.gov.



JIRA_CONTENT_HERE",4.175909998094527
"How do I build a container image from scratch on Summit?
","You can view the Dockerfiles used to build the MPI base image at the code.ornl.gov repository. These Dockerfiles are buildable on Summit yourself by cloning the repository and running the ./build in the individual directories in the repository. This allows you the freedom to modify these base images to your own needs if you don't need all the components in the base images. You may run into the cgroup memory limit when building so kill the podman process, log out, and try running the build again if that happens when building.",4.323217268743351
"How do I build a container image from scratch on Summit?
","Users will be building and running containers on Summit without root permissions i.e. containers on Summit are rootless.  This means users can get the benefits of containers without needing additional privileges. This is necessary for a shared system like Summit. And this is part of the reason why Docker doesn't work on Summit. Podman and Singularity provides rootless support but to different extents hence why users need to use a combination of both.

Users will need to set up a file in their home directory /ccs/home/<username>/.config/containers/storage.conf with the following content:",4.320706791620595
"How do I build a container image from scratch on Summit?
","Podman is a container framework from Red Hat that functions as a drop in replacement for Docker. On Summit, we will use Podman for building container images and converting them into tar files for Singularity to use. Podman makes use of Dockerfiles to describe the images to be built. We cannot use Podman to run containers as it doesn't properly support MPI on Summit, and Podman does not support storing its container images on GPFS or NFS.",4.268495378953927
"How can I copy the file from the NVMe to my local space?
","#Copy the file from the NVMe to your local space
jsrun -n 1 cp /mnt/bb/$USER/test_file .

#Check the file locally
ls -l test_file",4.518415096707845
"How can I copy the file from the NVMe to my local space?
","Remember that by default NVMe support one file per MPI process up to one file per compute node. If users desire a single file as output from data staged on the NVMe they will need to construct it.  Tools to save automatically checkpoint files from NVMe to GPFS as also methods that allow automatic n to 1 file writing with NVMe staging are under development.   Tutorials about NVME:   Burst Buffer on Summit (slides, video) Summit Burst Buffer Libraries (slides, video).",4.122969521135462
"How can I copy the file from the NVMe to my local space?
",NVMes will need to be copied back to the parallel filesystem before the job ends. This largely manual mode of usage will not be the recommended way to use the burst buffer for most applications because tools are actively being developed to automate and improve the NVMe transfer and data management process. Here are the basic steps for using the BurstBuffers in their current limited mode of usage:,4.1071791694950175
"How does Spectral manage file transfers on Summit?
","Spectral is a portable and transparent middleware library to enable use of the node-local burst buffers for accelerated application output on Summit. It is used to transfer files from node-local NVMe back to the parallel GPFS file system without the need of the user to interact during the job execution. Spectral runs on the isolated core of each reserved node, so it does not occupy resources and based on some parameters the user could define which folder to be copied to the GPFS. In order to use Spectral, the user has to do the following steps in the submission script:",4.424009286027682
"How does Spectral manage file transfers on Summit?
","Summit is connected to an IBM Spectrum Scale™ filesystem providing 250PB of storage capacity with a peak write speed of 2.5 TB/s. Summit also has access to the center-wide NFS-based filesystem (which provides user and project home areas) and has access to the center’s High Performance Storage System (HPSS) for user and project archival storage.

Summit is running Red Hat Enterprise Linux (RHEL) version 8.2.",4.234701023575174
"How does Spectral manage file transfers on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.191127745936302
"What is the purpose of the Conda quick-reference list?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly. For those that just want a quick-reference list of common conda commands, see the https://docs.olcf.ornl.gov/systems/conda_basics.html#conda-quick section.",4.427592789570899
"What is the purpose of the Conda quick-reference list?
","Because there is no conda module on Frontier, this guide only applies if you installed a personal Miniconda first. See our https://docs.olcf.ornl.gov/software/python/miniconda.html for more details.

This guide has been shortened and adapted from a challenge in OLCF's Hands-On with Summit GitHub repository (Python: Conda Basics).",4.155137854857146
"What is the purpose of the Conda quick-reference list?
","This guide introduces a user to the basic workflow of using conda environments, as well as providing an example of how to create a conda environment that uses a different version of Python than the base environment uses on Summit. Although Summit is being used in this guide, all of the concepts still apply to other OLCF systems. If using Frontier, this assumes you already have installed your own version of conda (e.g., by https://docs.olcf.ornl.gov/software/python/miniconda.html).

OLCF Systems this guide applies to:

Summit

Andes

Frontier (if using conda)",4.124219037708635
"Can we use the -c option to specify the number of physical CPU cores to allocate to each MPI rank in Crusher?
","This example launches 16 MPI ranks (-n16), each with 4 physical CPU cores (-c4) to launch 1 OpenMP thread (OMP_NUM_THREADS=1) on. Because it is using 4 physical CPU cores per task, core specialization needs to be disabled (-S 0). The MPI ranks will be assigned to GPUs in a packed fashion so that each of the 8 GPUs on the node are shared by 2 MPI ranks. Similar to Example 5, -ntasks-per-gpu=2 will be used, but a new srun flag will be used to change the default round-robin (cyclic) distribution of MPI ranks across NUMA domains:",4.305334483114839
"Can we use the -c option to specify the number of physical CPU cores to allocate to each MPI rank in Crusher?
","The intent with both of the following examples is to launch 8 MPI ranks across the node where each rank is assigned its own logical (and, in this case, physical) core.  Using the -m distribution flag, we will cover two common approaches to assign the MPI ranks -- in a ""round-robin"" (cyclic) configuration and in a ""packed"" (block) configuration. Slurm's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive method was used to request an allocation of 1 compute node for these examples: salloc -A <project_id> -t 30 -p <parition> -N 1",4.295994545712258
"Can we use the -c option to specify the number of physical CPU cores to allocate to each MPI rank in Crusher?
","If CAAR or ECP teams require a temporary exception to this policy, please email help@olcf.ornl.gov with your request and it will be given to the OLCF Resource Utilization Council (RUC) for review.

This section describes how to map processes (e.g., MPI ranks) and process threads (e.g., OpenMP threads) to the CPUs and GPUs on Crusher. The https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#crusher-compute-nodes diagram will be helpful when reading this section to understand which physical CPU cores (and hardware threads) your processes and threads run on.",4.284371739366538
"Are backups enabled for the open member work directory on Slate?
","Files within ""Work"" directories (i.e., Member Work, Project Work, World Work) are not backed up and are purged on a regular basis according to the timeframes listed above.",4.186069156254588
"Are backups enabled for the open member work directory on Slate?
","Footnotes

1

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

2

Retention is not applicable as files will follow purge cycle.",4.109605669647471
"Are backups enabled for the open member work directory on Slate?
","The backups are accessed via the .snapshot subdirectory. Note that ls alone (or even ls -a) will not show the .snapshot subdirectory exists, though ls .snapshot will show its contents. The .snapshot feature is available in any subdirectory of your project home directory and will show the online backups available for that subdirectory.

To retrieve a backup, simply copy it into your desired destination with the cp command.",4.085172833157438
"How does the May 21, 2019 upgrade address the issue of internal compiler error with XL and the `-g` flag?
","export GPFSMPIO_COMM=1

This command will use non-blocking MPI calls and not MPI_Alltoallv for exchange of data between the MPI I/O aggregators which requires significant more amount of memory.



The following issues were resolved with the May 21, 2019 upgrade:

Some users have reported an internal compiler error when compiling their code with XL with the `-g` flag. This has been reported to IBM and they are investigating.

This bug was fixed in xl/16.1.1-3",4.120958604407731
"How does the May 21, 2019 upgrade address the issue of internal compiler error with XL and the `-g` flag?
","| Package | New Version | | --- | --- | | pgi | 20.1 | | xl | 16.1.1-6 | | kokkos | 3.0.0 |

Finally, the FFTW installations on Summit for the XL compiler have been rebuilt using -O2 to address an issue observed when running the FFTW suite using the default optimization options. All builds of the fftw package that use the XL compiler have been rebuilt.

If you encounter any issues, please contact help@olcf.ornl.gov.



<p style=""font-size:20px""><b>Rhea: OpenMPI Upgrade (February 18, 2020)</b></p>",4.090582007598342
"How does the May 21, 2019 upgrade address the issue of internal compiler error with XL and the `-g` flag?
","When using OpenMP with IBM XL compilers, the thread-safe compiler variant is required; These variants have the same name as the non-thread-safe compilers with an additional _r suffix. e.g. to compile OpenMPI C code one would use xlc_r

OpenMP offloading support is still under active development. Performance and debugging capabilities in particular are expected to improve as the implementations mature.",4.001692749399154
"How can I set the environment variable PFS_DIR on Summit?
","This will create a .condarc file in your $HOME directory if you do not have one already, which will now contain this new envs_dirs location. This will now enable you to use the --name env_name flag when using conda commands for environments stored in the summit directory, instead of having to use the -p /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/env_name flag and specifying the full path to the environment. For example, you can do source activate py3711-summit instead of source activate /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit/py3711-summit.",4.205487326300149
"How can I set the environment variable PFS_DIR on Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.161981133651379
"How can I set the environment variable PFS_DIR on Summit?
","It is highly recommended to create new environments in the ""Project Home"" directory (on Summit, this is /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>). This space avoids purges, allows for potential collaboration within your project, and works better with the compute nodes. It is also recommended, for convenience, that you use environment names that indicate the hostname, as virtual environments created on one system will not necessarily work on others.",4.154548084900996
"What is the name of the Singularity sif file that I can use to run the converted image?
","You can now create a Singularity sif file with singularity build --disable-cache --docker-login simple.sif docker://docker.io/<username>/simple.

This will ask you to enter your Docker username and password again for Singularity to download the image from Dockerhub and convert it to a sif file.",4.276253596007863
"What is the name of the Singularity sif file that I can use to run the converted image?
","Singularity is a container framework from Sylabs. On Summit, we will use Singularity solely as the runtime. We will convert the tar files of the container images Podman creates into sif files, store those sif files on GPFS, and run them with Jsrun. Singularity also allows building images but ordinary users cannot utilize that on Summit due to additional permissions not allowed for regular users.",4.254424250757733
"What is the name of the Singularity sif file that I can use to run the converted image?
","Users can build container images with Podman, convert it to a SIF file, and run the container using the Singularity runtime. This page is intended for users with some familiarity with building and running containers.

Users will make use of two applications on Summit - Podman and Singularity - in their container build and run workflow. Both are available without needing to load any modules.",4.193292984731243
"What is the purpose of the ""targetPort"" field in the Route object in Slate?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.164177384602931
"What is the purpose of the ""targetPort"" field in the Route object in Slate?
","In general, using FQDNs to access a service is more convenient. If your service communicates over HTTP or HTTPS, you can set up a https://docs.olcf.ornl.gov/systems/services.html#slate_routes to achieve this. If your service uses another protocol, you can use https://docs.olcf.ornl.gov/systems/services.html#slate_nodeports.

NodePort is a type of Service that reserves a port across the cluster that can route traffic to your pods. This is more flexible than a Route and can handle any TCP or UDP traffic.",4.153008170916812
"What is the purpose of the ""targetPort"" field in the Route object in Slate?
","A NodePort reserves a port across all nodes of the cluster. This port routes traffic to a service, which points to the pods that match the service's label selector.

NodePorts are given in the 30000-32767 range. These are ports you can use from outside the cluster to access resources inside of OpenShift.

For the Openshift clusters you will additionally need to create a https://docs.olcf.ornl.gov/systems/nodeport.html#network policy <slate_network_policies> file to allow external traffic into your namespace.",4.043330848573965
"How can I install a package named package_name using pip in a Conda environment at OLCF?
","One way to install packages into your conda environment is to build packages from source using pip. This approach is useful if a specific package or package version is not available in the conda repository, or if the pre-compiled binaries don't work on the HPC resources (which is common). However, building from source means you need to take care of some of the dependencies yourself, especially for optimization. Pip is available to use after installing Python into your conda environment, which you have already done.",4.301760406484666
"How can I install a package named package_name using pip in a Conda environment at OLCF?
","Using pip:

$ pip install package_name
$ pip uninstall package_name
$ pip install --no-binary=package_name package_name # builds from source



Conda User Guide

Anaconda Package List

Pip User Guide

Using Pip In A Conda Environment",4.280253017654447
"How can I install a package named package_name using pip in a Conda environment at OLCF?
","This page describes how a user can successfully install specific quantum software tools to run on select OLCF HPC systems. This allows a user to utilize our systems to run a simulator, or even as a ""frontend"" to a vendor's quantum machine (""backend"").

For most of these instructions, we use conda environments. For more detailed information on how to use Python modules and conda environments on OLCF systems, see our :doc:`Python on OLCF Systems page</software/python/index>`, and our https://docs.olcf.ornl.gov/software/python/conda_basics.html.",4.266797981939672
"What is the purpose of the new scratch filesystem being introduced in 2024?
","Please note, Summit will mount a new filesystem once returned to service.  Data stored on Alpine at the time of its decommission on January 01 will not be available.  Users will be responsible for transferring data onto Summit's new filesystem



Alpine II will be available early 2024.

The previous center-wide GPFS scratch filesystem, Alpine, will be decommissioned in January 2024. A new scratch filesystem will be made available for projects with 2024 Summit allocations in early 2024. Users will be responsible for transferring any needed data onto the new scratch filesystem once available.",4.2075288982930745
"What is the purpose of the new scratch filesystem being introduced in 2024?
","Each project has a World Work directory that resides in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. World Work areas can be accessed by all users of the system and are intended for sharing of data between projects. World Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but rather should be",4.095310265254196
"What is the purpose of the new scratch filesystem being introduced in 2024?
","Each project is granted a Project Work directory; these reside in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. Project Work directories can be accessed by all members of a project and are intended for sharing data within a project. Project Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but",4.047766723593045
"What is the quota for user directories on OLCF systems?
","Home directories for each user are NFS-mounted on all OLCF systems and are intended to store long-term, frequently-accessed user data. User Home areas are backed up on a daily basis. This file system does not generally provide the input/output (I/O) performance required by most compute jobs, and is not available to compute jobs on most systems. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for more details on applicable quotas, backups, purge, and retention timeframes.",4.424629198759514
"What is the quota for user directories on OLCF systems?
","The following table details quota, backup, purge, and retention information for each user-centric and project-centric storage area available at the OLCF.

User-Centric Storage Areas

| Area | Path | Type | Permissions | Quota | Backups | Purged | Retention | On Compute Nodes | | --- | --- | --- | --- | --- | --- | --- | --- | --- | | User Home | /ccs/home/[userid] | NFS | User set | 50 GB | Yes | No | 90 days | Read-only | | User Archive 1 | /home/[userid] | HPSS | User set | 2TB | No | No | 90 days | No | | User Archive 2 | /home/[userid] | HPSS | 700 | N/A | N/A | N/A | N/A | No |",4.359459002900104
"What is the quota for user directories on OLCF systems?
","System reservation (a dedicated set of nodes at a specific date/time)

Increased disk quota

Purge exemption for User/Group/World Work areas

Special requests are reviewed weekly by the OLCF Resource Utilization Council. Please contact help@olcf.ornl.gov for more information.



This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title: Computing Policy Version: 12.10",4.343873393326218
"What is the difference between ""hipcc --amdgpu-target=gfx90a"" and ""CC --offload-arch=gfx90a""?
",hipcc --amdgpu-target=gfx90a:xnack- --amdgpu-target=gfx90a:xnack+ -x hip or CC --offload-arch=gfx90a:xnack- --offload-arch=gfx90a:xnack+ -x hip,4.501720074558133
"What is the difference between ""hipcc --amdgpu-target=gfx90a"" and ""CC --offload-arch=gfx90a""?
",hipcc --amdgpu-target=gfx90a:xnack- --amdgpu-target=gfx90a:xnack+ -x hip or CC --offload-arch=gfx90a:xnack- --offload-arch=gfx90a:xnack+ -x hip,4.501720074558133
"What is the difference between ""hipcc --amdgpu-target=gfx90a"" and ""CC --offload-arch=gfx90a""?
","hipcc --amdgpu-target=gfx90a:xnack+ or CC --offload-arch=gfx90a:xnack+ -x hip

Kernels are compiled in ""xnack plus"" mode and will only be able to run on GPUs with HSA_XNACK=1 to enable XNACK. Performance may be better than ""xnack any"", but attempts to run with XNACK disabled will fail.

hipcc --amdgpu-target=gfx90a:xnack- or CC --offload-arch=gfx90a:xnack- -x hip

Kernels are compiled in ""xnack minus"" mode and will only be able to run on GPUs with HSA_XNACK=0 and XNACK disabled. Performance may be better than ""xnack any"", but attempts to run with XNACK enabled will fail.",4.420573863782693
"How can I launch a job on Summit using jsrun?
","summit>



Jsrun provides the ability to launch multiple jsrun job launches within a single batch job allocation. This can be done within a single node, or across multiple nodes.

By default, multiple invocations of jsrun in a job script will execute serially in order. In this configuration, jobs will launch one at a time and the next one will not start until the previous is complete. The batch node allocation is equal to the largest jsrun submitted, and the total walltime must be equal to or greater then the sum of all jsruns issued.",4.497532064603741
"How can I launch a job on Summit using jsrun?
","This section describes tools that users might find helpful to better understand the jsrun job launcher.

hello_jsrun is a ""Hello World""-type program that users can run on Summit nodes to better understand how MPI ranks and OpenMP threads are mapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A screencast showing how to use Hello_jsrun is also available: https://vimeo.com/261038849

Job Step Viewer provides a graphical view of an application's runtime layout on Summit. It allows users to preview and quickly iterate with multiple jsrun options to understand and optimize job launch.",4.475107869959791
"How can I launch a job on Summit using jsrun?
","to service nodes on that system). All commands within your job script (or the commands you run in an interactive job) will run on a launch node. Like login nodes, these are shared resources so you should not run multiprocessor/threaded programs on Launch nodes. It is appropriate to launch the jsrun command from launch nodes. | | Compute | Most of the nodes on Summit are compute nodes. These are where your parallel job executes. They're accessed via the jsrun command. |",4.409562157496612
"Can I use the DTNs for both local-area transfers and wide-area data transfers?
",The Data Transfer Nodes (DTNs) are hosts specifically designed to provide optimized data transfer between OLCF systems and systems outside of the OLCF network. These nodes perform well on local-area transfers as well as the wide-area data transfers for which they are tuned. The OLCF recommends that users use these nodes to improve transfer speed and reduce load on computational systems’ login and service nodes. OLCF provides two sets of DTNs: one for systems in our moderate enclave and a second for systems in the open enclave.,4.325298528842436
"Can I use the DTNs for both local-area transfers and wide-area data transfers?
","The OLCF offers a number of dedicated data transfer nodes to users. The nodes have been tuned specifically for wide area data transfers, and also perform well on the local area. There are also several utilities that the OLCF recommends for data transfer. Please refer to our https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#system-user-guides for information about the DTNs and available utilities.

Titan Scheduling Policy

=======================



.. note::

This details an official policy of the OLCF, and must be",4.1826542965021645
"Can I use the DTNs for both local-area transfers and wide-area data transfers?
","DTNs are also accessible via the ""OLCF DTN"" (for moderate) and ""NCCS Open DTN"" (for open) Globus endpoints. For more information on using Globus at OLCF see https://docs.olcf.ornl.gov/systems/dtn_user_guide.html#data-transferring-data-globus.

The moderate DTNs also support batch jobs. The system contains 8 nodes accessible through the DTN batch system.

Most OLCF resources now use the Slurm batch scheduler, including the DTNs. Below is a table of useful commands for Slurm.",4.182065743348163
"What is the nickname of the current R version that I am using?
","Several versions of R are available on Summit. You can see which by entering the command module spider r. Throughout this example, we will be using R version 3.6.1.

If you have logged in with the default modules, then you need to swap xl for gcc and the load R:

module swap xl gcc/6.4.0
module load r/3.6.1

If we do that and launch R, then we see:",4.12474767725414
"What is the nickname of the current R version that I am using?
","If we do that and launch R, then we see:

version
## platform       powerpc64le-unknown-linux-gnu
## arch           powerpc64le
## os             linux-gnu
## system         powerpc64le, linux-gnu
## status
## major          3
## minor          6.1
## year           2019
## month          07
## day            05
## svn rev        76782
## language       R
## version.string R version 3.6.1 (2019-07-05)
## nickname       Action of the Toes",4.112690652577598
"What is the nickname of the current R version that I am using?
","sessionInfo()
## R version 3.6.1 (2019-07-05)
## Platform: powerpc64le-unknown-linux-gnu (64-bit)
## Running under: Red Hat Enterprise Linux Server 7.6 (Maipo)
##
## Matrix products: default
## BLAS/LAPACK: /autofs/nccs-svm1_sw/summit/r/3.6.1/rhel7.6_gnu6.4.0/lib64/R/lib/libRblas.so
##
## locale:
## [1] C
##
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base
##
## loaded via a namespace (and not attached):
## [1] compiler_3.6.1",4.033927031726319
"What is the difference between the open-ce v1.5.0 and v1.5.2 releases?
",| 1.5.0 Software Packages https://github.com/open-ce/open-ce/releases/tag/open-ce-v1.5.0 | 1.5.2 Software Packages https://github.com/open-ce/open-ce/releases/tag/open-ce-v1.5.2 |,4.388058528393025
"What is the difference between the open-ce v1.5.0 and v1.5.2 releases?
","OpenCE 1.5.2 is now available on Summit. OpenCE 1.5.2 is available for python versions 3.9, 3.8, and 3.7. These builds can be accessed by loading the open-ce/1.5.2-py39-0, open-ce/1.5.2-py38-0, and open-ce/1.5.2-py37-0 modules, respectively.

The following packages are available in this release of OpenCE:",4.190154499174803
"What is the difference between the open-ce v1.5.0 and v1.5.2 releases?
","OpenCE 1.5.0 is now available on Summit. OpenCE 1.5.0 is available for python versions 3.7, 3.8, and 3.9. These builds can be accessed by loading the open-ce/1.5.0-py37-0, open-ce/1.5.0-py38-0, and open-ce/1.5.0-py39-0 modules, respectively.

The following packages are available in this release of OpenCE:",4.18542003098493
"How do I ensure a fresh login shell for setting up my parallel h5py environment?
","Before setting up your environment, you must exit and log back in so that you have a fresh login shell. This is to ensure that no previously activated environments exist in your $PATH environment variable. Additionally, you should execute module reset.

Building h5py from source is highly sensitive to the current environment variables set in your profile. Because of this, it is extremely important that all the modules and environments you plan to load are done in the correct order, so that all the environment variables are set correctly.",4.368499494451158
"How do I ensure a fresh login shell for setting up my parallel h5py environment?
","Frontier

.. code-block:: bash

   #!/bin/bash
   #SBATCH -A <PROJECT_ID>
   #SBATCH -J h5py
   #SBATCH -N 1
   #SBATCH -p batch
   #SBATCH -t 0:05:00

   unset SLURM_EXPORT_ENV

   cd $SLURM_SUBMIT_DIR
   date

   module load PrgEnv-gnu
   module load hdf5
   export PATH=""/path/to/your/miniconda/bin:$PATH""

   source activate /ccs/proj/<project_id>/<user_id>/envs/frontier/h5pympi-frontier

   srun -n42 python3 hdf5_parallel.py",4.145365377521007
"How do I ensure a fresh login shell for setting up my parallel h5py environment?
","source activate /ccs/proj/<project_id>/<user_id>/envs/summit/h5pympi-summit

   jsrun -n1 -r1 -a42 -c42 python3 hdf5_parallel.py

Andes

.. code-block:: bash

   #!/bin/bash
   #SBATCH -A <PROJECT_ID>
   #SBATCH -J h5py
   #SBATCH -N 1
   #SBATCH -p gpu
   #SBATCH -t 0:05:00

   unset SLURM_EXPORT_ENV

   cd $SLURM_SUBMIT_DIR
   date

   module load gcc
   module load hdf5
   module load python

   source activate /ccs/proj/<project_id>/<user_id>/envs/andes/h5pympi-andes

   srun -n42 python3 hdf5_parallel.py

Frontier

.. code-block:: bash",4.133970093617271
"What is the relationship between workgroups, wavefronts, and work-items in the Frontier system?
","| (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/Data-and-Storage-areas-3.pdf https://vimeo.com/803622140 | | 2023-02-15 | Using the Frontier Programming Environment | Matt Belhorn, HPC Engineer, ORNL | Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/frontier_programming_environment_20230215.pdf https://vimeo.com/803621185 | | 2023-02-15 | Frontier Programming Environment | Wael Elwasif, Computer Scientist, ORNL | Frontier Training Workshop",4.1153060775230825
"What is the relationship between workgroups, wavefronts, and work-items in the Frontier system?
",User Guide<summit-user-guide> and https://docs.olcf.ornl.gov/systems/index.html#Frontier User Guide<frontier-user-guide> can be used when building workflows for the non-SPI as well as the Citadel framework.,4.114494327734504
"What is the relationship between workgroups, wavefronts, and work-items in the Frontier system?
","The question ""what is a workflow system"" usually starts a spirited debate. Here, we will be referring to software or sets of tools used to automate processes and tasks on Slate. In addition to the Slate platform, these processes and tasks may utilize other NCCS compute and storage systems.",4.096245053896804
"How can I find more information about the Spark capabilities?
","Spark also provides a web UI to monitor cluster, and you can access it on your local machine by port forwarding the master node to local machine.

For example, if master node is running on andes338, you can run the following code on your local machine terminal.

ssh -N <USERNAME>@andes-login1.olcf.ornl.gov -L 8080:andes338.olcf.ornl.gov:8080

Then access the Spark dashboard using address http://localhost:8080/ on a web browser on your local machine.

The spark documentation is very useful tool, go through it to find the Spark capabilities.",4.18121120937221
"How can I find more information about the Spark capabilities?
",for more information.,4.142553153936677
"How can I find more information about the Spark capabilities?
",and view the results of your past jobs. More information about using these IBM quantum resources can be found on the IBM's Documentation or our https://docs.olcf.ornl.gov/quantum/quantum_systems/ibm_quantum.html.,4.046442085753395
"What should I do if ArgoCD does not automatically detect possible kustomize environment choices in the repository?
","Before going into how ArgoCD will use a kustomize configuration setup, a word about organizing the code repository. Prior to starting work with kustomize, take some time to consider what makes sense for setting up the directory of repository. Looking at the GitHub repository for kustomize, there is a kustomize Hello World document illustrating the basic layout to start with:",4.394449054509736
"What should I do if ArgoCD does not automatically detect possible kustomize environment choices in the repository?
","With a git repository defined in the ArgoCD Repositories settings that has kustomize environments ready for use, one can start using ArgoCD to deploy and manage kubernetes resources.",4.358685709599921
"What should I do if ArgoCD does not automatically detect possible kustomize environment choices in the repository?
","Image of the ArgoCD applications tab.

Prior to using ArgoCD, a git repository containing Kubernetes custom resources needs to be added for use.

The git repository containing Kubernetes custom resources is typically not the same git repository used to build the application.

There are three ways for ArgoCD to connect to a git repository:

connect using ssh

connect using https

connect using GitHub App",4.261765848078739
"What is the purpose of using pip install --no-cache-dir --no-binary=mpi4py mpi4py?
","Andes

.. code-block:: bash

   $ MPICC=""mpicc -shared"" pip install --no-cache-dir --no-binary=mpi4py mpi4py

Frontier

.. code-block:: bash

   $ MPICC=""cc -shared"" pip install --no-cache-dir --no-binary=mpi4py mpi4py

The MPICC flag ensures that you are using the correct C wrapper for MPI on the system. Building from source typically takes longer than a simple conda install, so the download and installation may take a couple minutes. If everything goes well, you should see a ""Successfully installed mpi4py"" message.

Next, install h5py from source.

Summit

.. code-block:: bash",4.306457029801677
"What is the purpose of using pip install --no-cache-dir --no-binary=mpi4py mpi4py?
","Because issues can arise when using conda and pip together (see link in https://docs.olcf.ornl.gov/systems/conda_basics.html#conda-refs), it is recommended to do this only if absolutely necessary.

To build a package from source, use pip install --no-binary=<package_name> <package_name>:

$ CC=gcc pip install --no-binary=numpy numpy

The CC=gcc flag will ensure that you are using the proper compiler and wrapper. Building from source results in a longer installation time for packages, so you may need to wait a few minutes for the install to finish.",4.23918287704895
"What is the purpose of using pip install --no-cache-dir --no-binary=mpi4py mpi4py?
","Summit

.. code-block:: bash

   $ HDF5_MPI=""ON"" CC=mpicc pip install --no-cache-dir --no-binary=h5py h5py

Andes

.. code-block:: bash

   $ HDF5_MPI=""ON"" CC=mpicc pip install --no-cache-dir --no-binary=h5py h5py

Frontier

.. code-block:: bash

   $ HDF5_MPI=""ON"" CC=cc HDF5_DIR=${OLCF_HDF5_ROOT} pip install --no-cache-dir --no-binary=h5py h5py",4.212679622634272
"Which library provides the com_err functionality?
","OLCF systems provide many software packages and scientific libraries pre-installed at the system-level for users to take advantage of. In order to link these libraries into an application, users must direct the compiler to their location. The module show command can be used to determine the location of a particular library. For example",3.95358007930698
"Which library provides the com_err functionality?
","libk5crypto.so.3 => /usr/lib64/libk5crypto.so.3 (0x00007ffef5bde000)
    libcom_err.so.2 => /lib64/libcom_err.so.2 (0x00007ffef59da000)
    libkrb5support.so.0 => /usr/lib64/libkrb5support.so.0 (0x00007ffef57cb000)
    libresolv.so.2 => /lib64/libresolv.so.2 (0x00007ffef55b3000)
    libsasl2.so.3 => /usr/lib64/libsasl2.so.3 (0x00007ffef5396000)
    libbrotlicommon.so.1 => /usr/lib64/libbrotlicommon.so.1 (0x00007ffef5175000)
    libkeyutils.so.1 => /usr/lib64/libkeyutils.so.1 (0x00007ffef4f70000)
    libselinux.so.1 => /lib64/libselinux.so.1 (0x00007ffef4d47000)",3.950640092443179
"Which library provides the com_err functionality?
","libk5crypto.so.3 => /usr/lib64/libk5crypto.so.3 (0x00007ffef5bde000)
    libcom_err.so.2 => /lib64/libcom_err.so.2 (0x00007ffef59da000)
    libkrb5support.so.0 => /usr/lib64/libkrb5support.so.0 (0x00007ffef57cb000)
    libresolv.so.2 => /lib64/libresolv.so.2 (0x00007ffef55b3000)
    libsasl2.so.3 => /usr/lib64/libsasl2.so.3 (0x00007ffef5396000)
    libbrotlicommon.so.1 => /usr/lib64/libbrotlicommon.so.1 (0x00007ffef5175000)
    libkeyutils.so.1 => /usr/lib64/libkeyutils.so.1 (0x00007ffef4f70000)
    libselinux.so.1 => /lib64/libselinux.so.1 (0x00007ffef4d47000)",3.950640092443179
"How do I run my executable under GDB on Frontier?
","GDB, the GNU Project Debugger, is a command-line debugger useful for traditional debugging and investigating code crashes. GDB lets you debug programs written in Ada, C, C++, Objective-C, Pascal (and many other languages).

GDB is available on Summit under all compiler families:

module load gdb

To use GDB to debug your application run:

gdb ./path_to_executable

Additional information about GDB usage can befound on the GDB Documentation Page.",4.281887541734858
"How do I run my executable under GDB on Frontier?
","GDB, the GNU Project Debugger, is a command-line debugger useful for traditional debugging and investigating code crashes. GDB lets you debug programs written in Ada, C, C++, Objective-C, Pascal (and many other languages).

GDB is availableon Summit under all compiler families:

module load gdb

To use GDB to debug your application run:

gdb ./path_to_executable

Additional information about GDB usage can befound on the GDB Documentation Page.",4.2762250830419255
"How do I run my executable under GDB on Frontier?
","GDB, the GNU Project Debugger, is a command-line debugger useful for traditional debugging and investigating code crashes. GDB lets you debug programs written in Ada, C, C++, Objective-C, Pascal (and many other languages).

GDB is available on andes via the gdb module:

module load gdb

To use GDB to debug your application run:

gdb ./path_to_executable

Additional information about GDB usage can befound on the GDB Documentation Page.",4.249074955524092
"What is the advantage of using the Spectral library on Summit?
","Spectral is a portable and transparent middleware library to enable use of the node-local burst buffers for accelerated application output on Summit. It is used to transfer files from node-local NVMe back to the parallel GPFS file system without the need of the user to interact during the job execution. Spectral runs on the isolated core of each reserved node, so it does not occupy resources and based on some parameters the user could define which folder to be copied to the GPFS. In order to use Spectral, the user has to do the following steps in the submission script:",4.302200396966559
"What is the advantage of using the Spectral library on Summit?
","Summit is connected to an IBM Spectrum Scale™ filesystem providing 250PB of storage capacity with a peak write speed of 2.5 TB/s. Summit also has access to the center-wide NFS-based filesystem (which provides user and project home areas) and has access to the center’s High Performance Storage System (HPSS) for user and project archival storage.

Summit is running Red Hat Enterprise Linux (RHEL) version 8.2.",4.14807826981832
"What is the advantage of using the Spectral library on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.141557728483436
"What is the issue with parallel HDF5 and ROMIO 3.2.1?
","A performance issue has been identified using parallel HDF5 with the default version of ROMIO provided in spectrum-mpi/10.2.0.10-20181214. To fully take advantage of parallel HDF5, users need to switch to the newer version of ROMIO and use ROMIO hints. The following shows recommended variables and hints for a 2 node job. Please note that hints must be tuned for a specific job.

$ module unload darshan-runtime
$ export OMPI_MCA_io=romio321
$ export ROMIO_HINTS=./my_romio_hints
$ cat $ROMIO_HINTS
romio_cb_write enable
romio_ds_write enable
cb_buffer_size 16777216
cb_nodes 2",4.411876447260019
"What is the issue with parallel HDF5 and ROMIO 3.2.1?
","Unload the darshan-runtime modulefile.

Alternatively, set export OMPI_MCA_io=romio314 in your environment to use the previous version of ROMIO. Please note that this version has known performance issues with parallel HDF5 (see ""Slow performance using parallel HDF5"" issue below).



The following issues were resolved with the February 19, 2019 upgrade:",4.180725403679663
"What is the issue with parallel HDF5 and ROMIO 3.2.1?
","There is a known issue in Spectrum MPI 10.2.0.10 provided by the spectrum-mpi/10.2.0.10-20181214 modulefile that causes a hang in MPI_Finalize when ROMIO 3.2.1 is being used and the darshan-runtime modulefile is loaded. The recommended and default Spectrum MPI version as of March 3, 2019 is Spectrum MPI 10.2.0.11 provided by the spectrum-mpi/10.2.0.11-20190201 modulefile. If you are seeing this issue, please make sure that you are using the latest version of Spectrum MPI. If you need to use a previous version of Spectrum MPI, your options are:

Unload the darshan-runtime modulefile.",4.1475903893297925
"Is there a way to search for modulefiles containing a specific string using the spider sub-command?
",| Command | Description | | --- | --- | | module spider | Shows the entire possible graph of modules | | module spider <modulename> | Searches for modules named <modulename> in the graph of possible modules | | module spider <modulename>/<version> | Searches for a specific version of <modulename> in the graph of possible modules | | module spider <string> | Searches for modulefiles containing <string> |,4.302041131530475
"Is there a way to search for modulefiles containing a specific string using the spider sub-command?
",| Command | Description | | --- | --- | | module spider | Shows the entire possible graph of modules | | module spider <modulename> | Searches for modules named <modulename> in the graph of possible modules | | module spider <modulename>/<version> | Searches for a specific version of <modulename> in the graph of possible modules | | module spider <string> | Searches for modulefiles containing <string> |,4.302041131530475
"Is there a way to search for modulefiles containing a specific string using the spider sub-command?
",| Command | Description | | --- | --- | | module spider | Shows the entire possible graph of modules | | module spider <modulename> | Searches for modules named <modulename> in the graph of possible modules | | module spider <modulename>/<version> | Searches for a specific version of <modulename> in the graph of possible modules | | module spider <string> | Searches for modulefiles containing <string> |,4.302041131530475
"What is the role of Slurm in managing the GPUs in Frontier?
","Due to the unique architecture of Frontier compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.

Before jumping into the examples, it is helpful to understand the output from the hello_jobstep program:",4.326488589435027
"What is the role of Slurm in managing the GPUs in Frontier?
","To override this default layout (not recommended), set -S 0 at job allocation.



Frontier uses SchedMD's Slurm Workload Manager for scheduling and managing jobs. Slurm maintains similar functionality to other schedulers such as IBM's LSF, but provides unique control of Frontier's resources through custom commands and options specific to Slurm. A few important commands can be found in the conversion table below, but please visit SchedMD's Rosetta Stone of Workload Managers for a more complete conversion reference.",4.320986699658582
"What is the role of Slurm in managing the GPUs in Frontier?
","The Slurm workload manager and the ROCr runtime treat each GCD as a separate GPU and visibility can be controlled using the ROCR_VISIBLE_DEVICES environment variable. Therefore, from this point on, the Frontier guide simply refers to a GCD as a GPU.",4.2801267019997855
"Is memory on Crusher migratable between CPU and GPU memory?
","The AMD MI250X and operating system on Crusher supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Crusher. In the following section we'll discuss how the heterogenous memory space on a Crusher node is surfaced within your application.",4.396376132710234
"Is memory on Crusher migratable between CPU and GPU memory?
","The page migration behavior summarized by the following tables represents the current, observable behavior. Said behavior will likely change in the near future.  CPU accesses to migratable memory may behave differently than other platforms you're used to. On Crusher, pages will not migrate from GPU HBM to CPU DDR4 based on access patterns alone. Once a page has migrated to GPU HBM it will remain there even if the CPU accesses it, and all accesses which do not resolve in the CPU cache will occur over the Infinity Fabric between the AMD ""Optimized 3rd Gen EPYC"" CPU and AMD MI250X GPU. Pages",4.303595240255469
"Is memory on Crusher migratable between CPU and GPU memory?
","The Crusher system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.292565203833473
"How can I ensure that my data is protected from unauthorized access on OLCF systems?
","The OLCF systems provide protections to maintain the confidentiality, integrity, and availability of user data. Measures include: the availability of file permissions, archival systems with access control lists, and parity/CRC checks on data paths/files. It is the user’s responsibility to set access controls appropriately for data. In the event of system failure or malicious actions, the OLCF makes no guarantee against loss of data nor makes a guarantee that a user’s data could not be potentially accessed, changed, or deleted by another individual. It is the user’s responsibility to insure",4.543709853690714
"How can I ensure that my data is protected from unauthorized access on OLCF systems?
","The OLCF systems provide protections to maintain the confidentiality, integrity, and availability of user data. Measures include the availability of file permissions, archival systems with access control lists, and parity and CRC checks on data paths and files. It is the user’s responsibility to set access controls appropriately for the data. In the event of system failure or malicious actions, the OLCF makes no guarantee against loss of data or that a user’s data can be accessed, changed, or deleted by another individual. It is the user’s responsibility to insure the appropriate level of",4.528298518501264
"How can I ensure that my data is protected from unauthorized access on OLCF systems?
","Never allow access to your sensitive data to anyone outside of your group.

Transfer of sensitive data must be through the use encrypted methods (scp, sftp, etc).

All sensitive data must be removed from all OLCF resources when your project has concluded.",4.453316855465566
"What is the maximum duration for a batch job on the DTNs at OLCF?
","DTNs are also accessible via the ""OLCF DTN"" (for moderate) and ""NCCS Open DTN"" (for open) Globus endpoints. For more information on using Globus at OLCF see https://docs.olcf.ornl.gov/systems/dtn_user_guide.html#data-transferring-data-globus.

The moderate DTNs also support batch jobs. The system contains 8 nodes accessible through the DTN batch system.

Most OLCF resources now use the Slurm batch scheduler, including the DTNs. Below is a table of useful commands for Slurm.",4.308611359234076
"What is the maximum duration for a batch job on the DTNs at OLCF?
","The OLCF offers a number of dedicated data transfer nodes to users. The nodes have been tuned specifically for wide area data transfers, and also perform well on the local area. There are also several utilities that the OLCF recommends for data transfer. Please refer to our https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#system-user-guides for information about the DTNs and available utilities.

Titan Scheduling Policy

=======================



.. note::

This details an official policy of the OLCF, and must be",4.151478240204803
"What is the maximum duration for a batch job on the DTNs at OLCF?
","strongly encourage users to run jobs on Titan that are as large as their

code will warrant. To that end, the OLCF implements queue policies that

enable large jobs to run in a timely fashion.



.. note::

The OLCF implements queue policies that encourage the

submission and timely execution of large, leadership-class jobs on

Titan.



The basic priority-setting mechanism for jobs waiting in the queue is

the time a job has been waiting relative to other jobs in the queue.

However, several factors are applied by the batch system to modify the",4.144951950208078
"What is the name of the job script that is being run on Summit?
","As is the case on other OLCF systems, computational work on Summit is performed within jobs. A typical job consists of several components:

A submission script

An executable

Input files needed by the executable

Output files created by the executable

In general, the process for running a job is to:

Prepare executables and input files

Write the batch script

Submit the batch script

Monitor the job's progress before and during execution",4.333354329987909
"What is the name of the job script that is being run on Summit?
","Summit has a node hierarchy that can be very confusing for the uninitiated. The Summit User Guide explains this in depth. This has some consequences that may be unusual for R programmers. A few important ones to note are:

You must have a script that you can run in batch, e.g. with Rscript or R CMD BATCH

All data that needs to be visible to the R process (including the script and your packages) must be on gpfs (not your home directory!).

You must launch your script from the launch nodes with jsrun.

We'll start with something very simple. The script is:

""hello world""",4.27682318637186
"What is the name of the job script that is being run on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.260543094970577
"How can we ensure that the new image has the correct permissions?
","We will use this Dockerfile to generate a BuildConfig and then build a new image in our project that has the correct permissions.

cat Dockerfile | oc new-build --dockerfile=- --to=my-image:tag

The build should start automatically, monitor it with oc logs bc/my-image -f.

Now that we have a new image with our /opt/application-data directory owned by the right user we can either update an existing deployment or create a new one with the image.",4.204896421493824
"How can we ensure that the new image has the correct permissions?
","We will use OpenShift to build a new image based on the upstream one and change owner of the directories that need to be writable during container execution. Here is an example Dockerfile which derives from an upstream image and changes ownership of directories to the user id that the container will run as in the cluster.

For example, if we are using the UID 63114 for our NCCS project user and we need to write to /opt/application-data during the runtime of the container image we could do this:

FROM upstream-image:tag
USER 0
RUN chown -R 63114 /opt/application-data
USER 63114",4.113842603177897
"How can we ensure that the new image has the correct permissions?
","Note that the above applies even to project PIs. In general, the OLCF will not overwrite existing UNIX permissions on data files owned by project members for the purpose of granting access to the project PI. Project PIs should work closely with project members throughout the duration of the project to ensure UNIX permissions are set appropriately.",4.077218187103434
"Can I use the NCCL backend with the open-ce v1.4.0 software packages on Summit?
","| Component | Old Version | New Version | | --- | --- | --- | | Red Hat Enterprise Linux | 8.3 | 8.4 | | Mellanox InfiniBand Driver | 5.3-1.0.0.1 | 5.4-1.0.3.0 | | NVIDIA driver | 450.36.06 | 460.106.00-1 | | Slurm | 20.02.6 | 20.02.7-1 |



<p style=""font-size:20px""><b>Summit: OpenCE 1.4.0 (October 13, 2021)</b></p>

OpenCE 1.4.0 is now available on Summit. OpenCE 1.4.0 is available for python versions 3.7, 3.8, and 3.9. These builds can be accessed by loading the open-ce/1.4.0-py37-0, open-ce/1.4.0-py38-0, and open-ce/1.4.0-py39-0 modules, respectively.",4.230798313897949
"Can I use the NCCL backend with the open-ce v1.4.0 software packages on Summit?
","The packages installed on Summit include only those that meet OLCF policy requirements for facility-provided software; build successfully on Summit's hardware architectures for a given toolchain and runtime environment; and are otherwise appropriate for use with the resource scheduler and communication fabrics supported by the OLCF.  Therefore, not all the packages in the E4S project collection are necessarily provided by the OLCF nor are these packages necessarily configured the same way for each toolchain and runtime environment  Not all packages are built for all compilers.",4.185238614526406
"Can I use the NCCL backend with the open-ce v1.4.0 software packages on Summit?
",| | 2020-02-10 | NCCL on Summit | Sylvain Jeaugey (NVIDIA) | Scaling Up Deep Learning Applications on Summit https://www.olcf.ornl.gov/calendar/scaling-up-deep-learning-applications-on-summit/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/12/Summit-NCCL.pdf https://vimeo.com/391520479 | | 2020-02-10 | Introduction to Watson Machine Learning CE | Brad Nemanich & Bryant Nelson (IBM) | Scaling Up Deep Learning Applications on Summit https://www.olcf.ornl.gov/calendar/scaling-up-deep-learning-applications-on-summit/ | (slides | recording),4.149286237188155
"Can I create a secured HTTPS route in Slate using the ""oc create route"" command without specifying a certificate?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.379081302955935
"Can I create a secured HTTPS route in Slate using the ""oc create route"" command without specifying a certificate?
","If no hostname is provided when creating routes, the default will be {SERVICE_NAME}-{PROJECT_NAME}.apps.<cluster>.ccs.ornl.gov. Any hostname that follows the pattern *.apps.<cluster>.ccs.ornl.gov can be provided, as long as another service isn't using that hostname already.

Secured routes (over HTTPS) offer encryption to keep connections private. You can use oc create route to create a secured HTTPS route.

Secured routes can use 3 different types of secure TLS termination.",4.342626339884476
"Can I create a secured HTTPS route in Slate using the ""oc create route"" command without specifying a certificate?
","The following command will create a secured route with re-encryption termination.

oc create route reencrypt --service=my-project \
  --hostname=my-project.apps.<cluster>.ccs.ornl.gov \
  --dest-ca-cert=ca.crt

Note that the --dest-ca-cert flag for the destination CA certificate is required for re-encryption.

The outputted YAML will look like this example:",4.246860319725092
"Can I choose any hostname for the Route object in Slate?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.2315534802187775
"Can I choose any hostname for the Route object in Slate?
","If no hostname is provided when creating routes, the default will be {SERVICE_NAME}-{PROJECT_NAME}.apps.<cluster>.ccs.ornl.gov. Any hostname that follows the pattern *.apps.<cluster>.ccs.ornl.gov can be provided, as long as another service isn't using that hostname already.

Secured routes (over HTTPS) offer encryption to keep connections private. You can use oc create route to create a secured HTTPS route.

Secured routes can use 3 different types of secure TLS termination.",4.156505520238355
"Can I choose any hostname for the Route object in Slate?
","In general, using FQDNs to access a service is more convenient. If your service communicates over HTTP or HTTPS, you can set up a https://docs.olcf.ornl.gov/systems/services.html#slate_routes to achieve this. If your service uses another protocol, you can use https://docs.olcf.ornl.gov/systems/services.html#slate_nodeports.

NodePort is a type of Service that reserves a port across the cluster that can route traffic to your pods. This is more flexible than a Route and can handle any TCP or UDP traffic.",4.144689630349342
"How do I access the Persistent Volume Claim menu in OpenShift?
","From the main screen of a project go to the Storage option in the hamburger menu on the left hand side of the screen, then click Persistent Volume Claims

Persistent Volume Claim Menu

In the upper right click the Create Persistent Volume Claim button.

Create Storage

This will take you to a screen that allows you to select what you need for your PVC. Give your claim a name and request an amount of storage and click Create. You can also click Edit YAML to edit the PVC object directly.

PV Settings",4.474520837463935
"How do I access the Persistent Volume Claim menu in OpenShift?
","Add Storage Menu

You should see a green popup appear in the upper right saying that the storage was added. This should additionally trigger a new deployment. To make sure a new deployment happened look at the Created time of the top most deployment.

A PersistentVolume is backed up by creating a VolumeSnapshot object. The VolumeSnapshot will create a point in time backup of your PersistentVolume and is something that you would likely do before an upgrade.

A Volume Snapshot will bind to a PersistentVolumeClaim. An example PersistentVolumeClaim to bind a VolumeSnapshot to follows:",4.386652732118092
"How do I access the Persistent Volume Claim menu in OpenShift?
","Once the Persistent Volume is created its allocated memory is available to be claimed by a project. This is done through a Persistent Volume Claim. PVC's are created using a YAML file in the same manner that PV's are created. An example of a PVC that claims the PV created in the previous section follows:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: storage-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

The claim is then placed using:

oc create -f storage-1.yaml",4.316898342033491
"How can I move the test_file from my local space to the NVMe?
","#Copy the file from the NVMe to your local space
jsrun -n 1 cp /mnt/bb/$USER/test_file .

#Check the file locally
ls -l test_file",4.423757183478139
"How can I move the test_file from my local space to the NVMe?
","date

# Change directory to user scratch space (GPFS)
cd /gpfs/alpine/<projid>/scratch/<userid>

echo "" ""
echo ""*****ORIGINAL FILE*****""
cat test.txt
echo ""***********************""

# Move file from GPFS to SSD
mv test.txt /mnt/bb/<userid>

# Edit file from compute node
srun -n1 hostname >> /mnt/bb/<userid>/test.txt

# Move file from SSD back to GPFS
mv /mnt/bb/<userid>/test.txt .

echo "" ""
echo ""*****UPDATED FILE******""
cat test.txt
echo ""***********************""

And here is the output from the script:

$ cat nvme_test-<jobid>.out
Mon May 17 12:28:18 EDT 2021",4.193132043198103
"How can I move the test_file from my local space to the NVMe?
","date

# Change directory to user scratch space (GPFS)
cd /gpfs/alpine/<projid>/scratch/<userid>

echo "" ""
echo ""*****ORIGINAL FILE*****""
cat test.txt
echo ""***********************""

# Move file from GPFS to SSD
mv test.txt /mnt/bb/<userid>

# Edit file from compute node
srun -n1 hostname >> /mnt/bb/<userid>/test.txt

# Move file from SSD back to GPFS
mv /mnt/bb/<userid>/test.txt .

echo "" ""
echo ""*****UPDATED FILE******""
cat test.txt
echo ""***********************""

And here is the output from the script:

$ cat nvme_test-<jobid>.out",4.163961389068072
"What is the advantage of using MPS on Summit?
",For Summit:,4.2636989277236506
"What is the advantage of using MPS on Summit?
","Summit is connected to an IBM Spectrum Scale™ filesystem providing 250PB of storage capacity with a peak write speed of 2.5 TB/s. Summit also has access to the center-wide NFS-based filesystem (which provides user and project home areas) and has access to the center’s High Performance Storage System (HPSS) for user and project archival storage.

Summit is running Red Hat Enterprise Linux (RHEL) version 8.2.",4.184857918410379
"What is the advantage of using MPS on Summit?
","Volta GPUs improve MPS with new capabilities. For instance, each Volta MPS client (MPI rank) is assigned a ""subcontext"" that has its own GPU address space, instead of sharing the address space with other clients. This isolation helps protect MPI ranks from out-of-range reads/writes performed by other ranks within CUDA kernels. Because each subcontext manages its own GPU resources, it can submit work directly to the GPU without the need to first pass through the MPS server. In addition, Volta GPUs support up to 48 MPS clients (up from 16 MPS clients on Pascal).",4.184728046544032
"What is the performance difference between using mma.sync() and CUTLASS on Summit?
","The example above performs a 16-bit accumulate operation, but 32-bit is also supported. Please see the provided samples and the WMMA documentation for more details.

CUDA 10 introduced a lower-level alternative to WMMA with the mma.sync() instruction. This is a very low-level instruction that requires the programmer handle the data movement provided by WMMA explicitly, but is capable of higher performance. Details of mma.sync can be found in the PTX documentation and examples for using this feature via CUTLASS cane be found in the second half of this GTC presentation.",4.164068332318964
"What is the performance difference between using mma.sync() and CUTLASS on Summit?
","The CUTLASS library provides a variety of primitives that are optimized for proper data layout and movement to achieve the maximum possible performance of a matrix multiplation on an NVIDIA GPU. These include iterators for blocking, loading, and storing matrix tiles, plus optimized classes for transforming the data and performing the actual multiplication. CUTLASS provides extensive documentation of these features and examples have been provided. Interested developers are also encouraged to watch the CUTLASS introduction video from GTC2018.",4.055292233154501
"What is the performance difference between using mma.sync() and CUTLASS on Summit?
",For Summit:,3.996882569522992
"Can I collaborate with other users on a project within the OLCF QCUP program?
","Welcome! The information below introduces how we structure projects and user accounts for access to the Quantum resources within the QCUP program. In general, OLCF QCUP resources are granted to projects, which are in turn made available to the approved users associated with each project.",4.4497248522703465
"Can I collaborate with other users on a project within the OLCF QCUP program?
","Collaborators involved with an approved and activated OLCF project can apply for a user account associated with it. There are several steps in receiving a user account, and we're here to help you through them.

Project PIs do not receive a user account with project creation, and must also apply.",4.391036266260462
"Can I collaborate with other users on a project within the OLCF QCUP program?
","The OLCF will then establish a QCUP project and notify the PI of its creation along with the 6-character OLCF QCUP Project ID and resources allocation details. At this time project participants may proceed with applying for their individual user accounts.

QCUP Projects have a finite duration; when starting, projects get however many months are left in that allocation period and then must be renewed for subsequent 6 month intervals. Projects can be renewed by filling out a renewal form (:download:`Accounts Renewal Form <Quantum-Renewal-Form.docx>`) and emailing it to accounts@ccs.ornl.gov.",4.349774466878425
"How can I render the scene and save the resulting image in Paraview?
","# Render scene and save resulting image
Render()
SaveScreenshot('pvbatch-test.png',ImageResolution=[1080, 1080])

For older versions of ParaView (e.g., 5.9.1), line 23 should be 'AnyLocation' (no space).



If everything is working properly, the above image should be generated after the batch job is complete.

All of the above can also be achieved in an interactive batch job through the use of the salloc command on Andes or the bsub -Is command on Summit. Recall that login nodes should not be used for memory- or compute-intensive tasks, including ParaView.",4.372277243082856
"How can I render the scene and save the resulting image in Paraview?
","VisIt developers have been notified, and at this time the current workaround is to disable Scalable Rendering from being used. To do this, go to Options→Rendering→Advanced and set the ""Use Scalable Rendering"" option to ""Never"".

However, this workaround has been reported to affect VisIt's ability to save images, as scalable rendering is utilized to save plots as image files (which can result in another compute engine crash). To avoid this, screen capture must be enabled. Go to File→""Set save options"" and check the box labeled ""Screen capture"".",4.145090556797976
"How can I render the scene and save the resulting image in Paraview?
","Submitting one of the above scripts will submit a job to the batch partition for five minutes using 28 MPI tasks across 1 node. As rendering speeds and memory issues widely vary for different datasets and MPI tasks, users are encouraged to find the optimal amount of MPI tasks to use for their data. Users with large datasets may also find a slight increase in performance by using the gpu partition on Andes, or by utilizing the GPUs on Summit. Once the batch job makes its way through the queue, the script will launch the loaded ParaView module (specified with module load) and execute a python",4.08793659129148
"What is the purpose of the StatefulSet?
","We will be deploying MongoDB with a StatefulSet. This is a special kind of deployment controller that is different from a normal Deployment in a few distinct ways and is primarily meant for for applications that rely on well known names for each pod.

We will create a Service with the StatefulSet because the StatefulSet controller requires a headless service in order to provide well-known DNS identifiers.",4.387456301806919
"What is the purpose of the StatefulSet?
","apiVersion: apps/v1
kind: StatefulSet
metadata:
  # statefulset name
  name: test-pod-stateful-set
spec:
  # number of replicas
  replicas: 3
  selector:
    # this sets the label the stateful set is looking for
    matchLabels:
      app: test-pod
  template:
    metadata:
      # labels are how the stateful set keep track of their objects. This sets a label on the pod
      labels:
        app: test-pod
    spec:
      containers:
        # container name
      - name: test-pod
        # using the base image",4.234361433662616
"What is the purpose of the StatefulSet?
","By design pods are ephemeral. They are meant to be something that can be killed with relatively little impact to the application. Since containers are ephemeral, we need a way to consume storage that is persistent and can hold data for our applications. One option that Kubernetes has for storing data is with Persistent Volumes. PersistentVolume objects are created by the cluster administrator and reference storage that is on a resilient storage platform such as NetApp. A user can request storage by creating a PersistentVolumeClaim which requests a desired size for a PersistentVolume. The",4.125470041950837
"How do I compute gradients with AMP in Summit?
","NVIDIA has also integrated a technology called Automatic Mixed Precision (AMP) into several common frameworks, TensorFlow, PyTorch, and MXNet at time of writing. In most cases AMP can be enabled via a small code change or via setting and environment variable. AMP does not strictly replace all matrix multiplication operations with half precision, but uses graph optimization techniques to determine whether a given layer is best run in full or half precision.

Examples are provided for using AMP, but the following sections summarize the usage in the three supported frameworks.",4.166687697527093
"How do I compute gradients with AMP in Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.161422127371157
"How do I compute gradients with AMP in Summit?
",For Summit:,4.157089254202255
"What is the purpose of the jsrun -n 1 python3 demo.py command?
","This section describes tools that users might find helpful to better understand the jsrun job launcher.

hello_jsrun is a ""Hello World""-type program that users can run on Summit nodes to better understand how MPI ranks and OpenMP threads are mapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A screencast showing how to use Hello_jsrun is also available: https://vimeo.com/261038849

Job Step Viewer provides a graphical view of an application's runtime layout on Summit. It allows users to preview and quickly iterate with multiple jsrun options to understand and optimize job launch.",4.153676239061211
"What is the purpose of the jsrun -n 1 python3 demo.py command?
","Finally, run the demo program by executing the following commands from a Summit login node:

$ source setup.bash
$ python3 demo.py3

Congratulations! You should now see interactive output from EnTK while it launches and monitors your job on Summit.",4.126060611077336
"What is the purpose of the jsrun -n 1 python3 demo.py command?
","jsrun --nrs 1 --tasks_per_rs 1 --cpu_per_rs 1 --gpu_per_rs 1 --smpiargs=""-disable_gpu_hooks"" \
      python /my_path/my_rapids_script.py dataset_part01 &
jsrun --nrs 1 --tasks_per_rs 1 --cpu_per_rs 1 --gpu_per_rs 1 --smpiargs=""-disable_gpu_hooks"" \
      python /my_path/my_rapids_script.py dataset_part02 &
jsrun --nrs 1 --tasks_per_rs 1 --cpu_per_rs 1 --gpu_per_rs 1 --smpiargs=""-disable_gpu_hooks"" \
      python /my_path/my_rapids_script.py dataset_part03 &
...
wait

Be aware of different OLCF's queues and scheduling policies to make best use of regular and high memory Summit nodes.",4.084959325120016
"How do I specify the project for my high-throughput computing environment in Parsl?
","Parsl needs to be able to write to the working directory from compute nodes, so we will work from within the member work directory and assume a project ID ABC123:

$ mkdir -p ${MEMBERWORK}/abc123/parsl-demo/
$ cd ${MEMBERWORK}/abc123/parsl-demo/

To run an example ""Hello world"" program with Parsl on Summit, create a file called hello-parsl.py with the following contents, but with your own project ID in the line specified:",4.256529201159687
"How do I specify the project for my high-throughput computing environment in Parsl?
","Parsl is a flexible and scalable parallel programming library for Python which is being developed at the University of Chicago. It augments Python with simple constructs for encoding parallelism. For more information about Parsl, please refer to its documentation.

Parsl can be installed with Conda for use on Summit by running the following from a login node:

$ module load workflows
$ module load parsl/1.1.0

The following instructions illustrate how to run a ""Hello world"" program with Parsl on Summit.",4.231562018623069
"How do I specify the project for my high-throughput computing environment in Parsl?
","It is highly recommended to create new environments in the ""Project Home"" directory (on Summit, this is /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>). This space avoids purges, allows for potential collaboration within your project, and works better with the compute nodes. It is also recommended, for convenience, that you use environment names that indicate the hostname, as virtual environments created on one system will not necessarily work on others.",4.156955399051057
"How do I execute an MPI binary on multiple compute nodes in parallel using Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.398120575332015
"How do I execute an MPI binary on multiple compute nodes in parallel using Frontier?
","Because a Frontier compute node has two hardware threads available (2 logical cores per physical core), this enables the possibility of multithreading your application (e.g., with OpenMP threads). Although the additional hardware threads can be assigned to additional MPI tasks, this is not recommended. It is highly recommended to only use 1 MPI task per physical core and to use OpenMP threads instead on any additional logical cores gained when using both hardware threads.",4.331715854962149
"How do I execute an MPI binary on multiple compute nodes in parallel using Frontier?
","There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.

Assigning MPI ranks in a ""round-robin"" (cyclic) manner across L3 cache regions (sockets) is the default behavior on Frontier. This mode will assign consecutive MPI tasks to different sockets before it tries to ""fill up"" a socket.",4.300252502860245
"Are there any measures in place to prevent unauthorized access to my data on OLCF systems?
","The OLCF systems provide protections to maintain the confidentiality, integrity, and availability of user data. Measures include: the availability of file permissions, archival systems with access control lists, and parity/CRC checks on data paths/files. It is the user’s responsibility to set access controls appropriately for data. In the event of system failure or malicious actions, the OLCF makes no guarantee against loss of data nor makes a guarantee that a user’s data could not be potentially accessed, changed, or deleted by another individual. It is the user’s responsibility to insure",4.461487507724978
"Are there any measures in place to prevent unauthorized access to my data on OLCF systems?
","The OLCF systems provide protections to maintain the confidentiality, integrity, and availability of user data. Measures include the availability of file permissions, archival systems with access control lists, and parity and CRC checks on data paths and files. It is the user’s responsibility to set access controls appropriately for the data. In the event of system failure or malicious actions, the OLCF makes no guarantee against loss of data or that a user’s data can be accessed, changed, or deleted by another individual. It is the user’s responsibility to insure the appropriate level of",4.4421088512641935
"Are there any measures in place to prevent unauthorized access to my data on OLCF systems?
","OLCF resources are federal computer systems, and as such, users should have no explicit or implicit expectation of privacy. OLCF employees and authorized vendor personnel with “root” privileges have access to all data on OLCF systems. Such employees can also login to OLCF systems as other users. As a general rule, OLCF employees will not discuss your data with any unauthorized entities nor grant access to data files to any person other than the UNIX “owner” of the data file, except in the following situations:",4.405301690247091
"How many hardware threads will be created for each physical CPU core when using the --threads-per-core=2 option in Crusher?
","Now the output shows that each OpenMP thread ran on (one of the hardware threads of) its own physical CPU core. More specifically (see the Crusher Compute Node diagram), OpenMP thread 000 of MPI rank 000 ran on hardware thread 001 (i.e., physical CPU core 01), OpenMP thread 001 of MPI rank 000 ran on hardware thread 002 (i.e., physical CPU core 02), OpenMP thread 000 of MPI rank 001 ran on hardware thread 009 (i.e., physical CPU core 09), and OpenMP thread 001 of MPI rank 001 ran on hardware thread 010 (i.e., physical CPU core 10) - as intended.",4.26901047550751
"How many hardware threads will be created for each physical CPU core when using the --threads-per-core=2 option in Crusher?
","physical CPU cores available, in which case the remaining OpenMP threads will share hardware threads and a WARNING will be issued as shown in the previous example.",4.232755130611172
"How many hardware threads will be created for each physical CPU core when using the --threads-per-core=2 option in Crusher?
","Comparing this output to the Crusher Compute Node diagram, we see that each pair of OpenMP threads is contained within a single physical core. MPI rank 000 ran on hardware threads 001 and 065 (i.e. physical CPU core 01) and MPI rank 001 ran on hardware threads 009 and 073 (i.e. physical CPU core 09).

There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.",4.208768079515027
"What is the purpose of the World Work Directory on Slate?
","Each project has a World Work directory that resides in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. World Work areas can be accessed by all users of the system and are intended for sharing of data between projects. World Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but rather should be",4.06130244159739
"What is the purpose of the World Work Directory on Slate?
","World Work Directory: 775

For example, if you have data that must be restricted only to yourself, keep them in your Member Work directory for that project (and leave the default permissions unchanged). If you have data that you intend to share with researchers within your project, keep them in the project’s Project Work directory. If you have data that you intend to share with researchers outside of a project, keep them in the project’s World Work directory.",4.050440432798927
"What is the purpose of the World Work Directory on Slate?
",The difference between the three lies in the accessibility of the data to project members and to researchers outside of the project. Member Work directories are accessible only by an individual project member by default. Project Work directories are accessible by all project members. World Work directories are readable by any user on the system.,4.023550635438308
"How do I access the GPFS or NFS file systems on Summit from within a container?
","Summit is connected to an IBM Spectrum Scale™ filesystem providing 250PB of storage capacity with a peak write speed of 2.5 TB/s. Summit also has access to the center-wide NFS-based filesystem (which provides user and project home areas) and has access to the center’s High Performance Storage System (HPSS) for user and project archival storage.

Summit is running Red Hat Enterprise Linux (RHEL) version 8.2.",4.233264024829615
"How do I access the GPFS or NFS file systems on Summit from within a container?
","Singularity is a container framework from Sylabs. On Summit, we will use Singularity solely as the runtime. We will convert the tar files of the container images Podman creates into sif files, store those sif files on GPFS, and run them with Jsrun. Singularity also allows building images but ordinary users cannot utilize that on Summit due to additional permissions not allowed for regular users.",4.232040358144944
"How do I access the GPFS or NFS file systems on Summit from within a container?
","Please note, Summit will mount a new filesystem once returned to service.  Data stored on Alpine at the time of its decommission on January 01 will not be available.  Users will be responsible for transferring data onto Summit's new filesystem



Alpine II will be available early 2024.

The previous center-wide GPFS scratch filesystem, Alpine, will be decommissioned in January 2024. A new scratch filesystem will be made available for projects with 2024 Summit allocations in early 2024. Users will be responsible for transferring any needed data onto the new scratch filesystem once available.",4.229917407816762
"How many nodes have a GPU of 3?
","We are targeting use cases that need GPUs for long running services. For batch access to GPUs we recommend using the standard HPC clusters in NCCS

The Slate Marble cluster has nodes with three NVIDIA Tesla V100 GPUs per node available for scheduling so a single pod could request from 1 to 3 GPUs",4.2423021073682605
"How many nodes have a GPU of 3?
","Login nodes have (2) 16-core Power9 CPUs and (4) V100 GPUs. Compute nodes have (2) 22-core Power9 CPUs and (6) V100 GPUs.

Summit nodes are connected to a dual-rail EDR InfiniBand network providing a node injection bandwidth of 23 GB/s. Nodes are interconnected in a Non-blocking Fat Tree topology. This interconnect is a three-level tree implemented by a switch to connect nodes within each cabinet (first level) along with Director switches (second and third level) that connect cabinets together.",4.23886452282799
"How many nodes have a GPU of 3?
","Crusher node architecture diagram

There are [4x] NUMA domains per node and [2x] L3 cache regions per NUMA for a total of [8x] L3 cache regions. The 8 GPUs are each associated with one of the L3 regions as follows:  NUMA 0:  hardware threads 000-007, 064-071 | GPU 4  hardware threads 008-015, 072-079 | GPU 5  NUMA 1:  hardware threads 016-023, 080-087 | GPU 2  hardware threads 024-031, 088-095 | GPU 3  NUMA 2:  hardware threads 032-039, 096-103 | GPU 6  hardware threads 040-047, 104-111 | GPU 7  NUMA 3:  hardware threads 048-055, 112-119 | GPU 0  hardware threads 056-063, 120-127 | GPU 1",4.213629348701982
"Who can access Crusher?
",Crusher is connected to the center-wide IBM Spectrum Scale™ filesystem providing 250 PB of storage capacity with a peak write speed of 2.5 TB/s. Crusher also has access to the center-wide NFS-based filesystem (which provides user and project home areas). While Crusher does not have direct access to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#dtn-user-guide to move data to/from HPSS.,4.1808725302541525
"Who can access Crusher?
","Crusher is an National Center for Computational Sciences (NCCS) moderate-security system that contains identical hardware and similar software as the upcoming Frontier system. It is used as an early-access testbed for Center for Accelerated Application Readiness (CAAR) and Exascale Computing Project (ECP) teams as well as NCCS staff and our vendor partners. The system has 2 cabinets, the first with 128 compute nodes and the second with 64 compute nodes, for a total of 192 compute nodes.",4.152853508422058
"Who can access Crusher?
","Crusher users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.119433649978224
"Can I install software on OLCF resources myself?
","Order is for convenience and no implication of priority is implied.

Products installed should be limited to those explicitly listed in the project application and approved by the OLCF.

The project application is reviewed by the Export Control Office. If you would like to install additional packages not listed in your original application, the Project PI must contact the OLCF at help@olcf.ornl.gov before making changes.

Products must provide appropriate modules for their software.",4.356274278722473
"Can I install software on OLCF resources myself?
","Users should acknowledge the OLCF in all publications and presentations that speak to work performed on OLCF resources:

This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725.



To request software installation, please contact help@olcf.ornl.gov with a description of the software, including the following details:

Software name.

Description of the software and its purpose. Is it export controlled?",4.343628285681456
"Can I install software on OLCF resources myself?
","All software used on OLCF computers must be appropriately acquired and used according to the appropriate software license agreement. Possession, use, or transmission of illegally obtained software is prohibited. Likewise, users shall not copy, store, or transfer copyrighted software, except as permitted by the owner of the copyright. Only export-controlled codes approved by the Export Control Office may be run by parties with sensitive data agreements.

Users must not intentionally introduce or use malicious software such as computer viruses, Trojan horses, or worms.",4.324678374891235
"Is it allowed to copy, store, or transfer copyrighted software on OLCF computers?
","All software used on OLCF computers must be appropriately acquired and used according to the appropriate software license agreement. Possession, use, or transmission of illegally obtained software is prohibited. Likewise, users shall not copy, store, or transfer copyrighted software, except as permitted by the owner of the copyright. Only export-controlled codes approved by the Export Control Office may be run by parties with sensitive data agreements.

Users must not intentionally introduce or use malicious software such as computer viruses, Trojan horses, or worms.",4.581486441100853
"Is it allowed to copy, store, or transfer copyrighted software on OLCF computers?
","Users are not allowed to reconstruct information or software for which they are not authorized. This includes but is not limited to any reverse engineering of copyrighted software or firmware present on OLCF computing resources.

Users are accountable for their actions and may be held accountable to applicable administrative or legal sanctions.",4.475186602232074
"Is it allowed to copy, store, or transfer copyrighted software on OLCF computers?
","All software used on OLCF computers must be appropriately acquired and used according to the appropriate software license agreement. Possession, use, or transmission of illegally obtained software is prohibited. Likewise, users shall not copy, store, or transfer copyrighted software, except as permitted by the owner of the copyright. Only export-controlled codes approved by the Export Control Office may be run by parties with sensitive data agreements.

<string>:645: (INFO/1) Duplicate implicit target name: ""malicious software"".",4.464262113140714
"How do I report any issues or concerns related to the OLCF QCUP program?
","Welcome! The information below introduces how we structure projects and user accounts for access to the Quantum resources within the QCUP program. In general, OLCF QCUP resources are granted to projects, which are in turn made available to the approved users associated with each project.",4.30642560170249
"How do I report any issues or concerns related to the OLCF QCUP program?
","A QCUP proposal describes the nature, methodology, and merits of the project, explains why it requires access to QCUP resources, and outlines any other essential information that might be needed for its consideration. Project applications are submitted using the Project Application Form. Select ""OLCF Quantum Computing User Program"" from the dropdown menu.

For QCUP Projects, all proposed work must be open, fundamental research and no Export Control, PHI, or other controlled data can be used.",4.295525580236901
"How do I report any issues or concerns related to the OLCF QCUP program?
","To gain access, you must first submit a project proposal to the OLCF QCUP (see https://docs.olcf.ornl.gov/systems/quantum_access.html#quantum-proj) or join an existing QCUP project (see https://docs.olcf.ornl.gov/systems/quantum_access.html#quantum-user). The Quantum Resource Utilization Council (QRUC), as well as independent referees, review and approve all QCUP project proposals.  Applications to QCUP are accepted year-round via the project application form found below. Once a project is approved, then all of the users associated with the project will need to apply for a",4.289082658513417
"How do I submit a batch job on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.372455373837562
"How do I submit a batch job on Summit?
","As is the case on other OLCF systems, computational work on Summit is performed within jobs. A typical job consists of several components:

A submission script

An executable

Input files needed by the executable

Output files created by the executable

In general, the process for running a job is to:

Prepare executables and input files

Write the batch script

Submit the batch script

Monitor the job's progress before and during execution",4.3500524805129785
"How do I submit a batch job on Summit?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.332386949189932
"How do I optimize the use of hardware FP atomic instructions on Frontier?
","The fast hardware-based Floating point (FP) atomic operations available on MI250X are assumed to be working on coarse grained memory regions; when these instructions are applied to a fine-grained memory region, they will silently produce a no-op. To avoid returning incorrect results, the compiler never emits hardware-based FP atomics instructions by default, even when applied to coarse grained memory regions. Currently, users can use the -munsafe-fp-atomics flag to force the compiler to emit hardware-based FP atomics. Using hardware-based FP atomics translates in a substantial performance",4.3287603262675125
"How do I optimize the use of hardware FP atomic instructions on Frontier?
","The fast hardware-based Floating point (FP) atomic operations available on MI250X are assumed to be working on coarse grained memory regions; when these instructions are applied to a fine-grained memory region, they will silently produce a no-op. To avoid returning incorrect results, the compiler never emits hardware-based FP atomics instructions by default, even when applied to coarse grained memory regions. Currently, users can use the -munsafe-fp-atomics flag to force the compiler to emit hardware-based FP atomics. Using hardware-based FP atomics translates in a substantial performance",4.3287603262675125
"How do I optimize the use of hardware FP atomic instructions on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.2869636537591225
"Are there any restrictions on the number of nodes that can be requested for a job in the OLCF Policy?
","System reservation (a dedicated set of nodes at a specific date/time)

Increased disk quota

Purge exemption for User/Group/World Work areas

Special requests are reviewed weekly by the OLCF Resource Utilization Council. Please contact help@olcf.ornl.gov for more information.



This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title: Computing Policy Version: 12.10",4.344357933288209
"Are there any restrictions on the number of nodes that can be requested for a job in the OLCF Policy?
","Allocation Overuse Policy

Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.318347837422478
"Are there any restrictions on the number of nodes that can be requested for a job in the OLCF Policy?
","As a DOE Leadership Computing Facility, OLCF has a mandate that a large portion of Frontier's usage come from large, leadership-class (a.k.a. capability) jobs. To ensure that OLCF complies with this directive, we strongly encourage users to run jobs on Frontier that are as large as their code will allow. To that end, OLCF implements queue policies that enable large jobs to run in a timely fashion.

The OLCF implements queue policies that encourage the submission and timely execution of large, leadership-class jobs on Frontier.",4.2769216943316986
"Can the SPI handle large-scale workloads?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads. Because of this, many processes used within the SPI are very similar to those used for standard non-SPI. This page lists the differences you may see when using OLCF resources to execute SPI resources.",4.23568409882888
"Can the SPI handle large-scale workloads?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.  This page lists the differences you may see when using OLCF resources to execute SPI workflows.  The page will also point to sections within the site where more information on standard non-SPI use can be found.",4.232353702317835
"Can the SPI handle large-scale workloads?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.

The SPI provides access to the OLCF's Summit resource for compute.  To safely separate SPI and non-SPI workflows, SPI workflows must use a separate login node named https://docs.olcf.ornl.gov/systems/citadel_user_guide.html#Citadel<spi-compute-citadel>.  Citadel provides a login node specifically for SPI workflows.",4.194552234274966
"What is the count_value for the date 2022-01-03?
","+----------+----+-----------+
|      date| org|count_value|
+----------+----+-----------+
|2022-01-03| BIO|         37|
|2022-01-02| ENV|         53|
|2022-01-03| CHE|         39|
|2022-01-03| PHY|         46|
|2022-01-01| CSC|         45|
|2022-01-03| CSC|         48|
|2022-01-01| BIO|         39|
|2022-01-01| MAT|         42|
|2022-01-02| CHE|         44|
|2022-01-03| ENV|         33|
|2022-01-01| ENG|         33|
|2022-01-02| ENG|         28|
|2022-01-01| ENV|         33|
|2022-01-02| CSC|         45|
|2022-01-02| MAT|         51|
|2022-01-01| PHY|         38|",4.279969875525746
"What is the count_value for the date 2022-01-03?
","+=======+=============+=============+========================+======================+

| 1     | 11,250      | --          | 24.0                   | 15                   |

+-------+-------------+-------------+------------------------+----------------------+

| 2     | 3,750       | 11,249      | 24.0                   | 5                    |

+-------+-------------+-------------+------------------------+----------------------+

| 3     | 313         | 3,749       | 12.0                   | 0                    |",3.946046601581334
"What is the count_value for the date 2022-01-03?
","|2022-01-01| PHY|         38|
|2022-01-01|PHRM|         40|
|2022-01-03|PHRM|         42|
|2022-01-02|PHRM|         43|
|2022-01-03| ENG|         56|
+----------+----+-----------+
only showing top 20 rows",3.922887993698049
"What is the purpose of the ""proxy_temp_path"" directive in Slate?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",3.923181710675993
"What is the purpose of the ""proxy_temp_path"" directive in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",3.888976977230641
"What is the purpose of the ""proxy_temp_path"" directive in Slate?
","access_log  /tmp/access.log  main;

    client_body_temp_path /tmp/nginx 1 2;
    proxy_temp_path /tmp/nginx-proxy;
    fastcgi_temp_path /tmp/nginx-fastcgi;
    uwsgi_temp_path /tmp/nginx-uwsgi;
    scgi_temp_path /tmp/nginx-scgi;

    sendfile            on;
    tcp_nopush          on;
    tcp_nodelay         on;
    keepalive_timeout   65;
    types_hash_max_size 2048;

    include             /etc/nginx/mime.types;
    default_type        application/octet-stream;",3.8778720516806993
"What is the role of the IonQ CLI tool?
","Information on submitting jobs to IonQ systems, system availability, checking job status, and tracking usage can be found via the IonQ Cloud Console.

A recommended workflow for running on IonQ's quantum computers is to utilize the emulator first, then run on one of the quantum computers. This is highlighted in the examples.",4.230845967357597
"What is the role of the IonQ CLI tool?
","IonQ backends are available via the IonQ cloud interface via the API and also via many quantum Software Development Kits (SDK’s)



Users can access information about IonQ’s systems, view submitted jobs, look up machine availability, and update job notification preferences via the IonQ Cloud Console.

Jupyter at OLCF: Access to the IonQ queues can also be obtained via OLCF JupyterHub, a web-based interactive computing environment. See examples of common use case notebooks at IonQ Notebook Samples.",4.229979488599507
"What is the role of the IonQ CLI tool?
","After submitting the OLCF quantum account application and receiving approval, you will receive an email from IonQ inviting you to create your quantum account. Once logged in, users will have access to IonQ's User Interface, https://cloud.ionq.com/, their online platform for managing jobs and accessing the available quantum systems, including the Harmony and Aria-1 systems, as well as the simulator, via the cloud. From the UI, users can view system status and upcoming system availability, as well as monitor batch submissions and job history.",4.223440192318915
"How can I add a server to mongoku using the MongoDB CLI?
","Steps to configure mongoku

Navigate to http://localhost:3100

Add Server -> ""admin:password@mongo:27017""

Click ""mongo""

We could use the port forwarding technique but that uses a connection that goes through the API server for the cluster which is not very performant. We will change the Service/mongo object so that it creates a NodePort that we can access from outside of the cluster.",4.260374360512564
"How can I add a server to mongoku using the MongoDB CLI?
","oc apply -f deployment.yaml

Snippet created with oc create deployment mongoku --image huggingface/mongoku --dry-run -o yaml

Run some commands to check on the Deployment

oc describe deployment mongoku

oc logs deployment/mongoku

oc port-forward deployment/mongoku 3100:3100

The oc port-forward command runs in the foreground. To test connectivity, one would need to use the MongoDB CLI from a second terminal.

Since we created the mongo service with the StatefulSet, all pods in our namespace will be able to resolve that ClusterIP so we can add a server to mongoku with just the service name.",4.124729551439926
"How can I add a server to mongoku using the MongoDB CLI?
","MongoDB is a common ""NoSQL"" database. We will be creating a Deployment to run the MongoDB service and expose it external to the cluster after setting up authentication. We will also be deploying a management Web UI for viewing queries.

Access to an allocation in Slate, the NCCS Kubernetes service

oc client installed

CLI client is logged into the cluster (oc login https://api.<cluster>.ccs.ornl.gov)

<string>:18: (INFO/1) Duplicate implicit target name: ""deploy mongodb"".",4.09887698758773
"Can you list all the version changes that took place during the upgrade on Tuesday, July 18, 2023?
","This page lists significant changes to software provided on OLCF systems. The most recent changes are listed first.



<p style=""font-size:20px""><b>Frontier and Crusher: System Software Upgrade (July 18, 2023)</b></p>

The Crusher TDS and Frontier systems were upgraded to a new version of the system software stack. This stack introduces ROCm 5.5.1 and HPE/Cray Programming Environment 23.05. For more information, please see:

Crusher System Updates.

Frontier System Updates.

Please contact help@olcf.ornl.gov with any issues or questions.",4.064981750764174
"Can you list all the version changes that took place during the upgrade on Tuesday, July 18, 2023?
","On Tuesday, September 19, 2023, Crusher's system software was upgraded. The following changes took place:

The system was upgraded to Slingshot Host Software 2.1.0.

ROCm 5.6.0 and 5.7.0 are now available via the rocm/5.6.0 and rocm/5.7.0 modulefiles, respectively.

HPE/Cray Programming Environments (PE) 23.09 is now available via the cpe/23.09 modulefile.

ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.

On Tuesday, July 18, 2023, the Crusher TDS was upgraded to a new version of the system software stack. During the upgrade, the following changes took place:",4.0417701859374
"Can you list all the version changes that took place during the upgrade on Tuesday, July 18, 2023?
","<p style=""font-size:20px""><b>Summit: Software Upgrade (July 16, 2019)</b></p>

The following modules will be installed and will become the default on July 16, 2019. The new stack requires Spectrum MPI 10.3 PTF 1 and as a result previous versions of Spectrum MPI have been deprecated.

| Package | Default | | --- | --- | | cuda | 10.1.168 | | spectrum-mpi | 10.3.0.1-20190716 |

Details about the software stack upgrade can be found in the IBM Service Pack 3.1 site and the Spectrum MPI 10.3.0.1 release notes.",4.039262678081936
"Can I use a previous version of Spectrum MPI if I need to?
","<p style=""font-size:20px""><b>Summit: Software Upgrade (July 16, 2019)</b></p>

The following modules will be installed and will become the default on July 16, 2019. The new stack requires Spectrum MPI 10.3 PTF 1 and as a result previous versions of Spectrum MPI have been deprecated.

| Package | Default | | --- | --- | | cuda | 10.1.168 | | spectrum-mpi | 10.3.0.1-20190716 |

Details about the software stack upgrade can be found in the IBM Service Pack 3.1 site and the Spectrum MPI 10.3.0.1 release notes.",4.210965684948463
"Can I use a previous version of Spectrum MPI if I need to?
","Spectrum MPI relies on CUDA Inter-process Communication (CUDA IPC) to provide fast on-node between GPUs. At present this capability cannot function with more than one resource set per node.

Set the environment variable PAMI_DISABLE_IPC=1 to force Spectrum MPI to not use fast GPU Peer-to-peer communication. This option will allow your code to run with more than one resource set per host, but you may see slower GPU to GPU communication.

Run in a single resource set per host, i.e. with jsrun --gpu_per_rs 6",4.189567906138365
"Can I use a previous version of Spectrum MPI if I need to?
","Upon login, the default versions of the XL compiler suite and Spectrum Message Passing Interface (MPI) are added to each user's environment through the modules system. No changes to the environment are needed to make use of the defaults.

Multiple versions of each compiler family are provided, and can be inspected using the modules system:

summit$ module -t avail gcc
/sw/summit/spack-envs/base/modules/site/Core:
gcc/7.5.0
gcc/9.1.0
gcc/9.3.0
gcc/10.2.0
gcc/11.1.0

type char is unsigned by default",4.183251054163565
"How is a batch job's usage calculated on Summit?
","Jobs on Summit are scheduled in full node increments; a node's cores cannot be allocated to multiple jobs. Because the OLCF charges based on what a job makes unavailable to other users, a job is charged for an entire node even if it uses only one core on a node. To simplify the process, users request and are allocated multiples of entire nodes through LSF.

Allocations on Summit are separate from those on Andes and other OLCF resources.

The node-hour charge for each batch job will be calculated as follows:

node-hours = nodes requested * ( batch job endtime - batch job starttime )",4.341161559197111
"How is a batch job's usage calculated on Summit?
","The batch-hm queue (and the batch-hm-spi queue for Moderate Enhanced security enclave projects) is used to access Summit's high-memory nodes.  Jobs may use all 54 nodes. It enforces the following policies:

Limit of (4) eligible-to-run jobs per user.

Jobs in excess of the per user limit above will be placed into a held state, but will change to eligible-to-run at the appropriate time.

Users may have only (25) jobs queued in the batch-hm queue at any state at any time. Additional jobs will be rejected at submit time.

batch-hm job limits:",4.338096590634515
"How is a batch job's usage calculated on Summit?
","| Bin | Min Nodes | Max Nodes | Max Walltime (Hours) | Aging Boost (Days) | | --- | --- | --- | --- | --- | | 1 | 2,765 | 4,608 | 24.0 | 15 | | 2 | 922 | 2,764 | 24.0 | 10 | | 3 | 92 | 921 | 12.0 | 0 | | 4 | 46 | 91 | 6.0 | 0 | | 5 | 1 | 45 | 2.0 | 0 |

The batch queue (and the batch-spi queue for Moderate Enhanced security enclave projects) is the default queue for production work on Summit.  Most work on Summit is handled through this queue. It enforces the following policies:

Limit of (4) eligible-to-run jobs per user.",4.333680549221878
"How can I optimize the use of RocBLAS device libraries on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.258097209351631
"How can I optimize the use of RocBLAS device libraries on Frontier?
","As mentioned above, HIP can be used on systems running on either the ROCm or CUDA platform, so OLCF users can start preparing their applications for Frontier today on Summit. To use HIP on Summit, you must load the HIP module:

$ module load hip-cuda

CUDA 11.4.0 or later must be loaded in order to load the hip-cuda module. hipBLAS, hipFFT, hipSOLVER, hipSPARSE, and hipRAND have also been added to hip-cuda.",4.188108321136112
"How can I optimize the use of RocBLAS device libraries on Frontier?
","results from these libraries, it is recommended to turn the atomic operations off by setting the mode via the rocBLAS or hipBLAS handle:",4.166376810982418
"Can I transfer data automatically from Alpine to Orion on Frontier?
",Data will not be automatically transferred from Alpine to Orion. Users should consider the data needed from Alpine and transfer it. Globus is the preferred method and there is access to both Orion and Alpine through the Globus OLCF DTN endpoint. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-transferring-data-globus. Use of HPSS to specifically stage data for the Alpine to Orion transfer is discouraged.,4.391640653889406
"Can I transfer data automatically from Alpine to Orion on Frontier?
","To assist you with moving your data off of Alpine, the DTN's mount the new Orion filesystem and all projects with access to Alpine have now been granted access to the Orion filesystem.

Please do not wait to migrate needed data, begin migrating all needed data now.",4.352113859628539
"Can I transfer data automatically from Alpine to Orion on Frontier?
",The following example is intended to help users who are making the transition from Summit to Frontier to move their data between Alpine GPFS and Orion Lustre. We strongly recommend using Globus for this transfer as it is the method that is most efficient for users and that causes the least contention on filesystems and data transfer nodes.,4.329826146582207
"What is the purpose of the -qlanglvl=90std flag in Summit?
",For Summit:,4.077352486510932
"What is the purpose of the -qlanglvl=90std flag in Summit?
","The xlflang module currently conflicts with the clang module. This restriction is expected to be lifted in future releases.

MPI on Summit is provided by IBM Spectrum MPI. Spectrum MPI provides compiler wrappers that automatically choose the proper compiler to build your application.

The following compiler wrappers are available:

C: mpicc

C++: mpic++, mpiCC

Fortran: mpifort, mpif77, mpif90",4.021729242016528
"What is the purpose of the -qlanglvl=90std flag in Summit?
","As a DOE Leadership Computing Facility, the OLCF has a mandate that a large portion of Summit's usage come from large, leadership-class (aka capability) jobs. To ensure the OLCF complies with DOE directives, we strongly encourage users to run jobs on Summit that are as large as their code will warrant. To that end, the OLCF implements queue policies that enable large jobs to run in a timely fashion.

The OLCF implements queue policies that encourage the submission and timely execution of large, leadership-class jobs on Summit.",4.014794585776901
"How do I access Summit's high-memory nodes?
","Most Summit nodes contain 512 GB of DDR4 memory for use by the POWER9 processors, 96 GB of High Bandwidth Memory (HBM2) for use by the accelerators, and 1.6TB of non-volatile memory that can be used as a burst buffer. A small number of nodes (54) are configured as ""high memory"" nodes. These nodes contain 2TB of DDR4 memory, 192GB of HBM2, and 6.4TB of non-volatile memory.",4.417828163226069
"How do I access Summit's high-memory nodes?
","Summit node architecture diagram

The basic building block of Summit is the IBM Power System AC922 node. Each of the approximately 4,600 compute nodes on Summit contains two IBM POWER9 processors and six NVIDIA Tesla V100 accelerators and provides a theoretical double-precision capability of approximately 40 TF. Each POWER9 processor is connected via dual NVLINK bricks, each capable of a 25GB/s transfer rate in each direction.",4.3533210941753815
"How do I access Summit's high-memory nodes?
","On Summit, there are three major types of nodes you will encounter: Login, Launch, and Compute. While all of these are similar in terms of hardware (see: https://docs.olcf.ornl.gov/systems/summit_user_guide.html#summit-nodes), they differ considerably in their intended use.",4.337628945294582
"What are some best practices for optimizing memory usage on Frontier?
","Most applications that use ""managed"" or ""unified"" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Frontier. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.",4.208413482971326
"What are some best practices for optimizing memory usage on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.186154083696586
"What are some best practices for optimizing memory usage on Frontier?
","The AMD MI250X and operating system on Frontier supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Frontier. In the following section we'll discuss how the heterogenous memory space on a Frontier node is surfaced within your application.",4.174036506983581
"Are there any software development kits (SDKs) available for IonQ?
","IonQ backends are available via the IonQ cloud interface via the API and also via many quantum Software Development Kits (SDK’s)



Users can access information about IonQ’s systems, view submitted jobs, look up machine availability, and update job notification preferences via the IonQ Cloud Console.

Jupyter at OLCF: Access to the IonQ queues can also be obtained via OLCF JupyterHub, a web-based interactive computing environment. See examples of common use case notebooks at IonQ Notebook Samples.",4.322374497994598
"Are there any software development kits (SDKs) available for IonQ?
","Information on submitting jobs to IonQ systems, system availability, checking job status, and tracking usage can be found via the IonQ Cloud Console.

A recommended workflow for running on IonQ's quantum computers is to utilize the emulator first, then run on one of the quantum computers. This is highlighted in the examples.",4.100299601496481
"Are there any software development kits (SDKs) available for IonQ?
","IonQ offers access to trapped ion quantum computers and emulators, accessible via their IonQ Quantum Cloud API. For the complete set of currently available devices, qubit numbers, etc. see IonQ Documentation. This guide describes how to use the system once you have access. For instructions on how to gain access, see our :doc:`Quantum Access </quantum/quantum_access>` page instead.

<string>:8: (INFO/1) No role entry for ""doc"" in module ""docutils.parsers.rst.languages.en"". Trying ""doc"" as canonical role name.

<string>:8: (ERROR/3) Unknown interpreted text role ""doc"".",4.089303517106727
"How do I select files and folders to transfer in Frontier?
","When the installation has finished, click on the Globus icon and select Web: Transfer Files as below



Globus will ask you to login. If your institution does not have an organizational login, you may choose to either Sign in with Google or Sign in with ORCiD iD.



In the main Globus web page, select the two-panel view, then set the source and destination endpoints. (Left/Right order does not matter)



Next, navigate to the appropriate source and destination paths to select the files you want to transfer. Click the ""Start"" button to begin the transfer.",4.231988465337412
"How do I select files and folders to transfer in Frontier?
","When prompted, authenticate into the OLCF DTN endpoint using your OLCF username and PIN followed by your RSA passcode.

Click in the left side “Path” box in the File Manager and enter the path to your data on Alpine. For example, /gpfs/alpine/stf007/proj-shared/my_alpine_data. You should see a list of your files and folders under the left “Path” Box.

Click on all files or folders that you want to transfer in the list. This will highlight them.

Click on the right side “Collection” box in the File Manager and type “OLCF DTN”",4.178638637442819
"How do I select files and folders to transfer in Frontier?
","transfers across all the users. Each user has a limit of 3 active transfers, so it is required to transfer a lot of data on each transfer than less data across many transfers.  If a folder is constituted with mixed files including thousands of small files (less than 1MB each one), it would be better to tar the small files.  Otherwise, if the files are larger, Globus will handle them.",4.078137029425458
"Can I create objects inside my project using the GUI?
","Now that you have a project you can create objects inside that project. We will be doing this with the Openshift Web GUI and the oc CLI client so you can use whichever interface you are more comfortable with in this tutorial. If you are more comfortable using the command line than you are using a GUI you can now https://docs.olcf.ornl.gov/systems/guided_tutorial.html#jump to the oc portion of this document<slate_guided_tutorial_cli>. Otherwise, continue with the GUI based tutorial below.

Go to https://console-openshift-console.apps.<cluster>.ccs.ornl.gov/k8s/cluster/projects",4.246450098744766
"Can I create objects inside my project using the GUI?
","To start tracing from the GUI, click on Tools→Start Trace. An options window will pop up and prompt for specific Trace settings other than the default. Upon starting the trace, any time you modify properties, create filters, open files, and hit Apply, etc., your actions will be translated into Python syntax. Once you are finished tracing the actions you want to script, click Tools→Stop Trace. A Python script should then be displayed to you and can be saved.",4.040579444333456
"Can I create objects inside my project using the GUI?
","Depending on you how configured your deployment, this could be your NFS or GPFS project space or an isolated volume dedicated/isolated to this MinIO server.

Within the GUI you can create buckets and upload/download data. If you are running this on NFS or GPFS the bucket will map to a directory.",4.03847267684757
"What could be the reason for VisIt not asking for my passcode and hanging after trying to connect to one of your systems?
","If VisIt never asks for your passcode and hangs after trying to connect to one of our systems, then this means VisIt is unable to establish a proper SSH connection. Here are a few different approaches to fix this issue:

Double check your host profile, especially the ""remote host name"", ""host name aliases"", and ""tunnel data connections through SSH"" sections.

If you are using a VPN (including GlobalProtect VPN), try turning it off.",4.528461308164957
"What could be the reason for VisIt not asking for my passcode and hanging after trying to connect to one of your systems?
","If the pop-up box called ""metadata server launch progress"" never goes away after entering your passcode, you may need to check if you have enough storage space available in your home directory (/ccs/home/[user id]). When connecting to OLCF systems, VisIt creates some small temporary files in your home directory that are unable to be created if you are over your quota (50 GB is the default quota limit).",4.249295915285796
"What could be the reason for VisIt not asking for my passcode and hanging after trying to connect to one of your systems?
","If VisIt will not connect to Andes or Summit when you try to draw an image, you should login to the system and check if a job is in the queue. To do this on Andes, enter squeue from the command line. To do this on Summit, enter bjobs from the command line. Your VisIt job should appear in the queue. If you see it in a state marked ""PD"" or ""PEND"" you should wait a bit longer to see if it will start. If you do not see your job listed in the queue, check to make sure your project ID is entered in your VisIt host profile. See the https://docs.olcf.ornl.gov/systems/visit.html#visit-modify-host",4.14661505942587
"How can I create resource sets of various common use cases on Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.256143995339395
"How can I create resource sets of various common use cases on Summit?
","summit>

The following example will create 4 resource sets each with 6 tasks and 3 GPUs. Each set of 6 MPI tasks will have access to 3 GPUs. Ranks 0 - 5 will have access to GPUs 0 - 2 on the first socket of the first node ( red resource set). Ranks 6 - 11 will have access to GPUs 3 - 5 on the second socket of the first node ( green resource set). This pattern will continue until 4 resource sets have been created. The following jsrun command will request 4 resource sets (-n4). Each resource set will contain 6 MPI tasks (-a6), 3 GPUs (-g3), and 6 cores (-c6).",4.190874947513603
"How can I create resource sets of various common use cases on Summit?
",For Summit:,4.161402610674164
"How do I check the status of my job on Summit?
",For Summit:,4.253491909557356
"How do I check the status of my job on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.187141142230926
"How do I check the status of my job on Summit?
","If VisIt will not connect to Andes or Summit when you try to draw an image, you should login to the system and check if a job is in the queue. To do this on Andes, enter squeue from the command line. To do this on Summit, enter bjobs from the command line. Your VisIt job should appear in the queue. If you see it in a state marked ""PD"" or ""PEND"" you should wait a bit longer to see if it will start. If you do not see your job listed in the queue, check to make sure your project ID is entered in your VisIt host profile. See the https://docs.olcf.ornl.gov/systems/visit.html#visit-modify-host",4.121124986592792
"How can I scale a deployment in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.234091826308754
"How can I scale a deployment in Slate?
","It is assumed you have already gone through https://docs.olcf.ornl.gov/systems/minio.html#slate_getting_started and established the https://docs.olcf.ornl.gov/systems/minio.html#helm_prerequisite. Please do that before attempting to deploy anything on Slate's clusters.

This example uses Helm version 3 to deploy a MinIO standalone Helm chart on Slate's Marble Cluster. This is the cluster in OLCF's Moderate enclave, the same enclave as Summit.

To start, clone the slate helm examples repository , containing the MinIO standalone Helm chart, and navigate into the charts directory:",4.145241527227453
"How can I scale a deployment in Slate?
","The following items need to be established before deploying applications on Slate systems (Marble or Onyx OpenShift clusters).

Follow https://docs.olcf.ornl.gov/systems/helm_prerequisite.html#slate_getting_started if you do not already have a project/namespace established on Onyx or Marble.

It is recommended to use Helm version 3.

Helm enables application deployment via Helm Charts. The high level Helm Architecture docs are also a great reference for understanding Helm.

Like oc, Helm is a single binary executable.

This can be installed on macOS with Homebrew :

$ brew install helm",4.134935365274676
"What is the name of the file that contains the code that is being executed?
","$ export SCOREP_FILTERING_FILE=scorep.filter

Now you are ready to submit your instrumented code to run with tracing enabled. This measurement will generate files of the form traces.otf. The .otf2 file format can be analyzed by a tool called Vampir .

<string>:16: (INFO/1) Duplicate explicit target name: ""vampir"".

Vampir provides a visual GUI to analyze the .otf2 trace file generated with Score-P.",4.00813312572911
"What is the name of the file that contains the code that is being executed?
","When your job reaches the top of the queue, the main window will be returned to your control. At this point you are connected and can open files that reside there and visualize them interactively.",3.960060504491517
"What is the name of the file that contains the code that is being executed?
","""hello world""

Save this in a file hw.r, somewhere on gpfs. So say your project is abc123. You might have hw.r in /gpfs/alpine/abc123/proj-shared/my_hw_path/.",3.9485530268762266
"What is the purpose of the `resources` field in a Persistent Volume Claim (PVC) YAML file?
","Once the Persistent Volume is created its allocated memory is available to be claimed by a project. This is done through a Persistent Volume Claim. PVC's are created using a YAML file in the same manner that PV's are created. An example of a PVC that claims the PV created in the previous section follows:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: storage-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

The claim is then placed using:

oc create -f storage-1.yaml",4.398842907676199
"What is the purpose of the `resources` field in a Persistent Volume Claim (PVC) YAML file?
","PV Settings

This will redirect you to the status page of your new PVC. You should see Bound in the Status field.

PV Status

Once the claim has been granted to the project a pod within that project can access that storage. To do this, edit the YAML of the pod and under the volume portion of the file add a name and the name of the claim. It will look similar to the following.

volumes:
  name: pvol
  persistentVolumeClaim:
    claimName: storage-1

Then all that is left to do is mount the storage into a container:

volumeMounts:
  - mountPath: /tmp/
    name: pvol",4.290222367529885
"What is the purpose of the `resources` field in a Persistent Volume Claim (PVC) YAML file?
","From the main screen of a project go to the Storage option in the hamburger menu on the left hand side of the screen, then click Persistent Volume Claims

Persistent Volume Claim Menu

In the upper right click the Create Persistent Volume Claim button.

Create Storage

This will take you to a screen that allows you to select what you need for your PVC. Give your claim a name and request an amount of storage and click Create. You can also click Edit YAML to edit the PVC object directly.

PV Settings",4.280253342806619
"What happens if GPU HBM becomes full and there is no room for new data?
","If ``HSA_XNACK=1``, page faults in GPU kernels will trigger a page table lookup. If the memory location can be made accessible to the GPU, either by being migrated to GPU HBM or being mapped for remote access, the appropriate action will occur and the access will be replayed. Once a memory region has been migrated to GPU HBM it typically stays there rather than migrating back to CPU DDR4. The exceptions are if the programmer uses a HIP library call such as ``hipPrefetchAsync`` to request migration, or if GPU HBM becomes full and the page must forcibly be evicted back to CPU DDR4 to make room",4.163676813728253
"What happens if GPU HBM becomes full and there is no room for new data?
","If ``HSA_XNACK=1``, page faults in GPU kernels will trigger a page table lookup. If the memory location can be made accessible to the GPU, either by being migrated to GPU HBM or being mapped for remote access, the appropriate action will occur and the access will be replayed. Once a memory region has been migrated to GPU HBM it typically stays there rather than migrating back to CPU DDR4. The exceptions are if the programmer uses a HIP library call such as ``hipPrefetchAsync`` to request migration, or if GPU HBM becomes full and the page must forcibly be evicted back to CPU DDR4 to make room",4.163676813728253
"What happens if GPU HBM becomes full and there is no room for new data?
","If HSA_XNACK=0, page faults in GPU kernels are not handled and will terminate the kernel. Therefore all memory locations accessed by the GPU must either be resident in the GPU HBM or mapped by the HIP runtime. Memory regions may be migrated between the host DDR4 and GPU HBM using explicit HIP library functions such as hipMemAdvise and hipPrefetchAsync, but memory will not be automatically migrated based on access patterns alone.",4.152997686899248
"What is the purpose of the High Performance Storage System (HPSS) on Frontier?
",The High Performance Storage System (HPSS) is the tape-archive storage system at the OLCF and is the storage technology that supports the User Archive areas. HPSS is intended for data that do not require day-to-day access.,4.419103753157375
"What is the purpose of the High Performance Storage System (HPSS) on Frontier?
","System Overview





The High Performance Storage System (HPSS) provides tape storage for large amounts of data created on OLCF systems. The HPSS can be accessed from any OLCF system through the hsi utility. More information about using HPSS can be found in the https://docs.olcf.ornl.gov/systems/hpss_user_guide.html#data-hpss section of the https://docs.olcf.ornl.gov/systems/hpss_user_guide.html#data-storage-and-transfers page.",4.3867023665272376
"What is the purpose of the High Performance Storage System (HPSS) on Frontier?
","Frontier connects to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#dtn-user-guide to move data to/from HPSS.

Frontier is running Cray OS 2.4 based on SUSE Linux Enterprise Server (SLES) version 15.4.",4.385400951638497
"Is stdin enabled for the container?
","command: [""/bin/sh"",""-c""]

args: [""echo 'Hello World!'; cat""]

Finally, we need a tty. This will give us the ability to open a shell in our  Pod and get a better understanding of what is happing. To do this, add the following two lines under the command line that you just added:

tty: true

stdin: true

Your page should now look as follows:



You can now click the 'Create' button in the lower left which will take you to the screen where the   Pod is created.",4.025497693153154
"Is stdin enabled for the container?
","Source to image is a tool to build docker formatted container images. This is accomplished by injecting source code into a container image and then building a new image. The new image will contain the source code ready to run, via the $ docker run command inside the newly built image.

Using the S2I model of building images, it is possible to have your source hosted on a git repository and then pulled and built by Openshift.

Click on the newly created project and then on the blue Add to Project button on the center of the page.",3.9884961464681226
"Is stdin enabled for the container?
","Secondly, the  Pod needs something to do when it starts. For an nginx server this would be running nginx, for a flask app this would be running the app.py file etc. For illustrative purposes this  Pod is going to be starting a shell with the /bin/sh command, echoing a ""Hello World!"" prompt then running a cat command as a means to keep the pod running. Without the addition of the cat at the end the echo command would end causing the /bin/sh to end causing the  Pod to go from a status of Running to Completed.  To make these changes add the following lines below the image line:",3.9769567083557247
"How do I configure the ArgoCD deployment to use a directory of YAML or JSON files for deploying Kubernetes resources on Slate?
","directory of YAML or JSON files

kustomize applications

helm charts

This section will focus on the deployment of Kubernetes resources using kustomize. If the use of helm is preferred, refer to the Continuous Delivery with Helm and ArgoCD blog post as well as the App of Apps Pattern discussed on the ArgoCD Cluster Bootstrapping page.

References to ksonnet for deployment of Kubernetes resources may be mentioned in some documentation. However, the use if ksonnet is no longer supported by ArgoCD.

In order to deploy resources, one should have the following to start with:",4.438448751833807
"How do I configure the ArgoCD deployment to use a directory of YAML or JSON files for deploying Kubernetes resources on Slate?
","The Red Hat OpenShift GitOps operator is already installed onto each of the Slate clusters. However, to use ArgoCD an instance will need to be added into a project namespace. If one is to use ArgoCD to manage resources in a single project, then the ArgoCD instance could be located in the same project as the resources being managed. However, if the application being managed is resource (CPU and memory) sensitive or if resources in multiple projects are to be managed, it may be better to have the ArgoCD instance deployed into a separate project to allow for better control of resources allocated",4.256697400354033
"How do I configure the ArgoCD deployment to use a directory of YAML or JSON files for deploying Kubernetes resources on Slate?
","With a git repository defined in the ArgoCD Repositories settings that has kustomize environments ready for use, one can start using ArgoCD to deploy and manage kubernetes resources.",4.247904513689998
"What is the role of Data Transfer Nodes (DTNs) in the OLCF Policy?
",The Data Transfer Nodes (DTNs) are hosts specifically designed to provide optimized data transfer between OLCF systems and systems outside of the OLCF network. These nodes perform well on local-area transfers as well as the wide-area data transfers for which they are tuned. The OLCF recommends that users use these nodes to improve transfer speed and reduce load on computational systems’ login and service nodes. OLCF provides two sets of DTNs: one for systems in our moderate enclave and a second for systems in the open enclave.,4.641824829568259
"What is the role of Data Transfer Nodes (DTNs) in the OLCF Policy?
","The OLCF offers a number of dedicated data transfer nodes to users. The nodes have been tuned specifically for wide area data transfers, and also perform well on the local area. There are also several utilities that the OLCF recommends for data transfer. Please refer to our https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#system-user-guides for information about the DTNs and available utilities.

Titan Scheduling Policy

=======================



.. note::

This details an official policy of the OLCF, and must be",4.56130443879328
"What is the role of Data Transfer Nodes (DTNs) in the OLCF Policy?
","DTNs are also accessible via the ""OLCF DTN"" (for moderate) and ""NCCS Open DTN"" (for open) Globus endpoints. For more information on using Globus at OLCF see https://docs.olcf.ornl.gov/systems/dtn_user_guide.html#data-transferring-data-globus.

The moderate DTNs also support batch jobs. The system contains 8 nodes accessible through the DTN batch system.

Most OLCF resources now use the Slurm batch scheduler, including the DTNs. Below is a table of useful commands for Slurm.",4.338166311299847
"What is the difference between --stdio_stdout and --stdio_stderr in Summit?
","When using --stdio_stdout or --stdio_stderr users must use absolute paths. Using relative paths (e.g. ./my_stdout) will not successfully create the file in the user's current working directory. An bug has been filed with IBM to fix this issue and allow relative paths.

In some cases users will encounter a segmentation fault when running job steps that have uneven number of resource sets per node. For example:",4.122237109578848
"What is the difference between --stdio_stdout and --stdio_stderr in Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.072422260226806
"What is the difference between --stdio_stdout and --stdio_stderr in Summit?
","More information on the Programming Environment, Compiling, and Running Batch Jobs can be found in the https://docs.olcf.ornl.gov/systems/citadel_user_guide.html#Summit User Guide<summit-documentation-resources>.

For notable differences between Citadel and Summit, please see the https://docs.olcf.ornl.gov/systems/citadel_user_guide.html#SPI<spi-compute-citadel> documentation.

The login node used by Citadel mirrors the Summit login nodes in hardare and software.  The login node also provides access to the same compute resources as are accessible from Summit.",4.062759796603585
"How do I link my program with the appropriate libraries when using the Cray compiler wrappers on Crusher?
","Cray, AMD, and GCC compilers are provided through modules on Crusher. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers.

It is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) whenever possible. See the next section for more details.",4.318804875010221
"How do I link my program with the appropriate libraries when using the Cray compiler wrappers on Crusher?
","Cray provides PrgEnv-<compiler> modules (e.g., PrgEnv-cray) that load compatible components of a specific compiler toolchain. The components include the specified compiler as well as MPI, LibSci, and other libraries. Loading the PrgEnv-<compiler> modules also defines a set of compiler wrappers for that compiler toolchain that automatically add include paths and link in libraries for Cray software. Compiler wrappers are provided for C (cc), C++ (CC), and Fortran (ftn).",4.318051494728855
"How do I link my program with the appropriate libraries when using the Cray compiler wrappers on Crusher?
","Cray provides PrgEnv-<compiler> modules (e.g., PrgEnv-cray) that load compatible components of a specific compiler toolchain. The components include the specified compiler as well as MPI, LibSci, and other libraries. Loading the PrgEnv-<compiler> modules also defines a set of compiler wrappers for that compiler toolchain that automatically add include paths and link in libraries for Cray software. Compiler wrappers are provided for C (cc), C++ (CC), and Fortran (ftn).",4.318051494728855
"How does the performance of FP32 LDS atomicAdd() operations compare to the performance of FP64 LDS atomicAdd() operations when contention is high?
","In cases when contention is very low, a FP32 CAS loop implementing an atomicAdd() operation could be faster than an hardware FP32 LDS atomicAdd(). Applications using single precision FP atomicAdd() are encouraged to experiment with the use of double precision to evaluate the trade-off between high atomicAdd() performance vs. potential lower occupancy due to higher LDS usage.",4.632045656434537
"How does the performance of FP32 LDS atomicAdd() operations compare to the performance of FP64 LDS atomicAdd() operations when contention is high?
","In cases when contention is very low, a FP32 CAS loop implementing an atomicAdd() operation could be faster than an hardware FP32 LDS atomicAdd(). Applications using single precision FP atomicAdd() are encouraged to experiment with the use of double precision to evaluate the trade-off between high atomicAdd() performance vs. potential lower occupancy due to higher LDS usage.",4.632045656434537
"How does the performance of FP32 LDS atomicAdd() operations compare to the performance of FP64 LDS atomicAdd() operations when contention is high?
","Hardware FP atomic operations performed in LDS memory are usually always faster than an equivalent CAS loop, in particular when contention on LDS memory locations is high. Because of a hardware design choice, FP32 LDS atomicAdd() operations can be slower than equivalent FP64 LDS atomicAdd(), in particular when contention on memory locations is low (e.g. random access pattern). The aforementioned behavior is only true for FP atomicAdd() operations. Hardware atomic operations for CAS/Min/Max on FP32 are usually faster than the FP64 counterparts. In cases when contention is very low, a FP32 CAS",4.604273787419398
"Can I use the Notary Token Verification Form for replacement tokens?
","Notary Token Verification Form (See Notary Instructions) For certain resources, ORNL requires identity proofing to authenticate a user’s identity and possession of the token prior to activation. The Notary Token Verification Form verifies the identity of the applicant and maintains records of the identity proofing credentials that may be disclosed in the future to an authorized individual or investigative agency. Alternatively, you may schedule this verification with a member of our staff virtually my filling out the Conference Scheduler Form. You will need your Application Confirmation",4.398414533752179
"Can I use the Notary Token Verification Form for replacement tokens?
","Form. You will need your Application Confirmation Number that was emailed to you by our accounts team to schedule in this manner. If you do not possess a confirmation number (you are verifying a replacement token, for example), please email us at help@olcf.ornl.gov to schedule.",4.120995235474606
"Can I use the Notary Token Verification Form for replacement tokens?
","If you are processing sensitive or proprietary data, additional paperwork is required and will be sent to you.

If you need an RSA SecurID token from our facility, the token and additional paperwork will be sent to you via email to complete identity proofing.",4.048078360968734
"Can you provide an example of how to generate an instrumented executable using Perftools?
","There are three programming interfaces available: (1) Perftools-lite, (2) Perftools, and (3) Perftools-preload.

Below are two examples that generate an instrumented executable using Perftools, which is an advanced interface that provides full-featured data collection and analysis capability, including full traces with timeline displays.

The first example generates an instrumented executable using a PrgEnv-amd build:

module load PrgEnv-amd
module load craype-accel-amd-gfx90a
module load rocm
module load perftools",4.525390590474211
"Can you provide an example of how to generate an instrumented executable using Perftools?
","There are three programming interfaces available: (1) Perftools-lite, (2) Perftools, and (3) Perftools-preload.

Below are two examples that generate an instrumented executable using Perftools, which is an advanced interface that provides full-featured data collection and analysis capability, including full traces with timeline displays.

The first example generates an instrumented executable using a PrgEnv-amd build:

module load PrgEnv-amd
module load craype-accel-amd-gfx90a
module load rocm
module load perftools",4.525390590474211
"Can you provide an example of how to generate an instrumented executable using Perftools?
","To instrument your code, you need to compile the code using the Score-P instrumentation command (scorep), which is added as a prefix to your compile statement. In most cases the Score-P instrumentor is able to automatically detect the programming paradigm from the set of compile and link options given to the compiler. Some cases will, however, require some additional link options within the compile statement e.g. CUDA instrumentation.

Below are some basic examples of the different instrumentation scenarios:",4.24109121959539
"How do I troubleshoot issues with SPI resources?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads. Because of this, many processes used within the SPI are very similar to those used for standard non-SPI. This page lists the differences you may see when using OLCF resources to execute SPI resources.",4.204085316020678
"How do I troubleshoot issues with SPI resources?
","https://docs.olcf.ornl.gov/systems/index.html#SPI resources mount SPI filesystems<spi-file-systems>.  The SPI resources do not mount the non-SPI's scratch filesystems, home areas, or mass storage.

https://docs.olcf.ornl.gov/systems/index.html#SPI compute resources cannot access external resources<spi-data-transfer>.  Needed data must be transferred to the SPI resources through the SPI's DTN.

https://docs.olcf.ornl.gov/systems/index.html#The Citadel login nodes<spi-compute-citadel> and batch queues must be used to access Summit and Frontier for SPI workflows.",4.182970070439499
"How do I troubleshoot issues with SPI resources?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.  This page lists the differences you may see when using OLCF resources to execute SPI workflows.  The page will also point to sections within the site where more information on standard non-SPI use can be found.",4.170977580637971
"Can I use a Python script with the visit command?
","the -s flag will launch the CLI in the form of a Python shell, which can be useful for interactive batch jobs.  The -np and -nn flags represent the number of processors and nodes VisIt will use to execute the Python script, while the -l flag specifies the specific parallel method to do so (required). Execute visit -fullhelp to get a list of all command line options.",4.2732170776942
"Can I use a Python script with the visit command?
","Following one of the methods above will submit a batch job for five minutes to either Summit, Andes, or Frontier.  Once the batch job makes its way through the queue, the script will launch VisIt version X.Y.Z (specified with the -v flag, required on Andes) and execute a python script called visit_example.py (specified with the -s flag, required if using a Python script). Note that the -nowin -cli options are also required, which launches the CLI and tells VisIt to not launch the GUI. Although a Python script is used for this example, not calling the -s flag will launch the CLI in the form of",4.220571227170808
"Can I use a Python script with the visit command?
",of how to run a Python script using PvBatch on Andes and Summit.,4.169133567910262
"Are there additional file systems or protections for sensitive data on OLCF systems?
","The OLCF uses a standard file system structure to assist users with data organization on OLCF systems. Complete details about all file systems available to OLCF users can be found in the Data Management Policy section.

Additional file systems and file protections may be employed for sensitive data. If you are a user on a project producing sensitive data, further instructions will be given by the OLCF. The following guidelines apply to sensitive data:

Only store sensitive data in designated locations. Do not store sensitive data in your User Home directory.",4.611909972728869
"Are there additional file systems or protections for sensitive data on OLCF systems?
","The OLCF systems provide protections to maintain the confidentiality, integrity, and availability of user data. Measures include: the availability of file permissions, archival systems with access control lists, and parity/CRC checks on data paths/files. It is the user’s responsibility to set access controls appropriately for data. In the event of system failure or malicious actions, the OLCF makes no guarantee against loss of data nor makes a guarantee that a user’s data could not be potentially accessed, changed, or deleted by another individual. It is the user’s responsibility to insure",4.489050272309651
"Are there additional file systems or protections for sensitive data on OLCF systems?
","The OLCF systems provide protections to maintain the confidentiality, integrity, and availability of user data. Measures include the availability of file permissions, archival systems with access control lists, and parity and CRC checks on data paths and files. It is the user’s responsibility to set access controls appropriately for the data. In the event of system failure or malicious actions, the OLCF makes no guarantee against loss of data or that a user’s data can be accessed, changed, or deleted by another individual. It is the user’s responsibility to insure the appropriate level of",4.476176495648683
"How many FP64 (double-precision) cores does each SM on the V100 contain?
","Each SM on the V100 contains 32 FP64 (double-precision) cores, 64 FP32 (single-precision) cores, 64 INT32 cores, and 8 tensor cores. A 128-KB combined memory block for shared memory and L1 cache can be configured to allow up to 96 KB of shared memory. In addition, each SM has 4 texture units which use the (configured size of the) L1 cache.",4.589056459717679
"How many FP64 (double-precision) cores does each SM on the V100 contain?
","For more information, please see the following section of NVIDIA's CUDA Programming Guide: http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#independent-thread-scheduling-7-x

The Tesla V100 contains 640 tensor cores (8 per SM) intended to enable faster training of large neural networks. Each tensor core performs a D = AB + C operation on 4x4 matrices. A and B are FP16 matrices, while C and D can be either FP16 or FP32:",4.299974899490016
"How many FP64 (double-precision) cores does each SM on the V100 contain?
","The NVIDIA Tesla V100 GPUs in Summit are capable of over 7TF/s of double-precision and 15 TF/s of single-precision floating point performance. Additionally, the V100 is capable of over 120 TF/s of half-precision floating point performance when using its Tensor Core feature. The Tensor Cores are purpose-built accelerators for half-precision matrix multiplication operations. While they were designed especially to accelerate machine learning workflows, they are exposed through several other APIs that are useful to other HPC applications. This section provides information for using the V100",4.265833277556183
"Can you explain the concept of quantum parallelism?
","A recommended workflow for running on Quantinuum's quantum computers is to utilize the syntax checker first, run on the emulator, then run on one of the quantum computers. This is highlighted in the examples.",4.134993737471148
"Can you explain the concept of quantum parallelism?
","Conventional/classical computing utilizes information storage based on digital devices storing “bits”, which are in either of two distinct states at a given time, i.e. 0 or 1. Quantum computers utilize properties of quantum mechanics, such as superposition and entanglement, in order to exceed certain capabilities of classical computers. Superposition means that the units of information storage can be in multiple states at the same time, and entanglement means the states can depend on each other.  In quantum computing systems, information is stored not using “bits”, but instead using “qubits”.",4.120221437972075
"Can you explain the concept of quantum parallelism?
","Laser based quantum gates

Linear trap Quantum Charge-Coupled Device (QCCD) architecture with three or more parallel gate zones

Mid-circuit measurement conditioned circuit branching

Qubit reuse after mid-circuit measurement

Native gate set: single-qubit rotations, two-qubit ZZ-gates



Users can access information about Quantinuum's systems, view submitted jobs, look up machine availability, and update job notification preferences on the cloud dashboard on the Quantinuum User Portal.",4.078641555821309
"How can I ensure that my job script sends an email to the submitter when it ends?
","| Sends email to the submitter when the job begins. | |  | #SBATCH --mail-type=END | Sends email to the submitter when the job ends. | | --mail-user | #SBATCH --mail-user=<address> | Specifies email address to use for --mail-type options. | | -J | #SBATCH -J <name> | Sets the job name to <name> instead of the name of the job script. | | --get-user-env | #SBATCH --get-user-env | Exports all environment variables from the submitting shell into the batch job shell. Since the login nodes differ from the service nodes, using the –get-user-env option is not recommended. Users should create the",4.079892984878759
"How can I ensure that my job script sends an email to the submitter when it ends?
","| Sends email to the submitter when the job begins. | |  | #SBATCH --mail-type=END | Sends email to the submitter when the job ends. | | --mail-user | #SBATCH --mail-user=<address> | Specifies email address to use for --mail-type options. | | -J | #SBATCH -J <name> | Sets the job name to <name> instead of the name of the job script. | | --get-user-env | #SBATCH --get-user-env | Exports all environment variables from the submitting shell into the batch job shell. Since the login nodes differ from the service nodes, using the –get-user-env option is not recommended. Users should create the",4.079892984878759
"How can I ensure that my job script sends an email to the submitter when it ends?
","Most users will find batch jobs an easy way to use the system, as they allow you to ""hand off"" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.",4.05584258649532
"Can I use the myOLCF self-service portal to manage my project's allocation?
","Project members

The myOLCF site is provided to aid in the utilization and management of OLCF allocations. See the https://docs.olcf.ornl.gov/services_and_applications/myolcf/index.html for more information.

If you have any questions or have a request for additional data, please contact the OLCF User Assistance Center.",4.482994080130112
"Can I use the myOLCF self-service portal to manage my project's allocation?
","If you already have a user account at the OLCF, your existing credentials can be leveraged across multiple projects.

If your user account has an associated RSA SecurID (i.e. you have an ""OLCF Moderate"" account), you gain access to another project by logging in to the myOLCF self-service portal and filling out the application under My Account > Join Another Project. For more information, see the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#myOLCF self-service portal documentation<myolcf-overview>.",4.408423087098819
"Can I use the myOLCF self-service portal to manage my project's allocation?
","When all of the above steps are completed, your user account will be created and you will be notified by email. Now that you have a user account and it has been associated with a project, you're ready to get to work. This website provides extensive documentation for OLCF systems, and can help you efficiently use your project's allocation. We recommend reading the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#System User Guides<system-user-guides> for the machines you will be using often.",4.399254802698229
"How do I access Summit and Frontier's compute resources for SPI workflows?
","To access Summit and Frontier's compute resources for SPI workflows, you must first log into a https://docs.olcf.ornl.gov/systems/index.html#Citadel login node<citadel-login-nodes> and then submit a batch job to one of the SPI specific batch queues.",4.655514146671801
"How do I access Summit and Frontier's compute resources for SPI workflows?
","This section covers differences between SPI and non-SPI workflows, but the existing resource user guides cover the majority of system use methods.  Please use the https://docs.olcf.ornl.gov/systems/index.html#Summit User Guide<summit-documentation-resources> and https://docs.olcf.ornl.gov/systems/index.html#Frontier User Guide<frontier-user-guide> for resource use details.



To help separate data and processes, the Citadel framework provides separate login nodes to reach Summit and Frontier's compute resources:",4.444569130720556
"How do I access Summit and Frontier's compute resources for SPI workflows?
","The SPI provides access to the OLCF's Summit resource for compute.  To safely separate SPI and non-SPI workflows, SPI workflows must use a separate login node named https://docs.olcf.ornl.gov/systems/summit_user_guide.html#Citadel<spi-compute-citadel>. Because Citadel is largely a front end for Summit, you can use the Summit documentation when using Citadel. The https://docs.olcf.ornl.gov/systems/summit_user_guide.html#SPI<spi-compute-citadel> page can be used to see notable differences when using the Citadel resource.",4.400165182553646
"What is the benefit of using zero-copy read/write operations in Crusher?
",Crusher is connected to the center-wide IBM Spectrum Scale™ filesystem providing 250 PB of storage capacity with a peak write speed of 2.5 TB/s. Crusher also has access to the center-wide NFS-based filesystem (which provides user and project home areas). While Crusher does not have direct access to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#dtn-user-guide to move data to/from HPSS.,4.149831114093578
"What is the benefit of using zero-copy read/write operations in Crusher?
","BytesMoved = BytesWritten + BytesRead

where

BytesWritten = 64 * TCC\_EA\_WRREQ\_64B\_sum + 32 * (TCC\_EA\_WRREQ\_sum - TCC\_EA\_WRREQ\_64B\_sum)

BytesRead = 32 * TCC\_EA\_RDREQ\_32B\_sum + 64 * (TCC\_EA\_RDREQ\_sum - TCC\_EA\_RDREQ\_32B\_sum)



This section details 'tips and tricks' and information of interest to users when porting from Summit to Crusher.",4.073363230686603
"What is the benefit of using zero-copy read/write operations in Crusher?
","Each Crusher compute node has [2x] 1.92 TB NVMe devices (SSDs) with a peak sequential performance of 5500 MB/s (read) and 2000 MB/s (write). To use the NVMe, users must request access during job allocation using the -C nvme option to sbatch, salloc, or srun. Once the devices have been granted to a job, users can access them at /mnt/bb/<userid>. Users are responsible for moving data to/from the NVMe before/after their jobs. Here is a simple example script:",4.040849026105965
"What is the email address of the person who can help me with pbdR on Summit?
","George Ostrouchov - ostrouchovg AT ornl DOT gov

Drew Schmidt - schmidtda AT ornl DOT gov

We are happy to provide support and collaboration for R and pbdR users on Summit.",4.365991179684304
"What is the email address of the person who can help me with pbdR on Summit?
","If you want to do GPU computing on Summit with R, we would love to collaborate with you (see contact details at the bottom of this document).

For more information about using R and pbdR effectively in an HPC environment such as Summit, please see the R and pbdR articles. These are long-form articles that introduce HPC concepts like MPI programming in much more detail than we do here.

Also, if you want to use R and/or pbdR on Summit, please feel free to contact us directly:

Mike Matheson - mathesonma AT ornl DOT gov

George Ostrouchov - ostrouchovg AT ornl DOT gov",4.297392472484232
"What is the email address of the person who can help me with pbdR on Summit?
",For Summit:,4.0761133884392535
"What is the retention policy for files in the Project Work directory?
","Retention - Period of time, post-account-deactivation or post-project-end, after which data will be marked as eligible for permanent deletion.

Important! Files within ""Work"" directories (i.e., Member Work, Project Work, World Work) are not backed up and are purged on a regular basis according to the timeframes listed above.

Footnotes

1

This entry is for legacy User Archive directories which contained user data on January 14, 2020. There is also a quota/limit of 2,000 files on this directory.

2",4.406816672508988
"What is the retention policy for files in the Project Work directory?
","Footnotes

1

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

2

Retention is not applicable as files will follow purge cycle.",4.397644933311766
"What is the retention policy for files in the Project Work directory?
","3

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

4

Retention is not applicable as files will follow purge cycle.",4.385816337320065
"How do I optimize the performance of my deep learning model on Summit?
","Large workloads.

Long runtimes on Summit's high memory nodes.

Your Python script has support for multi-gpu/multi-node execution via dask-cuda.

Your Python script is single GPU but requires simultaneous job steps.

RAPIDS is provided in Jupyter following  these instructions.

Note that Python scripts prepared on Jupyter can be deployed on Summit if they use the same RAPIDS version. Use !jupyter nbconvert --to script my_notebook.ipynb to convert notebook files to Python scripts.

RAPIDS is provided on Summit through the module load command:",4.236571066134615
"How do I optimize the performance of my deep learning model on Summit?
","The https://docs.olcf.ornl.gov/systems/summit_user_guide.html#OLCF Training Archive<training-archive> provides a list of previous training events, including multi-day Summit Workshops. Some examples of topics addressed during these workshops include using Summit's NVME burst buffers, CUDA-aware MPI, advanced networking and MPI, and multiple ways of programming multiple GPUs per node. You can also find simple tutorials and code examples for some common programming and running tasks in our Github tutorial page .",4.2261991617141295
"How do I optimize the performance of my deep learning model on Summit?
","Summit is an IBM system located at the Oak Ridge Leadership Computing Facility. With a theoretical peak double-precision performance of approximately 200 PF, it is one of the most capable systems in the world for a wide range of traditional computational science applications. It is also one of the ""smartest"" computers in the world for deep learning applications with a mixed-precision capability in excess of 3 EF.



Summit node architecture diagram",4.223026716051805
"How can I specify the XNACK mode for a binary during compilation?
","If no XNACK flag is specificed at compilation the default is ""xnack any"", and objects in `roc-obj-ls` with not have an XNACK mode specified.

.. code::
    $ hipcc --amdgpu-target=gfx90a square.hipref.cpp -o xnack_any.exe
    $ roc-obj-ls -v xnack_any.exe
    Bundle# Entry ID:                                                              URI:
    1       host-x86_64-unknown-linux                                           file://xnack_any.exe#offset=8192&size=0
    1       hipv4-amdgcn-amd-amdhsa--gfx90a                                     file://xnack_any.exe#offset=8192&size=9752",4.292665353058751
"How can I specify the XNACK mode for a binary during compilation?
","If no XNACK flag is specificed at compilation the default is ""xnack any"", and objects in `roc-obj-ls` with not have an XNACK mode specified.

.. code::
    $ hipcc --amdgpu-target=gfx90a square.hipref.cpp -o xnack_any.exe
    $ roc-obj-ls -v xnack_any.exe
    Bundle# Entry ID:                                                              URI:
    1       host-x86_64-unknown-linux                                           file://xnack_any.exe#offset=8192&size=0
    1       hipv4-amdgcn-amd-amdhsa--gfx90a                                     file://xnack_any.exe#offset=8192&size=9752",4.292665353058751
"How can I specify the XNACK mode for a binary during compilation?
","Two versions of each kernel will be generated, one that runs with XNACK disabled and one that runs if XNACK is enabled. This is different from ""xnack any"" in that two versions of each kernel are compiled and HIP picks the appropriate one at runtime, rather than there being a single version compatible with both. A ""fat binary"" compiled in this way will have the same performance of ""xnack+"" with HSA_XNACK=1 and as ""xnack-"" with HSA_XNACK=0, but the final executable will be larger since it contains two copies of every kernel.",4.228756570047901
"What is the difference between a route and a service in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.398431240566318
"What is the difference between a route and a service in Slate?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.278102742707671
"What is the difference between a route and a service in Slate?
","In general, using FQDNs to access a service is more convenient. If your service communicates over HTTP or HTTPS, you can set up a https://docs.olcf.ornl.gov/systems/services.html#slate_routes to achieve this. If your service uses another protocol, you can use https://docs.olcf.ornl.gov/systems/services.html#slate_nodeports.

NodePort is a type of Service that reserves a port across the cluster that can route traffic to your pods. This is more flexible than a Route and can handle any TCP or UDP traffic.",4.224198740424251
"What is the purpose of the {0:4} syntax in Summit?
",For Summit:,4.181269809504828
"What is the purpose of the {0:4} syntax in Summit?
","The above procedure can also be followed to connect to Summit or Frontier, with the main difference being the number of available processors. The time limit syntax for Andes, Summit, and Frontier also differ. Summit uses the format HH:MM while Andes and Frontier follow HH:MM:SS.

Please do not run VisIt's GUI client from an OLCF machine. You will get much better performance if you install a client on your workstation and launch locally. You can directly connect to OLCF machines from inside VisIt and access your data remotely.",4.065435599499879
"What is the purpose of the {0:4} syntax in Summit?
",Figure 1. An example of the NDS servers on Summit,4.042453448481849
"How can I run the date command to write a timestamp to the standard output file for my job on Summit?
","6: This line is left blank, so it will be ignored.

7: This command will change the current directory to the directory from where the script was submitted.

8: This command will run the date command.

9: This command will run (8) MPI instances of the executable a.out on the compute nodes allocated by the batch system.

Batch scripts can be submitted for execution using the sbatch command. For example, the following will submit the batch script named test.slurm:

sbatch test.slurm",4.2003773039258965
"How can I run the date command to write a timestamp to the standard output file for my job on Summit?
","output to a file named RunSim123.#, where # is the job ID assigned by LSF | | 9 | Optional | Write standard error to a file named RunSim123.#, where # is the job ID assigned by LSF | | 10 |  | Blank line | | 11 |  | Change into one of the scratch filesystems | | 12 |  | Copy input files into place | | 13 |  | Run the date command to write a timestamp to the standard output file | | 14 |  | Run the executable on the allocated compute nodes | | 15 |  | Copy output files from the scratch area into a more permanent location |",4.197424590880016
"How can I run the date command to write a timestamp to the standard output file for my job on Summit?
","This method on Summit requires the user to be present until the job completes. For users who have long scripts or are unable to monitor the job, you can submit the above lines in a batch script. However, you will wait in the queue twice, so this is not recommended. Alternatively, one can use Andes.

For Andes/Frontier (Slurm Script):

Andes

.. code-block:: bash


   #!/bin/bash
   #SBATCH -A XXXYYY
   #SBATCH -J visit_test
   #SBATCH -N 1
   #SBATCH -p gpu
   #SBATCH -t 0:05:00

   cd $SLURM_SUBMIT_DIR
   date

   module load visit",4.195769013213961
"How do I create a network policy in Slate?
","Network policies are specifications of how groups of pods are allowed to communicate with each other and other network endpoints. A pod is selected in a Network Policy based on a label so the Network Policy can define rules to specify traffic to that pod.

When you create a namespace in Slate with the Quota Dashboard some Network Policies are added to your newly created namespace. This means that pods inside your project are isolated from connections not defined in these Network Policies.",4.368217636448522
"How do I create a network policy in Slate?
","Network Policies do not conflict, they are additive. This means that if two policies match a pod the pod will be restricted to what is allowed by the union of those policies' ingress/egress rules.

To create a Network policy using the GUI click the Networking tab followed by the Network Policy tab:

Creating Network Policies

This will place you in an editor with some boiler plate YAML. From here you can define the network policy that you need for your namespace. Below is an example Network Policy that you would use to allow all external traffic to access a nodePort in your namespace.",4.228283143467134
"How do I create a network policy in Slate?
","to view object's YAML.

To create a Network Policy, define one in YAML similar to the output of the previous command and run:

oc create -f FILENAME

For a more complex example of a Network Policy please see the Kubernetes doc.

A full reference of Network Policies can be found here.",4.178228971500132
"How does the hardware atomic operations for CAS/Min/Max on FP32 compare to the FP64 counterparts in Frontier?
","Hardware FP atomic operations performed in LDS memory are usually always faster than an equivalent CAS loop, in particular when contention on LDS memory locations is high. Because of a hardware design choice, FP32 LDS atomicAdd() operations can be slower than equivalent FP64 LDS atomicAdd(), in particular when contention on memory locations is low (e.g. random access pattern). The aforementioned behavior is only true for FP atomicAdd() operations. Hardware atomic operations for CAS/Min/Max on FP32 are usually faster than the FP64 counterparts. In cases when contention is very low, a FP32 CAS",4.436158200578328
"How does the hardware atomic operations for CAS/Min/Max on FP32 compare to the FP64 counterparts in Frontier?
","Hardware FP atomic operations performed in LDS memory are usually always faster than an equivalent CAS loop, in particular when contention on LDS memory locations is high. Because of a hardware design choice, FP32 LDS atomicAdd() operations can be slower than equivalent FP64 LDS atomicAdd(), in particular when contention on memory locations is low (e.g. random access pattern). The aforementioned behavior is only true for FP atomicAdd() operations. Hardware atomic operations for CAS/Min/Max on FP32 are usually faster than the FP64 counterparts. In cases when contention is very low, a FP32 CAS",4.436158200578328
"How does the hardware atomic operations for CAS/Min/Max on FP32 compare to the FP64 counterparts in Frontier?
","In cases when contention is very low, a FP32 CAS loop implementing an atomicAdd() operation could be faster than an hardware FP32 LDS atomicAdd(). Applications using single precision FP atomicAdd() are encouraged to experiment with the use of double precision to evaluate the trade-off between high atomicAdd() performance vs. potential lower occupancy due to higher LDS usage.",4.361782404612419
"How does the batch system determine the apparent submit time for jobs?
","number of processors requested above, this is an adjustment to the

apparent submit time of the job. However, this adjustment has the effect

of making jobs appear much younger than jobs submitted under projects

that have not exceeded their allocation. In addition to the priority

change, these jobs are also limited in the amount of wall time that can

be used. For example, consider that ``job1`` is submitted at the same

time as ``job2``. The project associated with ``job1`` is over its

allocation, while the project for ``job2`` is not. The batch system will",4.422986977821425
"How does the batch system determine the apparent submit time for jobs?
","For example, consider that job1 is submitted at the same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. In addition, projects that are at 125% of their allocated time will be limited to only one running job at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.398665342338849
"How does the batch system determine the apparent submit time for jobs?
","For example, consider that job1 is submitted at the same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. In addition, projects that are at 125% of their allocated time will be limited to only one running job at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.398665342338849
"How does the MI250X GPU's denormal handling affect the performance of deep learning models using FP16 precision?
","The MI250X has different denormal handling for FP16 and BF16 datatypes, which is relevant for ML training. Prefer using the BF16 over the FP16 datatype for ML models as you are more likely to encounter denormal values with FP16 (which get flushed to zero, causing failure in convergence for some ML models). See more in https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#using-reduced-precision.",4.502316448845882
"How does the MI250X GPU's denormal handling affect the performance of deep learning models using FP16 precision?
","Users leveraging BF16 and FP16 datatypes for applications such as ML/AI training and low-precision matrix multiplication should be aware that the AMD MI250X GPU has different denormal handling than the V100 GPUs on Summit. On the MI250X, the V_DOT2 and the matrix instructions for FP16 and BF16 flush input and output denormal values to zero. FP32 and FP64 MFMA instructions do not flush input and output denormal values to zero.",4.486945085640215
"How does the MI250X GPU's denormal handling affect the performance of deep learning models using FP16 precision?
","Users leveraging BF16 and FP16 datatypes for applications such as ML/AI training and low-precision matrix multiplication should be aware that the AMD MI250X GPU has different denormal handling than the V100 GPUs on Summit. On the MI250X, the V_DOT2 and the matrix instructions for FP16 and BF16 flush input and output denormal values to zero. FP32 and FP64 MFMA instructions do not flush input and output denormal values to zero.",4.486945085640215
"How do I use the Conda quick-reference list?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly. For those that just want a quick-reference list of common conda commands, see the https://docs.olcf.ornl.gov/systems/conda_basics.html#conda-quick section.",4.43987923768359
"How do I use the Conda quick-reference list?
","Because there is no conda module on Frontier, this guide only applies if you installed a personal Miniconda first. See our https://docs.olcf.ornl.gov/software/python/miniconda.html for more details.

This guide has been shortened and adapted from a challenge in OLCF's Hands-On with Summit GitHub repository (Python: Conda Basics).",4.210249329934867
"How do I use the Conda quick-reference list?
","This guide introduces a user to the basic workflow of using conda environments, as well as providing an example of how to create a conda environment that uses a different version of Python than the base environment uses on Summit. Although Summit is being used in this guide, all of the concepts still apply to other OLCF systems. If using Frontier, this assumes you already have installed your own version of conda (e.g., by https://docs.olcf.ornl.gov/software/python/miniconda.html).

OLCF Systems this guide applies to:

Summit

Andes

Frontier (if using conda)",4.18431347763306
"Where can users find the Crusher quick start guide?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.172944811005979
"Where can users find the Crusher quick start guide?
","Crusher users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.105959412759884
"Where can users find the Crusher quick start guide?
",This section of the Summit User Guide is intended to show current OLCF users how to start preparing their applications to run on the upcoming Frontier system. We will continue to add more topics to this section in the coming months. Please see the topics below to get started.,4.089496025801033
"How can I efficiently use my project's allocation?
","Before you can do anything you need a Project to do your things in. Fortunately, when you get an allocation on one of our clusters a Project is automatically created for you with the same name as the allocation. By using our own distinct Project we are ensuring that we will not interfere with anyone else's work.

NOTE: Everywhere that you see <cluster> replace that with the cluster that you will be running on (marble or onyx).",4.326940652352308
"How can I efficiently use my project's allocation?
","The requesting project's allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Summit would be equivalent to using 16,000 Summit node-hours of a project's allocation.



As is the case with many other queuing systems, it is possible to place dependencies on jobs to prevent them from running until other jobs have started/completed/etc. Several possible dependency settings are described in the table below:",4.205292866244861
"How can I efficiently use my project's allocation?
","The requesting project’s allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Frontier would be equivalent to using 16,000 Frontier node-hours of a project’s allocation.

Reservations should not be confused with priority requests. If quick turnaround is needed for a few jobs or for a period of time, a priority boost should be requested. A reservation should only be requested if users need to guarantee availability of a set of nodes at a given time, such as for a live demonstration at a conference.",4.163769915519495
"How long does it typically take for a Persistent Volume Claim (PVC) to transition from the Pending state to the Bound state?
","PV Settings

This will redirect you to the status page of your new PVC. You should see Bound in the Status field.

PV Status

Once the claim has been granted to the project a pod within that project can access that storage. To do this, edit the YAML of the pod and under the volume portion of the file add a name and the name of the claim. It will look similar to the following.

volumes:
  name: pvol
  persistentVolumeClaim:
    claimName: storage-1

Then all that is left to do is mount the storage into a container:

volumeMounts:
  - mountPath: /tmp/
    name: pvol",4.319696175925893
"How long does it typically take for a Persistent Volume Claim (PVC) to transition from the Pending state to the Bound state?
","oc create -f storage-1.yaml

Once this command has run the claim will go into the Pending state. Typically it will only be in this state for a brief amount of time before transitioning to the Bound state. You can check the state of your PVC with the following command:

oc get pvc storage-1

This will return the state of your claim. That output should look something like this once the claim has been bound:

NAME      STATUS      VOLUME                 CAPACITY   ACCESSMODES   STORAGECLASS   AGE
storage-1 Bound       openshift-data-v0019   100Gi      RWO,ROX,RWX                  7s",4.302668722068528
"How long does it typically take for a Persistent Volume Claim (PVC) to transition from the Pending state to the Bound state?
","Once the Persistent Volume is created its allocated memory is available to be claimed by a project. This is done through a Persistent Volume Claim. PVC's are created using a YAML file in the same manner that PV's are created. An example of a PVC that claims the PV created in the previous section follows:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: storage-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

The claim is then placed using:

oc create -f storage-1.yaml",4.260090413233685
"How do I check the status of a job in Frontier?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.254526289118458
"How do I check the status of a job in Frontier?
","The most straightforward monitoring is with the bjobs command. This command will show the current queue, including both pending and running jobs. Running bjobs -l will provide much more detail about a job (or group of jobs). For detailed output of a single job, specify the job id after the -l. For example, for detailed output of job 12345, you can run bjobs -l 12345 . Other options to bjobs are shown below. In general, if the command is specified with -u all it will show information for all users/all jobs. Without that option, it only shows your jobs. Note that this is not an exhaustive list.",4.147557929697819
"How do I check the status of a job in Frontier?
","In addition to holding, releasing, and updating the job, the scontrol command can show detailed job information via the show job subcommand. For example, scontrol show job 12345.



The default job launcher for Frontier is srun . The srun command is used to execute an MPI binary on one or more compute nodes in parallel.

srun  [OPTIONS... [executable [args...]]]

Single Command (non-interactive)

$ srun -A <project_id> -t 00:05:00 -p <partition> -N 2 -n 4 --ntasks-per-node=2 ./a.out
<output printed to terminal>",4.144813368116179
"Can you explain the concept of ""aging"" in the context of the OLCF Policy?
","By default, there is no lifetime retention for any data on OLCF resources. The OLCF specifies a limited post-deactivation timeframe during which user and project data will be retained. When the retention timeframe expires, the OLCF retains the right to delete data. If you have data retention needs outside of the default policy, please notify the OLCF.",4.192967651151863
"Can you explain the concept of ""aging"" in the context of the OLCF Policy?
","The project data retention policy exists to reclaim storage space after a project ends. By default, the OLCF will retain data in project-centric storage areas only for a designated amount of time after the project end date. During this time, a project member can request a temporary user account extension for data access. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for details on purge and retention timeframes for each project-centric storage area.",4.181424356280264
"Can you explain the concept of ""aging"" in the context of the OLCF Policy?
","The user data retention policy exists to reclaim storage space after a user account is deactivated, e.g., after the user’s involvement on all OLCF projects concludes. By default, the OLCF will retain data in user-centric storage areas only for a designated amount of time after the user’s account is deactivated. During this time, a user can request a temporary user account extension for data access. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for details on retention timeframes for each user-centric storage area.",4.1742409072019
"How can I request 1024 nodes for my job on Summit?
","Jobs on Summit are scheduled in full node increments; a node's cores cannot be allocated to multiple jobs. Because the OLCF charges based on what a job makes unavailable to other users, a job is charged for an entire node even if it uses only one core on a node. To simplify the process, users request and are allocated multiples of entire nodes through LSF.

Allocations on Summit are separate from those on Andes and other OLCF resources.

The node-hour charge for each batch job will be calculated as follows:

node-hours = nodes requested * ( batch job endtime - batch job starttime )",4.269207025108765
"How can I request 1024 nodes for my job on Summit?
","The requesting project's allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Summit would be equivalent to using 16,000 Summit node-hours of a project's allocation.



As is the case with many other queuing systems, it is possible to place dependencies on jobs to prevent them from running until other jobs have started/completed/etc. Several possible dependency settings are described in the table below:",4.261598967801195
"How can I request 1024 nodes for my job on Summit?
","To access Summit and Frontier's compute resources for SPI workflows, you must first log into a https://docs.olcf.ornl.gov/systems/index.html#Citadel login node<citadel-login-nodes> and then submit a batch job to one of the SPI specific batch queues.",4.248213080569051
"How can I determine the optimal number of GPUs to use for my job?
","Based on how your code expects to interact with the system, you can create resource sets containing the needed GPU and core resources. If a code expects to utilize one GPU per task, a resource set would contain one core and one GPU. If a code expects to pass work to a single GPU from two tasks, a resource set would contain two cores and one GPU.

Decide on the number of resource sets needed

Once you understand tasks, threads, and GPUs in a resource set, you simply need to decide the number of resource sets needed.",4.24099100183705
"How can I determine the optimal number of GPUs to use for my job?
","Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.",4.218729015246713
"How can I determine the optimal number of GPUs to use for my job?
","Due to the unique architecture of Frontier compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.

Before jumping into the examples, it is helpful to understand the output from the hello_jobstep program:",4.199628918603765
"How do I access the SPI resource for maintenance and troubleshooting purposes?
","https://docs.olcf.ornl.gov/systems/index.html#SPI resources mount SPI filesystems<spi-file-systems>.  The SPI resources do not mount the non-SPI's scratch filesystems, home areas, or mass storage.

https://docs.olcf.ornl.gov/systems/index.html#SPI compute resources cannot access external resources<spi-data-transfer>.  Needed data must be transferred to the SPI resources through the SPI's DTN.

https://docs.olcf.ornl.gov/systems/index.html#The Citadel login nodes<spi-compute-citadel> and batch queues must be used to access Summit and Frontier for SPI workflows.",4.162880693465788
"How do I access the SPI resource for maintenance and troubleshooting purposes?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.

The SPI provides access to the OLCF's Summit resource for compute.  To safely separate SPI and non-SPI workflows, SPI workflows must use a separate login node named https://docs.olcf.ornl.gov/systems/citadel_user_guide.html#Citadel<spi-compute-citadel>.  Citadel provides a login node specifically for SPI workflows.",4.113415272283612
"How do I access the SPI resource for maintenance and troubleshooting purposes?
","Access to the SPI resources is allowed to approved IP addresses only.

Direct access to SPI resources require the connecting IP address to be whitelisted.  The OLCF must know your IP before you can directly connect to SPI resources.

Project using ORNL's KDI must following KDI access procedures and cannot access SPI resources directly.  If your project uses both KDI and SPI, you do not need to provide an IP.



To add an IP or range of IPs to your project’s whitelist, please contact help@olcf.ornl.gov",4.111406868297855
"How do I launch a batch job on Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.416282926723181
"How do I launch a batch job on Andes?
","Batch Queues on Andes

The compute nodes on Andes are separated into two partitions the ""batch partition"" and the ""GPU partition"" as described in the https://docs.olcf.ornl.gov/systems/your_file.html#andes-compute-nodes section. The scheduling policies for the individual partitions are as follows:

Batch Partition Policy (default)

Jobs that do not specify a partition will run in the 704 node batch partition:",4.318062767758001
"How do I launch a batch job on Andes?
","For example, this is how you would modify the Andes profile to use the gpu partition:

Under Andes' ""Launch Profiles"":

Click on ""New Profile""

Name the profile something like ""gpu"" (arbitrary)

Click on ""Parallel""

Check ""Launch Parallel Engine""

Set ""Launch Method"" to sbatch/srun (required)

Set ""Partition/Pool/Queue"" to gpu (required)

Set default number of processors to 28 (max without hyperthreading) (arbitrary)

Set default number of nodes to 1 (arbitrary)

Set default ""Bank/Account"" to your OLCF project with Andes allocation

Set a default ""Time Limit"" in format of (HH:MM:SS)",4.224771748619324
"How can I see a list of all available metrics and events on Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.18532946186354
"How can I see a list of all available metrics and events on Summit?
","If you want to collect information on just a specific performance measurement, for example the number of bytes written to DRAM, you can do so with the --metrics option:

summit> jsrun -n1 -a1 -g1 nv-nsight-cu-cli -k vectorAdd --metrics dram__bytes_write.sum ./vectorAdd

The list of available metrics can be obtained with nv-nsight-cu-cli --query-metrics. Most metrics have both a base name and suffix. Together these  make up the full metric name to pass to nv-nsight-cu-cli. To list the full names for a collection of metrics, use --query-metrics-mode suffix --metrics <metrics list>.",4.179343759265083
"How can I see a list of all available metrics and events on Summit?
",For Summit:,4.151931353366046
"How do I launch the compiler server for pyQuil?
","With the way pyQuil works, you need to launch its compiler server, launch the virtual machine / simulator QVM server, and then launch your pyQuil Python program on the same host. Running a Python script will ping and utilize both the compiler and QVM servers. As a proof of concept, this has been done on a single login node and the steps are outlined below.

Using your already created ENV_NAME virtual environment (outlined above):

(ENV_NAME)$ quilc -P -S > quilc.log 2>&1 & qvm -S > qvm.log 2>&1 & python3 script.py ; kill $(jobs -p)",4.466534902137406
"How do I launch the compiler server for pyQuil?
","Quil is the Rigetti-developed quantum instruction/assembly language. PyQuil is a Python library for writing and running quantum programs using Quil.

Installing pyQuil requires installing the Forest SDK. To quote Rigetti: ""pyQuil, along with quilc, the QVM, and other libraries, make up what is called the Forest SDK"". Because we don't have Docker functionality and due to normal users not having sudo privileges, this means that you will have to install the SDK via the ""bare-bones"" method. The general info below came from:

https://pyquil-docs.rigetti.com/en/stable/start.html",4.232370040891912
"How do I launch the compiler server for pyQuil?
","Rigetti provides system access via a cloud-based JupyterLab development environment: https://docs.rigetti.com/qcs/getting-started/jupyterlab-ide.  From there, users can access a JupyterLab server loaded with Rigetti's PyQuil programming framework, Rigetti's Forest Software Development Kit, and associated program examples and tutorials.  This is the method that allows access to Rigetti's QPU's directly, as opposed to simulators.",4.188743313643744
"How can I view a specific revision of a deployment in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.122002210078719
"How can I view a specific revision of a deployment in Slate?
","To roll back a deployment, run

oc rollout undo deploy/{NAME}

When using the web interface, you can view and edit a Deployment, from the sidebar, go to Applications, then Deployments.

Deployment Menu

You can get info on any deployment by clicking on it.

To edit this configuration, click Actions in the upper right hand corner, then Edit on whatever you wish to edit, or Edit Deployment if you'd rather edit the YAML directly.

Edit Deployment Config",4.105098859432804
"How can I view a specific revision of a deployment in Slate?
","As with other objects in OpenShift, you can create this Deployment with oc create -f {FILE_NAME}.

Once created, you can check the status of the ongoing deployment process with the command

oc rollout status deploy/{NAME}

To get basic information about the available revisions, if you had done any updates to the deployment since, run

oc rollout history deploy/{NAME}

You can view a specific revision with

oc rollout history deploy/{NAME} --revision=1

For more detailed information about a Deployment, use

oc describe deploy {NAME}

To roll back a deployment, run",4.086753418044694
"Can I use a pre-compiled binary of CuPy instead of building it from source?
","If you wish to use CuPy within a clone of the OpenCE environment, the installation process is very similar to what we do in the regular CuPy installation we saw above.

The open-ce/1.2.0-pyXY-0 (which is the current default) will not support this. So make sure you are using open-ce/1.5.0-pyXY-0 or higher.

The contents of the open-ce module cannot be modified so you need to make your own clone of the open-ce environment.",4.215341191586427
"Can I use a pre-compiled binary of CuPy instead of building it from source?
","Because issues can arise when using conda and pip together (see link in https://docs.olcf.ornl.gov/systems/conda_basics.html#conda-refs), it is recommended to do this only if absolutely necessary.

To build a package from source, use pip install --no-binary=<package_name> <package_name>:

$ CC=gcc pip install --no-binary=numpy numpy

The CC=gcc flag will ensure that you are using the proper compiler and wrapper. Building from source results in a longer installation time for packages, so you may need to wait a few minutes for the install to finish.",4.171775812996341
"Can I use a pre-compiled binary of CuPy instead of building it from source?
","You have now discovered what CuPy can provide! Now you can try speeding up your own codes by swapping CuPy and NumPy where you can.

CuPy User Guide

CuPy Website

CuPy API Reference

CuPy Release Notes",4.163597027894156
"Can I use HPCToolkit to analyze the performance of GPU-accelerated applications?
","Programming models supported by HPCToolkit include MPI, OpenMP, OpenACC, CUDA, OpenCL, DPC++, HIP, RAJA, Kokkos, and others.

Below is an example that generates a profile and loads the results in their GUI-based viewer.

module use /gpfs/alpine/csc322/world-shared/modulefiles/ppc64le
module load hpctoolkit

# 1. Profile and trace an application using CPU time and GPU performance counters
jsrun <jsrun_options> hpcrun -o <measurement_dir> -t -e CPUTIME -e gpu=nvidia <application>

# 2. Analyze the binary of executables and its dependent libraries
hpcstruct <measurement_dir>",4.466060103176973
"Can I use HPCToolkit to analyze the performance of GPU-accelerated applications?
","Programming models supported by HPCToolkit include MPI, OpenMP, OpenACC, CUDA, OpenCL, DPC++, HIP, RAJA, Kokkos, and others.

Below is an example that generates a profile and loads the results in their GUI-based viewer.

module use /gpfs/alpine/csc322/world-shared/modulefiles/x86_64
module load hpctoolkit

# 1. Profile and trace an application using CPU time and GPU performance counters
srun <srun_options> hpcrun -o <measurement_dir> -t -e CPUTIME -e gpu=amd <application>

# 2. Analyze the binary of executables and its dependent libraries
hpcstruct <measurement_dir>",4.455235338798804
"Can I use HPCToolkit to analyze the performance of GPU-accelerated applications?
","Programming models supported by HPCToolkit include MPI, OpenMP, OpenACC, CUDA, OpenCL, DPC++, HIP, RAJA, Kokkos, and others.

Below is an example that generates a profile and loads the results in their GUI-based viewer.

A full list of available HPCToolkit versions can be seen with the module spider hpctoolkit command.

module load hpctoolkit/2022.05.15-rocm

# 1. Profile and trace an application using CPU time and GPU performance counters
srun <srun_options> hpcrun -o <measurement_dir> -t -e CPUTIME -e gpu=amd <application>",4.417429884456011
"Can I respond to a previous ticket with a new, unrelated issue?
","Please do not respond to previous tickets with new, unrelated issues. This can slow down response time and make finding relevant information harder thereby slowing down time to resolution.

Let us know if you've solved the issue yourself (and let us know what worked!)

MOST IMPORTANT: do not hesitate to contact us (help@olcf.ornl.gov); we will work through the details with you.",4.1845301129600925
"Can I respond to a previous ticket with a new, unrelated issue?
","Where possible, provide helpful details that can help speed the process. For example: Project ID, relevant directories, job scripts, jobIDs, modules at compile/runtime, host name, etc.

When replying to a ticket, do not modify the subject line.

Do not piggyback unrelated questions on existing tickets. This leads to slower response times and inflates ticket history.

Do not open multiple tickets on the same unresolved topic. Doing so can fragment resources and slow down the time to resolution.",4.097205619024143
"Can I respond to a previous ticket with a new, unrelated issue?
","*****ORIGINAL FILE*****
This is my file. There are many like it but this one is mine.
***********************

*****UPDATED FILE******
This is my file. There are many like it but this one is mine.
spock25
***********************



If you have problems or need helping running on Spock, please submit a ticket by emailing help@olcf.ornl.gov.



JIRA_CONTENT_HERE",3.8363156028347056
"What is the purpose of the RT_GPU_ID in the output from the program?
","The output from the program contains a lot of information, so let's unpack it. First, there are different IDs associated with the GPUs so it is important to describe them before moving on. GPU_ID is the node-level (or global) GPU ID, which is labeled as one might expect from looking at the Crusher Node Diagram: 0, 1, 2, 3, 4, 5, 6, 7. RT_GPU_ID is the HIP runtime GPU ID, which can be though of as each MPI rank's local GPU ID number (with zero-based indexing). So in the output above, each MPI rank has access to only 1 unique GPU - where MPI 000 has access to ""global"" GPU 4, MPI 001 has access",4.43911070822062
"What is the purpose of the RT_GPU_ID in the output from the program?
","Here is a summary of the different GPU IDs reported by the example program:

GPU_ID is the node-level (or global) GPU ID read from ROCR_VISIBLE_DEVICES. If this environment variable is not set (either by the user or by Slurm), the value of GPU_ID will be set to N/A by this program.

RT_GPU_ID is the HIP runtime GPU ID (as reported from, say hipGetDevice).

Bus_ID is the physical bus ID associated with the GPUs. Comparing the bus IDs is meant to definitively show that different GPUs are being used.",4.407221224588156
"What is the purpose of the RT_GPU_ID in the output from the program?
","Here is a summary of the different GPU IDs reported by the example program:

GPU_ID is the node-level (or global) GPU ID read from ROCR_VISIBLE_DEVICES. If this environment variable is not set (either by the user or by Slurm), the value of GPU_ID will be set to N/A.

RT_GPU_ID is the HIP runtime GPU ID (as reported from, say hipGetDevice).

Bus_ID is the physical bus ID associated with the GPUs. Comparing the bus IDs is meant to definitively show that different GPUs are being used.",4.389044887791318
"How can I create a secured route with re-encryption termination in Slate?
","The following command will create a secured route with re-encryption termination.

oc create route reencrypt --service=my-project \
  --hostname=my-project.apps.<cluster>.ccs.ornl.gov \
  --dest-ca-cert=ca.crt

Note that the --dest-ca-cert flag for the destination CA certificate is required for re-encryption.

The outputted YAML will look like this example:",4.273364121788798
"How can I create a secured route with re-encryption termination in Slate?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.235929503638905
"How can I create a secured route with re-encryption termination in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.201088984726185
"How do I download data from a bucket in MinIO?
","Depending on you how configured your deployment, this could be your NFS or GPFS project space or an isolated volume dedicated/isolated to this MinIO server.

Within the GUI you can create buckets and upload/download data. If you are running this on NFS or GPFS the bucket will map to a directory.",4.234530784734504
"How do I download data from a bucket in MinIO?
","MinIO is a high-performance software-defined object storage suite that enables the ability to easily deploy cloud-native data infrastructure for various data workloads. In this example we are deploying a simple, standalone implementation of MinIO on our cloud-native platform, Slate (https://docs.olcf.ornl.gov/systems/minio.html#slate_overview).

This service will only be accessible from inside of ORNL's network.

If your project requires an externally facing service available to the Internet, please contact User Assistance by submitting a help ticket. There is a process to get such approval.",4.2039156138982285
"How do I download data from a bucket in MinIO?
","You can also go to it by logging into the Marble GUI. Once logged in, go to Networking->Routes and click the URL in the ""Location"" column of your MinIO applications row.

You will be greeted with the NCCS SSO page. Continue through that with your normal NCCS login credentials.

After the NCCS login, you will be greeted with MinIO's login page. Here you will enter the access-key and secret-key you created with the secret-token.yaml file.

At this point, you should be inside the MinIO Browser.",4.132206624757877
"Can rocprof profile multiple GPUs simultaneously?
","Integer instructions and cache levels are currently not documented here.

To get started, you will need to make an input file for rocprof, to be passed in through rocprof -i <input_file> --timestamp on -o my_output.csv <my_exe>. Below is an example, and contains the information needed to roofline profile GPU 0, as seen by each rank:",4.284498320763795
"Can rocprof profile multiple GPUs simultaneously?
","Integer instructions and cache levels are currently not documented here.

To get started, you will need to make an input file for rocprof, to be passed in through rocprof -i <input_file> --timestamp on -o my_output.csv <my_exe>. Below is an example, and contains the information needed to roofline profile GPU 0, as seen by each rank:",4.284498320763795
"Can rocprof profile multiple GPUs simultaneously?
","srun -N 2 -n 16 --ntasks-per-node=8 --gpus-per-node=8 --gpu-bind=closest bash -c 'rocprof -o ${SLURM_JOBID}_${SLURM_PROCID}.csv -i <input_file> --timestamp on <exe>'

The gpu: filter in the rocprof input file identifies GPUs by the number the MPI rank would see them as. In the srun example above, each MPI rank only has 1 GPU, so each rank sees its GPU as GPU 0.

The theoretical (not attainable) peak roofline constructs a theoretical maximum performance for each operational intensity.",4.27598158884376
"Where are User Archive directories located?
","Use of User Archive areas for data storage is deprecated as of January 14, 2020. The user archive area for any user account created after that date (or for any user archive directory that is empty of user files after that date) will contain only symlinks to the top-level directories for each of the user's projects on HPSS. Users with existing data in a User Archive directory are encouraged to move that data to an appropriate project-based directory as soon as possible.  The information below is simply for reference for those users with existing data in User Archive directories.",4.435573678669543
"Where are User Archive directories located?
","2

User Archive directories that were created (or had no user data) after January 14, 2020. Settings other than permissions are not applicable because directories are root-owned and contain no user files.

3",4.4140332215408105
"Where are User Archive directories located?
","User archive areas on HPSS are intended for storage of data not immediately needed in either User Home directories (NFS) or User Work directories (GPFS). User Archive directories should not be used to store project-related data. Rather, Project Archive directories should be used for project data.

User archive directories are located at /home/$USER.",4.378447430746845
"What is the benefit of attending a Summit Workshop?
",For Summit:,4.290218497331336
"What is the benefit of attending a Summit Workshop?
",| (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_20190211_summit_workshop_python.pdf https://vimeo.com/346452419 | | 2019-02-11 | Practical Tips for Running on Summit | David Appelhans (IBM) | Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/02/STW_Feb_GettingStartedExamples_169ratio.pdf https://vimeo.com/346452176 | | 2018-12-06 | ML/DL Frameworks on Summit | Junqi Yin (OLCF) | Summit Training Workshop,4.244107905089293
"What is the benefit of attending a Summit Workshop?
",https://vimeo.com/306435487 | | 2018-12-03 | Experiences Porting/Optimizing Codes for Acceptance Testing | Bob Walkup (IBM) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording 1 recording 2) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_walkup.pdf https://vimeo.com/306890861 https://vimeo.com/306890949 | | 2018-12-03 | Practical Tips for Running on Summit | David Appelhans (IBM) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording),4.196510744478772
"How do I upload data to the HPSS?
","Currently, HSI and HTAR are offered for archiving data into HPSS or retrieving data from the HPSS archive. For optimal transfer performance, we recommend sending a file of 768 GB or larger to HPSS. The minimum file size that we recommend sending is 512 MB. HPSS will handle files between 0K and 512 MB, but write and read performance will be negatively affected. For files smaller than 512 MB we recommend bundling them with HTAR to achieve an archive file of at least 512 MB.",4.34689993432803
"How do I upload data to the HPSS?
",Please note that the HPSS is not mounted directly onto Frontier nodes. There are two main methods for accessing and moving data to/from the HPSS. The first is to use the command line utilities hsi and htar. The second is to use the Globus data transfer service. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-hpss for more information on both of these methods.,4.345906039605898
"How do I upload data to the HPSS?
","Copying data to the HPSS archive system

The hsi and htar utilities can be used to to transfer data from the Orion filesystem to the HPSS. The tools can also be used to transfer data from the HPSS to the Orion filesystem.

Globus is also available to transfer data directly to the HPSS

Please do not use the HPSS as a method to migrate data

Due to the large amounts of data on the Alpine scratch filesystem and the limited available space on the HPSS archive system, we strongly recommend not using the HPSS to transfer data between Alpine and Orion.",4.260968648152223
"What is the advantage of using the AMD GPU architecture for computing?
","The Crusher system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.239777395222244
"What is the advantage of using the AMD GPU architecture for computing?
","The Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.21596578571193
"What is the advantage of using the AMD GPU architecture for computing?
","Frontier has a total of 9,408 AMD compute nodes, with each node consisting of [1x] 64-core AMD “Optimized 3rd Gen EPYC” CPU with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X accelerators, each with 2 Graphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can think of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth memory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU, allowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of 36+36 GB/s. The 2 GCDs on the same MI250X are connected with",4.1912142825529335
"How do I specify the GPU mapping for my job in Frontier?
","This section describes how to map processes (e.g., MPI ranks) and process threads (e.g., OpenMP threads) to the CPUs, GPUs, and NICs on Frontier.

Users are highly encouraged to use the CPU- and GPU-mapping programs used in the following sections to check their understanding of the job steps (i.e., srun commands) they intend to use in their actual jobs.

For the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-cpu-map and https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-multi-map sections:",4.342706509648776
"How do I specify the GPU mapping for my job in Frontier?
","In this sub-section, an MPI+OpenMP+HIP ""Hello, World"" program (hello_jobstep) will be used to show how to make only specific GPUs available to processes - which we will refer to as ""GPU mapping"". Again, Slurm's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive method was used to request an allocation of 2 compute nodes for these examples: salloc -A <project_id> -t 30 -p <parition> -N 2. The CPU mapping part of this example is very similar to the example used above in the Multithreading sub-section, so the focus here will be on the GPU mapping part.",4.308076080534036
"How do I specify the GPU mapping for my job in Frontier?
","As has been pointed out previously in the Frontier documentation, notice that GPUs are NOT mapped to MPI ranks in sequential order (e.g., MPI rank 0 is mapped to physical CPU cores 1-7 and GPU 4, MPI rank 1 is mapped to physical CPU cores 9-15 and GPU 5), but this IS expected behavior. It is simply a consequence of the Frontier node architectures as shown in the Frontier Node Diagram and subsequent https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Note on NUMA domains <numa-note>.

Example 2: 1 MPI rank with 7 CPU cores and 1 GPU (single-node)",4.2679844790804085
"How can we monitor the build process?
","Openshift will automatically pull and build your source. If you make a change and need an updated build simply navigate to the build option on the menu to the left and select your project and click Start Build in the upper right.

The first step to creating the the build is to create a build config. This is done in one of three ways depending on how your code is structured. If you have a git repository already configured and it is public then the command

oc new-build .

will create your build config from that repository.",4.072276082420744
"How can we monitor the build process?
","What happens is that oc pulls in the provided repository, in this example Django, and automatically configures everything needed to build the image. You should now be able to go to the Openshift web GUI and under the builds tab see your newly built build.

Now, since everything has been configured, you can click the Start Build button in the upper right hand side of the Web GUI anytime that you need to make another build. You can also start a another build from the command line with either:

oc start-build <buildconfig_name>

Or, if you would like to receive logs from the build:",4.061441200583143
"How can we monitor the build process?
","Taking the GitLab CI/CD concepts documentation as a start point, Continuous Integration (CI) completes tasks necessary to test and build software resulting in a container image. Example tasks performed could be code linting, test coverage, unit testing, functional testing, code compiling or integration testing. Tasks would be triggered whenever code is pushed into a repository.",4.042149824394575
"How can I compile the `square.hipref.cpp` file for the AMD GPU architecture using the `hipcc` compiler?
","| Vendor | Module | Language | Compiler | OpenMP flag (GPU) | | --- | --- | --- | --- | --- | | Cray | cce | C C++ |  | -fopenmp | | Fortran | ftn |  | | AMD | rocm |  |  |  |

This section shows how to compile HIP codes using the Cray compiler wrappers and hipcc compiler driver.

Make sure the craype-accel-amd-gfx908 module is loaded when using HIP.

| Compiler | Compile/Link Flags, Header Files, and Libraries | | --- | --- | | CC |  | | hipcc |  |",4.249662110725187
"How can I compile the `square.hipref.cpp` file for the AMD GPU architecture using the `hipcc` compiler?
","| Vendor | Module | Language | Compiler | OpenMP flag (GPU) | | --- | --- | --- | --- | --- | | Cray | cce | C C++ |  | -fopenmp | | Fortran | ftn (wraps crayftn) |  | | AMD | rocm |  |  | -fopenmp |

If invoking amdclang, amdclang++, or amdflang directly, or using hipcc you will need to add: -fopenmp -target x86_64-pc-linux-gnu -fopenmp-targets=amdgcn-amd-amdhsa -Xopenmp-target=amdgcn-amd-amdhsa -march=gfx90a.

This section shows how to compile HIP codes using the Cray compiler wrappers and hipcc compiler driver.",4.234934587321306
"How can I compile the `square.hipref.cpp` file for the AMD GPU architecture using the `hipcc` compiler?
","Make sure the craype-accel-amd-gfx90a module is loaded when compiling HIP with the Cray compiler wrappers.

| Compiler | Compile/Link Flags, Header Files, and Libraries | | --- | --- | |  |  | | hipcc |  |

hipcc requires the ROCm Toolclain, See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#exposing-the-rocm-toolchain-to-your-programming-environment",4.17934944649358
"How do I ensure that my Score-P instrumentation is accurate?
","To instrument your code, you need to compile the code using the Score-P instrumentation command (scorep), which is added as a prefix to your compile statement. In most cases the Score-P instrumentor is able to automatically detect the programming paradigm from the set of compile and link options given to the compiler. Some cases will, however, require some additional link options within the compile statement e.g. CUDA instrumentation.

Below are some basic examples of the different instrumentation scenarios:",4.299983312426833
"How do I ensure that my Score-P instrumentation is accurate?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations. It works in combination with Periscope, Scalasca, Vampir, and Tau.

Steps in a typical Score-P workflow to run on https://docs.olcf.ornl.gov/systems/Scorep.html#summit-user-guide:",4.174600621738759
"How do I ensure that my Score-P instrumentation is accurate?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi-processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations.

For detailed information about using Score-P on Summit and the builds available, please see the Score-P Software Page.",4.165254473959787
"Can I use Score-P with other performance analysis tools?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi-processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations.

For detailed information about using Score-P on Summit and the builds available, please see the Score-P Software Page.",4.418793546561935
"Can I use Score-P with other performance analysis tools?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations. It works in combination with Periscope, Scalasca, Vampir, and Tau.

Steps in a typical Score-P workflow to run on https://docs.olcf.ornl.gov/systems/Scorep.html#summit-user-guide:",4.413241677294966
"Can I use Score-P with other performance analysis tools?
","To instrument your code, you need to compile the code using the Score-P instrumentation command (scorep), which is added as a prefix to your compile statement. In most cases the Score-P instrumentor is able to automatically detect the programming paradigm from the set of compile and link options given to the compiler. Some cases will, however, require some additional link options within the compile statement e.g. CUDA instrumentation.

Below are some basic examples of the different instrumentation scenarios:",4.291443815685045
"What does the warning message ""Requested total thread count and/or thread affinity may result in oversubscription of available CPU resources!"" mean in Crusher?
","WARNING: Requested total thread count and/or thread affinity may result in
oversubscription of available CPU resources!  Performance may be degraded.
Explicitly set OMP_WAIT_POLICY=PASSIVE or ACTIVE to suppress this message.
Set CRAY_OMP_CHECK_AFFINITY=TRUE to print detailed thread-affinity messages.
WARNING: Requested total thread count and/or thread affinity may result in
oversubscription of available CPU resources!  Performance may be degraded.
Explicitly set OMP_WAIT_POLICY=PASSIVE or ACTIVE to suppress this message.",4.35454806994015
"What does the warning message ""Requested total thread count and/or thread affinity may result in oversubscription of available CPU resources!"" mean in Crusher?
","WARNING: Requested total thread count and/or thread affinity may result in
oversubscription of available CPU resources!  Performance may be degraded.
Explicitly set OMP_WAIT_POLICY=PASSIVE or ACTIVE to suppress this message.
Set CRAY_OMP_CHECK_AFFINITY=TRUE to print detailed thread-affinity messages.
WARNING: Requested total thread count and/or thread affinity may result in
oversubscription of available CPU resources!  Performance may be degraded.
Explicitly set OMP_WAIT_POLICY=PASSIVE or ACTIVE to suppress this message.",4.35454806994015
"What does the warning message ""Requested total thread count and/or thread affinity may result in oversubscription of available CPU resources!"" mean in Crusher?
","WARNING: Requested total thread count and/or thread affinity may result in
oversubscription of available CPU resources!  Performance may be degraded.
Explicitly set OMP_WAIT_POLICY=PASSIVE or ACTIVE to suppress this message.
Set CRAY_OMP_CHECK_AFFINITY=TRUE to print detailed thread-affinity messages.
WARNING: Requested total thread count and/or thread affinity may result in
oversubscription of available CPU resources!  Performance may be degraded.
Explicitly set OMP_WAIT_POLICY=PASSIVE or ACTIVE to suppress this message.",4.35454806994015
"How can I see the environment changes made by a module in Summit?
","below), you can choose between the rendering options by loading its corresponding module on the system you're connected to. For example, to see these modules on Summit:",4.323298751238252
"How can I see the environment changes made by a module in Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.149363110091542
"How can I see the environment changes made by a module in Summit?
",For Summit:,4.111465806122766
"Are there any workarounds for the TMPDIR environment variable issue until the bug is fixed?
","Setting the TMPDIR environment variable causes jobs to fail with JSM (jsrun) errors and can also cause jobs to bounce back and forth between eligible and running states until a retry limit has been reached and the job is placed in a blocked state (NOTE: This ""bouncing"" of job state can be caused for multiple reasons. Please see the known issue Jobs suspended due to retry limit / Queued job flip-flops between queued/running states if you are not setting TMPDIR). A bug has been filed with IBM to address this issue.",4.279536018475104
"Are there any workarounds for the TMPDIR environment variable issue until the bug is fixed?
","When TMPDIR is set before submitting a job (i.e., in the shell/environment where a job is submitted from), the job will bounce back and forth between a running and eligible state until its retry limit has been reached and the job will end up in a blocked state. This is true for both interactive jobs and jobs submitted with a batch script, but interactive jobs will hang without dropping you into your interactive shell. In both cases, JSM log files (e.g., jsm-lsf-wait.username.1004985.log) will be created in the location set for TMPDIR containing the same error message as shown above.",4.133914921972909
"Are there any workarounds for the TMPDIR environment variable issue until the bug is fixed?
","When TMPDIR is set within a running job (i.e., in an interactive session or within a batch script), any attempt to call jsrun will lead to a job failure with the following error message:

Error: Remote JSM server is not responding on host batch503-25-2020 15:29:45:920 90012 main: Error initializing RM connection. Exiting.",4.049352357972969
"What is XNACK and how does it relate to the MI250X GPU operating mode?
","XNACK (pronounced X-knack) refers to the AMD GPU's ability to retry memory accesses that fail due to a page fault. The XNACK mode of an MI250X can be changed by setting the environment variable HSA_XNACK before starting a process that uses the GPU. Valid values are 0 (disabled) and 1 (enabled), and all processes connected to a GPU must use the same XNACK setting. The default MI250X on Frontier is HSA_XNACK=0.",4.532974654296117
"What is XNACK and how does it relate to the MI250X GPU operating mode?
","XNACK (pronounced X-knack) refers to the AMD GPU's ability to retry memory accesses that fail due to a page fault. The XNACK mode of an MI250X can be changed by setting the environment variable HSA_XNACK before starting a process that uses the GPU. Valid values are 0 (disabled) and 1 (enabled), and all processes connected to a GPU must use the same XNACK setting. The default MI250X on Crusher is HSA_XNACK=0.",4.529913818161332
"What is XNACK and how does it relate to the MI250X GPU operating mode?
","The accessibility of memory from GPU kernels and whether pages may migrate depends three factors: how the memory was allocated; the XNACK operating mode of the GPU; whether the kernel was compiled to support page migration. The latter two factors are intrinsically linked, as the MI250X GPU operating mode restricts the types of kernels which may run.",4.406790892093186
"What is the format of the file exported by pprof?
","If you do not declare the TAU_METRICS variable, then TIME is used by default, and the profiling files are not in a directory. When the execution ends there will be one file per process called profile.X.Y.Z. In this case there is just one file, called profile.0.0.0

We can export a text file with some information through the pprof tool or visualize it by using paraprof.

If an application has no MPI at all, use the argument --smpiargs=""off"" for jsrun. Otherwise, TAU will fail as MPI is active by default.

$ pprof profile.0.0.0
Reading Profile files in profile.*",4.209818564526169
"What is the format of the file exported by pprof?
","The output files generated when the profile measurement runs are successful will be placed in a folder uniquely named:

$ scorep-yyyymmdd_hhmm_<Unique ID created>

A file will be placed within the above mentioned folder with the name profile.cubex. This type of file can be analyzed using a tool called Cube developed by Scalasca.

For a more detailed description of profiling measurements with Score-P, please visit the ScorepP_Profiling homepage.

To run a tracing measurement, we will need to enable this through the environment variable SCOREP_ENABLE_TRACING:",4.060398519417062
"What is the format of the file exported by pprof?
","As with Nsight Systems, there is a graphical user interface you can load a report file into (The GUI is only available for Windows, x86_64 Linux and Mac). Use the -o flag to create a file (the added report extension will be .nsight-cuprof-report), copy it to your local system, and use the File > Open File menu item. If you are using multiple MPI ranks, make sure you name each one independently. Nsight Compute does not yet support the %q syntax (this will come in a future release), so your job script will have to do the naming manually; for example, you can create a simple shell script:",4.041538429920203
"What is the purpose of the ""jsrun"" command?
","This section describes tools that users might find helpful to better understand the jsrun job launcher.

hello_jsrun is a ""Hello World""-type program that users can run on Summit nodes to better understand how MPI ranks and OpenMP threads are mapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A screencast showing how to use Hello_jsrun is also available: https://vimeo.com/261038849

Job Step Viewer provides a graphical view of an application's runtime layout on Summit. It allows users to preview and quickly iterate with multiple jsrun options to understand and optimize job launch.",4.407153621350611
"What is the purpose of the ""jsrun"" command?
","While jsrun performs similar job launching functions as aprun and mpirun, its syntax is very different. A large reason for syntax differences is the introduction of the resource set concept. Through resource sets, jsrun can control how a node appears to each job. Users can, through jsrun command line flags, control which resources on a node are visible to a job. Resource sets also allow the ability to run multiple jsruns simultaneously within a node. Under the covers, a resource set is a cgroup.

At a high level, a resource set allows users to configure what a node look like to their job.",4.339354004356772
"What is the purpose of the ""jsrun"" command?
",Below is a comparison table between srun and jsrun.,4.337998186274088
"Can you explain the expected behavior of jsrun for Summit when the -l flag is given a lowercase value?
","Expected behavior:

When capitalized, jsrun should not compromise on the resource layout, and will wait to begin the job step until the ideal resources are available. When given a lowercase value, jsrun will not wait, but initiate the job step with the most ideal layout as is available at the time. This also means that when there's no resource contention, such as running a single job step at a time, capitalization should not matter, as they should both yield the same resources.

Actual behavior:",4.246372169462572
"Can you explain the expected behavior of jsrun for Summit when the -l flag is given a lowercase value?
","Actual behavior:

Capitalizing the latency priority value may allocate incorrect resources, or even cause the job step to fail entirely.

Recommendation:

It is currently recommended to only use the lowercase values to (-l / --latency_priority). The system default is: gpu-cpu,cpu-mem,cpu-cpu. Since this ordering is used implicitly when the -l flag is omitted, this issue only impacts submissions which explicitly include a latency priority in the jsrun command.

Users have reported errors when using complex datatypes with MPI Collectives and GPUDirect:",4.175901582049167
"Can you explain the expected behavior of jsrun for Summit when the -l flag is given a lowercase value?
","This section describes tools that users might find helpful to better understand the jsrun job launcher.

hello_jsrun is a ""Hello World""-type program that users can run on Summit nodes to better understand how MPI ranks and OpenMP threads are mapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A screencast showing how to use Hello_jsrun is also available: https://vimeo.com/261038849

Job Step Viewer provides a graphical view of an application's runtime layout on Summit. It allows users to preview and quickly iterate with multiple jsrun options to understand and optimize job launch.",4.164237631620161
"How can I check the dependencies of my executable in Frontier?
",One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Frontier from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Frontier. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.,4.123023826696196
"How can I check the dependencies of my executable in Frontier?
",One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Frontier from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Frontier. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.,4.123023826696196
"How can I check the dependencies of my executable in Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.0728804531997564
"How can I view the status of a Slate route?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.167595600711732
"How can I view the status of a Slate route?
","By default, a route will only expose your https://docs.olcf.ornl.gov/systems/route.html#slate_services to NCCS networks. If you need your service exposed to the world outside ORNL, you will first need to get your project approved for external routes. To do this, submit a systems ticket. In the description, give us your project name and a brief reasoning for why exposing externally is needed.

We will let you know once your project is able to set up external routes.",4.147760183316012
"How can I view the status of a Slate route?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.117172007918185
"Can I clone a persistent volume from a VolumeSnapshot?
","Cloning a persistent volume is just as easy as implementing a snapshot. First, find a Persistent Volume Claim in the same namespace that you would like to clone for your new persistent volume. Then it's as simple as adding the trident.netapp.io/cloneFromPVC annotation with a value of the name of the Persistent Volume Claim you would like to clone.

In the below example, we clone a persistent volume named source-clone-pvc into a new volume called destination-clone-pvc",4.463466895552099
"Can I clone a persistent volume from a VolumeSnapshot?
","Now, if the data in the PersistentVolume becomes corrupted or lost in some way, we can create a new PersistentVolume that is identical to the original PersistentVolume at the time the VolumeSnapshot was created.

To do this, create a PersistentVolumeClaim that contains a dataSource field in the Pod's spec. An example follows:",4.389662296414693
"Can I clone a persistent volume from a VolumeSnapshot?
","apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: snapshot-pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Gi

We can now create a VolumeSnapshot, which will capture the data in the PersistentVolume that was provisioned by the named PersistentVolumeClaim at the time the VolumeSnapshot is created, to backup the data.

apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshot
metadata:
  # Name of VolumeSnapshot
  name: pvc-snap
spec:
  source:
    # Name of persistentVolumeClaim to snapshot
    persistentVolumeClaimName: snapshot-pvc",4.278745531040072
"How many MI250X OAMs are available in a single Frontier node?
","Each Frontier compute node contains 4 AMD MI250X. The AMD MI250X has a peak performance of 53 TFLOPS in double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 26.5 TFLOPS (double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in each direction simultaneously).

To connect to Frontier, ssh to frontier.olcf.ornl.gov. For example:

$ ssh <username>@frontier.olcf.ornl.gov",4.3728473017880525
"How many MI250X OAMs are available in a single Frontier node?
","The 2 GCDs on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The OLCF Director's Discretionary (DD) program allocates approximately 10% of the available Frontier hours in a calendar year. Frontier is allocated in *node* hours, and a typical DD project is awarded between 15,000 - 20,000 *node* hours. For more information about Frontier, please visit the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#frontier-user-guide.",4.305535440652895
"How many MI250X OAMs are available in a single Frontier node?
",The AMD Instinct MI200 is built on advanced packaging technologies enabling two Graphic Compute Dies (GCDs) to be integrated into a single package in the Open Compute Project (OCP) Accelerator Module (OAM) in the MI250 and MI250X products. Each GCD is build on the AMD CDNA 2 architecture. A single Frontier node contains 4 MI250X OAMs for the total of 8 GCDs.,4.27929249754821
"How can I access data stored on the HPSS from Frontier?
",Please note that the HPSS is not mounted directly onto Frontier nodes. There are two main methods for accessing and moving data to/from the HPSS. The first is to use the command line utilities hsi and htar. The second is to use the Globus data transfer service. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-hpss for more information on both of these methods.,4.513457016343756
"How can I access data stored on the HPSS from Frontier?
","Frontier connects to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#dtn-user-guide to move data to/from HPSS.

Frontier is running Cray OS 2.4 based on SUSE Linux Enterprise Server (SLES) version 15.4.",4.3774195182387
"How can I access data stored on the HPSS from Frontier?
","For more detailed information about center-wide file systems and data archiving available on Frontier, please refer to the pages on https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers. The subsections below give a quick overview of NFS, Lustre,and HPSS storage spaces as well as the on node NVMe ""Burst Buffers"" (SSDs).",4.24853279645273
"How can I load a collection in Andes?
",For Andes:,4.235093580430092
"How can I load a collection in Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.12541814454351
"How can I load a collection in Andes?
","Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-user-guide.

For a full list of software available at the OLCF, please see the Software section (coming soon).",4.09777181654121
"What should I do if I need to use a different cluster than the one assigned to my allocation?
","Before you can do anything you need a Project to do your things in. Fortunately, when you get an allocation on one of our clusters a Project is automatically created for you with the same name as the allocation. By using our own distinct Project we are ensuring that we will not interfere with anyone else's work.

NOTE: Everywhere that you see <cluster> replace that with the cluster that you will be running on (marble or onyx).",4.322874145667929
"What should I do if I need to use a different cluster than the one assigned to my allocation?
","Users are not allowed to access cluster compute nodes directly from a login node. Instead, users must use an interactive batch job to allocate and gain access to compute resources. This is done by using the Slurm salloc command. Other Slurm options are passed to salloc on the command line as well:

$ salloc -A abc123 -p gpu -N 4 -t 1:00:00

This request will:

| salloc | Start an interactive session | | --- | --- | | -A | Charge to the abc123 project | | -p gpu | Run in the gpu partition | | -N 4 | request (4) nodes... | | -t 1:00:00 | ...for (1) hour |",4.059354405090493
"What should I do if I need to use a different cluster than the one assigned to my allocation?
","Users are not allowed to access cluster compute nodes directly from a login node. Instead, users must use an interactive batch job to allocate and gain access to compute resources. This is done by using the Slurm salloc command. Other Slurm options are passed to salloc on the command line as well:

$ salloc -A abc123 -p gpu -N 4 -t 1:00:00

This request will:

| salloc | Start an interactive session | | --- | --- | | -A | Charge to the abc123 project | | -p gpu | Run in the gpu partition | | -N 4 | request (4) nodes... | | -t 1:00:00 | ...for (1) hour |",4.059354405090493
"What compilers are available on Summit?
","The following compilers are available on Summit:

XL: IBM XL Compilers (loaded by default)

LLVM: LLVM compiler infrastructure

PGI: Portland Group compiler suite

NVHPC: Nvidia HPC SDK compiler suite

GNU: GNU Compiler Collection

NVCC: CUDA C compiler

PGI was bought out by Nvidia and have rebranded their compilers, incorporating them into the NVHPC compiler suite. There will be no more new releases of the PGI compilers.",4.562569838652967
"What compilers are available on Summit?
","The E4S software list is installed along side the existing software on Summit and can be access via lmod modulefiles.

To access the installed software, load the desired compiler via:

module load < compiler/version >
.. ie ..
module load gcc/9.1.0
.. or ..
module load gcc/7.5.0

Then use module avail to see the installed list of packages.

List of installed packages on Summit for E4S release 21.08:",4.315925429636317
"What compilers are available on Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.286988239875909
"Is there a way to increase the resource limits for my user on Summit?
","Because the login nodes are resources shared by all Summit users, we utilize cgroups to help better ensure resource availability for all users of the shared nodes. By default each user is limited to 16 hardware-threads, 16GB of memory, and 1 GPU.  Please note that limits are set per user and not individual login sessions. All user processes on a node are contained within a single cgroup and share the cgroup's limits.",4.253178925814267
"Is there a way to increase the resource limits for my user on Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.1889381853535
"Is there a way to increase the resource limits for my user on Summit?
","Summit is connected to an IBM Spectrum Scale™ filesystem providing 250PB of storage capacity with a peak write speed of 2.5 TB/s. Summit also has access to the center-wide NFS-based filesystem (which provides user and project home areas) and has access to the center’s High Performance Storage System (HPSS) for user and project archival storage.

Summit is running Red Hat Enterprise Linux (RHEL) version 8.2.",4.165832751855133
"How can I access the MinIO server?
","You can also go to it by logging into the Marble GUI. Once logged in, go to Networking->Routes and click the URL in the ""Location"" column of your MinIO applications row.

You will be greeted with the NCCS SSO page. Continue through that with your normal NCCS login credentials.

After the NCCS login, you will be greeted with MinIO's login page. Here you will enter the access-key and secret-key you created with the secret-token.yaml file.

At this point, you should be inside the MinIO Browser.",4.392749564035676
"How can I access the MinIO server?
","MinIO is a high-performance software-defined object storage suite that enables the ability to easily deploy cloud-native data infrastructure for various data workloads. In this example we are deploying a simple, standalone implementation of MinIO on our cloud-native platform, Slate (https://docs.olcf.ornl.gov/systems/minio.html#slate_overview).

This service will only be accessible from inside of ORNL's network.

If your project requires an externally facing service available to the Internet, please contact User Assistance by submitting a help ticket. There is a process to get such approval.",4.302510531747393
"How can I access the MinIO server?
","Depending on you how configured your deployment, this could be your NFS or GPFS project space or an isolated volume dedicated/isolated to this MinIO server.

Within the GUI you can create buckets and upload/download data. If you are running this on NFS or GPFS the bucket will map to a directory.",4.249832385049382
"How do I create a workflow in libEnsemble?
","libEnsemble is a complete https://docs.olcf.ornl.gov/systems/libensemble.html#Python<py-index> toolkit for steering dynamic ensembles of calculations. Workflows are highly portable and detect/integrate heterogeneous resources with little effort. For instance, libEnsemble can automatically detect, assign, and reassign allocated processors and GPUs to ensemble members.",4.300201961218939
"How do I create a workflow in libEnsemble?
","Users select or supply generator and simulator functions to express their ensembles; the generator typically steers the ensemble based on prior simulator results. Such functions can also launch and monitor external executables at any scale.

Begin by loading the python module:

$ module load cray-python

libEnsemble is available on PyPI, conda-forge, the xSDK, and E4S. Most users install libEnsemble via pip:

$ pip install libensemble",4.203514034733172
"How do I create a workflow in libEnsemble?
","See this video for an example workflow on https://docs.olcf.ornl.gov/systems/libensemble.html#Spock<spock-quick-start-guide>. The channel will soon publish a Frontier-specific guide.

import numpy as np
from tutorial_gen import gen_random_sample
from tutorial_sim import sim_find_sine

from libensemble.libE import libE
from libensemble.tools import add_unique_random_streams

libE_specs = {""nworkers"": 4, ""comms"": ""local""}",4.176179465703803
"What types of quantum systems are available through IBM Quantum Services?
","IBM Quantum Services provides access to more than 20 currently available quantum systems (known as backends).  IBM's quantum processors are made up of superconducting transmon qubits, and users can utilize these systems via the universal, gate-based, circuit model of quantum computation.  Additionally, users have access to 5 different types of simulators, simulating from 32 up to 5000 qubits to represent different aspects of the quantum backends.",4.557783337139601
"What types of quantum systems are available through IBM Quantum Services?
","Simulator backends currently available: https://quantum-computing.ibm.com/services?services=simulators

IBM's Documentation

IBM Quantum Insider",4.4220391315091865
"What types of quantum systems are available through IBM Quantum Services?
",and view the results of your past jobs. More information about using these IBM quantum resources can be found on the IBM's Documentation or our https://docs.olcf.ornl.gov/quantum/quantum_systems/ibm_quantum.html.,4.391052068972049
"How can I optimize my job submission strategy to maximize resource utilization on Frontier?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.2803948287304285
"How can I optimize my job submission strategy to maximize resource utilization on Frontier?
","Users may have only 100 jobs queued in the batch queue at any time (this includes jobs in all states). Additional jobs will be rejected at submit time.

The debug quality of service (QOS) class can be used to access Frontier's compute resources for short non-production debug tasks. The QOS provides a higher priority compare to jobs of the same job size bin in production queues. Production work and job chaining using the debug QOS is prohibited. Each user is limited to one job in any state at any one point. Attempts to submit multiple jobs to this QOS will be rejected upon job submission.",4.2373173797946775
"How can I optimize my job submission strategy to maximize resource utilization on Frontier?
","As a DOE Leadership Computing Facility, OLCF has a mandate that a large portion of Frontier's usage come from large, leadership-class (a.k.a. capability) jobs. To ensure that OLCF complies with this directive, we strongly encourage users to run jobs on Frontier that are as large as their code will allow. To that end, OLCF implements queue policies that enable large jobs to run in a timely fashion.

The OLCF implements queue policies that encourage the submission and timely execution of large, leadership-class jobs on Frontier.",4.155843288692775
"How can I make use of the GPU with Singularity on Summit?
","Singularity is a container framework from Sylabs. On Summit, we will use Singularity solely as the runtime. We will convert the tar files of the container images Podman creates into sif files, store those sif files on GPFS, and run them with Jsrun. Singularity also allows building images but ordinary users cannot utilize that on Summit due to additional permissions not allowed for regular users.",4.331286689018621
"How can I make use of the GPU with Singularity on Summit?
","If you want to do GPU computing on Summit with R, we would love to collaborate with you (see contact details at the bottom of this document).

For more information about using R and pbdR effectively in an HPC environment such as Summit, please see the R and pbdR articles. These are long-form articles that introduce HPC concepts like MPI programming in much more detail than we do here.

Also, if you want to use R and/or pbdR on Summit, please feel free to contact us directly:

Mike Matheson - mathesonma AT ornl DOT gov

George Ostrouchov - ostrouchovg AT ornl DOT gov",4.306107214808341
"How can I make use of the GPU with Singularity on Summit?
","The --nv flag is needed to tell Singularity to make use of the GPU.

You can run containers with CUDA-aware MPI as well. CUDA-aware MPI allows transferring GPU data with MPI without needing to copy the data over to CPU memory first. Read more https://docs.olcf.ornl.gov/systems/containers_on_summit.html#CUDA-Aware MPI.

Let's build and run a container that will demonstrate CUDA-aware MPI.

Create a new directory cudaawarempiexample.

Run the below wget commands to obtain the example code and Makefile from the OLCF tutorial example page.",4.261591761234228
"What is the benefit of using managed or unified memory on Crusher with XNACK enabled?
","Most applications that use ""managed"" or ""unified"" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Crusher. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.",4.630926602704306
"What is the benefit of using managed or unified memory on Crusher with XNACK enabled?
","Most applications that use ""managed"" or ""unified"" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Frontier. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.",4.487617360177295
"What is the benefit of using managed or unified memory on Crusher with XNACK enabled?
","The AMD MI250X and operating system on Crusher supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Crusher. In the following section we'll discuss how the heterogenous memory space on a Crusher node is surfaced within your application.",4.31889593564101
"What is the peak sequential performance of the NVMe devices on Crusher compute nodes?
","Each Crusher compute node has [2x] 1.92 TB NVMe devices (SSDs) with a peak sequential performance of 5500 MB/s (read) and 2000 MB/s (write). To use the NVMe, users must request access during job allocation using the -C nvme option to sbatch, salloc, or srun. Once the devices have been granted to a job, users can access them at /mnt/bb/<userid>. Users are responsible for moving data to/from the NVMe before/after their jobs. Here is a simple example script:",4.490294977762861
"What is the peak sequential performance of the NVMe devices on Crusher compute nodes?
","Each Spock compute node has [2x] 3.2 TB NVMe devices (SSDs) with a peak sequential performance of 6900 MB/s (read) and 4200 MB/s (write). To use the NVMe, users must request access during job allocation using the -C nvme option to sbatch, salloc, or srun. Once the devices have been granted to a job, users can access them at /mnt/bb/<userid>. Users are responsible for moving data to/from the NVMe before/after their jobs. Here is a simple example script:

#!/bin/bash
#SBATCH -A <projid>
#SBATCH -J nvme_test
#SBATCH -o %x-%j.out
#SBATCH -t 00:05:00
#SBATCH -p batch
#SBATCH -N 1
#SBATCH -C nvme",4.375831004712868
"What is the peak sequential performance of the NVMe devices on Crusher compute nodes?
","Each compute node on Frontier has [2x] 1.92TB Non-Volatile Memory (NVMe) storage devices (SSDs), colloquially known as a ""Burst Buffer"" with a peak sequential performance of 5500 MB/s (read) and 2000 MB/s (write). The purpose of the Burst Buffer system is to bring improved I/O performance to appropriate workloads. Users are not required to use the NVMes. Data can also be written directly to the parallel filesystem.



The NVMes on Frontier are local to each node.",4.332881368602204
"How can I remove a saved collection of modules in Summit?
","below), you can choose between the rendering options by loading its corresponding module on the system you're connected to. For example, to see these modules on Summit:",4.205170932347272
"How can I remove a saved collection of modules in Summit?
","Defining custom module collections

Lmod supports caching commonly used collections of environment modules on a per-user basis in $home/.lmod.d. To create a collection called ""NAME"" from the currently loaded modules, simply call module save NAME. omitting ""NAME"" will set the user’s default collection. Saved collections can be recalled and examined with the commands summarized in the following table.",4.143973070280737
"How can I remove a saved collection of modules in Summit?
","Lmod supports caching commonly used collections of environment modules on a per-user basis in $home/.lmod.d. To create a collection called ""NAME"" from the currently loaded modules, simply call module save NAME. omitting ""NAME"" will set the user’s default collection. Saved collections can be recalled and examined with the commands summarized in the following table.",4.127116319753867
"Can I create a Singularity sif file from the container image?
","You can now create a Singularity sif file with singularity build --disable-cache --docker-login simple.sif docker://docker.io/<username>/simple.

This will ask you to enter your Docker username and password again for Singularity to download the image from Dockerhub and convert it to a sif file.",4.483531203777443
"Can I create a Singularity sif file from the container image?
","Singularity is a container framework from Sylabs. On Summit, we will use Singularity solely as the runtime. We will convert the tar files of the container images Podman creates into sif files, store those sif files on GPFS, and run them with Jsrun. Singularity also allows building images but ordinary users cannot utilize that on Summit due to additional permissions not allowed for regular users.",4.359053444349106
"Can I create a Singularity sif file from the container image?
","Users can build container images with Podman, convert it to a SIF file, and run the container using the Singularity runtime. This page is intended for users with some familiarity with building and running containers.

Users will make use of two applications on Summit - Podman and Singularity - in their container build and run workflow. Both are available without needing to load any modules.",4.345709008584945
"What is the source of the Volume Snapshot?
","Now, if the data in the PersistentVolume becomes corrupted or lost in some way, we can create a new PersistentVolume that is identical to the original PersistentVolume at the time the VolumeSnapshot was created.

To do this, create a PersistentVolumeClaim that contains a dataSource field in the Pod's spec. An example follows:",4.193543126268561
"What is the source of the Volume Snapshot?
","apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshot
metadata:
  # Snapshot name
  name: pvc1-snap
spec:
  source:
    # Persistent Volume to snapshot
    persistentVolumeClaimName: test-pod-pvc",4.166034805353124
"What is the source of the Volume Snapshot?
","apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: snapshot-pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Gi

We can now create a VolumeSnapshot, which will capture the data in the PersistentVolume that was provisioned by the named PersistentVolumeClaim at the time the VolumeSnapshot is created, to backup the data.

apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshot
metadata:
  # Name of VolumeSnapshot
  name: pvc-snap
spec:
  source:
    # Name of persistentVolumeClaim to snapshot
    persistentVolumeClaimName: snapshot-pvc",4.125935458254857
"Can EnTK be run from within a batch job?
","The Ensemble Toolkit (EnTK) is a Python library developed by the RADICAL Research Group at Rutgers University for developing and executing large-scale ensemble-based workflows. This tutorial shows how to get up and running with EnTK 1.13.0 on Summit specifically. For in-depth information about EnTK itself, please refer to its documentation.

Before using EnTK itself, you will need MongoDB and RabbitMQ services running on https://docs.olcf.ornl.gov/systems/entk.html#Slate<slate>. There are tutorials for MongoDB in this documentation, but the tutorial for RabbitMQ is forthcoming.",4.198764539929219
"Can EnTK be run from within a batch job?
","You will need to know the connection information for both MongoDB and RabbitMQ so that EnTK can be configured to connect to the services.

Then, to use EnTK on Summit, load the module as shown below:

$ module load workflows
$ module load entk/1.13.0

Run the following command to verify that EnTK is available:

$ radical-utils-version
1.13.0

To run EnTK on Summit, you will create two files and then execute two commands from a Summit login node. Currently, EnTK must be run from a Summit login node, rather than within a batch job.",4.157555010884007
"Can EnTK be run from within a batch job?
","Finally, run the demo program by executing the following commands from a Summit login node:

$ source setup.bash
$ python3 demo.py3

Congratulations! You should now see interactive output from EnTK while it launches and monitors your job on Summit.",4.148910083692559
"How do I unload the darshan-runtime module?
","You will need to unload the darshan-runtime module. In some instances you may need to unload the xalt and xl modules.  $ module unload darshan-runtime

Serial

..  C

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load scorep
        $ module load gcc
        $ scorep gcc -c test.c
        $ scorep gcc -o test test.o

..  C++

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load scorep
        $ module load gcc
        $ scorep g++ -c test.cpp main.cpp
        $ scorep g++ -o test test.o main.o

..  Fortran",4.51046410573441
"How do I unload the darshan-runtime module?
","On Tuesday, May 9, 2023, the darshan-runtime modulefile was added to DefApps and is now loaded by default on Frontier. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>. Unloading darshan-runtime is recommended for users profiling their applications with other profilers to prevent conflicts.



JIRA_CONTENT_HERE",4.29885220824757
"How do I unload the darshan-runtime module?
","<p style=""font-size:20px""><b>Frontier: Darshan Runtime 3.4.0 (May 10, 2023)</b></p>

The Darshan Runtime modulefile darshan-runtime/3.4.0 on Frontier is now loaded by default. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>.

Unloading darshan-runtime modulefile is recommended for users profiling their applications with other profilers to prevent conflicts.",4.286337261624195
"How do I know when a permanent fix for an issue in Thrust packed with CUDA 10.1 update 1 will be available on Summit?
","Using the hip-cuda/5.1.0 module on Summit requires CUDA v11.4.0 or later. However, only CUDA v11.0.3 and earlier currently support GPU-aware MPI, so GPU-aware MPI is currently not available when using HIP on Summit.

Any codes compiled with post 11.0.3 CUDA (cuda/11.0.3) will not work with MPS enabled (-alloc_flags ""gpumps"") on Summit. The code will hang indefinitely. CUDA v11.0.3 is the latest version that is officially supported by IBM's software stack installed on Summit. We are continuing to look into this issue.",4.2101388194879
"How do I know when a permanent fix for an issue in Thrust packed with CUDA 10.1 update 1 will be available on Summit?
","The following issues were resolved with the July 16, 2019 software upgrade:

CUDA 10 adds a new feature to profile CPU side OpenMP constructs (see https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openmp). This feature is enabled by default and has a bug which will cause it to overwrite the contents of LD_PRELOAD. SpectrumMPI requires a library (libpami_cuda_hook.so) to be preloaded in order to function. All MPI applications on Summit will break when run in nvprof with default settings. The workaround is to disable the new OpenMP profiling feature:",4.160673755066896
"How do I know when a permanent fix for an issue in Thrust packed with CUDA 10.1 update 1 will be available on Summit?
","Although there are newer CUDA modules on Summit, cuda/11.0.3 is the latest version that is officially supported by the version of IBM's software stack installed on Summit. When loading the newer CUDA modules, a message is printed to the screen stating that the module is for “testing purposes only”. These newer unsupported CUDA versions might work with some users’ applications, but importantly, they are known not to work with GPU-aware MPI.",4.152623856437011
"What is the difference between the ""Server"" and ""Route"" settings in the ArgoCD instance creation form?
","Next Click the ""Create ArgoCD"" button. You will be presented with a form view similar to:

Image of the form view for ArgoCD instance creation.

Starting with the form view of the process, make the following changes allowing for access to the ArgoCD web UI via a route:

Server -> Insecure -> true

Server -> Route -> Enabled -> true

Server -> Route -> Tls -> Termination -> edge

Image of the form view for ArgoCD instance creation with the route settings configured.",4.4023990511507884
"What is the difference between the ""Server"" and ""Route"" settings in the ArgoCD instance creation form?
","This enables access to the ArgoCD instance once deployed via the web browser more easily. In the above images, notice that the instance name is argocd. By default, the route name to the web UI will be <<instanceName>>-server-<<projectName>>.apps.<<clusterName>>.ccs.ornl.gov. If a different host name is desired to access the instance, enter the desired name in the Host parameter while maintaining the pattern new-name.apps.<cluster>.ccs.ornl.gov. For example,

Image of the form view with a custom host name set.",4.229397185648676
"What is the difference between the ""Server"" and ""Route"" settings in the ArgoCD instance creation form?
","NAME                                     HOST/PORT                                PATH   SERVICES        PORT   TERMINATION   WILDCARD
route.route.openshift.io/argocd-server   argocd-stf042.apps.marble.ccs.ornl.gov          argocd-server   http   edge          None

When one navigates to the route in a web browser, the ArgoCD login screen will be presented:

Image  of the ArgoCD login screen.

For ArgoCD authentication, the default user is admin with the password stored in the <<instanceName>>-cluster secret in the project. Following login, the instance is ready for configuration:",4.208044251466605
"What is the purpose of a Slate Helm Chart?
","The following items need to be established before deploying applications on Slate systems (Marble or Onyx OpenShift clusters).

Follow https://docs.olcf.ornl.gov/systems/helm_prerequisite.html#slate_getting_started if you do not already have a project/namespace established on Onyx or Marble.

It is recommended to use Helm version 3.

Helm enables application deployment via Helm Charts. The high level Helm Architecture docs are also a great reference for understanding Helm.

Like oc, Helm is a single binary executable.

This can be installed on macOS with Homebrew :

$ brew install helm",4.218327201973813
"What is the purpose of a Slate Helm Chart?
","It is also possible to write your own charts for helm, if you have an application that can be deployed to many namespaces or that could benefit from templating objects. How to write charts is outside the scope of this documentation, but the upstream docs are a great place to start.",4.157331545395098
"What is the purpose of a Slate Helm Chart?
","It is assumed you have already gone through https://docs.olcf.ornl.gov/systems/minio.html#slate_getting_started and established the https://docs.olcf.ornl.gov/systems/minio.html#helm_prerequisite. Please do that before attempting to deploy anything on Slate's clusters.

This example uses Helm version 3 to deploy a MinIO standalone Helm chart on Slate's Marble Cluster. This is the cluster in OLCF's Moderate enclave, the same enclave as Summit.

To start, clone the slate helm examples repository , containing the MinIO standalone Helm chart, and navigate into the charts directory:",4.144424033696225
"What should a user do if they suspect that their account has been compromised?
","that any of the accounts used to access OLCF have been compromised. Users should inform the OLCF promptly of any changes in their contact information (E-mail, phone, affiliation, etc.) Updates should be sent to accounts@ccs.ornl.gov.",4.200143530941041
"What should a user do if they suspect that their account has been compromised?
","be disabled immediately. Users are not to attempt to receive unintended messages or access information by some unauthorized means, such as imitating another system, impersonating another user or other person, misuse of legal user credentials (usernames, tokens, etc.), or by causing some system component to function incorrectly. Users are prohibited from changing or circumventing access controls to allow themselves or others to perform actions outside their authorized privileges. Users must notify the OLCF immediately when they become aware that any of the accounts used to access OLCF have",4.144597821136121
"What should a user do if they suspect that their account has been compromised?
",It is the user’s responsibility to insure the appropriate level of backup and integrity checks on critical data and programs.,4.123049567064304
"How do I run a script that writes an HDF5 file in parallel?
","srun -n42 python3 hdf5_parallel.py

Provided there are no errors, you should see ""42 MPI ranks have finished writing!"" in your output file, and there should be a new file called ""output.h5"" in your directory. To see explicitly that the MPI tasks did their job, you can use the h5dump command to view the dataset named ""test"" in output.h5:

$ h5dump output.h5",4.3167223774186985
"How do I run a script that writes an HDF5 file in parallel?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.

This guide teaches you how to build a personal, parallel-enabled version of h5py and how to write an HDF5 file in parallel using mpi4py and h5py.

In this guide, you will:

Learn how to install mpi4py

Learn how to install parallel h5py

Test your build with Python scripts

OLCF Systems this guide applies to:

Summit

Andes

Frontier",4.291302749991461
"How do I run a script that writes an HDF5 file in parallel?
","Both HDF5 and h5py can be compiled with MPI support, which allows you to optimize your HDF5 I/O in parallel. MPI support in Python is accomplished through the mpi4py package, which provides complete Python bindings for MPI. Building h5py against mpi4py allows you to write to an HDF5 file using multiple parallel processes, which can be helpful for users handling large datasets in Python. h5Py is available after loading the default Python module on either Summit or Andes, but it has not been built with parallel support.",4.277519874664782
"What is the importance of following the correct order of steps in the Conda guide?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly. For those that just want a quick-reference list of common conda commands, see the https://docs.olcf.ornl.gov/systems/conda_basics.html#conda-quick section.",4.503293895351634
"What is the importance of following the correct order of steps in the Conda guide?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.

This guide teaches you how to build CuPy from source into a custom virtual environment. On Summit and Andes, this is done using conda, while on Frontier this is done using venv.

In this guide, you will:

Learn how to install CuPy

Learn the basics of CuPy

Compare speeds to NumPy

OLCF Systems this guide applies to:

Summit

Frontier

Andes",4.247313155724918
"What is the importance of following the correct order of steps in the Conda guide?
","Because there is no conda module on Frontier, this guide only applies if you installed a personal Miniconda first. See our https://docs.olcf.ornl.gov/software/python/miniconda.html for more details.

This guide has been shortened and adapted from a challenge in OLCF's Hands-On with Summit GitHub repository (Python: Conda Basics).",4.20344717925429
"What is the format for the subproject ID in OLCF?
","primary project users must be associated with a subproject(s). If you have any questions, or would like to request a subproject, please contact the OLCF Accounts Team at accounts@ccs.ornl.gov.",4.364120864568081
"What is the format for the subproject ID in OLCF?
","This sub-section describes the process of obtaining access to Ascent for an OLCF training event. Please follow the steps below to request access.

Once on the form, linked above, fill in the project ID in the ""Enter the Project ID of the project you wish to join"" field and click ""Next"".



After you enter the Project ID, use the sliders to select ""Yes"" for OLCF as the Project Organization and select ""Yes"" for Open as the Security Enclave.

<string>:5: (INFO/1) Enumerated list start value not ordinal-1: ""2"" (ordinal 2)",4.343658891308672
"What is the format for the subproject ID in OLCF?
","The ID for a subproject must follow the format of: <6 character primary project ID> + <1-4 character subproject suffix>. For example, project ABC123 could have subprojects ABC123XYZ6 and ABC123P3. The hours allocated for the primary project and subprojects must equal the awarded allocation. All hours can be allocated to subprojects, or some amount can be held as reserve as part of the primary project. It is recommended that all users be assigned to a subproject(s). If all of a primary project's awarded hours are allocated to its subprojects, all primary project users must be associated with a",4.307051011161621
"How can I get information about the available revisions of a Deployment in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.091601315074118
"How can I get information about the available revisions of a Deployment in Slate?
","Jenkins

OpenShift Pipelines

GitLab Runners

Deploying a GitLab Runner into a Slate project may be found in the https://docs.olcf.ornl.gov/systems/overview.html#slate_gitlab_runners document which leverages a localized Slate Helm Chart. The GitLab Pipelines documentation as well as GitLab CI/CD Examples provide more details on GitLab Runner capabilities and usage.

Jenkins

For Jenkins deployment, a Slate Helm Chart is planned to be released. Documentation concerning Jenkins Pipelines as well as Jenkins Pipeline Tutorials are available.

OpenShift Pipelines",4.083642602519133
"How can I get information about the available revisions of a Deployment in Slate?
","Versioned and Immutable: Desired state is stored in a way that enforces immutability, versioning and retains a complete version history.

Pulled Automatically: Software agents automatically pull the desired declarations from the source.

Continuously Reconciled: Software agents continuously observe actual system state and attempt to apply the desired state.

On Slate, Red Hat OpenShift GitOps is based on the open source ArgoCD project. For more information as well as how to install and use ArgoCD on Slate, see: https://docs.olcf.ornl.gov/systems/overview.html#slate_openshift_gitops.",4.069742143781848
"How do I know which compute nodes are available for my calculation?
","Compute nodes are the appropriate place for long-running, computationally-intensive tasks. When you start a batch job, your batch script (or interactive shell for batch-interactive jobs) runs on one of your allocated compute nodes.

Compute-intensive, memory-intensive, or other disruptive processes running on login nodes may be killed without warning.",4.266225298710676
"How do I know which compute nodes are available for my calculation?
","After running this command, the job will wait until enough compute nodes are available, just as any other batch job must. However, once the job starts, the user will be given an interactive prompt on the primary compute node within the allocated resource pool. Commands may then be executed directly (instead of through a batch script).",4.25283153089023
"How do I know which compute nodes are available for my calculation?
","After running this command, the job will wait until enough compute nodes are available, just as any other batch job must. However, once the job starts, the user will be given an interactive prompt on the primary compute node within the allocated resource pool. Commands may then be executed directly (instead of through a batch script).

Debugging",4.2314462307082765
"How do I check if my job has completed successfully on Summit?
",For Summit:,4.192472764501848
"How do I check if my job has completed successfully on Summit?
","Finally, run the demo program by executing the following commands from a Summit login node:

$ source setup.bash
$ python3 demo.py3

Congratulations! You should now see interactive output from EnTK while it launches and monitors your job on Summit.",4.123532992744513
"How do I check if my job has completed successfully on Summit?
","This method on Summit requires the user to be present until the job completes. For users who have long scripts or are unable to monitor the job, you can submit the above lines in a batch script. However, you will wait in the queue twice, so this is not recommended. Alternatively, one can use Andes.

For Andes/Frontier (Slurm Script):

Andes

.. code-block:: bash


   #!/bin/bash
   #SBATCH -A XXXYYY
   #SBATCH -J visit_test
   #SBATCH -N 1
   #SBATCH -p gpu
   #SBATCH -t 0:05:00

   cd $SLURM_SUBMIT_DIR
   date

   module load visit",4.118593746231191
"How can I request renewal of my membership on the current project?
",| Page | Content | | --- | --- | | Project Profile | General information about the current project | | Renew This Project | Form to request renewal of the current project | | Renew My Membership | Form to request renewal of your membership on the current project | | Current Users | A list of current projects members with contact and application role information | | Historical Users | A list of previous project members with contact information | | Current Allocations | A list of current project allocations | | Historical Allocations | A list of current project allocations | | Allocation Usage,4.295593572008287
"How can I request renewal of my membership on the current project?
","As a principal investigator of a project at the OLCF, you must approve (or reject) every potential user that requests membership on your project. myOLCF provides a mechanism for processing these requests via the ""For My Approval"" page.

Click the ""For My Approval"" link in the ""My Account"" top navigation dropdown:

for my approval link

You'll see a list of all pending requests that need your response:

<string>:6: (INFO/1) Enumerated list start value not ordinal-1: ""2"" (ordinal 2)

for my approval link",4.144416963560767
"How can I request renewal of my membership on the current project?
","The OLCF will then establish a QCUP project and notify the PI of its creation along with the 6-character OLCF QCUP Project ID and resources allocation details. At this time project participants may proceed with applying for their individual user accounts.

QCUP Projects have a finite duration; when starting, projects get however many months are left in that allocation period and then must be renewed for subsequent 6 month intervals. Projects can be renewed by filling out a renewal form (:download:`Accounts Renewal Form <Quantum-Renewal-Form.docx>`) and emailing it to accounts@ccs.ornl.gov.",4.1193541507292935
"What is the purpose of the patch in the given scenario?
","If more advanced patching is needed of a resources or field does not support the strategic merge process, use patchesJson6902 instead of patchesStrategicMerge as this provides for more operations and control over the merge process. Additionally, one may also be able to use a configuration transformation to modify the resulting resources. While not utilized the the helloWorld kustomize application, these are illustrated in some of the other examples it the same repository.

Review of the production environment is left as an exercise for the reader.",4.062237479445849
"What is the purpose of the patch in the given scenario?
","there are a few meta information blocks present: namePrefix, commonLabels, and commonAnnotations. Additionally, we see that there is a patch specified with the patchesStrategicMerge block where a patch file to be merged is specified:

$ cat overlays/staging/map.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: the-map
data:
  altGreeting: ""Have a pineapple!""
  enableRisky: ""true""",3.993413999106136
"What is the purpose of the patch in the given scenario?
","In this case, the patch will use a merge strategy to change the data entries for the specified apiVersion/kind/metadata.name object. Running kustomize build on the staging environment shows the result of the patch as well as the added meta:

$ kustomize build overlays/staging
apiVersion: v1
data:
  altGreeting: Have a pineapple!
  enableRisky: ""true""
kind: ConfigMap
metadata:
  annotations:
    note: Hello, I am staging!
  labels:
    app: hello
    org: acmeCorporation
    variant: staging
  name: staging-the-map
---
apiVersion: v1
kind: Service
...",3.980120691797833
"Can I use a variable in my job script to specify the account to charge my job time to?
","| Option | Use | Description | | --- | --- | --- | | -A | #SBATCH -A <account> | Causes the job time to be charged to <account>. The account string, e.g. pjt000 is typically composed of three letters followed by three digits and optionally followed by a subproject identifier. The utility showproj can be used to list your valid assigned project ID(s). This option is required by all jobs. | | -N | #SBATCH -N <value> | Number of compute nodes to allocate. Jobs cannot request partial nodes. | | -t | #SBATCH -t <time> | Maximum wall-clock time. <time> is in the format HH:MM:SS. | | -p | #SBATCH -p",4.142194376667993
"Can I use a variable in my job script to specify the account to charge my job time to?
","| Option | Use | Description | | --- | --- | --- | | -A | #SBATCH -A <account> | Causes the job time to be charged to <account>. The account string, e.g. pjt000 is typically composed of three letters followed by three digits and optionally followed by a subproject identifier. The utility showproj can be used to list your valid assigned project ID(s). This option is required by all jobs. | | -N | #SBATCH -N <value> | Number of compute nodes to allocate. Jobs cannot request partial nodes. | | -t | #SBATCH -t <time> | Maximum wall-clock time. <time> is in the format HH:MM:SS. | | -p | #SBATCH -p",4.142194376667993
"Can I use a variable in my job script to specify the account to charge my job time to?
",you can optionally specify a +time value for each jobid.,4.078553845224409
"How can you create a new namespace in Slate?
","Once your Slate Project Allocation Request is approved, you can create your own namespaces and move your allocation around those namespaces via the quota dashboard located at https://quota.marble.ccs.ornl.gov and https://quota.onyx.ccs.ornl.gov. The terms ""namespace"" and ""project"" may get used interchangeably when referring to your project's usable space within the requested resource boundaries (CPU/Memory/Storage).



The OC tool provides CLI access to the OpenShift cluster. It needs to be installed on your machine.",4.183597249230727
"How can you create a new namespace in Slate?
","This section will describe the project allocation process for Slate. This is applicable to both the Onyx and Marble OLCF OpenShift clusters.

In addition, we will go over the process to log into Onyx and Marble, and how namespaces work within the OpenShift context.

Please fill out the Slate Request Form in order to use Slate resources. This must be done in order to proceed.

If you have any question please contact User Assistance, via a Help Ticket Submission or by emailing help@olcf.ornl.gov.

Required information:

- Existing OLCF Project ID

- Project PI",4.118462865622336
"How can you create a new namespace in Slate?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",4.083173775022464
"How do I enable X11 forwarding for Vampir on Summit?
","VampirServer does not take advantage of GPU components

The following sections will cover the 3 different methods for using Vampir on Summit. For each method, you will need to enable X11 forwarding when logging in to Summit to allow for launching a GUI from Summit. To do so, you can use the ssh option -X as shown below

$ ssh -X <USERID>@summit.olcf.ornl.gov

Please visit this link if you need more information for logging onto Summit













Do not run Vampir on a login node for trace files > 1 GB! Please see the next 2 sections for running larger trace files.",4.565283369262662
"How do I enable X11 forwarding for Vampir on Summit?
","After connecting to Summit using X11 forwarding you will need to load the Vampir module and start the https://docs.olcf.ornl.gov/systems/Vampir.html#VampirServer. <vamps>

$ module load vampir

$ vampirserver start -- -P <projectID> -w <walltime> -q <queue>

#Example: vampirserver start -- -P 123456 -w 60 -q debug



Successful VampirServer startup message should appear in terminal window. You will need this information!",4.483630547277826
"How do I enable X11 forwarding for Vampir on Summit?
","<string>:17: (INFO/1) Duplicate explicit target name: ""summit"".

<string>:17: (INFO/1) Duplicate explicit target name: ""x11 forwarding"".

After logging onto Summit (with X11 forwarding), execute the series of commands below:

$ module load vampir

$ vampir &

Once the GUI pops up (might take a few seconds), you can load a file resident on the file system by selecting Local File for file selection.",4.370423761457933
"What are the differences between SPI workflows and standard non-SPI workflows when it comes to resource utilization?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.  This page lists the differences you may see when using OLCF resources to execute SPI workflows.  The page will also point to sections within the site where more information on standard non-SPI use can be found.",4.508367779516859
"What are the differences between SPI workflows and standard non-SPI workflows when it comes to resource utilization?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads. Because of this, many processes used within the SPI are very similar to those used for standard non-SPI. This page lists the differences you may see when using OLCF resources to execute SPI resources.",4.411820476144433
"What are the differences between SPI workflows and standard non-SPI workflows when it comes to resource utilization?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.

The SPI provides access to the OLCF's Summit resource for compute.  To safely separate SPI and non-SPI workflows, SPI workflows must use a separate login node named https://docs.olcf.ornl.gov/systems/citadel_user_guide.html#Citadel<spi-compute-citadel>.  Citadel provides a login node specifically for SPI workflows.",4.347311117085024
"How can I install a package and its dependencies manually using Spack?
","Alternatively, a user may install a package and its dependencies manually by:

$ spack install <my_app_dependencies@version%compiler>

## This may or may not add the spec to the spack.yaml depending on the Spack version being used.

For more information regarding Spack and its usage, please see the Spack documentation.

<string>:3: (INFO/1) Duplicate explicit target name: ""the spack 101 tutorial"".

For an extensive tutorial concerning Spack, go to the Spack 101 tutorial.

For more information concerning external packages, please see here.

Spack - package management tool",4.503281563256624
"How can I install a package and its dependencies manually using Spack?
","Spack - package management tool

Spack 101 tutorial - Spack tutorial",4.325231012343936
"How can I install a package and its dependencies manually using Spack?
","A dependency that is not already installed will be built via Spack once the environment is concretized and installed. These can be added to the spack.yaml by adding to the specs section.

specs:
- cmake@3.18.2                            ## example from above
- my_apps_dependency1@version%compiler    ## other explicitly defined specs
- my_apps_dependency2@version%compiler

When in the Spack environment, any packages that are added to the environment file can be installed via:

$ spack concretize -f  ## The -f flag here forces a reconcretization of the entire environment
$ spack install",4.274046637157911
"What is the difference between hipHostMalloc and hipMalloc on Frontier?
","+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+
| hipHostMalloc                               | CPU DDR4                  | Local read/write                           | Zero copy read/write over Infinity Fabric          |
+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+",4.240847999562391
"What is the difference between hipHostMalloc and hipMalloc on Frontier?
","+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+
| hipHostMalloc                               | CPU DDR4                  | Local read/write                           | Zero copy read/write over Infinity Fabric          |
+---------------------------------------------+---------------------------+--------------------------------------------+----------------------------------------------------+",4.240847999562391
"What is the difference between hipHostMalloc and hipMalloc on Frontier?
","| API | Flag | Results | | --- | --- | --- | | hipHostMalloc() | hipHostMallocDefault | Fine grained | | hipHostMalloc() | hipHostMallocNonCoherent | Coarse grained |

The following table shows the nature of the memory returned based on the flag passed as argument to hipExtMallocWithFlags().

| API | Flag | Result | | --- | --- | --- | | hipExtMallocWithFlags() | hipDeviceMallocDefault | Coarse grained | | hipExtMallocWithFlags() | hipDeviceMallocFinegrained | Fine grained |",4.184771140746655
"How can I use the second window to troubleshoot issues in my application in TAU?
","TAU is installed with Program Database Toolkit (PDT) on Summit. PDT is a framework for analyzing source code written in several programming languages. Moreover, Performance Application Programming Interface (PAPI) is supported. PAPI counters are used to assess the CPU performance. In this section, some approaches for profiling and tracing will be presented.

In most cases, we need to use wrappers to recompile the application:

For C: replace the compiler with the TAU wrapper tau_cc.sh

For C++: replace the compiler with the TAU wrapper tau_cxx.sh",4.106099810724067
"How can I use the second window to troubleshoot issues in my application in TAU?
","From the main window right click one label and select “Show User Event Statistics Window”. Then, we can see the data transfered to the devices



The CUDA Profiling Tools Interface (CUPTI) is used by profiling and tracing tools that target CUDA applications.



Matrix multiplication with MPI+OpenMP:

$ export TAU_METRICS=TIME,achieved_occupancy
$ jsrun -n 2 -r 2 -g 1  tau_exec -T mpi,pdt,papi,cupti,openmp -ompt -cupti  ./add

We choose to use tau_exec with MPI, PDT, PAPI, CUPTI, and OpenMP.

Output directories:",4.08686494435814
"How can I use the second window to troubleshoot issues in my application in TAU?
","TAU is a portable profiling and tracing toolkit that supports many programming languages. The instrumentation can be done by inserting in the source code using an automatic tool based on the Program Database Toolkit (PDT), on the compiler instrumentation, or manually using the instrumentation API.

Webpage: https://www.cs.uoregon.edu/research/tau/home.php",4.051730786219985
"What is the difference between scp and rsync in terms of performance?
","rsync -avz --dry-run mydir/ $USER@dtn.ccs.ornl.gov:/path/

See the manual pages for more information:

$ man scp
$ man rsync

Differences:

scp cannot continue if it is interrupted. rsync can.

rsync is optimized for performance.

By default, rsync checks if the transfer of the data was successful.

Standard file transfer protocol (FTP) and remote copy (RCP) should not be used to transfer files to the NCCS high-performance computing (HPC) systems due to security concerns.",4.245719200780544
"What is the difference between scp and rsync in terms of performance?
","Standard tools such as rsync and scp can also be used through the DTN, but may be slower and require more manual intervention than Globus

Copying data directly from Alpine (GPFS) to Orion (Lustre)

Globus is the suggested tool to transfer needed data from Alpine to Orion.

Globus should be used when transfer large amounts of data.

Standard tools such as rsync and cp can also be used. The DTN mounts both filesystems and should be used when transferring with rsync and cp tools. These methods should not be used to transfer large amounts of data.

Copying data to the HPSS archive system",4.070761762967228
"What is the difference between scp and rsync in terms of performance?
",Below is a comparison table between srun and jsrun.,4.062235170446836
"What is the storage class associated with the storage-1 Persistent Volume Claim?
",a desired size for a PersistentVolume. The cluster administrator or some automated mechanism will provision the storage on the backend and make it available to the cluster via the PersistentVolumeClaim.,4.36537105728717
"What is the storage class associated with the storage-1 Persistent Volume Claim?
","Once the Persistent Volume is created its allocated memory is available to be claimed by a project. This is done through a Persistent Volume Claim. PVC's are created using a YAML file in the same manner that PV's are created. An example of a PVC that claims the PV created in the previous section follows:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: storage-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

The claim is then placed using:

oc create -f storage-1.yaml",4.353058902072865
"What is the storage class associated with the storage-1 Persistent Volume Claim?
","apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  # The name of the claim
  name: test-pod-pvc
spec:
  # The type of storage being requested. This can be blank and it will be
  # set to the default value, which is netapp-nfs, but it is good practice
  # to explictly declare it.
  storageClassName: netapp-nfs
  # how the volume can be accessed. ReadWriteMany, or RWX as it is abbreviated,
  # means the volume can be mounted as Read Write by multiple nodes
  accessModes:
  - ReadWriteMany
  resources:
    # the amount of storage being requested
    requests:
      storage: 1Gi",4.352284701713598
"What is the maximum performance of the final production system for random I/O under FPP mode?
","I/O and 2.2 TB/s for random I/O under FPP mode, which means each process, writes its own file. Metada operations are improved with around to minimum 50,000 file access per sec and aggregated up to 2.6 million accesses of 32KB small files.",4.438625110381736
"What is the maximum performance of the final production system for random I/O under FPP mode?
","Summit mounts a POSIX-based IBM Spectrum Scale parallel filesystem called Alpine. Alpine's maximum capacity is 250 PB. It is consisted of 77 IBM Elastic Storage Server (ESS) GL4 nodes running IBM Spectrum Scale 5.x which are called Network Shared Disk (NSD) servers. Each IBM ESS GL4 node, is a scalable storage unit (SSU), constituted by two dual-socket IBM POWER9 storage servers, and a 4X EDR InfiniBand network for up to 100Gbit/sec of networking bandwidth.  The maximum performance of the final production system will be about 2.5 TB/s for sequential I/O and 2.2 TB/s for random I/O under FPP",4.0987790070650885
"What is the maximum performance of the final production system for random I/O under FPP mode?
","When a user occupies more than one compute node, then they are using more NVMes and the I/O can scale linearly. For example in the following plot you can observe the scalability of the IOR benchmark on 2048 compute nodes on Summit where the write performance achieves 4TB/s and the read 11.3 TB/s",4.0813956525826445
"What does the pat_build command do?
","pat_build -g hip,io,mpi -w -f <executable>

The pat_build command in the above examples generates an instrumented executable with +pat appended to the executable name (e.g., hello_jobstep+pat).

When run, the instrumented executable will trace HIP, I/O, MPI, and all user functions and generate a folder of results (e.g., hello_jobstep+pat+39545-2t).

To analyze these results, use the pat_report command, e.g.:

pat_report hello_jobstep+pat+39545-2t",4.288280264212135
"What does the pat_build command do?
","pat_build -g hip,io,mpi -w -f <executable>

The pat_build command in the above examples generates an instrumented executable with +pat appended to the executable name (e.g., hello_jobstep+pat).

When run, the instrumented executable will trace HIP, I/O, MPI, and all user functions and generate a folder of results (e.g., hello_jobstep+pat+39545-2t).

To analyze these results, use the pat_report command, e.g.:

pat_report hello_jobstep+pat+39545-2t",4.288280264212135
"What does the pat_build command do?
","Some libraries still resolved to paths outside of /mnt/bb, and the reason for that is that the executable may have several paths in RPATH.



The Performance Analysis Tools (PAT), formerly CrayPAT, are a suite of utilities that enable users to capture and analyze performance data generated during program execution. These tools provide an integrated infrastructure for measurement, analysis, and visualization of computation, communication, I/O, and memory utilization to help users optimize programs for faster execution and more efficient computing resource usage.",4.038107132123036
"What is the inclusive execution time of the subroutines?
","Explanation:

One process was running as it is a serial application, even MPI calls are executed from a single thread.

The total execution time is 70.733 seconds and only 9 msec are exclusive for the main routine. The rest are caused by subroutines.

The exclusive time is the time caused by the mentioned routine, and the inclusive is with the execution time from the subroutines.

The #Subrs is the number of the called subroutines.

There is also information about the parallel I/O if any exists, the bytes, and the bandwidth.",4.362966655528035
"What is the inclusive execution time of the subroutines?
","NODE 0;CONTEXT 0;THREAD 0:
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call
---------------------------------------------------------------------------------------
100.0        0.038     1:10.733           1           1   70733442 .TAU application
100.0            9     1:10.733           1        4654   70733404 int main(int, char **)",4.13171609720437
"What is the inclusive execution time of the subroutines?
","Execute the application as previously shown.

Now you can see the duration of all the loops



Select Options -> Select Metric… -> Exclusive… -> PAPI_TOT_INS/PAPI_TOT_CYC



The loops with less than 1.5 IPC have poor performance and could likely be improved.

Execute the MPI+OpenMP version

Now you can see the duration of parallelfor loops and decide when they should be improved or even removed.



When we instrument the MPI with OpenACC, we can see the following through paraprof

We can observe the duration of the OpenACC calls",4.130037506176727
"How does the OLCF ensure compliance with the data retention policy?
","By default, there is no lifetime retention for any data on OLCF resources. The OLCF specifies a limited post-deactivation timeframe during which user and project data will be retained. When the retention timeframe expires, the OLCF retains the right to delete data. If you have data retention needs outside of the default policy, please notify the OLCF.",4.399766556796197
"How does the OLCF ensure compliance with the data retention policy?
","The project data retention policy exists to reclaim storage space after a project ends. By default, the OLCF will retain data in project-centric storage areas only for a designated amount of time after the project end date. During this time, a project member can request a temporary user account extension for data access. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for details on purge and retention timeframes for each project-centric storage area.",4.397322638164788
"How does the OLCF ensure compliance with the data retention policy?
","The user data retention policy exists to reclaim storage space after a user account is deactivated, e.g., after the user’s involvement on all OLCF projects concludes. By default, the OLCF will retain data in user-centric storage areas only for a designated amount of time after the user’s account is deactivated. During this time, a user can request a temporary user account extension for data access. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for details on retention timeframes for each user-centric storage area.",4.388141601187176
"What is the compute capability of the device used in the example program?
",| Compute Capability | 7.0 | | --- | --- | | Peak double precision floating point performance | 7.8 TFLOP/s | | Peak single precision floating point performance | 15.7 TFLOP/s | | Single precision CUDA cores | 5120 | | Double precision CUDA cores | 2560 | | Tensor cores | 640 | | Clock frequency | 1530 MHz | | Memory Bandwidth | 900 GB/s | | Memory size (HBM2) | 16 or 32 GB | | L2 cache | 6 MB | | Shared memory size / SM | Configurable up to 96 KB | | Constant memory | 64 KB | | Register File Size | 256 KB (per SM) | | 32-bit Registers | 65536 (per SM) | | Max registers per thread | 255 | |,4.212039492494969
"What is the compute capability of the device used in the example program?
","There are a variety of programming models available to program GPUs (e.g. CUDA, OpenACC, OpenMP offloading, etc.) and you are welcome to use any of them at these events.

<p style=""font-size:20px""><b>Why participate?</b></p>",4.114527543187206
"What is the compute capability of the device used in the example program?
","Each Frontier compute node consists of [1x] 64-core AMD ""Optimized 3rd Gen EPYC"" CPU (with 2 hardware threads per physical core) with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X, each with 2 Graphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can think of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth memory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU, allowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of 36+36 GB/s. The 2 GCDs on the same MI250X are connected with Infinity",4.09516972409076
"Can I use --threads-per-core with other MPI implementations besides OpenMP?
","Comparing this output to the Crusher Compute Node diagram, we see that each pair of OpenMP threads is contained within a single physical core. MPI rank 000 ran on hardware threads 001 and 065 (i.e. physical CPU core 01) and MPI rank 001 ran on hardware threads 009 and 073 (i.e. physical CPU core 09).

There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.",4.284336828398852
"Can I use --threads-per-core with other MPI implementations besides OpenMP?
","There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.",4.2699967910177765
"Can I use --threads-per-core with other MPI implementations besides OpenMP?
","The following examples cover multithreading with hybrid MPI+OpenMP applications.  In these examples, Slurm's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive method was used to request an allocation of 1 compute node: salloc -A <project_id> -t 30 -p <parition> -N 1

There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.",4.256383187400207
"How can I display the files on each node in the allocation?
","echo
echo ""*****DISPLAYING FILES ON EACH NODE IN THE ALLOCATION*****""
# Check to see if file exists
srun -N ${SLURM_NNODES} -n ${SLURM_NNODES} --ntasks-per-node=1 bash -c ""echo \""\$(hostname): \$(ls -lh /mnt/bb/$USER/test.txt)\""""
echo ""*********************************************************""

echo
# Showing the file on the current node -- this will be the same on all other nodes in the allocation
echo ""*****SBCAST FILE ON CURRENT NODE******""
cat /mnt/bb/$USER/test.txt
echo ""**************************************""

and here is the output from that script:

Fri 03 Mar 2023 03:43:30 PM EST",4.342957162766775
"How can I display the files on each node in the allocation?
","echo
echo ""*****DISPLAYING FILES ON EACH NODE IN THE ALLOCATION*****""
# Check to see if file exists
srun -N ${SLURM_NNODES} -n ${SLURM_NNODES} --ntasks-per-node=1 bash -c ""echo \""\$(hostname): \$(ls -lh /mnt/bb/$USER/test.txt)\""""
echo ""*********************************************************""

echo
# Showing the file on the current node -- this will be the same on all other nodes in the allocation
echo ""*****SBCAST FILE ON CURRENT NODE******""
cat /mnt/bb/$USER/test.txt
echo ""**************************************""

and here is the output from that script:

Fri 03 Mar 2023 03:43:30 PM EST",4.342957162766775
"How can I display the files on each node in the allocation?
","Fri 03 Mar 2023 03:43:30 PM EST

*****ORIGINAL FILE*****
This is an example file
***********************

*****DISPLAYING FILES ON EACH NODE IN THE ALLOCATION*****
crusher001: -rw-r--r-- 1 hagertnl hagertnl 24 Mar  3 15:43 /mnt/bb/hagertnl/test.txt
crusher002: -rw-r--r-- 1 hagertnl hagertnl 24 Mar  3 15:43 /mnt/bb/hagertnl/test.txt
*********************************************************

*****SBCAST FILE ON CURRENT NODE******
This is an example file
**************************************

sbcast also handles binaries and their libraries:",4.129678153971273
"How can I unload all modules?
","You will need to unload the darshan-runtime module. In some instances you may need to unload the xalt and xl modules.  $ module unload darshan-runtime

Serial

..  C

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load scorep
        $ module load gcc
        $ scorep gcc -c test.c
        $ scorep gcc -o test test.o

..  C++

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load scorep
        $ module load gcc
        $ scorep g++ -c test.cpp main.cpp
        $ scorep g++ -o test test.o main.o

..  Fortran",4.185172682633123
"How can I unload all modules?
","module purge
module load DefApps
module unload xl
module load open-ce/1.5.2-py39-0
conda activate /ccs/proj/<project_id>/<user_id>/envs/summit/opence_cupy_summit

Assuming you are continuing from the previous sections, you do not need to load any modules. Otherwise, you need to load the modules associated with your system covered in the https://docs.olcf.ornl.gov/systems/cupy.html#Installing CuPy section <cupy-envs>.",4.097505679346956
"How can I unload all modules?
","Modules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the spider sub-command can be used as summarized in the following table.",4.088715889206133
"What is the difference between --gpu-bind=closest and --gpu-bind=map_gpu?
","--gpu-bind=closest binds each task to the GPU which is closest.

To further clarify, --gpus-per-task does not actually bind GPUs to MPI ranks. It allocates GPUs to the job step. The --gpu-bind=closest is what actually maps a specific GPU to each rank; namely, the ""closest"" one, which is the GPU on the same NUMA domain as the CPU core the MPI rank is running on (see the https://docs.olcf.ornl.gov/systems/spock_quick_start_guide.html#spock-compute-nodes section).

Without these additional flags, all MPI ranks would have access to all GPUs (which is the default behavior).",4.4434044874093175
"What is the difference between --gpu-bind=closest and --gpu-bind=map_gpu?
","The output shows the round-robin (cyclic) distribution of MPI ranks to GPUs. In fact, it is a round-robin distribution of MPI ranks to L3 cache regions (the default distribution). The GPU mapping is a consequence of where the MPI ranks are distributed; --gpu-bind=closest simply maps the GPU in an L3 cache region to the MPI ranks in the same L3 region.

Example 6: 32 MPI ranks - where 2 ranks share a GPU (round-robin, multi-node)

This example is an extension of Example 5 to run on 2 nodes.",4.299538482680524
"What is the difference between --gpu-bind=closest and --gpu-bind=map_gpu?
","This example will be very similar to Example 1, but instead of using --gpu-bind=closest to map each MPI rank to the closest GPU, --gpu-bind=map_gpu will be used to map each MPI rank to a specific GPU. The map_gpu option takes a comma-separated list of GPU IDs to specify how the MPI ranks are mapped to GPUs, where the form of the comma-separated list is <gpu_id_for_task_0>, <gpu_id_for_task_1>,....

$ export OMP_NUM_THREADS=2
$ srun -N1 -n8 -c2 --gpus-per-node=8 --gpu-bind=map_gpu:4,5,2,3,6,7,0,1 ./hello_jobstep | sort",4.297019373405902
"How can I specify the entry point for the container?
","Secondly, the  Pod needs something to do when it starts. For an nginx server this would be running nginx, for a flask app this would be running the app.py file etc. For illustrative purposes this  Pod is going to be starting a shell with the /bin/sh command, echoing a ""Hello World!"" prompt then running a cat command as a means to keep the pod running. Without the addition of the cat at the end the echo command would end causing the /bin/sh to end causing the  Pod to go from a status of Running to Completed.  To make these changes add the following lines below the image line:",4.069567639207513
"How can I specify the entry point for the container?
","Here you will have the option to select from a number of container images. Select the one that matches the source code in the git repository that you will be using. In this example we'll be using Python.

Using the drop down menu select the version of the language that your source is written in and click Select.

Then input the repo that contains the repository you wish to use.",4.050313019236071
"How can I specify the entry point for the container?
","name: the-container
        ports:
        - containerPort: 8080",4.03623379149119
"What information can I find on the IBM Quantum Hub Dashboard?
","Users can access information about IBM Quantum's systems, view queue information, and submit jobs on their cloud dashboard at https://quantum-computing.ibm.com. The cloud dashboard allows access to IBM Quantum's graphical circuit builder, Quantum Composer, IBM's Quantum Lab, and associated program examples.  A Jupyterlab server is provisioned with IBM Quantum's Qiskit programming framework for job submission.",4.476323398441649
"What information can I find on the IBM Quantum Hub Dashboard?
",and view the results of your past jobs. More information about using these IBM quantum resources can be found on the IBM's Documentation or our https://docs.olcf.ornl.gov/quantum/quantum_systems/ibm_quantum.html.,4.338899435948695
"What information can I find on the IBM Quantum Hub Dashboard?
","After submitting the OLCF quantum account application and receiving approval, proceed to https://quantum-computing.ibm.com/ and click on ""Create an IBMid account"". Your IBM Quantum Hub account email will be the email associated with your OLCF account. If sign-in fails, contact help@olcf.ornl.gov. Once logged in, users will have access to the IBM Quantum Hub, IBM’s online platform for QPU access, forums for quantum computing discussion, etc. From the IBM Quantum Hub Dashboard, users can manage system reservations, view system (backend) statuses, and view the results of your past jobs. More",4.323335048119319
"Can Perftools be used for profiling?
","There are three programming interfaces available: (1) Perftools-lite, (2) Perftools, and (3) Perftools-preload.

Below are two examples that generate an instrumented executable using Perftools, which is an advanced interface that provides full-featured data collection and analysis capability, including full traces with timeline displays.

The first example generates an instrumented executable using a PrgEnv-amd build:

module load PrgEnv-amd
module load craype-accel-amd-gfx90a
module load rocm
module load perftools",4.391476657529956
"Can Perftools be used for profiling?
","There are three programming interfaces available: (1) Perftools-lite, (2) Perftools, and (3) Perftools-preload.

Below are two examples that generate an instrumented executable using Perftools, which is an advanced interface that provides full-featured data collection and analysis capability, including full traces with timeline displays.

The first example generates an instrumented executable using a PrgEnv-amd build:

module load PrgEnv-amd
module load craype-accel-amd-gfx90a
module load rocm
module load perftools",4.391476657529956
"Can Perftools be used for profiling?
","pat_report hello_jobstep+pat+39545-2t

The resulting report includes profiles of functions, profiles of maximum function times, details on load imbalance, details on program energy and power usages, details on memory high water mark, and more.

More detailed information on the HPE Performance Analysis Tools can be found in the HPE Performance Analysis Tools User Guide.

When using perftools-lite-gpu, there is a known issue causing ld.lld not to be found. A workaround this issue can be found here.",4.231801863581409
"What should I do if I encounter issues with ParaView on Andes?
","Alternatively, the ParaView installations in /sw/andes/paraview (i.e., the paraview/5.9.1-egl and paraview/5.9.1-osmesa modules) can also be loaded to avoid this issue.



The ParaView at OLCF Tutorial highlights how to get started on Andes with example datasets.

The Official ParaView User's Guide and the Python API Documentation contain all information regarding the GUI and Python interfaces.

A full list of ParaView Documentation can be found on ParaView's website.

The ParaView Wiki contains extensive information about all things ParaView.",4.373354436340095
"What should I do if I encounter issues with ParaView on Andes?
","Andes: ParaView 5.9.1, 5.10.0, 5.11.0

Using a different version than what is listed above is not guaranteed to work properly.",4.353717299737042
"What should I do if I encounter issues with ParaView on Andes?
","ParaView was developed to analyze extremely large datasets using distributed memory computing resources. The OLCF provides ParaView server installs on Andes and Summit to facilitate large scale distributed visualizations. The ParaView server running on Andes and Summit may be used in a headless batch processing mode or be used to drive a ParaView GUI client running on your local machine.

For a tutorial of how to get started with ParaView on Andes, see our ParaView at OLCF Tutorial.",4.310091214304446
"How can I include a comment line in my submission script for Summit?
","The first line of a script can be used to specify the script’s interpreter; this line is optional. If not used, the submitter’s default shell will be used. The line uses the hash-bang syntax, i.e., #!/path/to/shell.

The Slurm submission options are preceded by the string #SBATCH, making them appear as comments to a shell. Slurm will look for #SBATCH options in a batch script from the script’s first line through the first non-comment line. A comment line begins with #. #SBATCH options entered after the first non-comment line will not be read by Slurm.",4.209081739933258
"How can I include a comment line in my submission script for Summit?
","Interpreter Line

The first line of a script can be used to specify the script’s interpreter; this line is optional. If not used, the submitter’s default shell will be used. The line uses the hash-bang syntax, i.e., #!/path/to/shell.

Slurm Submission Options",4.161018930217573
"How can I include a comment line in my submission script for Summit?
","Summit has a node hierarchy that can be very confusing for the uninitiated. The Summit User Guide explains this in depth. This has some consequences that may be unusual for R programmers. A few important ones to note are:

You must have a script that you can run in batch, e.g. with Rscript or R CMD BATCH

All data that needs to be visible to the R process (including the script and your packages) must be on gpfs (not your home directory!).

You must launch your script from the launch nodes with jsrun.

We'll start with something very simple. The script is:

""hello world""",4.147226068267073
"What is the name of the main function in the TAU application?
","TAU is installed with Program Database Toolkit (PDT) on Summit. PDT is a framework for analyzing source code written in several programming languages. Moreover, Performance Application Programming Interface (PAPI) is supported. PAPI counters are used to assess the CPU performance. In this section, some approaches for profiling and tracing will be presented.

In most cases, we need to use wrappers to recompile the application:

For C: replace the compiler with the TAU wrapper tau_cc.sh

For C++: replace the compiler with the TAU wrapper tau_cxx.sh",4.201977519167722
"What is the name of the main function in the TAU application?
","TAU is a portable profiling and tracing toolkit that supports many programming languages. The instrumentation can be done by inserting in the source code using an automatic tool based on the Program Database Toolkit (PDT), on the compiler instrumentation, or manually using the instrumentation API.

Webpage: https://www.cs.uoregon.edu/research/tau/home.php",4.172121452512054
"What is the name of the main function in the TAU application?
","Below, we'll look at using TAU to profile each case.

Edit the cmake_summit_pgi.sh and replace mpic++ with tau_cxx.sh. This applies only for the non-GPU versions.

TAU works with special TAU makefiles to declare what programming models are expected from the application:

The available makefiles are located inside TAU installation:",4.118761543248317
"What is the default partition (queue) that my job will be submitted to?
","Users may have only 100 jobs queued in the batch queue at any time (this includes jobs in all states). Additional jobs will be rejected at submit time.

The debug quality of service (QOS) class can be used to access Frontier's compute resources for short non-production debug tasks. The QOS provides a higher priority compare to jobs of the same job size bin in production queues. Production work and job chaining using the debug QOS is prohibited. Each user is limited to one job in any state at any one point. Attempts to submit multiple jobs to this QOS will be rejected upon job submission.",4.1282587872099
"What is the default partition (queue) that my job will be submitted to?
","There are special queue names when submitting jobs to citadel.ccs.ornl.gov (the Moderate Enhanced version of Summit). These queues are: batch-spi, batch-hm-spi, and debug-spi.  For example, to submit a job to the batch-spi queue on Citadel, you would need -q batch-spi when using the bsub command or #BSUB -q batch-spi when using a job script.

Except for the enhanced security policies for jobs in these queues, all other queue properties are the same as the respective Summit queues described above, such as maximum walltime and number of eligible running jobs.",4.125864944438877
"What is the default partition (queue) that my job will be submitted to?
","| Bin | Min Nodes | Max Nodes | Max Walltime (Hours) | Aging Boost (Days) | | --- | --- | --- | --- | --- | | 1 | 2,765 | 4,608 | 24.0 | 15 | | 2 | 922 | 2,764 | 24.0 | 10 | | 3 | 92 | 921 | 12.0 | 0 | | 4 | 46 | 91 | 6.0 | 0 | | 5 | 1 | 45 | 2.0 | 0 |

The batch queue (and the batch-spi queue for Moderate Enhanced security enclave projects) is the default queue for production work on Summit.  Most work on Summit is handled through this queue. It enforces the following policies:

Limit of (4) eligible-to-run jobs per user.",4.125224457166125
"Is Helm a single binary executable?
","$ brew install helm

Or can be pulled from the Helm Release Page. If downloading from the GitHub release page, you can copy this executable into /usr/local/bin to add it to $PATH.

NOTE: One nice feature of Helm is that it uses the underlying authentication credentials used with oc, so once you login with oc login, the helm client will authenticate automatically.

Once oc and helm are setup and you are logged in with oc login, test Helm:

$ helm ls",4.222171280595623
"Is Helm a single binary executable?
","Helm is the OLCF preferred package manager for Kubernetes. There are a variety of upstream applications that can be installed with helm, such as databases, web frontends, and monitoring tools. Helm has ""packages"" called ""charts"", which can essentially be thought of as kubernetes object templates. You can pass in values which helm uses to fill in these templates, and create the objects (pods, deployments, services, etc)

Follow https://docs.olcf.ornl.gov/systems/helm_example.html#helm_prerequisite for installing Helm.",4.170829250060307
"Is Helm a single binary executable?
","$ helm ls

You should get some output similar to this (although, you may not have any applications listed, if you have not deployed any):",4.144141517700229
"Can a user access OLCF resources from a personal device?
","Computers, software, and communications systems provided by the OLCF are to be used for work associated with and within the scope of the approved project. The use of OLCF resources for personal or non-work-related activities is prohibited. All computers, networks, E-mail, and storage systems are property of the United States Government. Any misuse or unauthorized access is prohibited, and is subject to criminal and civil penalties. OLCF systems are provided to our users without any warranty. OLCF will not be held liable in the event of any system failure or data loss or corruption for any",4.3806916648260525
"Can a user access OLCF resources from a personal device?
","The requirements outlined in this document apply to all individuals who have an OLCF account. It is your responsibility to ensure that all individuals have the proper need-to-know before allowing them access to the information on OLCF computing resources. This document will outline the main security concerns.

OLCF computing resources are for business use only. Installation or use of software for personal use is not allowed. Incidents of abuse will result in account termination. Inappropriate uses include, but are not limited to:

Sexually oriented information",4.3595211815692645
"Can a user access OLCF resources from a personal device?
","If you already have a user account at the OLCF, your existing credentials can be leveraged across multiple projects.

If your user account has an associated RSA SecurID (i.e. you have an ""OLCF Moderate"" account), you gain access to another project by logging in to the myOLCF self-service portal and filling out the application under My Account > Join Another Project. For more information, see the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#myOLCF self-service portal documentation<myolcf-overview>.",4.350266354330717
"How can I use Score-P with CMake?
","For CMake and Autotools based builds it is recommended to configure in the following way(s):

#Example for CMake

$ SCOREP_WRAPPER=off cmake .. \
     -DCMAKE_C_COMPILER=scorep-gcc \
     -DCMAKE_CXX_COMPILER=scorep-g++ \
     -DCMAKE_Fortran_COMPILER=scorep-ftn

#Example for autotools

$ SCOREP_WRAPPER=off  ../configure \
     CC=scorep-gcc \
     CXX=scorep-g++ \
     FC=scorep--ftn \
     --disable-dependency-tracking",4.315303672201333
"How can I use Score-P with CMake?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi-processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations.

For detailed information about using Score-P on Summit and the builds available, please see the Score-P Software Page.",4.300452610815274
"How can I use Score-P with CMake?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations. It works in combination with Periscope, Scalasca, Vampir, and Tau.

Steps in a typical Score-P workflow to run on https://docs.olcf.ornl.gov/systems/Scorep.html#summit-user-guide:",4.290843932764894
"What is the maximum memory bandwidth of Crusher?
","Crusher contains a total of 768 AMD MI250X. The AMD MI250X has a peak performance of 53 TFLOPS in double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 26.5 TFLOPS (double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in both directions simultaneously).



To connect to Crusher, ssh to crusher.olcf.ornl.gov. For example:

$ ssh <username>@crusher.olcf.ornl.gov",4.279672304471464
"What is the maximum memory bandwidth of Crusher?
",Crusher is connected to the center-wide IBM Spectrum Scale™ filesystem providing 250 PB of storage capacity with a peak write speed of 2.5 TB/s. Crusher also has access to the center-wide NFS-based filesystem (which provides user and project home areas). While Crusher does not have direct access to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#dtn-user-guide to move data to/from HPSS.,4.2696642490606544
"What is the maximum memory bandwidth of Crusher?
",The Crusher nodes are connected with [4x] HPE Slingshot 200 Gbps (25 GB/s) NICs providing a node-injection bandwidth of 800 Gbps (100 GB/s).,4.241285007415879
"How do I run GDB on Summit?
","GDB, the GNU Project Debugger, is a command-line debugger useful for traditional debugging and investigating code crashes. GDB lets you debug programs written in Ada, C, C++, Objective-C, Pascal (and many other languages).

GDB is available on Summit under all compiler families:

module load gdb

To use GDB to debug your application run:

gdb ./path_to_executable

Additional information about GDB usage can befound on the GDB Documentation Page.",4.325317622155885
"How do I run GDB on Summit?
","GDB, the GNU Project Debugger, is a command-line debugger useful for traditional debugging and investigating code crashes. GDB lets you debug programs written in Ada, C, C++, Objective-C, Pascal (and many other languages).

GDB is availableon Summit under all compiler families:

module load gdb

To use GDB to debug your application run:

gdb ./path_to_executable

Additional information about GDB usage can befound on the GDB Documentation Page.",4.3231095469121215
"How do I run GDB on Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.2897500851693176
"What is the purpose of the OLCF Policy?
","The OLCF provides a comprehensive suite of hardware and software resources for the creation, manipulation, and retention of scientific data. This document comprises guidelines for acceptable use of those resources. It is an official policy of the OLCF, and as such, must be agreed to by relevant parties as a condition of access to and use of OLCF computational resources.",4.350186529787915
"What is the purpose of the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to or use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title:Security Policy Version: 12.10",4.342313307372409
"What is the purpose of the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  Title: Project Reporting Policy Version: 12.10",4.326688663483319
"What is the cause of oversubscription in Summit jobs?
",For Summit:,4.167798891265052
"What is the cause of oversubscription in Summit jobs?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.161739669488423
"What is the cause of oversubscription in Summit jobs?
","The batch-hm queue (and the batch-hm-spi queue for Moderate Enhanced security enclave projects) is used to access Summit's high-memory nodes.  Jobs may use all 54 nodes. It enforces the following policies:

Limit of (4) eligible-to-run jobs per user.

Jobs in excess of the per user limit above will be placed into a held state, but will change to eligible-to-run at the appropriate time.

Users may have only (25) jobs queued in the batch-hm queue at any state at any time. Additional jobs will be rejected at submit time.

batch-hm job limits:",4.156553839303666
"How can I set the `TAU_DIR` environment variable?
",Environment variables to be used during compilation through the environment variable TAU_OPTIONS,4.430712232543152
"How can I set the `TAU_DIR` environment variable?
","$ module show tau
---------------------------------------------------------------
   /sw/summit/modulefiles/core/tau/2.28.1:
---------------------------------------------------------------
whatis(""TAU 2.28.1 github "")
setenv(""TAU_DIR"",""/sw/summit/tau/tau2/ibm64linux"")
prepend_path(""PATH"",""/sw/summit/tau/tau2/ibm64linux/bin"")
help([[https://www.olcf.ornl.gov/software_package/tau
]])

The available Makefiles are named per-compiler and are located in:",4.242622844196283
"How can I set the `TAU_DIR` environment variable?
","$ ls ${TAU_DIR}/lib/Makefile.tau*

For a serial application, we should not use a Makefile with a programming model such as MPI or OpenMP. However, as the source code for this specific case includes MPI headers that are not excluded during the compilation of the serial version, we should declare a Makefile with MPI. We can declare a TAU makefile with the environment variable TAU_MAKEFILE. Moreover, with TAU_OPTIONS below, we add options to the linker as the application depends on PNetCDF.",4.157904501502848
"How can I ensure my Summit shell sources the correct run control files?
","On Summit and Andes, batch-interactive jobs using bash (i.e. those submitted with bsub -Is or salloc) run as interactive, non-login shells (and therefore source ~/.bashrc, if it exists). Regular batch jobs using bash on those systems are non-interactive, non-login shells and source the file defined by the variable $BASH_ENV in the shell from which you submitted the job. This variable is not set by default, so this means that none of these files will be sourced for a regular batch job unless you explicitly set that variable.",4.199620492627771
"How can I ensure my Summit shell sources the correct run control files?
","Some users have noticed that their login shells, batch jobs, etc. are not sourcing shell run control files as expected. This is related to the way bash is initialized. The initialization process is discussed in the INVOCATION section of the bash manpage, but is summarized here.

Bash sources different files based on two attributes of the shell: whether or not it's a login shell, and whether or not it's an interactive shell. These attributes are not mutually exclusive (so a shell can be ""interactive login"", ""interactive non-login"", etc.):",4.191934923849809
"How can I ensure my Summit shell sources the correct run control files?
","OLCF systems provide many software packages and scientific libraries pre-installed at the system-level for users to take advantage of. To facilitate this, environment management tools are employed to handle necessary changes to the shell. The sections below provide information about using these management tools on Summit.

A user’s default shell is selected when completing the User Account Request form. The chosen shell is set across all OLCF resources, and is the shell interface a user will be presented with upon login to any OLCF system. Currently, supported shells include:

bash

tcsh",4.13988623008138
"What is the difference between Andes and Frontier?
",For Andes:,4.288741461042336
"What is the difference between Andes and Frontier?
","As the OLCF brings Frontier into full production and begins preparations for future resources, you should be aware of plans that will impact Summit, Andes, and the Alpine filesystem in 2023.

Please pay attention to the following key dates as you plan your science campaigns and data migration for the remainder of the year:",4.151823134708544
"What is the difference between Andes and Frontier?
","The above procedure can also be followed to connect to Summit or Frontier, with the main difference being the number of available processors. The time limit syntax for Andes, Summit, and Frontier also differ. Summit uses the format HH:MM while Andes and Frontier follow HH:MM:SS.

Please do not run VisIt's GUI client from an OLCF machine. You will get much better performance if you install a client on your workstation and launch locally. You can directly connect to OLCF machines from inside VisIt and access your data remotely.",4.140080201999513
"How do I keep my login session active?
","Access to systems is provided via Secure Shell version 2 (sshv2). You will need to ensure that your ssh client supports keyboard-interactive authentication. The method of setting up this authentication varies from client to client, so you may need to contact your local administrator for assistance. Most new implementations support this authentication type, and many ssh clients are available on the web. Login sessions will be automatically terminated after a period of inactivity. When you apply for an account, you will be mailed an RSA SecurID token. You will also be sent a request to complete",4.054832837203876
"How do I keep my login session active?
","This will give you a new opportunity to enter your PIN + token code and your username will appear in login request box as shown below. If you want you OLCF username to be filled in by default, go to ""Options→Host profiles"" and enter it under ""Username"".",4.016109540262345
"How do I keep my login session active?
","Login nodes should be used for basic tasks such as file editing, code compilation, data backup, and job submission. Login nodes should not be used for memory- or compute-intensive tasks. Users should also limit the number of simultaneous tasks performed on the login resources. For example, a user should not run (10) simultaneous tar processes on a login node.

Compute-intensive, memory-intensive, or otherwise disruptive processes running on login nodes may be killed without warning.",4.002855388515946
"How can I learn more about the ALCC program?
","ALCC – The ASCR Leadership Computing Challenge (ALCC) is open to scientists from the research community in national laboratories, academia and industry. The ALCC program allocates computational resources at the OLCF for special situations of interest to the Department with an emphasis on high-risk, high-payoff simulations in areas directly related to the Department’s energy mission in areas such as advancing the clean energy agenda and understanding the Earth’s climate, for national emergencies, or for broadening the community of researchers capable of using leadership computing resources.",4.286843048222501
"How can I learn more about the ALCC program?
","OLCF training events can go through a streamlined version of the approval process before gaining access to the system. The remainder of this section of the user guide describes ""Ascent-specific"" information intended for participants of OLCF training events.",4.118662288121652
"How can I learn more about the ALCC program?
","If you would like more information about how you can volunteer to mentor a team at an upcoming Open/GPU hackathon, please visit our Become a Mentor page.

<p style=""font-size:20px""><b>Who can I contact with questions?</b></p>

If you have any questions about the OLCF GPU Hackathon Series, please contact OLCF Help at (help@olcf.ornl.gov).",4.105298179828715
"How can I combine measurements with program structure information and generate a database on Summit?
","# 3. Combine measurements with program structure information and generate a database
hpcprof -o <database_dir> <measurement_dir>

# 4. Understand performance issues by analyzing profiles and traces with the GUI
hpcviewer <database_dir>

More detailed information on HPCToolkit can be found in the HPCToolkit User's Manual.

HPCToolkit does not require a recompile to profile the code. It is recommended to use the -g optimization flag for attribution to source lines.",4.2396437421799495
"How can I combine measurements with program structure information and generate a database on Summit?
","# 3. Combine measurements with program structure information and generate a database
hpcprof -o <database_dir> <measurement_dir>

# 4. Understand performance issues by analyzing profiles and traces with the GUI
hpcviewer <database_dir>

A quick summary of HPCToolkit options can be found in the HPCTookit wiki page. More detailed information on HPCToolkit can be found in the HPCToolkit User's Manual.

HPCToolkit does not require a recompile to profile the code. It is recommended to use the -g optimization flag for attribution to source lines.",4.225950377942626
"How can I combine measurements with program structure information and generate a database on Summit?
","Login to https://docs.olcf.ornl.gov/systems/Scorep.html#Summit <connecting-to-olcf>: ssh <user_id>@summit.olcf.ornl.gov

Instrument your code with Score-P

Perform a measurement run with profiling enabled

Perform a profile analysis with CUBE or cube_stat

Use scorep-score to define a filter

Perform a measurement run with tracing enabled and the filter applied

Perform in-depth analysis on the trace data with Vampir",4.148758118210751
"How can you use the compatibility table in Crusher?
",The compatibility table below was determined by linker testing with all current combinations of cray-mpich and rocm modules on Crusher.,4.2981646205005655
"How can you use the compatibility table in Crusher?
",Crusher is connected to the center-wide IBM Spectrum Scale™ filesystem providing 250 PB of storage capacity with a peak write speed of 2.5 TB/s. Crusher also has access to the center-wide NFS-based filesystem (which provides user and project home areas). While Crusher does not have direct access to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#dtn-user-guide to move data to/from HPSS.,4.02603370496363
"How can you use the compatibility table in Crusher?
","The compatibility table below was determined by linker testing with all current combinations of cray-mpich and ROCm-related modules on Frontier.

| cray-mpich | ROCm | | --- | --- | | 8.1.17 | 5.4.0, 5.3.0, 5.2.0, 5.1.0 | | 8.1.23 | 5.4.0, 5.3.0, 5.2.0, 5.1.0 |

This section shows how to compile with OpenMP using the different compilers covered above.

| Vendor | Module | Language | Compiler | OpenMP flag (CPU thread) | | --- | --- | --- | --- | --- | | Cray | cce | C, C++ |  | -fopenmp | | Fortran | ftn (wraps crayftn) |  | | AMD | amd |  |  | -fopenmp | | GCC | gcc |  |  | -fopenmp |",3.9926787741862513
"What is the purpose of using the `--ntasks-per-gpu=2` option in the `srun` command?
","So the job step (i.e., srun command) used above gave the desired output. Each MPI rank spawned 2 OpenMP threads and had access to a unique GPU. The --gpus-per-task=1 allocated 1 GPU for each MPI rank and the --gpu-bind=closest ensured that the closest GPU to each rank was the one used.

Example 2: 8 MPI ranks - each with 2 OpenMP threads and 1 GPU (multi-node)

This example will extend Example 1 to run on 2 nodes. As the output shows, it is a very straightforward exercise of changing the number of nodes to 2 (-N2) and the number of MPI ranks to 8 (-n8).",4.317963387836155
"What is the purpose of using the `--ntasks-per-gpu=2` option in the `srun` command?
","So the job step (i.e., srun command) used above gave the desired output. Each MPI rank spawned 2 OpenMP threads and had access to a unique GPU. The --gpus-per-node=8 allocated 8 GPUs for node and the --gpu-bind=closest ensured that the closest GPU to each rank was the one used.",4.284658362199452
"What is the purpose of using the `--ntasks-per-gpu=2` option in the `srun` command?
","| Slurm Option | Description | | --- | --- | | --ntasks-per-gpu | Specifies the number of MPI ranks that will share access to a GPU. |

On AMD's MI250X, multi-process service (MPS) is not needed since multiple MPI ranks per GPU is supported natively.

Example 4: 16 MPI ranks - where 2 ranks share a GPU (round-robin, single-node)

This example launches 16 MPI ranks (-n16), each with 1 physical CPU core (-c1) to launch 1 OpenMP thread (OMP_NUM_THREADS=1) on. The MPI ranks will be assigned to GPUs in a round-robin fashion so that each of the 8 GPUs on the node are shared by 2 MPI ranks.",4.2746197194956
"Can I use ArgoCD to manage multiple Kubernetes environments?
","With a git repository defined in the ArgoCD Repositories settings that has kustomize environments ready for use, one can start using ArgoCD to deploy and manage kubernetes resources.",4.431854432101036
"Can I use ArgoCD to manage multiple Kubernetes environments?
","Unlike the CI/CD tools mentioned above, ArgoCD is not used for testing and creating container images. Rather, ArgoCD manages Kubernetes application deployments in an automated and consistent manner using custom resource files versioned in a git repository. Additionally, individual development, test and production deployments across multiple projects may be accomplished using a singular git repository. Whenever a change occurs in the git repository, ArgoCD will make the necessary changes to a project by adding, reconfiguring, or removing resources. In other words, the CD in ArgoCD is for",4.332109452048865
"Can I use ArgoCD to manage multiple Kubernetes environments?
",allow for better control of resources allocated to ArgoCD.,4.31808817238095
"What happens if a file system associated with Project Work storage is nearing capacity?
","not be retained in this file system for long, but rather should be migrated to Project Home or Project Archive space as soon as the files are not actively being used. If a file system associated with Project Work storage is nearing capacity, the OLCF may contact the PI of the project to request that he or she reduce the size of the Project Work directory. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for more details on applicable quotas, backups, purge, and retention timeframes.",4.334583940818907
"What happens if a file system associated with Project Work storage is nearing capacity?
","this file system for long, but rather should be migrated to Project Home or Project Archive space as soon as the files are not actively being used. If a file system associated with World Work storage is nearing capacity, the OLCF may contact the PI of the project to request that he or she reduce the size of the World Work directory. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for more details on applicable quotas, backups, purge, and retention timeframes.",4.330081868325025
"What happens if a file system associated with Project Work storage is nearing capacity?
","and files are automatically purged on a regular basis. Files should not be retained in this file system for long, but rather should be migrated to Project Home or Project Archive space as soon as the files are not actively being used. If a file system associated with your Member Work directory is nearing capacity, the OLCF may contact you to request that you reduce the size of your Member Work directory. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for more details on applicable quotas, backups, purge, and retention timeframes.",4.301166927736543
"How do I make further inquiries about my application?
","When our accounts team begins processing your application, you will receive an automated email containing an unique 36-character confirmation code. Make note of it; you can use it to check the status of your application at any time.

The principal investigator (PI) of the project must approve your account and system access. We will make the project PI aware of your request.",4.186309147898768
"How do I make further inquiries about my application?
","On the ""Policies & Agreements"" page click the links to read the polices and then answer ""Yes"" to affirm you have read each. Certify your application by selecting ""Yes"" as well. Then Click ""Submit.""

<string>:5: (INFO/1) Enumerated list start value not ordinal-1: ""8"" (ordinal 8)

After submitting your application, it will need to pass through the approval process. Depending on when you submit, approval might not occur until the next business day.",4.183473013914243
"How do I make further inquiries about my application?
","Once your application is evaluated and approved, you will be notified via email of your account creation, and the quantum resource vendor will be contacted with instructions to grant you access.

You can check the general status of your application at any time using the myOLCF self-service portal's account status page. For more information, see our https://docs.olcf.ornl.gov/services_and_applications/myolcf/overview.html page. If you need to make further inquiries about your application, you may email our Accounts Team at accounts@ccs.ornl.gov.",4.142431238457019
"How can I ensure that my executable uses the correct version of RocBLAS on Frontier?
","Frontier

.. code-block:: bash

    $ module swap PrgEnv-cray PrgEnv-gnu
    $ module load cmake

# INSTALLING LAPACK (also installs BLAS)
$ cd
$ mkdir pack_temp/
$ cd pack_temp/
$ cp ../lapack-3.10.0.tar.gz .
$ tar -xvf lapack-3.10.0.tar.gz
$ cd lapack-3.10.0/
$ mkdir build
$ cd build/
$ cmake -DBUILD_SHARED_LIBS=ON -DCMAKE_INSTALL_LIBDIR=$HOME/lapackblas ..
$ cmake --build . -j --target install",4.173133094576364
"How can I ensure that my executable uses the correct version of RocBLAS on Frontier?
","...
rocblas_create_handle(handle);
rocblas_set_atomics_mode(handle, rocblas_atomics_not_allowed);

hipblasCreate(&handle);
hipblasSetAtomicsMode(handle, HIPBLAS_ATOMICS_NOT_ALLOWED);



On Tuesday, September 19, 2023, Frontier's system software was upgraded. The following changes took place:

The system was upgraded to Slingshot Host Software 2.1.0.

ROCm 5.6.0 and 5.7.0 are now available via the rocm/5.6.0 and rocm/5.7.0 modulefiles, respectively.

HPE/Cray Programming Environments (PE) 23.09 is now available via the cpe/23.09 modulefile.

ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.",4.152583972142525
"How can I ensure that my executable uses the correct version of RocBLAS on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.141954354251732
"What is the benefit of using the OMP_NUM_THREADS environment variable?
","In addition to specifying the SMT level, you can also control the number of threads per MPI task by exporting the OMP_NUM_THREADS environment variable. If you don't export it yourself, Jsrun will automatically set the number of threads based on the number of cores requested (-c) and the binding (-b) option. It is better to be explicit and set the OMP_NUM_THREADS value yourself rather than relying on Jsrun constructing it for you. Especially when you are using Job Step Viewer which relies on the presence of that environment variable to give you visual thread assignment information.",4.385921287473885
"What is the benefit of using the OMP_NUM_THREADS environment variable?
","In the below example, you could also do export OMP_NUM_THREADS=16 in your job script instead of passing it as a -E flag to jsrun. The below example starts 1 resource set with 2 tasks and 8 cores, 4 cores bound to each task, 16 threads for each task. We can set 16 threads since there are 4 cores per task and the default is smt4 for each core (4 * 4 = 16 threads).

jsrun -n1 -a2 -c8 -g1 -bpacked:4 -dpacked -EOMP_NUM_THREADS=16 csh -c 'echo $OMP_NUM_THREADS $OMP_PLACES'

16 0:4,4:4,8:4,12:4
16 16:4,20:4,24:4,28:4",4.229551099411772
"What is the benefit of using the OMP_NUM_THREADS environment variable?
","There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.",4.207260615228351
"What is the purpose of the export=NONE flag in SLURM batch jobs?
","When using Python environments with SLURM, it is always recommended to submit a batch script using the export=NONE flag to avoid $PATH issues and use unset SLURM_EXPORT_ENV in your job script (before calling srun); however, this means that previously set environment variables are NOT passed into the batch job, so you will have to set them again (and load modules again) if they are required by your workflow. Alternatively, you can try submitting your batch script from a fresh login shell.

$ sbatch --export=NONE submit.sl

Below are example batch scripts for running on Andes and Frontier:",4.462767747256669
"What is the purpose of the export=NONE flag in SLURM batch jobs?
","$ sbatch --export=NONE submit_hello.sl

Example ""submit_hello"" batch script:

Summit

.. code-block:: bash

   #!/bin/bash
   #BSUB -P <PROJECT_ID>
   #BSUB -W 00:05
   #BSUB -nnodes 1
   #BSUB -J mpi4py
   #BSUB -o mpi4py.%J.out
   #BSUB -e mpi4py.%J.err

   cd $LSB_OUTDIR
   date

   module load gcc
   module load hdf5
   module load python

   source activate /ccs/proj/<project_id>/<user_id>/envs/summit/h5pympi-summit

   jsrun -n1 -r1 -a42 -c42 python3 hello_mpi.py

Andes

.. code-block:: bash",4.086076564440543
"What is the purpose of the export=NONE flag in SLURM batch jobs?
","cd $SLURM_SUBMIT_DIR
date
srun -n 8 ./a.out

This batch script shows examples of the three sections outlined above:

<string>:509: (INFO/1) Duplicate implicit target name: ""interpreter line"".

1: This line is optional and can be used to specify a shell to interpret the script. In this example, the bash shell will be used.

2: The job will be charged to the “XXXYYY” project.

3: The job will be named test.

4: The job will request (2) nodes.

5: The job will request (1) hour walltime.

<string>:526: (INFO/1) Duplicate implicit target name: ""shell commands"".",4.079700889473493
"How do I load the necessary modules for my parallel h5py environment on Frontier?
","This guide has been adapted for Frontier only for a conda workflow. Using the default cray-python module on Frontier does not work with parallel h5py (because Python 3.9 is incompatible). Thus, this guide assumes that you are using a personal https://docs.olcf.ornl.gov/software/python/miniconda.html.

For venv users only interested in installing mpi4py, the pip command in this guide is still accurate.

This guide has been adapted from a challenge in OLCF's Hands-On with Summit GitHub repository (Python: Parallel HDF5).",4.377358411471729
"How do I load the necessary modules for my parallel h5py environment on Frontier?
","Frontier

.. code-block:: bash

   #!/bin/bash
   #SBATCH -A <PROJECT_ID>
   #SBATCH -J h5py
   #SBATCH -N 1
   #SBATCH -p batch
   #SBATCH -t 0:05:00

   unset SLURM_EXPORT_ENV

   cd $SLURM_SUBMIT_DIR
   date

   module load PrgEnv-gnu
   module load hdf5
   export PATH=""/path/to/your/miniconda/bin:$PATH""

   source activate /ccs/proj/<project_id>/<user_id>/envs/frontier/h5pympi-frontier

   srun -n42 python3 hdf5_parallel.py",4.365001195540155
"How do I load the necessary modules for my parallel h5py environment on Frontier?
","First, load the gnu compiler module (most Python packages assume GCC), hdf5 module (necessary for h5py), and the python module (allows you to create a new environment):

Summit

.. code-block:: bash

   $ module load gcc
   $ module load hdf5
   $ module load python

Andes

.. code-block:: bash

   $ module load gcc
   $ module load hdf5
   $ module load python

Frontier

.. code-block:: bash

   $ module load PrgEnv-gnu
   $ module load hdf5

   # Make sure your personal miniconda installation is in your path
   $ export PATH=""/path/to/your/miniconda/bin:$PATH""",4.351917464122099
"When will Alpine II filesystem be available?
","Please note, Summit will mount a new filesystem once returned to service.  Data stored on Alpine at the time of its decommission on January 01 will not be available.  Users will be responsible for transferring data onto Summit's new filesystem



Alpine II will be available early 2024.

The previous center-wide GPFS scratch filesystem, Alpine, will be decommissioned in January 2024. A new scratch filesystem will be made available for projects with 2024 Summit allocations in early 2024. Users will be responsible for transferring any needed data onto the new scratch filesystem once available.",4.422205789256687
"When will Alpine II filesystem be available?
","| Alpine becomes read-only and available only from DTNs. Please do not wait, begin transferring your Lustre data now. | | https://docs.olcf.ornl.gov/systems/2023_olcf_system_changes.html#Jan 01<alpine_decom> | Alpine decommissioned.  ALL REMAINING DATA WILL BE PERMANENTLY DELETED | | https://docs.olcf.ornl.gov/systems/2023_olcf_system_changes.html#Early 2024<summit_return_to_service> | Summit available for projects with 2024 allocation. | | https://docs.olcf.ornl.gov/systems/2023_olcf_system_changes.html#Early 2024<alpine_ii_available> | Alpine II filesystem available. |",4.266430833500035
"When will Alpine II filesystem be available?
","The Alpine GPFS file system remains available but will be permanently unmounted from Crusher on Tuesday, April 18, 2023. Please begin moving your data to the Orion file system as soon as possible.

On Thursday, December 29, 2022 the following system configuration settings will be updated on Crusher:

Low-Noise Mode will be enabled: as a result, system processes will be constrained to core 0 on every node.",4.237558654875082
"What happens to a memory region that is evicted from GPU HBM on Crusher?
","If ``HSA_XNACK=1``, page faults in GPU kernels will trigger a page table lookup. If the memory location can be made accessible to the GPU, either by being migrated to GPU HBM or being mapped for remote access, the appropriate action will occur and the access will be replayed. Once a memory region has been migrated to GPU HBM it typically stays there rather than migrating back to CPU DDR4. The exceptions are if the programmer uses a HIP library call such as ``hipPrefetchAsync`` to request migration, or if GPU HBM becomes full and the page must forcibly be evicted back to CPU DDR4 to make room",4.316236800447492
"What happens to a memory region that is evicted from GPU HBM on Crusher?
","If ``HSA_XNACK=1``, page faults in GPU kernels will trigger a page table lookup. If the memory location can be made accessible to the GPU, either by being migrated to GPU HBM or being mapped for remote access, the appropriate action will occur and the access will be replayed. Once a memory region has been migrated to GPU HBM it typically stays there rather than migrating back to CPU DDR4. The exceptions are if the programmer uses a HIP library call such as ``hipPrefetchAsync`` to request migration, or if GPU HBM becomes full and the page must forcibly be evicted back to CPU DDR4 to make room",4.316236800447492
"What happens to a memory region that is evicted from GPU HBM on Crusher?
","If HSA_XNACK=0, page faults in GPU kernels are not handled and will terminate the kernel. Therefore all memory locations accessed by the GPU must either be resident in the GPU HBM or mapped by the HIP runtime. Memory regions may be migrated between the host DDR4 and GPU HBM using explicit HIP library functions such as hipMemAdvise and hipPrefetchAsync, but memory will not be automatically migrated based on access patterns alone.",4.295086157892836
"What is the advantage of using a batch submission script in HPC?
","The most common way to interact with the batch system is via batch scripts. A batch script is simply a shell script with added directives to request various resoruces from or provide certain information to the scheduling system.  Aside from these directives, the batch script is simply the series of commands needed to set up and run your job.

To submit a batch script, use the command sbatch myjob.sl

Consider the following batch script:

#!/bin/bash
#SBATCH -A ABC123
#SBATCH -J RunSim123
#SBATCH -o %x-%j.out
#SBATCH -t 1:00:00
#SBATCH -p batch
#SBATCH -N 1024",4.341089632873188
"What is the advantage of using a batch submission script in HPC?
","Batch scripts, or job submission scripts, are the mechanism by which a user configures and submits a job for execution. A batch script is simply a shell script that also includes commands to be interpreted by the batch scheduling software (e.g. Slurm).",4.336329122083871
"What is the advantage of using a batch submission script in HPC?
","sbatch test.slurm

If successfully submitted, a Slurm job ID will be returned. This ID can be used to track the job. It is also helpful in troubleshooting a failed job; make a note of the job ID for each of your jobs in case you must contact the OLCF User Assistance Center for support.



Batch scripts are useful when one has a pre-determined group of commands to execute, the results of which can be viewed at a later time. However, it is often necessary to run tasks on compute resources interactively.",4.332856499716447
"Can foreign nationals from restricted countries access OLCF computers?
","Applicants who appear on a restricted foreign country listing in section 15 CFR 740.7 License Exceptions for Computers are denied access based on US Foreign Policy. The countries cited are Cuba, Iran, North Korea, Sudan, and Syria. Additionally, no work may be performed on OLCF computers on behalf of foreign nationals from these countries.

Users may not deliberately interfere with other users accessing system resources.",4.543725372925526
"Can foreign nationals from restricted countries access OLCF computers?
","Computers, software, and communications systems provided by the OLCF are to be used for work associated with and within the scope of the approved project. The use of OLCF resources for personal or non-work-related activities is prohibited. All computers, networks, E-mail, and storage systems are property of the United States Government. Any misuse or unauthorized access is prohibited, and is subject to criminal and civil penalties. OLCF systems are provided to our users without any warranty. OLCF will not be held liable in the event of any system failure or data loss or corruption for any",4.378596438594825
"Can foreign nationals from restricted countries access OLCF computers?
","The OLCF computer systems are operated as research systems and only contain data related to scientific research and do not contain personally identifiable information (data that falls under the Privacy Act of 1974 5U.S.C. 552a). Use of OLCF resources to store, manipulate, or remotely access any national security information is strictly prohibited. This includes, but is not limited to: classified information, unclassified controlled nuclear information (UCNI), naval nuclear propulsion information (NNPI), the design or development of nuclear, biological, or chemical weapons or any weapons of",4.344962171383735
"What is the maximum walltime for jobs in Bin 1 in the OLCF Policy?
","Preemptable job limits:

| Bin | Min Nodes | Max Nodes | Max Walltime (Hours) | Guaranteed Walltime | | --- | --- | --- | --- | --- | | 4 | 46 | 91 | 24.0 | 6.0 (hours) | | 5 | 1 | 45 | 24.0 | 2.0 (hours) |

If a job in the killable queue does not reach its requested walltime, it will continue to use allocation time with each automatic resubmission until it either reaches the requested walltime during a single continuous run, or is manually killed by the user. Allocations are always charged based on actual compute time used by all jobs.",4.336126598719923
"What is the maximum walltime for jobs in Bin 1 in the OLCF Policy?
","Allocation Overuse Policy

Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.3182536984730815
"What is the maximum walltime for jobs in Bin 1 in the OLCF Policy?
","enforces the following policies:



-  Production jobs are not allowed.

-  Maximum job walltime of (1) hour.

-  Limit of (1) job per user *regardless of the job's state*.

-  Jobs receive a (2)-day priority aging boost for scheduling.



.. warning::

Users who misuse the ``debug`` queue may have further

access to the queue denied.



Allocation Overuse Policy

-------------------------



Projects that overrun their allocation are still allowed to run on OLCF

systems, although at a reduced priority. Like the adjustment for the",4.310889694702316
"Can you create multiple resource sets for a single job in Summit?
","summit>

The following example will create 4 resource sets each with 6 tasks and 3 GPUs. Each set of 6 MPI tasks will have access to 3 GPUs. Ranks 0 - 5 will have access to GPUs 0 - 2 on the first socket of the first node ( red resource set). Ranks 6 - 11 will have access to GPUs 3 - 5 on the second socket of the first node ( green resource set). This pattern will continue until 4 resource sets have been created. The following jsrun command will request 4 resource sets (-n4). Each resource set will contain 6 MPI tasks (-a6), 3 GPUs (-g3), and 6 cores (-c6).",4.248754000471781
"Can you create multiple resource sets for a single job in Summit?
","Resource sets allow each jsrun to control how the node appears to a code. This method is unique to jsrun, and requires thinking of each job launch differently than aprun or mpirun. While the method is unique, the method is not complicated and can be reasoned in a few basic steps.",4.220382714152228
"Can you create multiple resource sets for a single job in Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.215259296660254
"What is the difference between the output of ""kustomize build overlays/staging"" and the base directory?
","base directory of YAML files that specify one or multiple kubernetes resources

kustomization.yaml file

one or more overlay directories

Unlike helm which is a template framework for deployment of kubernetes resources, kustomize is a patching framework. Once the base directory of YAML files is in place, kustomize patches those files to modify kubernetes resources for deployment with custom configurations for one or multiple environments such as dev, test, and prod.",4.203115446998392
"What is the difference between the output of ""kustomize build overlays/staging"" and the base directory?
","As seen above, in the simplest form a git repository contains two directories at the root: base and overlays. The base directory contains a set resources which deploy the application. These could be YAML files generated from the output of existing OpenShift project resources, new YAML files for resources that have not been deployed prior, YAML files generated by the helm template command, or even an existing git repository of YAML files located in GitHub or GitLab maintained by someone else. The overlays directory was added to contain patches for distinct deployments. In this case, two",4.135306967178401
"What is the difference between the output of ""kustomize build overlays/staging"" and the base directory?
","├── base
│   ├── configMap.yaml
│   ├── deployment.yaml
│   ├── kustomization.yaml
│   └── service.yaml
└── overlays
    ├── production
    │   ├── deployment.yaml
    │   └── kustomization.yaml
    └── staging
        ├── kustomization.yaml
        └── map.yaml",4.082342395617157
"How can I troubleshoot issues with singularity builds on Summit?
","Singularity is a container framework from Sylabs. On Summit, we will use Singularity solely as the runtime. We will convert the tar files of the container images Podman creates into sif files, store those sif files on GPFS, and run them with Jsrun. Singularity also allows building images but ordinary users cannot utilize that on Summit due to additional permissions not allowed for regular users.",4.263927137167062
"How can I troubleshoot issues with singularity builds on Summit?
","Users can build container images with Podman, convert it to a SIF file, and run the container using the Singularity runtime. This page is intended for users with some familiarity with building and running containers.

Users will make use of two applications on Summit - Podman and Singularity - in their container build and run workflow. Both are available without needing to load any modules.",4.23821540820365
"How can I troubleshoot issues with singularity builds on Summit?
","Users will be building and running containers on Summit without root permissions i.e. containers on Summit are rootless.  This means users can get the benefits of containers without needing additional privileges. This is necessary for a shared system like Summit. And this is part of the reason why Docker doesn't work on Summit. Podman and Singularity provides rootless support but to different extents hence why users need to use a combination of both.

Users will need to set up a file in their home directory /ccs/home/<username>/.config/containers/storage.conf with the following content:",4.205507097895151
"How can I ensure that my job is executed efficiently on Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.260801310708552
"How can I ensure that my job is executed efficiently on Andes?
","Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-user-guide.

For a full list of software available at the OLCF, please see the Software section (coming soon).",4.238648994997258
"How can I ensure that my job is executed efficiently on Andes?
",For Andes:,4.2231993365593095
"What is the API version of the Deployment?
","A/B Deployments are a popular way to try a new version of an application with a small subset of users in the production environment. With this strategy, you can specify that the older version gets most of the user requests while a limited fraction of users get sent to the new version. Since you can control the amount of users which get sent to the new version, you can gradually increase the volume of requests to the new version and eventually stop using the old version. Remember that deployment configurations don't do any autoscaling of pods, so you may have to adjust the number of pod",4.185717636547832
"What is the API version of the Deployment?
","Blue-green deployment requires that your application can handle both old and new versions running at the same time. Be sure to think about your application and if it can handle this. For example, if the new version of the software changes how a certain field in a database is read and written, then the old version of the software won't be able to read the database changes, and your production instance could break. This is known as ""N-1 compatibility"" or ""backward compatibility"".",4.177368070359731
"What is the API version of the Deployment?
","apiVersion: apps/v1
kind: Deployment
metadata:
  # deployment name
  name: test-pod-deployment
spec:
  # number of replicas
  replicas: 3
  selector:
    # this sets the label the deployment is looking for
    matchLabels:
      app: test-pod
  template:
    metadata:
      # labels are how the deployments keep track of their objects. This sets a label on the pod
      labels:
        app: test-pod
    spec:
      containers:
        # container name
      - name: test-pod
        # using the base image",4.142210116568737
"Can I use Persistent Volumes to store data for multiple applications?
","By design pods are ephemeral. They are meant to be something that can be killed with relatively little impact to the application. Since containers are ephemeral, we need a way to consume storage that is persistent and can hold data for our applications. One option that Kubernetes has for storing data is with Persistent Volumes. PersistentVolume objects are created by the cluster administrator and reference storage that is on a resilient storage platform such as NetApp. A user can request storage by creating a PersistentVolumeClaim which requests a desired size for a PersistentVolume. The",4.359910056206989
"Can I use Persistent Volumes to store data for multiple applications?
","Now, if the data in the PersistentVolume becomes corrupted or lost in some way, we can create a new PersistentVolume that is identical to the original PersistentVolume at the time the VolumeSnapshot was created.

To do this, create a PersistentVolumeClaim that contains a dataSource field in the Pod's spec. An example follows:",4.258510911744725
"Can I use Persistent Volumes to store data for multiple applications?
","Cloning a persistent volume is just as easy as implementing a snapshot. First, find a Persistent Volume Claim in the same namespace that you would like to clone for your new persistent volume. Then it's as simple as adding the trident.netapp.io/cloneFromPVC annotation with a value of the name of the Persistent Volume Claim you would like to clone.

In the below example, we clone a persistent volume named source-clone-pvc into a new volume called destination-clone-pvc",4.200781898726952
"What is the purpose of the -b flag in the VisIt command?
","the -s flag will launch the CLI in the form of a Python shell, which can be useful for interactive batch jobs.  The -np and -nn flags represent the number of processors and nodes VisIt will use to execute the Python script, while the -l flag specifies the specific parallel method to do so (required). Execute visit -fullhelp to get a list of all command line options.",4.235245662823969
"What is the purpose of the -b flag in the VisIt command?
","For Summit (module):

$ module load visit
$ visit -nowin -cli -v 3.1.4 -l bsub/jsrun -p batch -b XXXYYY -t 00:05 -np 42 -nn 1 -s visit_example.py

Due to the nature of the custom VisIt launcher for Summit, users are unable to solely specify -l jsrun for VisIt to work properly. Instead of manually creating a batch script, as seen in the Andes method outlined below, VisIt submits its own through -l bsub/jsrun. The -t flag sets the time limit, -b specifies the project to be charged, and -p designates the queue the job will submit to.",4.145294237481326
"What is the purpose of the -b flag in the VisIt command?
","All of the above can also be achieved in an interactive batch job through the use of the salloc command on Andes or the bsub -Is command on Summit. Recall that login nodes should not be used for memory- or compute-intensive tasks, including VisIt.",4.066946483525376
"How should users manage their files in the World Archive directory to comply with OLCF Policy?
","this file system for long, but rather should be migrated to Project Home or Project Archive space as soon as the files are not actively being used. If a file system associated with World Work storage is nearing capacity, the OLCF may contact the PI of the project to request that he or she reduce the size of the World Work directory. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for more details on applicable quotas, backups, purge, and retention timeframes.",4.46420994134483
"How should users manage their files in the World Archive directory to comply with OLCF Policy?
","and files are automatically purged on a regular basis. Files should not be retained in this file system for long, but rather should be migrated to Project Home or Project Archive space as soon as the files are not actively being used. If a file system associated with your Member Work directory is nearing capacity, the OLCF may contact you to request that you reduce the size of your Member Work directory. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for more details on applicable quotas, backups, purge, and retention timeframes.",4.442931730925966
"How should users manage their files in the World Archive directory to comply with OLCF Policy?
","Each project is granted a World Archive directory; these reside on the High Performance Storage System (HPSS), OLCF's tape-archive storage system. World Archive areas are shared among all users of the system and are intended for sharing data between projects. HPSS is intended for data that do not require day-to-day access. Users should not store data unrelated to OLCF projects on HPSS. Users should periodically review files and remove unneeded ones. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for more details on applicable quotas, backups, purge,",4.404385694704294
"Can I use conda environments on other OLCF systems besides Summit and Frontier?
","This guide introduces a user to the basic workflow of using conda environments, as well as providing an example of how to create a conda environment that uses a different version of Python than the base environment uses on Summit. Although Summit is being used in this guide, all of the concepts still apply to other OLCF systems. If using Frontier, this assumes you already have installed your own version of conda (e.g., by https://docs.olcf.ornl.gov/software/python/miniconda.html).

OLCF Systems this guide applies to:

Summit

Andes

Frontier (if using conda)",4.621535592672838
"Can I use conda environments on other OLCF systems besides Summit and Frontier?
","This page describes how a user can successfully install specific quantum software tools to run on select OLCF HPC systems. This allows a user to utilize our systems to run a simulator, or even as a ""frontend"" to a vendor's quantum machine (""backend"").

For most of these instructions, we use conda environments. For more detailed information on how to use Python modules and conda environments on OLCF systems, see our :doc:`Python on OLCF Systems page</software/python/index>`, and our https://docs.olcf.ornl.gov/software/python/conda_basics.html.",4.378832189179106
"Can I use conda environments on other OLCF systems besides Summit and Frontier?
","This guide has been adapted for Frontier following venv syntax. If you are using a personal https://docs.olcf.ornl.gov/software/python/miniconda.html, the workflow will be similar to the Summit or Andes conda scenario.  If that is your use-case, then ignore the mention of the cray-python module in the workflow (other modules still apply).

This guide has been adapted from a challenge in OLCF's Hands-On with Summit GitHub repository (Python: CuPy Basics).",4.374948203221978
"How can I start using ArgoCD to deploy and manage Kubernetes resources?
","With a git repository defined in the ArgoCD Repositories settings that has kustomize environments ready for use, one can start using ArgoCD to deploy and manage kubernetes resources.",4.5123626541404604
"How can I start using ArgoCD to deploy and manage Kubernetes resources?
","At this point, it should now be possible to use this git repository for deployment of resources into a project.

By default, OpenShift GitOps will automatically configure the project and add the necessary roles to allow for the deployment of Kubernetes resources to the same project that contains the ArgoCD deployment. If it is desired to manage resources in a project other than where ArgoCD is deployed, please contact the Platforms Group to assistance in configuring the additional projects.

ArgoCD supports multiple methods to deploy Kubernetes resource manifests:",4.451358558273895
"How can I start using ArgoCD to deploy and manage Kubernetes resources?
","Image of the ArgoCD applications tab.

Prior to using ArgoCD, a git repository containing Kubernetes custom resources needs to be added for use.

The git repository containing Kubernetes custom resources is typically not the same git repository used to build the application.

There are three ways for ArgoCD to connect to a git repository:

connect using ssh

connect using https

connect using GitHub App",4.438903638347129
"Are there any workarounds for this issue?
","VisIt developers have been notified, and at this time the current workaround is to disable Scalable Rendering from being used. To do this, go to Options→Rendering→Advanced and set the ""Use Scalable Rendering"" option to ""Never"".

However, this workaround has been reported to affect VisIt's ability to save images, as scalable rendering is utilized to save plots as image files (which can result in another compute engine crash). To avoid this, screen capture must be enabled. Go to File→""Set save options"" and check the box labeled ""Screen capture"".",3.917439835487456
"Are there any workarounds for this issue?
","This may result in a sub-optimal alignment of CPU and GPU on the node, as shown in the example output. Unfortunately, at the moment there is not a workaround for this, however improvements are possible in future SLURM updates.

As mentioned previously, all GPUs are accessible by all MPI ranks by default, so it is possible to programatically map any combination of GPUs to MPI ranks. It should be noted however that Cray MPICH does not support GPU-aware MPI for multiple GPUs per rank, so this binding is not suggested.",3.8970074640925736
"Are there any workarounds for this issue?
",for more information.,3.8849063386524767
"What is the purpose of DDT in Andes?
",One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Andes from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Andes. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.,4.213963466189802
"What is the purpose of DDT in Andes?
",For Andes:,4.189937011097093
"What is the purpose of DDT in Andes?
","One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Andes from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Andes. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.

GDB",4.158739352896276
"Can I use the same command to tag and push my image to the registry?
","Now, find the repository and tag information of the local image you want to add to the registry and tag it accordingly.

$ docker images
REPOSITORY                                TAG                 IMAGE ID            CREATED             SIZE
example:5000/streams                      v3.1.4              fd7673fdbe30        3 weeks ago         1.95GB

The command to tag your image is:

docker tag example:5000/streams:v3.1.4 registry.apps.<cluster>.ccs.ornl.gov/<namespace>/<image>:<tag>

Lastly, the image needs to be pushed to the registry.",4.357623603993803
"Can I use the same command to tag and push my image to the registry?
","Then you can push and pull from the integrated registry. In the following example we will pull busybox:latest from Docker Hub and push it to our namespace in the integrate registry.

$ docker pull busybox:latest
latest: Pulling from library/busybox
ee153a04d683: Pull complete
Digest: sha256:9f1003c480699be56815db0f8146ad2e22efea85129b5b5983d0e0fb52d9ab70
Status: Downloaded newer image for busybox:latest
docker.io/library/busybox:latest

$ docker tag busybox:latest registry.apps.marble.ccs.ornl.gov/stf002platform/busybox:latest",4.272426674159267
"Can I use the same command to tag and push my image to the registry?
","When tagging an image, you must use the format registry.apps.<cluster>.ccs.ornl.gov/<namespace>/<image> where:

Cluster is the name of the OpenShift cluster

Namespace is the name of the Kubernetes namespace you are using (Use oc status to see what OpenShift Project/Kubernetes Namespace you are currently in)

Image is the name of the image you want to push

Once you push the image into the registry, a OpenShift ImageStream will be automatically created",4.265142148870748
"What is the purpose of the OLCF Policy regarding quarterly progress reports?
","Principal Investigators of current OLCF projects must submit a quarterly progress report. The quarterly reports are essential as the OLCF must diligently track the use of the center's resources. In keeping with this, the OLCF (and DOE Leadership Computing Facilities in general) imposes the following penalties for late submission:

| Timeframe | Penalty | | --- | --- | | 1 Month Late | Job submissions against offending project will be suspended. | | 3 Months Late | Login privileges will be suspended for all OLCF resources for all users associated with offending project. |",4.428286274789262
"What is the purpose of the OLCF Policy regarding quarterly progress reports?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  Title: Project Reporting Policy Version: 12.10",4.357793500090871
"What is the purpose of the OLCF Policy regarding quarterly progress reports?
","Access to OLCF resources is limited to approved projects and their users. The type of project (INCITE, ALCC, or Director's Discretion) will determine the application and review procedures. *Quarterly reports are required from industrial Director's Discretion projects only.",4.315219686589224
"How can I ensure that my char type is unsigned by default in Summit?
","type char is unsigned by default

| Vendor | Module | Compiler | Enable C99 | Enable C11 | Default signed char | Define macro | | --- | --- | --- | --- | --- | --- | --- | | IBM | xl | xlc xlc_r | -std=gnu99 | -std=gnu11 | -qchar=signed | -WF,-D | | GNU | system default | gcc | -std=gnu99 | -std=gnu11 | -fsigned-char | -D | | GNU | gcc | gcc | -std=gnu99 | -std=gnu11 | -fsigned-char | -D | | LLVM | llvm | clang | default | -std=gnu11 | -fsigned-char | -D | | PGI | pgi | pgcc | -c99 | -c11 | -Mschar | -D | | NVHPC | nvhpc | nvc | -c99 | -c11 | -Mschar | -D |

type char is unsigned by default",4.184618713856641
"How can I ensure that my char type is unsigned by default in Summit?
","Upon login, the default versions of the XL compiler suite and Spectrum Message Passing Interface (MPI) are added to each user's environment through the modules system. No changes to the environment are needed to make use of the defaults.

Multiple versions of each compiler family are provided, and can be inspected using the modules system:

summit$ module -t avail gcc
/sw/summit/spack-envs/base/modules/site/Core:
gcc/7.5.0
gcc/9.1.0
gcc/9.3.0
gcc/10.2.0
gcc/11.1.0

type char is unsigned by default",4.002022286896759
"How can I ensure that my char type is unsigned by default in Summit?
",For Summit:,3.999373025317405
"Can I disable the sourcing of additional files in /etc in Summit?
","Login and compute resources in the Citadel framework can not access the internet.  This may impact workflows that attempt to access external repositories.

More information on building codes for Citadel including programming environments, compilers, and available software can be found on https://docs.olcf.ornl.gov/systems/index.html#Summit User Guide<summit-user-guide> and https://docs.olcf.ornl.gov/systems/index.html#Frontier User Guide<frontier-user-guide>.",4.089824238024128
"Can I disable the sourcing of additional files in /etc in Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.089153872706155
"Can I disable the sourcing of additional files in /etc in Summit?
","<string>:17: (INFO/1) Duplicate explicit target name: ""summit"".

<string>:17: (INFO/1) Duplicate explicit target name: ""x11 forwarding"".

After logging onto Summit (with X11 forwarding), execute the series of commands below:

$ module load vampir

$ vampir &

Once the GUI pops up (might take a few seconds), you can load a file resident on the file system by selecting Local File for file selection.",4.073722605478087
"What is the purpose of the replicas field in a Deployment?
","On top of Kubernetes ReplicaSets, OpenShift gives us even more support for software lifecycle with Deployments. A Deployment creates a ReplicaSet and has the added benefit of controlling how new deployments get triggered and deployed.

Deployments are sufficient for deploying a production service

Deployments manages ReplicaSets which in turn manages a set of Pods

Below is an example Deployment:",4.1567083007661205
"What is the purpose of the replicas field in a Deployment?
","Below is an example Deployment:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 5
  selector:
    matchLabels:
      name: my-app
  template: { ... }
  strategy:
    type: RollingUpdate
  revisionHistoryLimit: 2
  minReadySeconds: 0
  paused: false

Let's look at the individual parts of this definition, under spec.

replicas - the number of replicas to be passed down to the ReplicaSet

selector - the selector to determine which pods are managed by the ReplicaSet.",4.117895025342509
"What is the purpose of the replicas field in a Deployment?
","A recreate deployment scales the previous deployment down to 0 before starting the new deployment. This is best used when a downtime is acceptable, and your application cannot handle having the old and new versions running at the same time.

Here is an example recreate deployment:

strategy:
  type: Recreate

The recreate strategy follows this sequence:

Scale down the old deployment to 0 replicas.

Scale up the new deployment to the number of desired replicas.",4.09922522074676
"Can I use the Cray compiler wrappers to compile Fortran codes that use OpenMP?
","mpif77 or mpif90 to invoke appropriate versions of the fortran compiler

These wrapper programs are cognizant of your currently loaded modules, and will ensure that your code links against our openmpi installation.  more information about using openmpi at our center can be found in our software documentation.

Compiling threaded codes

When building threaded codes, compiler-specific flags must be included to ensure a proper build.

Openmp

For pgi, add ""-mp"" to the build line.

$ mpicc -mp test.c -o test.x
$ export OMP_NUM_THREADS=2

For gnu, add ""-fopenmp"" to the build line.",4.352859330525631
"Can I use the Cray compiler wrappers to compile Fortran codes that use OpenMP?
","| Vendor | Module | Language | Compiler | OpenMP flag (GPU) | | --- | --- | --- | --- | --- | | Cray | cce | C C++ |  | -fopenmp | | Fortran | ftn (wraps crayftn) |  | | AMD | rocm |  |  | -fopenmp |

If invoking amdclang, amdclang++, or amdflang directly, or using hipcc you will need to add: -fopenmp -target x86_64-pc-linux-gnu -fopenmp-targets=amdgcn-amd-amdhsa -Xopenmp-target=amdgcn-amd-amdhsa -march=gfx90a.

This section shows how to compile HIP codes using the Cray compiler wrappers and hipcc compiler driver.",4.351980471070807
"Can I use the Cray compiler wrappers to compile Fortran codes that use OpenMP?
","This section shows how to compile code with OpenACC. Currently only the Cray compiler supports OpenACC for Fortran. The AMD and GNU programming environments do not support OpenACC at all. C and C++ support for OpenACC is provided by clacc which maintains a fork of the LLVM compiler with added support for OpenACC. It can be obtained by loading the UMS modules ums, ums025, and clacc.",4.339235503372644
"What is the advantage of using Lmod over other module management systems?
","Environment management with lmod

The modules software package allows you to dynamically modify your user environment by using pre-written modulefiles. Environment modules are provided through Lmod, a Lua-based module system for dynamically altering shell environments.  By managing changes to the shell’s environment variables (such as path, ld_library_path, and pkg_config_path), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.",4.417330439525735
"What is the advantage of using Lmod over other module management systems?
","The modules software package allows you to dynamically modify your user environment by using pre-written modulefiles. Environment modules are provided through Lmod, a Lua-based module system for dynamically altering shell environments.  By managing changes to the shell’s environment variables (such as path, ld_library_path, and pkg_config_path), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.",4.3670891482863325
"What is the advantage of using Lmod over other module management systems?
","Environment modules are provided through Lmod, a Lua-based module system for dynamically altering shell environments. By managing changes to the shell’s environment variables (such as PATH, LD_LIBRARY_PATH, and PKG_CONFIG_PATH), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.

The interface to Lmod is provided by the module command:",4.36493741493149
"What is the remote hostname for Summit?
","Summit

**For Summit:**

- **Host nickname**: ``Summit`` (this is arbitrary)
- **Remote hostname**: ``summit.olcf.ornl.gov`` (required)
- **Host name aliases**: ``login#`` (required)
- **Maximum Nodes**: Unchecked
- **Maximum processors**: Unchecked (arbitrary)
- **Path to VisIt Installation**: ``/sw/summit/visit`` (required)
- **Username**: Your OLCF Username (required)
- **Tunnel data connections through SSH**: Checked (required)

Under the “Launch Profiles” tab create a launch profile. Most of these values
are arbitrary",4.2848152431800965
"What is the remote hostname for Summit?
",For Summit:,4.278069544629317
"What is the remote hostname for Summit?
",Figure 1. An example of the NDS servers on Summit,4.228449735667208
"How does the FairShare Scheduling Policy handle projects with multiple users?
","+-------+-------------+-------------+------------------------+----------------------+

| 4     | 126         | 312         | 6.0                    | 0                    |

+-------+-------------+-------------+------------------------+----------------------+

| 5     | 1           | 125         | 2.0                    | 0                    |

+-------+-------------+-------------+------------------------+----------------------+



FairShare Scheduling Policy

---------------------------



FairShare, as its name suggests, tries to push each user and project",4.455270974099805
"How does the FairShare Scheduling Policy handle projects with multiple users?
","towards their fair share of the system's utilization: in this case, 5%

of the system's utilization per user and 10% of the system's utilization

per project. To do this, the job scheduler adds (30) minutes priority

aging per user and (1) hour of priority aging per project for every (1)

percent the user or project is under its fair share value for the prior

(8) weeks. Similarly, the job scheduler subtracts priority in the same

way for users or projects that are over their fair share. For instance,

a user who has personally used 0.0% of the system's utilization over the",4.324246550454792
"How does the FairShare Scheduling Policy handle projects with multiple users?
","agreed to by the following persons as a condition of access to or use of

OLCF computational resources:



-  Principal Investigators (Non-Profit)

-  Principal Investigators (Industry)

-  All Users



**Title:** Titan Scheduling Policy **Version:** 13.02



In a simple batch queue system, jobs run in a first-in, first-out (FIFO)

order. This often does not make effective use of the system. A large job

may be next in line to run. If the system is using a strict FIFO queue,

many processors sit idle while the large job waits to run. *Backfilling*",4.1987269864644015
"How do I optimize the performance of my job on Andes?
","Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-user-guide.

For a full list of software available at the OLCF, please see the Software section (coming soon).",4.278776430572655
"How do I optimize the performance of my job on Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.277216479899713
"How do I optimize the performance of my job on Andes?
",For Andes:,4.237986870268408
"What is the purpose of the CuPy User Guide?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.

This guide teaches you how to build CuPy from source into a custom virtual environment. On Summit and Andes, this is done using conda, while on Frontier this is done using venv.

In this guide, you will:

Learn how to install CuPy

Learn the basics of CuPy

Compare speeds to NumPy

OLCF Systems this guide applies to:

Summit

Frontier

Andes",4.282603624163762
"What is the purpose of the CuPy User Guide?
","You have now discovered what CuPy can provide! Now you can try speeding up your own codes by swapping CuPy and NumPy where you can.

CuPy User Guide

CuPy Website

CuPy API Reference

CuPy Release Notes",4.238164976392676
"What is the purpose of the CuPy User Guide?
","CuPy is a library that implements NumPy arrays on NVIDIA GPUs by utilizing CUDA Toolkit libraries like cuBLAS, cuRAND, cuSOLVER, cuSPARSE, cuFFT, cuDNN and NCCL. Although optimized NumPy is a significant step up from Python in terms of speed, performance is still limited by the CPU (especially at larger data sizes) -- this is where CuPy comes in. Because CuPy's interface is nearly a mirror of NumPy, it acts as a replacement to run existing NumPy/SciPy code on NVIDIA CUDA platforms, which helps speed up calculations further. CuPy supports most of the array operations that NumPy provides,",4.1810501988902935
"What is the purpose of the compute nodes?
","Compute nodes are the appropriate place for long-running, computationally-intensive tasks. When you start a batch job, your batch script (or interactive shell for batch-interactive jobs) runs on one of your allocated compute nodes.

Compute-intensive, memory-intensive, or other disruptive processes running on login nodes may be killed without warning.",4.440146526702955
"What is the purpose of the compute nodes?
","Login nodes

Andes features 8 login nodes which are identical to the batch partition compute nodes.  The login nodes provide an environment for editing, compiling, and launching codes onto the compute nodes. All Andes users will access the system through these same login nodes, and as such, any CPU- or memory-intensive tasks on these nodes could interrupt service to other users. As a courtesy, we ask that you refrain from doing any analysis or visualization tasks on the login nodes.",4.293830731637801
"What is the purpose of the compute nodes?
","Recall from the System Overview that Frontier contains two node types: Login and Compute. When you connect to the system, you are placed on a login node. Login nodes are used for tasks such as code editing, compiling, etc. They are shared among all users of the system, so it is not appropriate to run tasks that are long/computationally intensive on login nodes. Users should also limit the number of simultaneous tasks on login nodes (e.g. concurrent tar commands, parallel make",4.252842186187137
"Is backup enabled for the Project Archive area?
","Member Work, Project Work, and World Work directories are not backed up. Project members are responsible for backing up these files, either to Project Archive areas (HPSS) or to an off-site location.

Moderate projects without export control restrictions are also allocated project-specific archival space on the High Performance Storage System (HPSS). The default quota is shown on the table below. If a higher quota is needed, contact the User Assistance Center.

There is no HPSS storage for Moderate Enhanced Projects, Moderate Projects subject to export control, or Open projects.",4.241807211656193
"Is backup enabled for the Project Archive area?
","Use of User Archive areas for data storage is deprecated as of January 14, 2020. The user archive area for any user account created after that date (or for any user archive directory that is empty of user files after that date) will contain only symlinks to the top-level directories for each of the user's projects on HPSS. Users with existing data in a User Archive directory are encouraged to move that data to an appropriate project-based directory as soon as possible.  The information below is simply for reference for those users with existing data in User Archive directories.",4.202184520531008
"Is backup enabled for the Project Archive area?
","As with the three project work areas, the difference between these three areas lies in the accessibility of data to project members and to researchers outside of the project. Member Archive directories are accessible only by an individual project member by default, Project Archive directories are accessible by all project members, and World Archive directories are readable by any user on the system.

<string>:194: (INFO/1) Duplicate implicit target name: ""permissions"".",4.171793357466079
"How can I track the progress of the bug fix for the TMPDIR environment variable issue?
","Setting the TMPDIR environment variable causes jobs to fail with JSM (jsrun) errors and can also cause jobs to bounce back and forth between eligible and running states until a retry limit has been reached and the job is placed in a blocked state (NOTE: This ""bouncing"" of job state can be caused for multiple reasons. Please see the known issue Jobs suspended due to retry limit / Queued job flip-flops between queued/running states if you are not setting TMPDIR). A bug has been filed with IBM to address this issue.",4.198992879553728
"How can I track the progress of the bug fix for the TMPDIR environment variable issue?
","When TMPDIR is set before submitting a job (i.e., in the shell/environment where a job is submitted from), the job will bounce back and forth between a running and eligible state until its retry limit has been reached and the job will end up in a blocked state. This is true for both interactive jobs and jobs submitted with a batch script, but interactive jobs will hang without dropping you into your interactive shell. In both cases, JSM log files (e.g., jsm-lsf-wait.username.1004985.log) will be created in the location set for TMPDIR containing the same error message as shown above.",4.111563931788375
"How can I track the progress of the bug fix for the TMPDIR environment variable issue?
","When TMPDIR is set within a running job (i.e., in an interactive session or within a batch script), any attempt to call jsrun will lead to a job failure with the following error message:

Error: Remote JSM server is not responding on host batch503-25-2020 15:29:45:920 90012 main: Error initializing RM connection. Exiting.",3.980153094463168
"Are backups enabled for the world work directory on Slate?
","Files within ""Work"" directories (i.e., Member Work, Project Work, World Work) are not backed up and are purged on a regular basis according to the timeframes listed above.",4.167370896174952
"Are backups enabled for the world work directory on Slate?
","Each project has a World Work directory that resides in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. World Work areas can be accessed by all users of the system and are intended for sharing of data between projects. World Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but rather should be",4.137671232411529
"Are backups enabled for the world work directory on Slate?
","The backups are accessed via the .snapshot subdirectory. Note that ls alone (or even ls -a) will not show the .snapshot subdirectory exists, though ls .snapshot will show its contents. The .snapshot feature is available in any subdirectory of your project home directory and will show the online backups available for that subdirectory.

To retrieve a backup, simply copy it into your desired destination with the cp command.",4.100076739255438
"How can a user or project increase their fair share of system utilization under the FairShare Scheduling Policy?
","towards their fair share of the system's utilization: in this case, 5%

of the system's utilization per user and 10% of the system's utilization

per project. To do this, the job scheduler adds (30) minutes priority

aging per user and (1) hour of priority aging per project for every (1)

percent the user or project is under its fair share value for the prior

(8) weeks. Similarly, the job scheduler subtracts priority in the same

way for users or projects that are over their fair share. For instance,

a user who has personally used 0.0% of the system's utilization over the",4.4652443178089625
"How can a user or project increase their fair share of system utilization under the FairShare Scheduling Policy?
","+-------+-------------+-------------+------------------------+----------------------+

| 4     | 126         | 312         | 6.0                    | 0                    |

+-------+-------------+-------------+------------------------+----------------------+

| 5     | 1           | 125         | 2.0                    | 0                    |

+-------+-------------+-------------+------------------------+----------------------+



FairShare Scheduling Policy

---------------------------



FairShare, as its name suggests, tries to push each user and project",4.38195614404744
"How can a user or project increase their fair share of system utilization under the FairShare Scheduling Policy?
","Because of the queuing method described above, users have no set allocation. Job throughput is only limited via the dynamic queue.

There is a time limit on program-wide usage of reservable systems (see below).

In addition to the fair-share queue, users may request a backend reservation for a certain period of time by contacting help@olcf.ornl.gov. If the reservation is granted, the reserved backend will be blocked from general use for a specified period of time, and the user will have sole use of the backend for that period.",4.267266467648788
"How long will my data be retained in my project's archive directory on HPSS?
","Each project is granted a Project Archive directory; these reside on the High Performance Storage System (HPSS), OLCF's tape-archive storage system. Project Archive directories are shared among all members of a project and are intended for sharing data within a project.  HPSS is intended for data that do not require day-to-day access. Users should not store data unrelated to OLCF projects on HPSS. Project members should also periodically review files and remove unneeded ones. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for more details on",4.440784384300901
"How long will my data be retained in my project's archive directory on HPSS?
","Each project is granted a World Archive directory; these reside on the High Performance Storage System (HPSS), OLCF's tape-archive storage system. World Archive areas are shared among all users of the system and are intended for sharing data between projects. HPSS is intended for data that do not require day-to-day access. Users should not store data unrelated to OLCF projects on HPSS. Users should periodically review files and remove unneeded ones. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for more details on applicable quotas, backups, purge,",4.438646736649384
"How long will my data be retained in my project's archive directory on HPSS?
","To keep the Spectrum Scale file system exceptionally performant, files that have not been accessed in the project and user areas are purged at the intervals shown in the table above. Please make sure that valuable data is moved off of these systems regularly. See https://docs.olcf.ornl.gov/systems/policies.html#data-hpss. for information about using the HSI and HTAR utilities to archive data on HPSS. Just to note that when you read a file, then the 90 days counter restarts.",4.434860795659729
"Can Valgrind be used for profiling programs?
","Valgrind

Valgrind is an instrumentation framework for building dynamic analysis tools. There are Valgrind tools that can automatically detect many memory management and threading bugs, and profile your programs in detail. You can also use Valgrind to build new tools.

The Valgrind distribution currently includes five production-quality tools: a memory error detector, a thread error detector, a cache and branch-prediction profiler, a call-graph generating cache profiler, and a heap profiler. It also includes two experimental tools: a data race detector, and an instant memory leak detector.",4.468322371478245
"Can Valgrind be used for profiling programs?
","Valgrind is an instrumentation framework for building dynamic analysis tools. There are Valgrind tools that can automatically detect many memory management and threading bugs, and profile your programs in detail. You can also use Valgrind to build new tools.

The Valgrind distribution currently includes five production-quality tools: a memory error detector, a thread error detector, a cache and branch-prediction profiler, a call-graph generating cache profiler, and a heap profiler. It also includes two experimental tools: a data race detector, and an instant memory leak detector.",4.458252189655164
"Can Valgrind be used for profiling programs?
","Valgrind is an instrumentation framework for building dynamic analysis tools. There are Valgrind tools that can automatically detect many memory management and threading bugs, and profile your programs in detail. You can also use Valgrind to build new tools.

The Valgrind distribution currently includes five production-quality tools: a memory error detector, a thread error detector, a cache and branch-prediction profiler, a call-graph generating cache profiler, and a heap profiler. It also includes two experimental tools: a data race detector, and an instant memory leak detector.",4.458252189655164
"How can I wrap POSIX I/O calls and calculate volume and bandwidth of I/O operations in TAU?
","A similar approach for other metrics, not all of them can be used. TAU provides a tool called tau_cupti_avail, where we can see the list of available metrics, then we have to figure out which CUPTI metrics use these ones.

Activate tracing and declare the data format to OTF2. OTF2 format is supported only by MPI and OpenSHMEM applications.

$ export TAU_TRACE=1
$ export TAU_TRACE_FORMAT=otf2

Use Vampir for visualization.

For example, do not instrument routine sort*(int *)

Create a file select.tau

BEGIN_EXCLUDE_LIST
void sort_#(int *)
END_EXCLUDE_LIST

Declare the TAU_OPTIONS variable",4.162588725091229
"How can I wrap POSIX I/O calls and calculate volume and bandwidth of I/O operations in TAU?
","For Fortran: replace the compiler with the TAU wrapper tau_f90.sh / tau_f77.sh

Even if you don't compile your application with a TAU wrapper, you can profile some basic functionalities with tau_exec, for example:

jsrun -n 4 –r 4 –a 1 –c 1 tau_exec -T mpi ./test

The above command profiles MPI for the binary test, which was not compiled with the TAU wrapper.

The following TAU environment variables may be useful in job submission scripts.",4.162580561304993
"How can I wrap POSIX I/O calls and calculate volume and bandwidth of I/O operations in TAU?
","When a user occupies more than one compute node, then they are using more NVMes and the I/O can scale linearly. For example in the following plot you can observe the scalability of the IOR benchmark on 2048 compute nodes on Summit where the write performance achieves 4TB/s and the read 11.3 TB/s",4.100399651068665
"How does the aging boost work in the OLCF Policy?
","By default, there is no lifetime retention for any data on OLCF resources. The OLCF specifies a limited post-deactivation timeframe during which user and project data will be retained. When the retention timeframe expires, the OLCF retains the right to delete data. If you have data retention needs outside of the default policy, please notify the OLCF.",4.150088958908559
"How does the aging boost work in the OLCF Policy?
","The user data retention policy exists to reclaim storage space after a user account is deactivated, e.g., after the user’s involvement on all OLCF projects concludes. By default, the OLCF will retain data in user-centric storage areas only for a designated amount of time after the user’s account is deactivated. During this time, a user can request a temporary user account extension for data access. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for details on retention timeframes for each user-centric storage area.",4.134347629471289
"How does the aging boost work in the OLCF Policy?
","The project data retention policy exists to reclaim storage space after a project ends. By default, the OLCF will retain data in project-centric storage areas only for a designated amount of time after the project end date. During this time, a project member can request a temporary user account extension for data access. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for details on purge and retention timeframes for each project-centric storage area.",4.132870260792058
"How do I set a breakpoint in GDB on Andes?
","One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Andes from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Andes. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.

GDB",4.372370606161121
"How do I set a breakpoint in GDB on Andes?
","GDB, the GNU Project Debugger, is a command-line debugger useful for traditional debugging and investigating code crashes. GDB lets you debug programs written in Ada, C, C++, Objective-C, Pascal (and many other languages).

GDB is available on andes via the gdb module:

module load gdb

To use GDB to debug your application run:

gdb ./path_to_executable

Additional information about GDB usage can befound on the GDB Documentation Page.",4.306321554027097
"How do I set a breakpoint in GDB on Andes?
","GDB

GDB, the GNU Project Debugger, is a command-line debugger useful for traditional debugging and investigating code crashes. GDB lets you debug programs written in Ada, C, C++, Objective-C, Pascal (and many other languages).

GDB is available on andes via the gdb module:

module load gdb

To use GDB to debug your application run:

gdb ./path_to_executable

Additional information about GDB usage can befound on the GDB Documentation Page.

Valgrind",4.270032363670435
"How do I compile my code with OpenMP support on Andes?
","For gnu, add ""-fopenmp"" to the build line.

$ mpicc -fopenmp test.c -o test.x
$ export OMP_NUM_THREADS=2

For intel, add ""-qopenmp"" to the build line.

$ mpicc -qopenmp test.c -o test.x
$ export OMP_NUM_THREADS=2

For information on running threaded codes, please see the https://docs.olcf.ornl.gov/systems/andes_user_guide.html#andes-thread-layout subsection of the https://docs.olcf.ornl.gov/systems/andes_user_guide.html#andes-running-jobs section in this user guide.",4.405703544009899
"How do I compile my code with OpenMP support on Andes?
","For gnu, add ""-fopenmp"" to the build line.

$ mpicc -fopenmp test.c -o test.x
$ export OMP_NUM_THREADS=2

For intel, add ""-qopenmp"" to the build line.

$ mpicc -qopenmp test.c -o test.x
$ export OMP_NUM_THREADS=2

For information on running threaded codes, please see the https://docs.olcf.ornl.gov/systems/your_file.html#andes-thread-layout subsection of the https://docs.olcf.ornl.gov/systems/your_file.html#andes-running-jobs section in this user guide.



Running Jobs",4.34628727939112
"How do I compile my code with OpenMP support on Andes?
","The OLCF provides hundreds of pre-installed software packages and scientific libraries for your use, in addition to taking software installation requests. See the software page for complete details on existing installs.

Compiling code on andes is typical of commodity or Beowulf-style HPC Linux clusters.

The following compilers are available on Andes:

intel, intel composer xe (default)

pgi, the portland group compilar suite

gcc, the gnu compiler collection",4.2551762603602
"How can I efficiently transfer data between the CPU and GPU on Frontier?
",interconnect to pass data between GPUs as well as from CPU-to-GPU.,4.230702671923138
"How can I efficiently transfer data between the CPU and GPU on Frontier?
","Each Frontier compute node contains 4 AMD MI250X. The AMD MI250X has a peak performance of 53 TFLOPS in double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 26.5 TFLOPS (double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in each direction simultaneously).

To connect to Frontier, ssh to frontier.olcf.ornl.gov. For example:

$ ssh <username>@frontier.olcf.ornl.gov",4.1980583974594445
"How can I efficiently transfer data between the CPU and GPU on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.1831834168658695
"What is the consequence of violating the OLCF Policy?
","authorized individual or investigative agency is in effect during the period of your access to information on a DOE computer and for a period of three years thereafter. OLCF personnel and users are required to address, safeguard against, and report misuse, abuse and criminal activities. Misuse of OLCF resources can lead to temporary or permanent disabling of accounts, loss of DOE allocations, and administrative or legal actions. Users who have not accessed a OLCF computing resource in at least 6 months will be disabled. They will need to reapply to regain access to their account. All users",4.263003088635424
"What is the consequence of violating the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to or use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title:Security Policy Version: 12.10",4.246945239473623
"What is the consequence of violating the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  Title: Project Reporting Policy Version: 12.10",4.243803511275571
"How do I create a Pod in the Slate tutorial?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.426993615059653
"How do I create a Pod in the Slate tutorial?
","From here, in the left hand hamburger menu click on the 'Workloads' tab and then the 'pods' tab:



Here you will be able to view all of the Pods in your Project. Since this is a new Project there will be no Pods in it. To create a  Pod click the 'Create Pod' button.

This will bring you to a screen of pre populated YAML that you can edit in the browser. This YAML is the basis of a podspec that will be sent to the API server once you click the 'Create' button in the lower left to create a  Pod in your Project. Here we will make a few slight modifications to the podspec.",4.303639872343897
"How do I create a Pod in the Slate tutorial?
","command: [""/bin/sh"",""-c""]

args: [""echo 'Hello World!'; cat""]

Finally, we need a tty. This will give us the ability to open a shell in our  Pod and get a better understanding of what is happing. To do this, add the following two lines under the command line that you just added:

tty: true

stdin: true

Your page should now look as follows:



You can now click the 'Create' button in the lower left which will take you to the screen where the   Pod is created.",4.227529451673547
"How can I request to reserve a set of processors for a period of time?
","+------------------------+----------------------+--------------------------+------------------+



System Reservation Policy

-------------------------



Projects may request to reserve a set of processors for a period of time

through the reservation request form, which can be found on the `Special

Requests <https://www.olcf.ornl.gov/support/getting-started/special-request-form/>`__

page. If the reservation is granted, the reserved processors will be

blocked from general use for a given period of time. Only users that",4.454091470034482
"How can I request to reserve a set of processors for a period of time?
","Projects may request to reserve a set of nodes for a period of time by contacting help@olcf.ornl.gov. If the reservation is granted, the reserved nodes will be blocked from general use for a given period of time. Only users that have been authorized to use the reservation can utilize those resources. To access the reservation, please add -U {reservation name} to bsub or job script. Since no other users can access the reserved resources, it is crucial that groups given reservations take care to ensure the utilization on those resources remains high. To prevent reserved resources from remaining",4.234511639368307
"How can I request to reserve a set of processors for a period of time?
","Because of the queuing method described above, users have no set allocation. Job throughput is only limited via the dynamic queue.

There is a time limit on program-wide usage of reservable systems (see below).

In addition to the fair-share queue, users may request a backend reservation for a certain period of time by contacting help@olcf.ornl.gov. If the reservation is granted, the reserved backend will be blocked from general use for a specified period of time, and the user will have sole use of the backend for that period.",4.234477119638934
"Can I share core/gpu resources between job steps when using backgrounded processes in Summit?
","To execute multiple job steps concurrently, standard UNIX process backgrounding can be used by adding a & at the end of the command. This will return control to the job script and execute the next command immediately, allowing multiple job launches to start at the same time. The jsruns will not share core/gpu resources in this configuration. The batch node allocation is equal to the sum of those of each jsrun, and the total walltime must be equal to or greater than that of the longest running jsrun task.",4.218549779634186
"Can I share core/gpu resources between job steps when using backgrounded processes in Summit?
","Summit's V100 GPUs are configured to have a default compute mode of EXCLUSIVE_PROCESS. In this mode, the GPU is assigned to only a single process at a time, and can accept work from multiple process threads concurrently.

It may be desirable to change the GPU's compute mode to DEFAULT, which enables multiple processes and their threads to share and submit work to it simultaneously. To change the compute mode to DEFAULT, use the -alloc_flags gpudefault option.",4.212599464160859
"Can I share core/gpu resources between job steps when using backgrounded processes in Summit?
","Note the ""RAPIDS basic execution"" option is for illustrative purposes and not recommended to run RAPIDS on Summit since it underutilizes resources. If your RAPIDS code is single GPU, consider Jupyter or the concurrent job steps option.

<string>:3: (INFO/1) Duplicate explicit target name: ""simultaneous job steps"".

In cases when a set of time steps need to be processed by single-GPU RAPIDS codes and each time step fits comfortably in GPU memory, it is recommended to execute simultaneous job steps.

The following script provides a general pattern to run job steps simultaneously with RAPIDS:",4.191110471128048
"What is the difference between running an MPI rank on a single GPU and running it on multiple GPUs?
","In the following examples, each MPI rank (and its OpenMP threads) will be mapped to a single GPU.

Example 1: 4 MPI ranks - each with 2 OpenMP threads and 1 GPU (single-node)

This example launches 4 MPI ranks (-n4), each with 2 physical CPU cores (-c2) to launch 2 OpenMP threads (OMP_NUM_THREADS=2) on. In addition, each MPI rank (and its 2 OpenMP threads) should have access to only 1 GPU. To accomplish the GPU mapping, two new srun options will be used:

--gpus-per-task specifies the number of GPUs required for the job on each task",4.433342450393155
"What is the difference between running an MPI rank on a single GPU and running it on multiple GPUs?
","In the following examples, each MPI rank (and its OpenMP threads) will be mapped to a single GPU.

Example 0: 1 MPI rank with 1 OpenMP thread and 1 GPU (single-node)

Somewhat counterintuitively, this common test case is currently among the most difficult. Slurm ignores GPU bindings for nodes with only a single task, so we do not use --gpu-bind here. We must allocate only a single GPU to ensure that only one GPU is available to the task, and since we get the first GPU available we should bind the task to the CPU closest to the allocated GPU.",4.380232482863669
"What is the difference between running an MPI rank on a single GPU and running it on multiple GPUs?
","When a CUDA program begins, each MPI rank creates a separate CUDA context on the GPU, but the scheduler on the GPU only allows one CUDA context (and so one MPI rank) at a time to launch on the GPU. This means that multiple MPI ranks can share access to the same GPU, but each rank gets exclusive access while the other ranks wait (time-slicing). This can cause the GPU to become underutilized if a rank (that has exclusive access) does not perform enough work to saturate the resources of the GPU. The following figure depicts such time-sliced access to a pre-Volta GPU.",4.373535534037181
"How do I specify the project name for my interactive batch job on Summit?
","For Summit (module):

$ module load visit
$ visit -nowin -cli -v 3.1.4 -l bsub/jsrun -p batch -b XXXYYY -t 00:05 -np 42 -nn 1 -s visit_example.py

Due to the nature of the custom VisIt launcher for Summit, users are unable to solely specify -l jsrun for VisIt to work properly. Instead of manually creating a batch script, as seen in the Andes method outlined below, VisIt submits its own through -l bsub/jsrun. The -t flag sets the time limit, -b specifies the project to be charged, and -p designates the queue the job will submit to.",4.265512979679662
"How do I specify the project name for my interactive batch job on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.233772677922839
"How do I specify the project name for my interactive batch job on Summit?
","On Summit and Andes, batch-interactive jobs using bash (i.e. those submitted with bsub -Is or salloc) run as interactive, non-login shells (and therefore source ~/.bashrc, if it exists). Regular batch jobs using bash on those systems are non-interactive, non-login shells and source the file defined by the variable $BASH_ENV in the shell from which you submitted the job. This variable is not set by default, so this means that none of these files will be sourced for a regular batch job unless you explicitly set that variable.",4.217374730599176
"What is the purpose of the hipPrefetchAsync function in Frontier?
","The Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.005506332464027
"What is the purpose of the hipPrefetchAsync function in Frontier?
","The AMD MI250X and operating system on Frontier supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Frontier. In the following section we'll discuss how the heterogenous memory space on a Frontier node is surfaced within your application.",3.9890193769684608
"What is the purpose of the hipPrefetchAsync function in Frontier?
","OLCF Tutorials – Simple HIP Examples

The links below point to event pages from previous Frontier training events. Under the ""Presentations"" tab on each event page, you will find the presentations given during the event.

Frontier Application Readiness Kick-Off Workshop (October 2019)

Please check back to this section regularly as we will continue to add new content for our users.",3.947039183875813
"How can I build CuPy from source?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.

This guide teaches you how to build CuPy from source into a custom virtual environment. On Summit and Andes, this is done using conda, while on Frontier this is done using venv.

In this guide, you will:

Learn how to install CuPy

Learn the basics of CuPy

Compare speeds to NumPy

OLCF Systems this guide applies to:

Summit

Frontier

Andes",4.379315684223816
"How can I build CuPy from source?
","If you wish to use CuPy within a clone of the OpenCE environment, the installation process is very similar to what we do in the regular CuPy installation we saw above.

The open-ce/1.2.0-pyXY-0 (which is the current default) will not support this. So make sure you are using open-ce/1.5.0-pyXY-0 or higher.

The contents of the open-ce module cannot be modified so you need to make your own clone of the open-ce environment.",4.322826391790244
"How can I build CuPy from source?
","Before setting up your environment, you must exit and log back in so that you have a fresh login shell. This is to ensure that no previously activated environments exist in your $PATH environment variable. Additionally, you should execute module reset.

Building CuPy from source is highly sensitive to the current environment variables set in your profile. Because of this, it is extremely important that all the modules and environments you plan to load are done in the correct order, so that all the environment variables are set correctly.",4.25089093995855
"Where can I find sample data for VisIt?
","For sample data and additional examples, explore the VisIt Data Archive and various VisIt Tutorials. Supplementary test data can be found in your local installation in the data directory:

Linux: /path/to/visit/data

macOS: /path/to/VisIt.app/Contents/Resources/data

Windows: C:\path\to\LLNL\VisIt x.y.z\data

Additionally, check out our beginner friendly OLCF VisIt Tutorial which uses Andes to visualize example datasets.",4.5246621437756165
"Where can I find sample data for VisIt?
","Past VisIt Tutorials are available on the Visit User's Wiki along with a set of Updated Tutorials available in the VisIt User Manual.

Sample data not pre-packaged with VisIt can be found in the VisIt Data Archive.

Older VisIt Versions with their release notes can be found on the old VisIt website, and Newer Versions can be found on the new VisIt website with release notes found on the VisIt Blog or VisIt Github Releases page.

Non-ORNL related bugs and issues in VisIt can be found and reported on Github.",4.389092414714961
"Where can I find sample data for VisIt?
","VisIt is now available on Frontier through the UMS022 module.

VisIt 3.3.3 is now available on Andes.

VisIt is an interactive, parallel analysis and visualization tool for scientific data. VisIt contains a rich set of visualization features so you can view your data in a variety of ways. It can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes. Further information regarding VisIt can be found at the links provided in the https://docs.olcf.ornl.gov/systems/visit.html#visit-resources section.",4.237874933955823
"What is the best way to get started with learning HIP programming?
",work to learn HIP. See here for a series of tutorials on programming with HIP and also converting existing CUDA code to HIP with the hipify tools .,4.4375462076206
"What is the best way to get started with learning HIP programming?
","The HIP API is very similar to CUDA, so if you are already familiar with using CUDA, the transition to using HIP should be fairly straightforward. Whether you are already familiar with CUDA or not, the best place to start learning about HIP is this Introduction to HIP webinar that was recently given by AMD:

Introduction to AMD GPU Programming with HIP: (slides | recording)

More useful resources, provided by AMD, can be found here:

HIP Programming Guide

HIP API Documentation

HIP Porting Guide

The OLCF is currently adding some simple HIP tutorials here as well:",4.39780263151596
"What is the best way to get started with learning HIP programming?
","Key features include:

HIP is a thin layer and has little or no performance impact over coding directly in CUDA.

HIP allows coding in a single-source C++ programming language including features such as templates, C++11 lambdas, classes, namespaces, and more.

The “hipify” tools automatically convert source from CUDA to HIP.

Developers can specialize for the platform (CUDA or HIP) to tune for performance or handle tricky cases.",4.225621951609506
"What is the total execution time of the process?
","Explanation:

One process was running as it is a serial application, even MPI calls are executed from a single thread.

The total execution time is 70.733 seconds and only 9 msec are exclusive for the main routine. The rest are caused by subroutines.

The exclusive time is the time caused by the mentioned routine, and the inclusive is with the execution time from the subroutines.

The #Subrs is the number of the called subroutines.

There is also information about the parallel I/O if any exists, the bytes, and the bandwidth.",4.191367401586379
"What is the total execution time of the process?
","The output shows that each independent process ran on its own CPU core and GPU on the same single node. To show that the ranks ran simultaneously, date was called before each job step and a 20 second sleep was added to the end of the hello_jobstep program. So the output also shows that the first job step was submitted at :45 and the subsequent job steps were all submitted between :46 and :52. But because each hello_jobstep sleeps for 20 seconds, the subsequent job steps must have all been running while the first job step was still sleeping (and holding up its resources). And the same argument",4.141176650806645
"What is the total execution time of the process?
","Execute the application as previously shown.

Now you can see the duration of all the loops



Select Options -> Select Metric… -> Exclusive… -> PAPI_TOT_INS/PAPI_TOT_CYC



The loops with less than 1.5 IPC have poor performance and could likely be improved.

Execute the MPI+OpenMP version

Now you can see the duration of parallelfor loops and decide when they should be improved or even removed.



When we instrument the MPI with OpenACC, we can see the following through paraprof

We can observe the duration of the OpenACC calls",4.092088252425794
"How does the sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_active metric relate to tensor core utilization?
","NVIDIA’s Nsight Compute may also be used to measure tensor core utilization via the sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_active metric, as follows:

$ nv-nsight-cu-cli --metrics sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_active ./cudaTensorCoreGemm",4.5307689250334295
"How does the sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_active metric relate to tensor core utilization?
","[  compute_gemm, 2019-Aug-08 12:48:39, Context 1, Stream 7
      Section: Command line profiler metrics
      ----------------------------------------------------------------------
      sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_active                    %                       43.44
      ----------------------------------------------------------------------",4.399089103916581
"How does the sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_active metric relate to tensor core utilization?
","When attempting to use Tensor Cores it is useful to measure and confirm that the Tensor Cores are being used within your code. For implicit use via a library like cuBLAS, the Tensor Cores will only be used above a certain threshold, so Tensor Core use should not be assumed. The NVIDIA Tools provide a performance metric to measure Tensor Core utilization on a scale from 0 (Idle) to 10 (Max) utilization.",4.180100219414953
"What is the best way to learn about VisIt's CLI and GUI?
","Using VisIt via the command line should always result in a batch job, and should always be executed on a compute node -- never the login or launch nodes.

Although most users find better performance following the approach outlined in https://docs.olcf.ornl.gov/systems/visit.html#visit-remote-gui, some users that don't require a GUI may find better performance using VisIt's CLI in a batch job. An example for doing this on OLCF systems is provided below.

For Summit (module):",4.206965120968346
"What is the best way to learn about VisIt's CLI and GUI?
","VisIt uses a client-server architecture. You will obtain the best performance by running the VisIt client on your local computer and running the server on OLCF resources. VisIt for your local computer can be obtained here: VisIt Installation.

Recommended VisIt versions on our systems:

Summit: VisIt 3.1.4

Andes: VisIt 3.3.3

Frontier: VisIt 3.3.3

Using a different version than what is listed above is not guaranteed to work properly.",4.192447657399583
"What is the best way to learn about VisIt's CLI and GUI?
","Past VisIt Tutorials are available on the Visit User's Wiki along with a set of Updated Tutorials available in the VisIt User Manual.

Sample data not pre-packaged with VisIt can be found in the VisIt Data Archive.

Older VisIt Versions with their release notes can be found on the old VisIt website, and Newer Versions can be found on the new VisIt website with release notes found on the VisIt Blog or VisIt Github Releases page.

Non-ORNL related bugs and issues in VisIt can be found and reported on Github.",4.187477642087186
"What is the purpose of the ""xinit &"" command?
","Some users have noticed that their login shells, batch jobs, etc. are not sourcing shell run control files as expected. This is related to the way bash is initialized. The initialization process is discussed in the INVOCATION section of the bash manpage, but is summarized here.

Bash sources different files based on two attributes of the shell: whether or not it's a login shell, and whether or not it's an interactive shell. These attributes are not mutually exclusive (so a shell can be ""interactive login"", ""interactive non-login"", etc.):",4.004858257356739
"What is the purpose of the ""xinit &"" command?
","<string>:17: (INFO/1) Duplicate explicit target name: ""summit"".

<string>:17: (INFO/1) Duplicate explicit target name: ""x11 forwarding"".

After logging onto Summit (with X11 forwarding), execute the series of commands below:

$ module load vampir

$ vampir &

Once the GUI pops up (might take a few seconds), you can load a file resident on the file system by selecting Local File for file selection.",3.997144395668872
"What is the purpose of the ""xinit &"" command?
","Shell Commands

<string>:526: (INFO/1) Duplicate implicit target name: ""shell commands"".

6: This line is left blank, so it will be ignored.

7: This command will change the current directory to the directory from where the script was submitted.

8: This command will run the date command.

9: This command will run (8) MPI instances of the executable a.out on the compute nodes allocated by the batch system.

Batch scripts can be submitted for execution using the sbatch command. For example, the following will submit the batch script named test.slurm:

sbatch test.slurm",3.951779988943548
"What is the name of the GPU architecture that Summit uses?
","Summit node architecture diagram

The basic building block of Summit is the IBM Power System AC922 node. Each of the approximately 4,600 compute nodes on Summit contains two IBM POWER9 processors and six NVIDIA Tesla V100 accelerators and provides a theoretical double-precision capability of approximately 40 TF. Each POWER9 processor is connected via dual NVLINK bricks, each capable of a 25GB/s transfer rate in each direction.",4.35780571609738
"What is the name of the GPU architecture that Summit uses?
","Each Summit Compute node has 6 NVIDIA V100 GPUs.  The NVIDIA Tesla V100 accelerator has a peak performance of 7.8 TFLOP/s (double-precision) and contributes to a majority of the computational work performed on Summit. Each V100 contains 80 streaming multiprocessors (SMs), 16 GB (32 GB on high-memory nodes) of high-bandwidth memory (HBM2), and a 6 MB L2 cache that is available to the SMs. The GigaThread Engine is responsible for distributing work among the SMs and (8) 512-bit memory controllers control access to the 16 GB (32 GB on high-memory nodes) of HBM2 memory. The V100 uses NVIDIA's",4.323144809453217
"What is the name of the GPU architecture that Summit uses?
",| (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_atchley.pdf https://vimeo.com/306002085 | | 2018-11-05 | Programming Methods for Summit's Multi-GPU Nodes | Jeff Larkin & Steve Abbott (NVIDIA) | Programming Methods for Summit's Multi-GPU Nodes https://www.olcf.ornl.gov/calendar/programming-methods-for-summits-multi-gpu-nodes/ | (slides | recording 1 recording 2) https://www.olcf.ornl.gov/wp-content/uploads/2018/11/multi-gpu-workshop.pdf https://vimeo.com/308290719 https://vimeo.com/308290811 | | 2018-06-28 | Intro to OpenACC | Steve Abbott (NVIDIA) |,4.273456156675362
"What is the maximum number of processors available for VisIt on Frontier?
","The above procedure can also be followed to connect to Summit or Frontier, with the main difference being the number of available processors. The time limit syntax for Andes, Summit, and Frontier also differ. Summit uses the format HH:MM while Andes and Frontier follow HH:MM:SS.

Please do not run VisIt's GUI client from an OLCF machine. You will get much better performance if you install a client on your workstation and launch locally. You can directly connect to OLCF machines from inside VisIt and access your data remotely.",4.272177041370576
"What is the maximum number of processors available for VisIt on Frontier?
","VisIt uses a client-server architecture. You will obtain the best performance by running the VisIt client on your local computer and running the server on OLCF resources. VisIt for your local computer can be obtained here: VisIt Installation.

Recommended VisIt versions on our systems:

Summit: VisIt 3.1.4

Andes: VisIt 3.3.3

Frontier: VisIt 3.3.3

Using a different version than what is listed above is not guaranteed to work properly.",4.259664991879944
"What is the maximum number of processors available for VisIt on Frontier?
","VisIt is now available on Frontier through the UMS022 module.

VisIt 3.3.3 is now available on Andes.

VisIt is an interactive, parallel analysis and visualization tool for scientific data. VisIt contains a rich set of visualization features so you can view your data in a variety of ways. It can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes. Further information regarding VisIt can be found at the links provided in the https://docs.olcf.ornl.gov/systems/visit.html#visit-resources section.",4.247845548202774
"What is the name of the fourth device listed in the variable ""devices""?
","The above log messages indicate the type of image required by each device, given its current mode (amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack-) and the images found in the binary (hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+).",3.9304867200183513
"What is the name of the fourth device listed in the variable ""devices""?
","The above log messages indicate the type of image required by each device, given its current mode (amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack-) and the images found in the binary (hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+).",3.927235030561626
"What is the name of the fourth device listed in the variable ""devices""?
","The information below the dashes which we omitted can be occasionally helpful for debugging, say if there is some kind of hardware problem..",3.919699104063472
"What is the retention policy for files on HPSS?
","To keep the Spectrum Scale file system exceptionally performant, files that have not been accessed in the project and user areas are purged at the intervals shown in the table above. Please make sure that valuable data is moved off of these systems regularly. See https://docs.olcf.ornl.gov/systems/policies.html#data-hpss. for information about using the HSI and HTAR utilities to archive data on HPSS. Just to note that when you read a file, then the 90 days counter restarts.",4.346235361240673
"What is the retention policy for files on HPSS?
","The High Performance Storage System (HPSS) at the OLCF provides longer-term storage for the large amounts of data created on the OLCF compute systems. The mass storage facility consists of tape and disk storage components, servers, and the HPSS software. After data is uploaded, it persists on disk for some period of time. The length of its life on disk is determined by how full the disk caches become. When data is migrated to tape, it is done so in a first-in, first-out fashion.",4.310591836320481
"What is the retention policy for files on HPSS?
","Each OLCF user receives an HPSS account automatically. Users can transfer data to HPSS from any OLCF system using the HSI or HTAR utilities. For more information on using HSI or HTAR, see the https://docs.olcf.ornl.gov/systems/user_centric.html#data-hpss .

Each file and directory on HPSS is associated with an HPSS storage allocation. For information on HPSS storage allocations, please visit the https://docs.olcf.ornl.gov/systems/user_centric.html#data-policy section.",4.288399768242601
"How do you optimize the performance of a kernel on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.295201914314321
"How do you optimize the performance of a kernel on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.2799912909946025
"How do you optimize the performance of a kernel on Frontier?
","The Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.230310855356146
"What information is shown in the first window that opens when running paraprof?
","$ paraprof --pack name.ppk
$ paraprof name.ppk &

The first window that opens when the paraprof name.ppk command is executed shows the experiment and the used metrics, for this case, TIME, PAPI_FP_OPS, PAPI_TOT_INS, PAPI_TOT_CYC.



The user is responsible for understanding which PAPI metrics should be used",4.154425934549924
"What information is shown in the first window that opens when running paraprof?
","You can see in the window with the profiling data after you pack them and execute paraprof, the profiling data are not across all the processes, it depends if a routine (color) is executed across all of them or not based on the type of the rourine CPU/GPU.





Select the metric achieved occupancy



Click on the colored bar

The achieved occupancy for this simple benchmark is 6.2%",4.065099363146436
"What information is shown in the first window that opens when running paraprof?
","Select the host in the left side of the window.

Select the ""Launch Profiles"" tab in the right side of the window. This will display the known launch profiles for this host.

Select a ""Launch Profile"" and the settings are displayed in the tabs below.

You can set your Project ID in the ""Default Bank/Account"" field in the ""Parallel"" tab.

You can change the queue used by modifying the ""Partition/Pool/Queue"" field in the ""Parallel"" tab.

Once you have made your changes, press the ""Apply"" button, and then save the settings (Options/Save Settings).",4.045326263876603
"How do I install parallel h5py on my personal conda environment on Frontier?
","This guide has been adapted for Frontier only for a conda workflow. Using the default cray-python module on Frontier does not work with parallel h5py (because Python 3.9 is incompatible). Thus, this guide assumes that you are using a personal https://docs.olcf.ornl.gov/software/python/miniconda.html.

For venv users only interested in installing mpi4py, the pip command in this guide is still accurate.

This guide has been adapted from a challenge in OLCF's Hands-On with Summit GitHub repository (Python: Parallel HDF5).",4.543838781286295
"How do I install parallel h5py on my personal conda environment on Frontier?
","Frontier

.. code-block:: bash

   #!/bin/bash
   #SBATCH -A <PROJECT_ID>
   #SBATCH -J h5py
   #SBATCH -N 1
   #SBATCH -p batch
   #SBATCH -t 0:05:00

   unset SLURM_EXPORT_ENV

   cd $SLURM_SUBMIT_DIR
   date

   module load PrgEnv-gnu
   module load hdf5
   export PATH=""/path/to/your/miniconda/bin:$PATH""

   source activate /ccs/proj/<project_id>/<user_id>/envs/frontier/h5pympi-frontier

   srun -n42 python3 hdf5_parallel.py",4.357545903176179
"How do I install parallel h5py on my personal conda environment on Frontier?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.

This guide teaches you how to build a personal, parallel-enabled version of h5py and how to write an HDF5 file in parallel using mpi4py and h5py.

In this guide, you will:

Learn how to install mpi4py

Learn how to install parallel h5py

Test your build with Python scripts

OLCF Systems this guide applies to:

Summit

Andes

Frontier",4.330926802784044
"What is the maximum number of jobs that can be allocated on Crusher compute nodes?
","Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.",4.413745570678088
"What is the maximum number of jobs that can be allocated on Crusher compute nodes?
","Each Crusher compute node has [2x] 1.92 TB NVMe devices (SSDs) with a peak sequential performance of 5500 MB/s (read) and 2000 MB/s (write). To use the NVMe, users must request access during job allocation using the -C nvme option to sbatch, salloc, or srun. Once the devices have been granted to a job, users can access them at /mnt/bb/<userid>. Users are responsible for moving data to/from the NVMe before/after their jobs. Here is a simple example script:",4.26833338425568
"What is the maximum number of jobs that can be allocated on Crusher compute nodes?
","Preemptable job limits:

| Bin | Min Nodes | Max Nodes | Max Walltime (Hours) | Guaranteed Walltime | | --- | --- | --- | --- | --- | | 4 | 46 | 91 | 24.0 | 6.0 (hours) | | 5 | 1 | 45 | 24.0 | 2.0 (hours) |

If a job in the killable queue does not reach its requested walltime, it will continue to use allocation time with each automatic resubmission until it either reaches the requested walltime during a single continuous run, or is manually killed by the user. Allocations are always charged based on actual compute time used by all jobs.",4.26715241502958
"How do I debug my HIP code?
",work to learn HIP. See here for a series of tutorials on programming with HIP and also converting existing CUDA code to HIP with the hipify tools .,4.329755852043285
"How do I debug my HIP code?
","Key features include:

HIP is a thin layer and has little or no performance impact over coding directly in CUDA.

HIP allows coding in a single-source C++ programming language including features such as templates, C++11 lambdas, classes, namespaces, and more.

The “hipify” tools automatically convert source from CUDA to HIP.

Developers can specialize for the platform (CUDA or HIP) to tune for performance or handle tricky cases.",4.209396374476805
"How do I debug my HIP code?
","The HIP API is very similar to CUDA, so if you are already familiar with using CUDA, the transition to using HIP should be fairly straightforward. Whether you are already familiar with CUDA or not, the best place to start learning about HIP is this Introduction to HIP webinar that was recently given by AMD:

Introduction to AMD GPU Programming with HIP: (slides | recording)

More useful resources, provided by AMD, can be found here:

HIP Programming Guide

HIP API Documentation

HIP Porting Guide

The OLCF is currently adding some simple HIP tutorials here as well:",4.17218934223969
"How can I see a list of all environments available in Conda at OLCF?
","This guide introduces a user to the basic workflow of using conda environments, as well as providing an example of how to create a conda environment that uses a different version of Python than the base environment uses on Summit. Although Summit is being used in this guide, all of the concepts still apply to other OLCF systems. If using Frontier, this assumes you already have installed your own version of conda (e.g., by https://docs.olcf.ornl.gov/software/python/miniconda.html).

OLCF Systems this guide applies to:

Summit

Andes

Frontier (if using conda)",4.315081095277852
"How can I see a list of all environments available in Conda at OLCF?
","This also is a great way to keep track of the locations and names of all other environments that have been created. The current environment is indicated by *.

To see what packages are installed in the active environment, use conda list:

$ conda list",4.265235097292372
"How can I see a list of all environments available in Conda at OLCF?
","$ conda config --show envs_dirs

On OLCF systems, the default location is your $HOME directory. If you plan to frequently create environments in a different location other than the default (such as /ccs/proj/...), then there is an option to add directories to the envs_dirs list.

For example, to track conda environments in a subdirectory called summit in Project Home you would execute:

$ conda config --append envs_dirs /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>/envs/summit",4.250495711457393
"What is the error message displayed in the provided code snippet?
","During handling of the above exception, another exception occurred:",3.989757811240169
"What is the error message displayed in the provided code snippet?
",One way to diagnose hipErrorNoBinaryForGpu messages is to set the environment variable AMD_LOG_LEVEL to 1 or greater:,3.8608712741745874
"What is the error message displayed in the provided code snippet?
",One way to diagnose hipErrorNoBinaryForGpu messages is to set the environment variable AMD_LOG_LEVEL to 1 or greater:,3.8608712741745874
"What is the purpose of the `ssh -4L` command in the instructions?
","Step 3 (terminal 2)

<string>:1418: (INFO/1) Duplicate implicit target name: ""step 3 (terminal 2)"".

In a second terminal on your local system open a tunneling connection following the instructions given by the vnc start-up script:

localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov

andes: ssh -4L 5901:localhost:5901 andes-gpu5

Step 4 (local system)

<string>:1427: (INFO/1) Duplicate implicit target name: ""step 4 (local system)"".",4.076243349782127
"What is the purpose of the `ssh -4L` command in the instructions?
","For Moderate Enhanced Projects, job scripts need to add ""-l"" (""ell"") to the shell specification, similar to interactive usage.",4.064069189872558
"What is the purpose of the `ssh -4L` command in the instructions?
","In a new terminal, open a tunneling connection with andes79.olcf.ornl.gov and port 5901
example:
     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov
     andes: ssh -4L 5901:localhost:5901 andes79

**************************************************************************

MATLAB is selecting SOFTWARE OPENGL rendering.

Step 3 (terminal 2)

In a second terminal on your local system open a tunneling connection following the instructions given by the vnc start-up script:

localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov",4.034757887239725
"What does the ""after"" dependency mean in Slurm?
","Oftentimes, a job will need data from some other job in the queue, but it's nonetheless convenient to submit the second job before the first finishes. Slurm allows you to submit a job with constraints that will keep it from running until these dependencies are met. These are specified with the -d option to Slurm. Common dependency flags are summarized below. In each of these examples, only a single jobid is shown but you can specify multiple job IDs as a colon-delimited list (i.e. #SBATCH -d afterok:12345:12346:12346). For the after dependency, you can optionally specify a +time value for",4.365683717113066
"What does the ""after"" dependency mean in Slurm?
","| Flag | Meaning (for the dependent job) | | --- | --- | | #SBATCH -d after:jobid[+time] | The job can start after the specified jobs start or are canceled. The optional +time argument is a number of minutes. If specified, the job cannot start until that many minutes have passed since the listed jobs start/are canceled. If not specified, there is no delay. | | #SBATCH -d afterany:jobid | The job can start after the specified jobs have ended (regardless of exit state) | | #SBATCH -d afternotok:jobid | The job can start after the specified jobs terminate in a failed (non-zero) state | | #SBATCH",4.077512314576896
"What does the ""after"" dependency mean in Slurm?
","Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.",4.023394306211169
"How do I find the name of my kernel?
","In an application with more than one kernel, you should strongly consider filtering by kernel name by adding a line like: kernel: <kernel_name> to the rocprof input file.",4.179696380321037
"How do I find the name of my kernel?
","In an application with more than one kernel, you should strongly consider filtering by kernel name by adding a line like: kernel: <kernel_name> to the rocprof input file.",4.179696380321037
"How do I find the name of my kernel?
","By default, Nsight Compute will collect this performance data for every kernel in your application. This will take a long time in a real-world application. It is recommended that you identify a specific kernel to profile and then use the -k argument to just profile that kernel. (If you don't know the name of your kernel, use nsys to obtain that. The flag will pattern match on any substring of the kernel name.) You can also use the -s option to skip some number of kernel calls and the -c option to specify how many invocations of that kernel you want to profile.",4.094704515742617
"What is an example of a project that CITADEL supports?
","CITADEL, having undergone comprehensive technical-, legal-, and policy-oriented reviews and received third-party accreditation, has presented new possibilities for research projects that previously could not access utilize OLCF systems due to the nature of their data.

If you are new to the OLCF or the OLCF's SPI, this page can help get you started.  Below are some of the high-level steps needed to begin use of the OLCF's SPI:",4.197659266987037
"What is an example of a project that CITADEL supports?
","The NCCS CITADEL security framework was originally conceived to facilitate the large-scale analysis of protected health information (PHI) data from the US Department of Veterans Affairs' (VA) Million Veteran Program. The NCCS SPI team, with assistance from ORNL Risk Management and ORNL’s Information Technology Services Division (ITSD), refined the initial prototype and expanded CITADEL's capabilities to accommodate a diverse array of programs, projects, and sponsors.",4.175214783358197
"What is an example of a project that CITADEL supports?
","The National Center for Computational Science (NCCS) and the Oak Ridge Leadership Computing Facility (OLCF) have implemented the CITADEL security framework as part of their Scalable Protected Infrastructure (SPI). This infrastructure provides resources and protocols that enable researchers to process protected data at scale. With the CITADEL framework, researchers can use the OLCF’s large HPC resources to compute data containing protected health information (PHI), personally identifiable information (PII), data protected under International Traffic in Arms Regulations, and other types of data",4.171558417378329
"How often are online backups performed?
","If you accidentally delete files from your home directory (/ccs/home/$USER), you may be able to retrieve them. Online backups are performed at regular intervals. Hourly backups for the past 24 hours, daily backups for the last 7 days, and once-weekly backups are available. It is possible that the deleted files are available in one of those backups. The backup directories are named hourly.*, daily.*, and weekly.* where * is the date/time stamp of backup creation. For example, hourly.2020-01-01-0905 is an hourly backup made on January 1st, 2020 at 9:05 AM.",4.2567039430243465
"How often are online backups performed?
","If you accidentally delete files from your project home directory (/ccs/proj/[projid]), you may be able to retrieve them. Online backups are performed at regular intervals.  Hourly backups for the past 24 hours, daily backups for the last 7 days, and once-weekly backups are available. It is possible that the deleted files are available in one of those backups. The backup directories are named hourly.*, daily.*, and weekly.* where * is the date/time stamp of backup creation. For example, hourly.2020-01-01-0905 is an hourly backup made on January 1st, 2020 at 9:05 AM.",4.236066176907961
"How often are online backups performed?
",It is the user’s responsibility to insure the appropriate level of backup and integrity checks on critical data and programs.,4.199165318204088
"How can project delegates and overseers ensure compliance with OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  Title: Project Reporting Policy Version: 12.10",4.346232501531056
"How can project delegates and overseers ensure compliance with OLCF Policy?
","Access to OLCF resources is limited to approved projects and their users. The type of project (INCITE, ALCC, or Director's Discretion) will determine the application and review procedures. *Quarterly reports are required from industrial Director's Discretion projects only.",4.295054408077598
"How can project delegates and overseers ensure compliance with OLCF Policy?
","Principal Investigators of current OLCF projects must submit a quarterly progress report. The quarterly reports are essential as the OLCF must diligently track the use of the center's resources. In keeping with this, the OLCF (and DOE Leadership Computing Facilities in general) imposes the following penalties for late submission:

| Timeframe | Penalty | | --- | --- | | 1 Month Late | Job submissions against offending project will be suspended. | | 3 Months Late | Login privileges will be suspended for all OLCF resources for all users associated with offending project. |",4.269701495283081
"What is the purpose of the ""submit_hello"" batch script in parallel h5py?
","Time to execute ""hdf5_parallel.py"" by submitting ""submit_h5py"" to the batch queue:

Summit

.. code-block:: bash

   $ bsub -L $SHELL submit_h5py.lsf

Andes

.. code-block:: bash

   $ sbatch --export=NONE submit_h5py.sl

Frontier

.. code-block:: bash

   $ sbatch --export=NONE submit_h5py.sl

Example ""submit_h5py"" batch script:

Summit

.. code-block:: bash

   #!/bin/bash
   #BSUB -P <PROJECT_ID>
   #BSUB -W 00:05
   #BSUB -nnodes 1
   #BSUB -J h5py
   #BSUB -o h5py.%J.out
   #BSUB -e h5py.%J.err

   cd $LSB_OUTDIR
   date

   module load gcc
   module load hdf5
   module load python",4.323789082099471
"What is the purpose of the ""submit_hello"" batch script in parallel h5py?
","$ sbatch --export=NONE submit_hello.sl

Example ""submit_hello"" batch script:

Summit

.. code-block:: bash

   #!/bin/bash
   #BSUB -P <PROJECT_ID>
   #BSUB -W 00:05
   #BSUB -nnodes 1
   #BSUB -J mpi4py
   #BSUB -o mpi4py.%J.out
   #BSUB -e mpi4py.%J.err

   cd $LSB_OUTDIR
   date

   module load gcc
   module load hdf5
   module load python

   source activate /ccs/proj/<project_id>/<user_id>/envs/summit/h5pympi-summit

   jsrun -n1 -r1 -a42 -c42 python3 hello_mpi.py

Andes

.. code-block:: bash",4.245608730748891
"What is the purpose of the ""submit_hello"" batch script in parallel h5py?
","cd $SLURM_SUBMIT_DIR
   date

   module load PrgEnv-gnu
   module load hdf5
   export PATH=""/path/to/your/miniconda/bin:$PATH""

   source activate /ccs/proj/<project_id>/<user_id>/envs/frontier/h5pympi-frontier

   srun -n42 python3 hello_mpi.py

If mpi4py is working properly, in mpi4py.<JOB_ID>.out you should see output similar to:

Hello from MPI rank 21 !
Hello from MPI rank 23 !
Hello from MPI rank 28 !
Hello from MPI rank 40 !
Hello from MPI rank 0 !
Hello from MPI rank 1 !
Hello from MPI rank 32 !
.
.
.",4.1183879441062725
"What is the purpose of the `module load nvidia-rapids/21.08` command?
","module load ums
module load ums-gen119
module load nvidia-rapids/21.08

conda create --clone nvrapids_21.08_gcc_9.3.0 -p <my_environment_path>

To activate the new environment you should still load the RAPIDS module first. This will ensure that all of the conda settings remain the same.

module load ums
module load ums-gen119
module load nvidia-rapids/21.08

conda activate <my_environment_path>

Running BlazingSQL multi-gpu/multi-node workloads requires a dask-cuda cluster as explained earlier.",4.279774619300501
"What is the purpose of the `module load nvidia-rapids/21.08` command?
","module load ums
module load ums-gen119
module load nvidia-rapids/21.08

Due different dependecies, cuCIM is available on Summit as a separate module using the next commands:

module load ums
module load ums-gen119
module load nvidia-rapids/cucim_21.08

The RAPIDS and cuCIM modules loads gcc/9.3.0 and cuda/11.0.3 modules. For a complete list of available packages, use conda list command.  After Summit's OS upgrade on August 7th, 2021. Older RAPIDS modules were deprecated.

As an example, the following LSF script will run a single-GPU RAPIDS script in one Summit node:",4.258621855477011
"What is the purpose of the `module load nvidia-rapids/21.08` command?
","#BSUB -P <PROJECT>
#BSUB -W 0:05
#BSUB -nnodes 1
#BSUB -q batch
#BSUB -J rapids_test
#BSUB -o rapids_test_%J.out
#BSUB -e rapids_test_%J.out

module load ums
module load ums-gen119
module load nvidia-rapids/21.08",4.194286141857331
"What step should I take after importing servers in Paraview?
","Although they can be separate files, both Andes and Summit server configurations can be combined and saved into one file following the hierarchy <Servers><Server name= >...<\Server><Server name= >...<\Server><\Servers>.

Step 2: Launch ParaView on your Desktop and Click on File -> Connect

Start ParaView and then select File/Connect to begin.



Step 3: Import Servers

Click Load Servers button and find the servers.pvsc file",4.275823704473874
"What step should I take after importing servers in Paraview?
","After installing, you must give ParaView the relevant server information to be able to connect to OLCF systems (comparable to VisIt's system of host profiles). The following provides an example of doing so. Although several methods may be used, the one described should work in most cases.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.

For Windows clients, it is necessary to install PuTTY to create an ssh connection in step 2.

Step 1: Save the following servers.pvsc file to your local computer",4.226904416710118
"What step should I take after importing servers in Paraview?
","Although in a single machine setup both the ParaView client and server run on the same host, this need not be the case. It is possible to run a local ParaView client to display and interact with your data while the ParaView server runs in an Andes or Summit batch job, allowing interactive analysis of very large data sets.",4.189818321302756
"How do I exit the 'Terminal' tab when I'm finished using it?
","The 'Events' tab is for the events that took place to create your pod. This is for things that happen outside of the code that is running inside your pod such as pulling the pod image, scheduling the pod onto a node etc.

The 'Terminal' tab will give you a tty inside your pod. Here you can run most commands as you normally would on a RedHat machine.",4.108190210510277
"How do I exit the 'Terminal' tab when I'm finished using it?
","Most Terminal applications have built-in shortcuts to directly open web addresses in the default browser.  MacOS Terminal.app: hold Command (⌘) and double-click on the URL  iTerm2: hold Command (⌘) and single-click on the URL

(currently) Compiled with GCC toolchain only

Does not support MPMD-mode via ERF

OpenMP only supported with use of the OMP_NUM_THREADS environment variable.",4.00494381635453
"How do I exit the 'Terminal' tab when I'm finished using it?
","When your job reaches the top of the queue, the main window will be returned to your control. At this point you are connected and can open files that reside there and visualize them interactively.",3.9532038828726352
"What is the purpose of the ""proj-shared"" directory?
","$ ls /gpfs/wolf/[projid]
proj-shared  scratch  world-shared

proj-shared can be accessed by all members of a project.

scratch contains directories for each user of a project and only that user can access their own directory.

world-shared can be accessed by any users on the system in any project.

Ascent is a training system that is not intended to be used as an OLCF user resource. Access to the system is only obtained through OLCF training events.",4.243210562357555
"What is the purpose of the ""proj-shared"" directory?
","fast, batch job access that's shared with other project members | Project Work | /gpfs/alpine/[projid]/proj-shared | | Short-term project data for fast, batch job access that's shared with those outside your project | World Work | /gpfs/alpine/[projid]/world-shared | | Long-term project data for archival access that you don't want to share | Member Archive | /hpss/prod/[projid]/users/$USER | | Long-term project data for archival access that's shared with other project members | Project Archive | /hpss/prod/[projid]/proj-shared | | Long-term project data for archival access that's shared with",4.235904331484906
"What is the purpose of the ""proj-shared"" directory?
","Each project is granted a Project Work directory; these reside in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. Project Work directories can be accessed by all members of a project and are intended for sharing data within a project. Project Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but",4.158691038711137
"How can I see the status of my job?
","The most straightforward monitoring is with the bjobs command. This command will show the current queue, including both pending and running jobs. Running bjobs -l will provide much more detail about a job (or group of jobs). For detailed output of a single job, specify the job id after the -l. For example, for detailed output of job 12345, you can run bjobs -l 12345 . Other options to bjobs are shown below. In general, if the command is specified with -u all it will show information for all users/all jobs. Without that option, it only shows your jobs. Note that this is not an exhaustive list.",4.174611410371911
"How can I see the status of my job?
","bjobs and jobstat help to identify what’s currently running and scheduled to run, but sometimes it’s beneficial to know how much of the system is not currently in use or scheduled for use.",4.152353350971662
"How can I see the status of my job?
","When your job reaches the top of the queue, the main window will be returned to your control. At this point you are connected and can open files that reside there and visualize them interactively.",4.109741472053579
"What is the benefit of having an OLCF Moderate account?
","myOLCF is currently available to OLCF Moderate user accounts; i.e., users that authenticate to OLCF systems with an RSA SecurID token. Visit https://my.olcf.ornl.gov and authenticate with your OLCF Moderate username and RSA SecurID PASSCODE (PIN followed by the 6-digit tokencode).

The myOLCF login page

OLCF Open user accounts, i.e., users that authenticate to OLCF systems with a password, cannot access myOLCF at this time, as we are still investigating the feasibility of supporting password-only authentication.",4.3061292035375685
"What is the benefit of having an OLCF Moderate account?
","If you already have a user account at the OLCF, your existing credentials can be leveraged across multiple projects.

If your user account has an associated RSA SecurID (i.e. you have an ""OLCF Moderate"" account), you gain access to another project by logging in to the myOLCF self-service portal and filling out the application under My Account > Join Another Project. For more information, see the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#myOLCF self-service portal documentation<myolcf-overview>.",4.29303601999369
"What is the benefit of having an OLCF Moderate account?
","At any time, you can view account pages by clicking on the ""My Account"" link in the top navigation menu:

link to my account page

There is only (1) account context in myOLCF: ""you"" as the currently-authenticated user. This account context is linked to the OLCF Moderate account that you used to authenticate to myOLCF.

account page left navigation menu

The left navigation menu also includes an expandable item with links to account-centric pages.",4.261583938934332
"How do I enable C11 with GCC on Summit?
","Using the hip-cuda/5.1.0 module on Summit requires CUDA v11.4.0 or later. However, only CUDA v11.0.3 and earlier currently support GPU-aware MPI, so GPU-aware MPI is currently not available when using HIP on Summit.

Any codes compiled with post 11.0.3 CUDA (cuda/11.0.3) will not work with MPS enabled (-alloc_flags ""gpumps"") on Summit. The code will hang indefinitely. CUDA v11.0.3 is the latest version that is officially supported by IBM's software stack installed on Summit. We are continuing to look into this issue.",4.180901834513259
"How do I enable C11 with GCC on Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.164542607306757
"How do I enable C11 with GCC on Summit?
","The E4S software list is installed along side the existing software on Summit and can be access via lmod modulefiles.

To access the installed software, load the desired compiler via:

module load < compiler/version >
.. ie ..
module load gcc/9.1.0
.. or ..
module load gcc/7.5.0

Then use module avail to see the installed list of packages.

List of installed packages on Summit for E4S release 21.08:",4.134002045872756
"Can I use Slurm's job scheduling features to run my interactive batch job for a specific duration?
","Since all compute resources are managed and scheduled by Slurm, it is not possible to simply log into the system and immediately begin running parallel codes interactively. Rather, you must request the appropriate resources from Slurm and, if necessary, wait for them to become available. This is done through an ""interactive batch"" job. Interactive batch jobs are submitted with the salloc command. Resources are requested via the same options that are passed via #SBATCH in a regular batch script (but without the #SBATCH prefix). For example, to request an interactive batch job with the same",4.364944251530773
"Can I use Slurm's job scheduling features to run my interactive batch job for a specific duration?
","sbatch test.slurm

If successfully submitted, a Slurm job ID will be returned. This ID can be used to track the job. It is also helpful in troubleshooting a failed job; make a note of the job ID for each of your jobs in case you must contact the OLCF User Assistance Center for support.



Interactive Batch Jobs on Commodity Clusters

Batch scripts are useful when one has a pre-determined group of commands to execute, the results of which can be viewed at a later time. However, it is often necessary to run tasks on compute resources interactively.",4.31881659167807
"Can I use Slurm's job scheduling features to run my interactive batch job for a specific duration?
","Use the sbatch --test-only command to see when a job of a specific size could be scheduled. For example, the snapshot below shows that a (2) node job would start at 10:54.

$ sbatch --test-only -N2 -t1:00:00 batch-script.slurm

  sbatch: Job 1375 to start at 2019-08-06T10:54:01 using 64 processors on nodes andes[499-500] in partition batch

The queue is fluid, the given time is an estimate made from the current queue state and load. Future job submissions and job completions will alter the estimate.



The following table summarizes frequently-used options to Slurm:",4.31696785246478
"How can I check for load imbalance across processes in TAU?
","The second window that is automatically loaded shows the TIME metric for each process (they are called ""nodes"") where each color is a different call. Each horizontal line is a process or Std.Dev./mean/max/min. The length of each color is related to the metric, if it is TIME, it is duration.



Select Options -> Uncheck Stack Bars Together

It is easier to check the load imbalance across the processes



If you click on any color, then a new window opens with information about the specific routine.",4.1369375932629415
"How can I check for load imbalance across processes in TAU?
","Below, we'll look at using TAU to profile each case.

Edit the cmake_summit_pgi.sh and replace mpic++ with tau_cxx.sh. This applies only for the non-GPU versions.

TAU works with special TAU makefiles to declare what programming models are expected from the application:

The available makefiles are located inside TAU installation:",4.108647711913004
"How can I check for load imbalance across processes in TAU?
","For Fortran: replace the compiler with the TAU wrapper tau_f90.sh / tau_f77.sh

Even if you don't compile your application with a TAU wrapper, you can profile some basic functionalities with tau_exec, for example:

jsrun -n 4 –r 4 –a 1 –c 1 tau_exec -T mpi ./test

The above command profiles MPI for the binary test, which was not compiled with the TAU wrapper.

The following TAU environment variables may be useful in job submission scripts.",4.074800043342862
"Can I enable CUDA-aware MPI and GPUDirect for all jobs on Summit?
","GPUDirect is a technology that can be implemented on a system to enhance CUDA-aware MPI by allowing data transfers directly between GPUs on the same node (peer-to-peer) and/or directly between GPUs on different nodes (with RDMA support) without the need to stage data through CPU memory. On Summit, both peer-to-peer and RDMA support are implemented. To enable CUDA-aware MPI in a job, use the following argument to jsrun:

jsrun --smpiargs=""-gpu"" ...",4.405053949306131
"Can I enable CUDA-aware MPI and GPUDirect for all jobs on Summit?
","Using the hip-cuda/5.1.0 module on Summit requires CUDA v11.4.0 or later. However, only CUDA v11.0.3 and earlier currently support GPU-aware MPI, so GPU-aware MPI is currently not available when using HIP on Summit.

Any codes compiled with post 11.0.3 CUDA (cuda/11.0.3) will not work with MPS enabled (-alloc_flags ""gpumps"") on Summit. The code will hang indefinitely. CUDA v11.0.3 is the latest version that is officially supported by IBM's software stack installed on Summit. We are continuing to look into this issue.",4.38831064443749
"Can I enable CUDA-aware MPI and GPUDirect for all jobs on Summit?
","https://vimeo.com/306436688 | | 2018-12-04 | GPU Direct, RDMA, CUDA-Aware MPI | Steve Abbott (NVIDIA) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_CUDA-Aware-MPI.pdf https://vimeo.com/306436248 | | 2018-12-04 | CUDA Unified Memory | Jeff Larkin (NVIDIA) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_UVM.pdf",4.351580915658168
"Can kernels compiled with ""hipcc --amdgpu-target=gfx90a:xnack-"" run on GPUs with XNACK enabled?
","Although XNACK is a capability of the MI250X GPU, it does require that kernels be able to recover from page faults. Both the ROCm and CCE HIP compilers will default to generating code that runs correctly with both XNACK enabled and disabled. Some applications may benefit from using the following compilation options to target specific XNACK modes.

hipcc --amdgpu-target=gfx90a or CC --offload-arch=gfx90a -x hip

Kernels are compiled to a single ""xnack any"" binary, which will run correctly with both XNACK enabled and XNACK disabled.",4.623155417838096
"Can kernels compiled with ""hipcc --amdgpu-target=gfx90a:xnack-"" run on GPUs with XNACK enabled?
","Although XNACK is a capability of the MI250X GPU, it does require that kernels be able to recover from page faults. Both the ROCm and CCE HIP compilers will default to generating code that runs correctly with both XNACK enabled and disabled. Some applications may benefit from using the following compilation options to target specific XNACK modes.

hipcc --amdgpu-target=gfx90a or CC --offload-arch=gfx90a -x hip

Kernels are compiled to a single ""xnack any"" binary, which will run correctly with both XNACK enabled and XNACK disabled.",4.623155417838096
"Can kernels compiled with ""hipcc --amdgpu-target=gfx90a:xnack-"" run on GPUs with XNACK enabled?
","hipcc --amdgpu-target=gfx90a:xnack+ or CC --offload-arch=gfx90a:xnack+ -x hip

Kernels are compiled in ""xnack plus"" mode and will only be able to run on GPUs with HSA_XNACK=1 to enable XNACK. Performance may be better than ""xnack any"", but attempts to run with XNACK disabled will fail.

hipcc --amdgpu-target=gfx90a:xnack- or CC --offload-arch=gfx90a:xnack- -x hip

Kernels are compiled in ""xnack minus"" mode and will only be able to run on GPUs with HSA_XNACK=0 and XNACK disabled. Performance may be better than ""xnack any"", but attempts to run with XNACK enabled will fail.",4.599952844320016
"How can I transfer the report file from Summit to my local machine?
","If you add the -o option, as above, the report will be saved to file with the extension .qdrep. That report file can later be analyzed in the Nsight Systems UI by selecting File > Open and locating the vectorAdd.qdrep file on your filesystem. Nsight Systems does not currently have a Power9 version of the UI, so you will need to download the UI for your local system, which is supported on Windows, Mac, and Linux (x86). Then use scp or some other file transfer utility for copying the report file from Summit to your local machine.",4.244343061651693
"How can I transfer the report file from Summit to my local machine?
",For Summit:,4.116361358142014
"How can I transfer the report file from Summit to my local machine?
","When the installation has finished, click on the Globus icon and select Web: Transfer Files as below



Globus will ask you to login. If your institution does not have an organizational login, you may choose to either Sign in with Google or Sign in with ORCiD iD.



In the main Globus web page, select the two-panel view, then set the source and destination endpoints. (Left/Right order does not matter)



Next, navigate to the appropriate source and destination paths to select the files you want to transfer. Click the ""Start"" button to begin the transfer.",4.112897388094913
"How can I run my instrumented code on Summit using Score-P?
","To instrument your code, you need to compile the code using the Score-P instrumentation command (scorep), which is added as a prefix to your compile statement. In most cases the Score-P instrumentor is able to automatically detect the programming paradigm from the set of compile and link options given to the compiler. Some cases will, however, require some additional link options within the compile statement e.g. CUDA instrumentation.

Below are some basic examples of the different instrumentation scenarios:",4.4382668391794216
"How can I run my instrumented code on Summit using Score-P?
","Login to https://docs.olcf.ornl.gov/systems/Scorep.html#Summit <connecting-to-olcf>: ssh <user_id>@summit.olcf.ornl.gov

Instrument your code with Score-P

Perform a measurement run with profiling enabled

Perform a profile analysis with CUBE or cube_stat

Use scorep-score to define a filter

Perform a measurement run with tracing enabled and the filter applied

Perform in-depth analysis on the trace data with Vampir",4.4368524721586375
"How can I run my instrumented code on Summit using Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations. It works in combination with Periscope, Scalasca, Vampir, and Tau.

Steps in a typical Score-P workflow to run on https://docs.olcf.ornl.gov/systems/Scorep.html#summit-user-guide:",4.361077623119617
"How can I view per MPI rank, per routine, the exclusive time and the floating operations in TAU?
","Click on the new metric, ""PAPI_TOT_INS / PAPI_TOT_CYC"" to see the instructions per cycle (IPC) across the various routines.



Click on the label mean:



For the non-MPI routines/calls, an IPC that is lower than 1.5 means that there is a potential for performance improvement.

Menu Windows -> 3D Visualization (3D demands OpenGL) will not work on Summit, and you will need to download the data on your laptop and install TAU locally to use this feature.

You can see per MPI rank, per routine, the exclusive time and the floating operations.",4.399109423200949
"How can I view per MPI rank, per routine, the exclusive time and the floating operations in TAU?
","Change the PAPI_FP_OPS to (PAPI_TOT_INS/PAPI_TOT_CYC)

You can see per MPI rank, per routine, the exclusive time and the corresponding IPC.



Create a file called, for example, select.tau with the content:

BEGIN_INSTRUMENT_SECTION
loops routine=""#""
END_INSTRUMENT_SECTION

Then declare the options in your submission script:

export TAU_OPTIONS=""-optTauSelectFile=select.tau -optLinking=lpnetcdf -optVerbose""

The linking option is required for this application, but may not be for all applications.

Do not forget to unset TAU_OPTIONS when it's not necessary.",4.35356922938937
"How can I view per MPI rank, per routine, the exclusive time and the floating operations in TAU?
","From the main window right click one label and select “Show User Event Statistics Window”. Then, we can see the data transfered to the devices



The CUDA Profiling Tools Interface (CUPTI) is used by profiling and tracing tools that target CUDA applications.



Matrix multiplication with MPI+OpenMP:

$ export TAU_METRICS=TIME,achieved_occupancy
$ jsrun -n 2 -r 2 -g 1  tau_exec -T mpi,pdt,papi,cupti,openmp -ompt -cupti  ./add

We choose to use tau_exec with MPI, PDT, PAPI, CUPTI, and OpenMP.

Output directories:",4.296628581250169
"What is the purpose of SLURM core specialization on Frontier?
","This example assumes the use of a core specialization of -S 0.  Because Frontier's default core specialization (-S 8) reserves the first core in each L3 region, the ""packed"" mode can be problematic because the 7 cores available in each L3 region won't necessarily divide evenly. This can lead to tasks potentially spanning multiple L3 regions with its assigned cores, which creates problems when Slurm tries to assign GPUs to a given task.",4.393136225680834
"What is the purpose of SLURM core specialization on Frontier?
","This example assumes the use of a core specialization of -S 0.  Because Frontier's default core specialization (-S 8) reserves the first core in each L3 region, the ""packed"" mode can be problematic because the 7 cores available in each L3 region won't necessarily divide evenly. This can lead to tasks potentially spanning multiple L3 regions with its assigned cores, which creates problems when Slurm tries to assign GPUs to a given task.",4.393136225680834
"What is the purpose of SLURM core specialization on Frontier?
","By default, Frontier reserves the first core in each L3 cache region. Frontier uses low-noise mode, which constrains all system processes to core 0. Low-noise mode cannot be disabled by users. In addition, Frontier uses SLURM core specialization (-S 8 flag at job allocation time, e.g., sbatch) to reserve one core from each L3 cache region, leaving 56 allocatable cores. Set -S 0 at job allocation to override this setting.",4.360086114987395
"How can I connect to Andes from my local system?
","In a new terminal, open a tunneling connection with andes79.olcf.ornl.gov and port 5901
example:
     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov
     andes: ssh -4L 5901:localhost:5901 andes79

**************************************************************************

MATLAB is selecting SOFTWARE OPENGL rendering.

In a second terminal on your local system open a tunneling connection following the instructions given by the vnc start-up script:

localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov

andes: ssh -4L 5901:localhost:5901 andes79",4.282863415209526
"How can I connect to Andes from my local system?
",For Andes:,4.276162915725722
"How can I connect to Andes from my local system?
","In a new terminal, open a tunneling connection with andes79.olcf.ornl.gov and port 5901
example:
     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov
     andes: ssh -4L 5901:localhost:5901 andes79

**************************************************************************

MATLAB is selecting SOFTWARE OPENGL rendering.

Step 3 (terminal 2)

In a second terminal on your local system open a tunneling connection following the instructions given by the vnc start-up script:

localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov",4.270122544188026
"How can I get a remote shell in a pod in Slate?
","Finally, to get a remote shell in the pod we run the oc rsh <POD_NAME> command. This will default to using /bin/sh in the pod. If a different shell is required, we can provide the optional --shell=/path/to/shell flag. For example, if we wanted to open a bash shell in the pod we would run the following command:

oc rsh --shell='/bin/bash' <POD_NAME>

If you have multiple containers in your pod, the oc rsh <POD_NAME> command will default to the first container. If you would like to start a remote shell in one of the other containers, you can use the optional -c <CONTAINER_NAME> flag.",4.367568964192275
"How can I get a remote shell in a pod in Slate?
","command: [""/bin/sh"",""-c""]

args: [""echo 'Hello World!'; cat""]

Finally, we need a tty. This will give us the ability to open a shell in our  Pod and get a better understanding of what is happing. To do this, add the following two lines under the command line that you just added:

tty: true

stdin: true

Your page should now look as follows:



You can now click the 'Create' button in the lower left which will take you to the screen where the   Pod is created.",4.282631425613456
"How can I get a remote shell in a pod in Slate?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.187625177001275
"What is the purpose of the `singularity exec` command in the Dockerfile?
","Singularity is a container framework from Sylabs. On Summit, we will use Singularity solely as the runtime. We will convert the tar files of the container images Podman creates into sif files, store those sif files on GPFS, and run them with Jsrun. Singularity also allows building images but ordinary users cannot utilize that on Summit due to additional permissions not allowed for regular users.",4.084157243082486
"What is the purpose of the `singularity exec` command in the Dockerfile?
","Users can build container images with Podman, convert it to a SIF file, and run the container using the Singularity runtime. This page is intended for users with some familiarity with building and running containers.

Users will make use of two applications on Summit - Podman and Singularity - in their container build and run workflow. Both are available without needing to load any modules.",4.079744211079362
"What is the purpose of the `singularity exec` command in the Dockerfile?
","You can now create a Singularity sif file with singularity build --disable-cache --docker-login simple.sif docker://docker.io/<username>/simple.

This will ask you to enter your Docker username and password again for Singularity to download the image from Dockerhub and convert it to a sif file.",4.066185651187496
"How do I ensure that my parallel job runs on a compute node on Summit?
","to service nodes on that system). All commands within your job script (or the commands you run in an interactive job) will run on a launch node. Like login nodes, these are shared resources so you should not run multiprocessor/threaded programs on Launch nodes. It is appropriate to launch the jsrun command from launch nodes. | | Compute | Most of the nodes on Summit are compute nodes. These are where your parallel job executes. They're accessed via the jsrun command. |",4.369609486687386
"How do I ensure that my parallel job runs on a compute node on Summit?
","Recall from the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#system-overview section that Summit has three types of nodes: login, launch, and compute. When you log into the system, you are placed on a login node. When your https://docs.olcf.ornl.gov/systems/summit_user_guide.html#batch-scripts or https://docs.olcf.ornl.gov/systems/summit_user_guide.html#interactive-jobs run, the resulting shell will run on a launch node. Compute nodes are accessed via the jsrun command. The jsrun command should only be issued from within an LSF job (either batch or interactive) on a launch node.",4.333960725848792
"How do I ensure that my parallel job runs on a compute node on Summit?
","(either batch or interactive) on a launch node. Otherwise, you will not have any compute nodes allocated and your parallel job will run on the login node. If this happens, your job will interfere with (and be interfered with by) other users' login node tasks. jsrun is covered in-depth in the Job Launcher (jsrun) section.",4.301367865008522
"Can I access a deployment in Slate using the ""oc port-forward"" command?
","Both the web UI and the API endpoint for the oc client are exposed outside of ORNL. However, you must log in with NCCS USERNAME AND PASSWORD rather than NCCS Single Sign On on the Web UI.

For production workloads, it is recommended to learn about https://docs.olcf.ornl.gov/systems/port_forwarding.html#services <slate_services> and https://docs.olcf.ornl.gov/systems/port_forwarding.html#routes <slate_routes> in order to gain access to your internal resources.

However, for testing and development, oc port-forward can be a powerful tool for quick access to internal cluster resources.",4.399738658970869
"Can I access a deployment in Slate using the ""oc port-forward"" command?
","Additionally, oc port-forward doesn't have to be given a pod name. This tool is aware of services and deployments as well. If you had a service called nginx-svc and a deployment called nginx, for example, the following commands would achieve the same result:

oc port-forward deployment/nginx 7777:8080
oc port-forward svc/nginx-svc 7777:8080

You will be forwarded to any of the pods matched by the service or deployment.

Furthermore, this doesn't only work for http traffic. You could also access other exposed services such as databases.",4.355271505469678
"Can I access a deployment in Slate using the ""oc port-forward"" command?
","For example, let's look at service that was created in the https://docs.olcf.ornl.gov/systems/nodeport.html#slate_services document. This document assumes that it was deployed to the my-project project.

If you run oc get services, you should see your service in the list.

$ oc get services
NAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)     AGE
my-service            ClusterIP   172.25.170.246   <none>        8080/TCP    8s

Then, get some information about the service with oc describe service my-service.",4.320706470997476
"What is the difference between ""docker pull"" and ""docker pull latest""?
","Then you can push and pull from the integrated registry. In the following example we will pull busybox:latest from Docker Hub and push it to our namespace in the integrate registry.

$ docker pull busybox:latest
latest: Pulling from library/busybox
ee153a04d683: Pull complete
Digest: sha256:9f1003c480699be56815db0f8146ad2e22efea85129b5b5983d0e0fb52d9ab70
Status: Downloaded newer image for busybox:latest
docker.io/library/busybox:latest

$ docker tag busybox:latest registry.apps.marble.ccs.ornl.gov/stf002platform/busybox:latest",4.14778758837619
"What is the difference between ""docker pull"" and ""docker pull latest""?
","The -t flag names the container image and the -f flag indicates the file to use for building the image.

Run podman image ls to see the list of images. localhost/simple should be among them. Any container created without an explicit url to a container registry in its name will automatically have the localhost prefix.

$ podman image ls
REPOSITORY             TAG      IMAGE ID      CREATED      SIZE
localhost/simple       latest   e47dbfde3e99  3 hours ago  687 MB
quay.io/centos/centos  stream8  ad6f8b5e7f64  8 days ago   497 MB",4.097274842127093
"What is the difference between ""docker pull"" and ""docker pull latest""?
","Check if your image is created

$ podman image ls
REPOSITORY                         TAG      IMAGE ID      CREATED      SIZE
docker.io/subilabrahamornl/simple  latest   e47dbfde3e99  3 hours ago  687 MB
localhost/simple                   latest   e47dbfde3e99  3 hours ago  687 MB
quay.io/centos/centos              stream8  ad6f8b5e7f64  8 days ago   497 MB

Run podman login docker.io and enter your account's username and password so that Podman is logged in to the container registry before pushing.

Push the container image to the registry with podman push docker.io/<username>/simple.",4.057841959292071
"How do I create an account on IBM's quantum computing platform?
","After submitting the OLCF quantum account application and receiving approval, proceed to https://quantum-computing.ibm.com/ and click on ""Create an IBMid account"". Your IBM Quantum Hub account email will be the email associated with your OLCF account. If sign-in fails, contact help@olcf.ornl.gov. Once logged in, users will have access to the IBM Quantum Hub, IBM’s online platform for QPU access, forums for quantum computing discussion, etc. From the IBM Quantum Hub Dashboard, users can manage system reservations, view system (backend) statuses, and view the results of your past jobs. More",4.508721588017922
"How do I create an account on IBM's quantum computing platform?
","Users can access information about IBM Quantum's systems, view queue information, and submit jobs on their cloud dashboard at https://quantum-computing.ibm.com. The cloud dashboard allows access to IBM Quantum's graphical circuit builder, Quantum Composer, IBM's Quantum Lab, and associated program examples.  A Jupyterlab server is provisioned with IBM Quantum's Qiskit programming framework for job submission.",4.405787266741098
"How do I create an account on IBM's quantum computing platform?
","After submitting the OLCF quantum account application and receiving approval, you will receive an email from Quantinuum inviting you to create your quantum account. Once logged in, users will have access to Quantinuum's User Interface, https://um.qapi.quantinuum.com, their online platform for managing jobs and accessing the available quantum systems, including the System Model H1, via the cloud. From the UI, users can view system status and upcoming system availability, as well as monitor batch submissions and job history. Information on using the quantum resources via Jupyter notebooks is",4.402854537028054
"What are some ways to efficiently use Summit's resources?
",For Summit:,4.385149496959568
"What are some ways to efficiently use Summit's resources?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.333175501162407
"What are some ways to efficiently use Summit's resources?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.220457221776824
"What is the relationship between arithmetic intensity and performance in Frontier?
","theoretical peak is determined by the hardware specifications and is not attainable in practice. attaiable peak is the performance as measured by in-situ microbenchmarks designed to best utilize the hardware. achieved performance is what the profiled application actually achieves.

The theoretical roofline can be constructed as:

FLOPS_{peak} = minimum(ArithmeticIntensity * BW_{HBM}, TheoreticalFLOPS)

On Frontier, the memory bandwidth for HBM is 1.6 TB/s, and the theoretical peak floating-point FLOPS/s when using vector registers is calculated by:",4.156131330453883
"What is the relationship between arithmetic intensity and performance in Frontier?
","Arithmetic intensity calculates the ratio of FLOPS to bytes moved between HBM and L2 cache. We calculated FLOPS above (FP64_FLOPS). We can calculate the number of bytes moved using the rocprof metrics TCC_EA_WRREQ_64B, TCC_EA_WRREQ_sum, TCC_EA_RDREQ_32B, and TCC_EA_RDREQ_sum. TCC refers to the L2 cache, and EA is the interface between L2 and HBM. WRREQ and RDREQ are write-requests and read-requests, respectively. Each of these requests is either 32 bytes or 64 bytes. So we calculate the number of bytes traveling over the EA interface as:

BytesMoved = BytesWritten + BytesRead

where",4.148481698816544
"What is the relationship between arithmetic intensity and performance in Frontier?
","Arithmetic intensity calculates the ratio of FLOPS to bytes moved between HBM and L2 cache. We calculated FLOPS above (FP64_FLOPS). We can calculate the number of bytes moved using the rocprof metrics TCC_EA_WRREQ_64B, TCC_EA_WRREQ_sum, TCC_EA_RDREQ_32B, and TCC_EA_RDREQ_sum. TCC refers to the L2 cache, and EA is the interface between L2 and HBM. WRREQ and RDREQ are write-requests and read-requests, respectively. Each of these requests is either 32 bytes or 64 bytes. So we calculate the number of bytes traveling over the EA interface as:

BytesMoved = BytesWritten + BytesRead

where",4.148481698816544
"How can I troubleshoot ParaView crashes when using the EGL version of the module via the command line?
","If problems persist and you do not need EGL, try using the OSMesa version of the module instead (e.g., paraview/5.9.1-osmesa instead of paraview/5.9.1-egl).

A command not found error occurs when trying to execute either PvBatch or PvPython after loading the default ParaView module on Andes. To fix this, you must load the equivalent ParaView module ending in ""pyapi"" instead (i.e., module load paraview/5.9.1-py3-pyapi instead of module load paraview/5.9.1-py3).",4.470863333714856
"How can I troubleshoot ParaView crashes when using the EGL version of the module via the command line?
","After installing, if you see a ""Can't open display"" or a ""DISPLAY is not set"" error, try restarting your computer. Sometimes XQuartz doesn't function properly if the computer was never restarted after installing.

If ParaView crashes when using the EGL version of the ParaView module via the command line and raises errors about OpenGL drivers or features, this is most likely due to not being connected to any GPUs.

Double check that you are either running on the GPU partition on Andes (i.e., -p gpu), or that you have -g set to a value greater than zero in your jsrun command on Summit.",4.445839101377406
"How can I troubleshoot ParaView crashes when using the EGL version of the module via the command line?
","If you plan on using the EGL version of the ParaView module (e.g., paraview/5.11.0-egl), then you must be connected to the GPUs. On Andes, this is done by using the gpu partition via #SBATCH -p gpu, while on Summit the -g flag in the jsrun command must be greater than zero.",4.32777995427163
"What is the advantage of building your own MPI base image on Summit?
","You can view the Dockerfiles used to build the MPI base image at the code.ornl.gov repository. These Dockerfiles are buildable on Summit yourself by cloning the repository and running the ./build in the individual directories in the repository. This allows you the freedom to modify these base images to your own needs if you don't need all the components in the base images. You may run into the cgroup memory limit when building so kill the podman process, log out, and try running the build again if that happens when building.",4.401494193996999
"What is the advantage of building your own MPI base image on Summit?
","Summit is connected to an IBM Spectrum Scale™ filesystem providing 250PB of storage capacity with a peak write speed of 2.5 TB/s. Summit also has access to the center-wide NFS-based filesystem (which provides user and project home areas) and has access to the center’s High Performance Storage System (HPSS) for user and project archival storage.

Summit is running Red Hat Enterprise Linux (RHEL) version 8.2.",4.211934066883757
"What is the advantage of building your own MPI base image on Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.206985390137508
"How do I load the ROCm Toolchain on Frontier?
","hipcc requires the ROCm Toolclain, See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#exposing-the-rocm-toolchain-to-your-programming-environment





Computational work on Frontier is performed by jobs. Jobs typically consist of several componenets:

A batch submission script

A binary executable

A set of input files for the executable

A set of output files created by the executable

In general, the process for running a job is to:

Prepare executables and input files.

Write a batch script.

Submit the batch script to the batch scheduler.",4.3667850191853965
"How do I load the ROCm Toolchain on Frontier?
","The following modules help you expose the ROCm Toolchain to your programming Environment:

| Programming Environment Module | Module that gets you ROCm Toolchain | How you load it: | | --- | --- | --- | | PrgEnv-amd | amd | amd  is loaded automatically with module load PrgEnv-amd | | PrgEnv-cray or PrgEnv-gnu | amd-mixed | module load amd-mixed |",4.274940043215778
"How do I load the ROCm Toolchain on Frontier?
","First, load the gnu compiler module (most Python packages assume GCC), relevant GPU module (necessary for CuPy), and the python module (allows you to create a new environment):

Summit

.. code-block:: bash

   $ module load gcc/7.5.0 # might work with other GCC versions
   $ module load cuda/11.0.2
   $ module load python

Frontier

.. code-block:: bash

   $ module load PrgEnv-gnu
   $ module load amd-mixed/5.3.0
   $ module load craype-accel-amd-gfx90a
   $ module load cray-python # only if not using Miniconda on Frontier",4.272108271207983
"What is the purpose of the User Archive areas for data storage?
","Use of User Archive areas for data storage is deprecated as of January 14, 2020. The user archive area for any user account created after that date (or for any user archive directory that is empty of user files after that date) will contain only symlinks to the top-level directories for each of the user's projects on HPSS. Users with existing data in a User Archive directory are encouraged to move that data to an appropriate project-based directory as soon as possible.  The information below is simply for reference for those users with existing data in User Archive directories.",4.419819350475952
"What is the purpose of the User Archive areas for data storage?
","User archive areas on HPSS are intended for storage of data not immediately needed in either User Home directories (NFS) or User Work directories (GPFS). User Archive directories should not be used to store project-related data. Rather, Project Archive directories should be used for project data.

User archive directories are located at /home/$USER.",4.347911842281205
"What is the purpose of the User Archive areas for data storage?
","The storage area to use in any given situation depends upon the activity you wish to carry out. Each user has a User Home area on a Network File System (NFS) and a User Archive area on the archival High Performance Storage System (HPSS). These user storage areas are intended to house user-specific files. Each project has a Project Home area on NFS, multiple Work areas on Spectrum Scale, and multiple Archive areas on HPSS. These project storage areas are intended to house project-centric files. We have defined several areas as listed below by function:",4.334566714116736
"How do I override the default thread allocation for a specific job step?
","In addition to specifying the SMT level, you can also control the number of threads per MPI task by exporting the OMP_NUM_THREADS environment variable. If you don't export it yourself, Jsrun will automatically set the number of threads based on the number of cores requested (-c) and the binding (-b) option. It is better to be explicit and set the OMP_NUM_THREADS value yourself rather than relying on Jsrun constructing it for you. Especially when you are using Job Step Viewer which relies on the presence of that environment variable to give you visual thread assignment information.",4.193898593527158
"How do I override the default thread allocation for a specific job step?
","Third attempt - Using multiple threads per core

To use both available hardware threads per core, the job must be allocated with --threads-per-core=2 (as opposed to only the job step - i.e., srun command). That value will then be inherited by srun unless explcitly overridden with --threads-per-core=1. Because we are using --threads-per-core=2, the usage of -c goes back to purely meaning the amount of logical cores (i.e., it is no longer equivalent to 1 physical core).

$ salloc -N1 -A <project_id> -t <time> -p <partition> --threads-per-core=2",4.104488044818785
"How do I override the default thread allocation for a specific job step?
","Allocation Overuse Policy

Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.0869120546924345
"How can I create a route to the A service in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.25346240394595
"How can I create a route to the A service in Slate?
","By default, a route will only expose your https://docs.olcf.ornl.gov/systems/route.html#slate_services to NCCS networks. If you need your service exposed to the world outside ORNL, you will first need to get your project approved for external routes. To do this, submit a systems ticket. In the description, give us your project name and a brief reasoning for why exposing externally is needed.

We will let you know once your project is able to set up external routes.",4.244142394949242
"How can I create a route to the A service in Slate?
","Routes can also be created from the web interface. On the hamburger menu, click Networking, then Routes.

Route in Hamburger Menu

If no routes have been created for a project, you will be presented with a Create Route button.

Create Route

On the Create Route screen, fill out the form, select your service in the service dropdown.",4.239424717678594
"How can I launch an interactive job on Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.317842471624561
"How can I launch an interactive job on Andes?
","All of the above can also be achieved in an interactive batch job through the use of the salloc command on Andes or the bsub -Is command on Summit. Recall that login nodes should not be used for memory- or compute-intensive tasks, including VisIt.",4.283482283199448
"How can I launch an interactive job on Andes?
","Nice DCV is currently undergoing maintenance. Instead, please use the VNC options detailed above.

Launch an interactive job:

localsytem: ssh username@andes.olcf.ornl.gov
andes: salloc -A PROJECT_ID -p gpu -N 1 -t 60:00 -M andes --constraint=DCV

Run the following commands:

$ xinit &
$ export DISPLAY=:0
$ dcv create-session --gl-display :0 mySessionName
$ hostname  // will be used to open a tunneling connection with this node
$ andes-gpuN

Open a tunneling connection with gpu node N, given by hostname:

localsystem: ssh username@andes.olcf.ornl.gov -L 8443:andes-gpuN:8443",4.248345610679708
"How does Frontier handle memory regions that are not migrated to GPU HBM?
","The AMD MI250X and operating system on Frontier supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Frontier. In the following section we'll discuss how the heterogenous memory space on a Frontier node is surfaced within your application.",4.332106660019478
"How does Frontier handle memory regions that are not migrated to GPU HBM?
","The page migration behavior summarized by the following tables represents the current, observable behavior. Said behavior will likely change in the near future.  CPU accesses to migratable memory may behave differently than other platforms you're used to. On Frontier, pages will not migrate from GPU HBM to CPU DDR4 based on access patterns alone. Once a page has migrated to GPU HBM it will remain there even if the CPU accesses it, and all accesses which do not resolve in the CPU cache will occur over the Infinity Fabric between the AMD ""Optimized 3rd Gen EPYC"" CPU and AMD MI250X GPU. Pages",4.295998598036265
"How does Frontier handle memory regions that are not migrated to GPU HBM?
","If HSA_XNACK=0, page faults in GPU kernels are not handled and will terminate the kernel. Therefore all memory locations accessed by the GPU must either be resident in the GPU HBM or mapped by the HIP runtime. Memory regions may be migrated between the host DDR4 and GPU HBM using explicit HIP library functions such as hipMemAdvise and hipPrefetchAsync, but memory will not be automatically migrated based on access patterns alone.",4.291593593014149
"How do you deploy a new version of the application in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.262284170831951
"How do you deploy a new version of the application in Slate?
","The following items need to be established before deploying applications on Slate systems (Marble or Onyx OpenShift clusters).

Follow https://docs.olcf.ornl.gov/systems/helm_prerequisite.html#slate_getting_started if you do not already have a project/namespace established on Onyx or Marble.

It is recommended to use Helm version 3.

Helm enables application deployment via Helm Charts. The high level Helm Architecture docs are also a great reference for understanding Helm.

Like oc, Helm is a single binary executable.

This can be installed on macOS with Homebrew :

$ brew install helm",4.205341484052863
"How do you deploy a new version of the application in Slate?
","A/B Deployments are a popular way to try a new version of an application with a small subset of users in the production environment. With this strategy, you can specify that the older version gets most of the user requests while a limited fraction of users get sent to the new version. Since you can control the amount of users which get sent to the new version, you can gradually increase the volume of requests to the new version and eventually stop using the old version. Remember that deployment configurations don't do any autoscaling of pods, so you may have to adjust the number of pod",4.179162317785692
"How many SIMD units are available in each CU in Frontier?
","Each CU has 4 Matrix Core Units (the equivalent of NVIDIA's Tensor core units) and 4 16-wide SIMD units. For a vector instruction that uses the SIMD units, each wavefront (which has 64 threads) is assigned to a single 16-wide SIMD unit such that the wavefront as a whole executes the instruction over 4 cycles, 16 threads per cycle. Since other wavefronts occupy the other three SIMD units at the same time, the total throughput still remains 1 instruction per cycle. Each CU maintains an instructions buffer for 10 wavefronts and also maintains 256 registers where each register is 64 4-byte wide",4.413216038090784
"How many SIMD units are available in each CU in Frontier?
","Each Frontier compute node contains 4 AMD MI250X. The AMD MI250X has a peak performance of 53 TFLOPS in double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 26.5 TFLOPS (double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in each direction simultaneously).

To connect to Frontier, ssh to frontier.olcf.ornl.gov. For example:

$ ssh <username>@frontier.olcf.ornl.gov",4.274287009327199
"How many SIMD units are available in each CU in Frontier?
","The 110 CUs in each GPU deliver peak performance of 26.5 TFLOPS in double precision. Also, each GPU contains 64 GB of high-bandwidth memory (HBM2) accessible at a peak bandwidth of 1.6 TB/s. The 2 GPUs in an MI250X are connected with [4x] GPU-to-GPU Infinity Fabric links providing 200+200 GB/s of bandwidth. (Consult the diagram in the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes section for information on how the accelerators are connected to each other, to the CPU, and to the network.",4.27428330830544
"How can I connect to Andes using SSH?
","In a new terminal, open a tunneling connection with andes-gpu5.olcf.ornl.gov and                                                                              port 5901
example:
     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov
     andes: ssh -4L 5901:localhost:5901 andes-gpu5

**************************************************************************",4.3329150129066765
"How can I connect to Andes using SSH?
","In a new terminal, open a tunneling connection with andes-gpu5.olcf.ornl.gov and                                                                              port 5901
example:
     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov
     andes: ssh -4L 5901:localhost:5901 andes-gpu5

**************************************************************************",4.3329150129066765
"How can I connect to Andes using SSH?
","In a new terminal, open a tunneling connection with andes79.olcf.ornl.gov and port 5901
example:
     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov
     andes: ssh -4L 5901:localhost:5901 andes79

**************************************************************************

MATLAB is selecting SOFTWARE OPENGL rendering.

In a second terminal on your local system open a tunneling connection following the instructions given by the vnc start-up script:

localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov

andes: ssh -4L 5901:localhost:5901 andes79",4.2977865804724855
"What is the purpose of the Ascent system?
","System Overview



Ascent is a stand-alone 18-node system with the same architecture and design as Summit. It's most often utilized as a resource for OLCF training events, workshops, and conferences. Ascent exists in the NCCS Open Security Enclave, which is subject to fewer restrictions than the Moderate Security Enclave that houses systems such as Summit. This means that participants in training events can go through a streamlined version of the approval process before being granted access.",4.412906311413913
"What is the purpose of the Ascent system?
",Ascent is a training system that is not intended to be used as an OLCF user resource. Access to the system is only obtained through OLCF training events.,4.354907397434194
"What is the purpose of the Ascent system?
","Ascent is an 18-node stand-alone system with the same architecture as Summit (see https://docs.olcf.ornl.gov/systems/summit_user_guide.html#summit-nodes section above), so most of this Summit User Guide can be referenced for Ascent as well. However, aside from the number of compute nodes, there are other differences between the two systems. Most notably, Ascent sits in the NCCS Open Security Enclave, which is subject to fewer restrictions than the Moderate Security Enclave that systems such as Summit belong to. This means that participants in OLCF training events can go through a streamlined",4.336061371610354
"How can I avoid oversubscribing cores in my Summit job?
","Jobs on Summit are scheduled in full node increments; a node's cores cannot be allocated to multiple jobs. Because the OLCF charges based on what a job makes unavailable to other users, a job is charged for an entire node even if it uses only one core on a node. To simplify the process, users request and are allocated multiples of entire nodes through LSF.

Allocations on Summit are separate from those on Andes and other OLCF resources.

The node-hour charge for each batch job will be calculated as follows:

node-hours = nodes requested * ( batch job endtime - batch job starttime )",4.250359776781097
"How can I avoid oversubscribing cores in my Summit job?
","You can use hello_jsrun or Job Step Viewer to see how the cores are being oversubscribed.

Because of how jsrun sets up OMP_NUM_THREADS based on -c and -b options if you don't specify the environment variable yourself, you can accidentally end up oversubscribing your cores. For example

jsrun -n1 -a2 -c8 -g1 -brs -dpacked  csh -c 'echo $OMP_NUM_THREADS $OMP_PLACES'

Warning: more than 1 task/rank assigned to a core
Warning: more than 1 task/rank assigned to a core
32 0:4,4:4,8:4,12:4,16:4,20:4,24:4,28:4
32 0:4,4:4,8:4,12:4,16:4,20:4,24:4,28:4",4.235565943933114
"How can I avoid oversubscribing cores in my Summit job?
","The first thing to notice here is the WARNING about oversubscribing the available CPU cores. Also, the output shows each MPI rank did spawn 2 OpenMP threads, but both OpenMP threads ran on the same logical core (for a given MPI rank). This was not the intended behavior; each OpenMP thread was meant to run on its own physical CPU core.",4.181582919118515
"Are there any tutorials or examples available for visualization on Andes?
","Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-user-guide.

For a full list of software available at the OLCF, please see the Software section (coming soon).",4.335586299426028
"Are there any tutorials or examples available for visualization on Andes?
","For sample data and additional examples, explore the VisIt Data Archive and various VisIt Tutorials. Supplementary test data can be found in your local installation in the data directory:

Linux: /path/to/visit/data

macOS: /path/to/VisIt.app/Contents/Resources/data

Windows: C:\path\to\LLNL\VisIt x.y.z\data

Additionally, check out our beginner friendly OLCF VisIt Tutorial which uses Andes to visualize example datasets.",4.264908390493073
"Are there any tutorials or examples available for visualization on Andes?
","Alternatively, the ParaView installations in /sw/andes/paraview (i.e., the paraview/5.9.1-egl and paraview/5.9.1-osmesa modules) can also be loaded to avoid this issue.



The ParaView at OLCF Tutorial highlights how to get started on Andes with example datasets.

The Official ParaView User's Guide and the Python API Documentation contain all information regarding the GUI and Python interfaces.

A full list of ParaView Documentation can be found on ParaView's website.

The ParaView Wiki contains extensive information about all things ParaView.",4.24624789116786
"How do I get started with deploying MinIO on Slate's Marble Cluster?
","It is assumed you have already gone through https://docs.olcf.ornl.gov/systems/minio.html#slate_getting_started and established the https://docs.olcf.ornl.gov/systems/minio.html#helm_prerequisite. Please do that before attempting to deploy anything on Slate's clusters.

This example uses Helm version 3 to deploy a MinIO standalone Helm chart on Slate's Marble Cluster. This is the cluster in OLCF's Moderate enclave, the same enclave as Summit.

To start, clone the slate helm examples repository , containing the MinIO standalone Helm chart, and navigate into the charts directory:",4.522952229625397
"How do I get started with deploying MinIO on Slate's Marble Cluster?
","We hope this provides a starting point for more robust implementations of MinIO, if your workload/project benefits from that. In addition, it gives insight into some of the core building blocks for establishing your own, different, applications on Slate.

This example deployment of MinIO enables two possible configurations (which we configure in the following sections):

MinIO running on a NCCS filesystem (GPFS or NFS - exposing filesystem project space through the MinIO GUI).",4.439918474498764
"How do I get started with deploying MinIO on Slate's Marble Cluster?
","MinIO is a high-performance software-defined object storage suite that enables the ability to easily deploy cloud-native data infrastructure for various data workloads. In this example we are deploying a simple, standalone implementation of MinIO on our cloud-native platform, Slate (https://docs.olcf.ornl.gov/systems/minio.html#slate_overview).

This service will only be accessible from inside of ORNL's network.

If your project requires an externally facing service available to the Internet, please contact User Assistance by submitting a help ticket. There is a process to get such approval.",4.402553002817377
"What libraries must be linked with my HIP program on Crusher?
","Make sure the craype-accel-amd-gfx90a module is loaded when compiling HIP with the Cray compiler wrappers.

| Compiler | Compile/Link Flags, Header Files, and Libraries | | --- | --- | |  |  | | hipcc |  |

hipcc requires the ROCm Toolclain, See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#exposing-the-rocm-toolchain-to-your-programming-environment",4.136172965711033
"What libraries must be linked with my HIP program on Crusher?
","| Implementation | Module | Compiler | Header Files & Linking | | --- | --- | --- | --- | | Cray MPICH | cray-mpich | cc, CC, ftn (Cray compiler wrappers) | MPI header files and linking is built into the Cray compiler wrappers | | hipcc |  |

To use GPU-aware Cray MPICH, users must set the following modules and environment variables:

module load craype-accel-amd-gfx90a
module load rocm

export MPICH_GPU_SUPPORT_ENABLED=1

There are extra steps needed to enable GPU-aware MPI on Crusher, which depend on the compiler that is used (see 1. and 2. below).",4.136169560171826
"What libraries must be linked with my HIP program on Crusher?
",The compatibility table below was determined by linker testing with all current combinations of cray-mpich and rocm modules on Crusher.,4.116531078982522
"How do I update my VisIt version to resolve the metadata server error on Andes?
","Using VisIt on Summit is also an option, as the scalable rendering problem is currently not an issue on Summit (as of Sept. 2021).

As of February 2022, this issue on Andes has been fixed (must use VisIt 3.2.2 or higher).",4.254223677646674
"How do I update my VisIt version to resolve the metadata server error on Andes?
","Navigate to the appropriate file.

Once you choose a file, you will be prompted for the number of nodes and processors you would like to use (remember that each node of Andes contains 32 processors, or 28 if using the high-memory GPU partition) and the Project ID, which VisIt calls a ""Bank"" as shown below.



Once specified, the server side of VisIt will be launched, and you can interact with your data.",4.247606764632205
"How do I update my VisIt version to resolve the metadata server error on Andes?
","If VisIt will not connect to Andes or Summit when you try to draw an image, you should login to the system and check if a job is in the queue. To do this on Andes, enter squeue from the command line. To do this on Summit, enter bjobs from the command line. Your VisIt job should appear in the queue. If you see it in a state marked ""PD"" or ""PEND"" you should wait a bit longer to see if it will start. If you do not see your job listed in the queue, check to make sure your project ID is entered in your VisIt host profile. See the https://docs.olcf.ornl.gov/systems/visit.html#visit-modify-host",4.229379266900078
"How does Crusher handle page faults when using remote access?
","Most applications that use ""managed"" or ""unified"" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Crusher. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.",4.119560241148944
"How does Crusher handle page faults when using remote access?
","The Crusher system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.096185528114775
"How does Crusher handle page faults when using remote access?
",Crusher is connected to the center-wide IBM Spectrum Scale™ filesystem providing 250 PB of storage capacity with a peak write speed of 2.5 TB/s. Crusher also has access to the center-wide NFS-based filesystem (which provides user and project home areas). While Crusher does not have direct access to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#dtn-user-guide to move data to/from HPSS.,4.094346468518531
"What is the purpose of the `--nnodes` flag in the `bsrun` command?
","In easy mode, the system software converts options such as -nnodes in a batch script into the resource string needed by the scheduling system. In expert mode, the user is responsible for creating this string and options such as -nnodes cannot be used. In easy mode, you will not be able to use bsub -R to create resource strings. The system will automatically create the resource string based on your other bsub options. In expert mode, you will be able to use -R, but you will not be able to use the following options to bsub: -ln_slots, -ln_mem, -cn_cu, or -nnodes.",4.193888807271708
"What is the purpose of the `--nnodes` flag in the `bsrun` command?
","| Option | jsrun (Summit) | srun  (Frontier) | | --- | --- | --- | | Number of nodes | -nnodes | -N, --nnodes | | Number of tasks | defined with resource set | -n, --ntasks | | Number of tasks per node | defined with resource set | --ntasks-per-node | | Number of CPUs per task | defined with resource set | -c, --cpus-per-task | | Number of resource sets | -n, --nrs | N/A | | Number of resource sets per host | -r, --rs_per_host | N/A | | Number of tasks per resource set | -a, --tasks_per_rs | N/A | | Number of CPUs per resource set | -c, --cpus_per_rs | N/A | | Number of GPUs per resource set",4.1685047141428
"What is the purpose of the `--nnodes` flag in the `bsrun` command?
","It's recommended to explicitly specify jsrun options and not rely on the default values. This most often includes --nrs,--cpu_per_rs, --gpu_per_rs, --tasks_per_rs, --bind, and --launch_distribution.

The below examples were launched in the following 2 node interactive batch job:

summit> bsub -nnodes 2 -Pprj123 -W02:00 -Is $SHELL

The following example will create 12 resource sets each with 1 MPI task and 1 GPU. Each MPI task will have access to a single GPU.",4.143882690846705
"How does the batch system determine the waiting time for jobs submitted under overallocated projects?
","For example, consider that job1 is submitted at the same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. In addition, projects that are at 125% of their allocated time will be limited to only one running job at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.6348071870891525
"How does the batch system determine the waiting time for jobs submitted under overallocated projects?
","For example, consider that job1 is submitted at the same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. In addition, projects that are at 125% of their allocated time will be limited to only one running job at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.6348071870891525
"How does the batch system determine the waiting time for jobs submitted under overallocated projects?
","same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. Additionally, projects that are at 125% of their allocated time will be limited to only 3 running jobs at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.590739819924324
"Can the allocation of hours be renewed or extended in the OLCF Policy?
","System reservation (a dedicated set of nodes at a specific date/time)

Increased disk quota

Purge exemption for User/Group/World Work areas

Special requests are reviewed weekly by the OLCF Resource Utilization Council. Please contact help@olcf.ornl.gov for more information.



This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title: Computing Policy Version: 12.10",4.305367861316941
"Can the allocation of hours be renewed or extended in the OLCF Policy?
","Allocation Overuse Policy

Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.304699225183218
"Can the allocation of hours be renewed or extended in the OLCF Policy?
",The OLCF has a pull-back policy for under-utilization of INCITE allocations. Under-utilized INCITE project allocations will have core-hours removed from their outstanding core-hour project balance at specific times during the INCITE calendar year. The following table summarizes the current under-utilization policy:,4.298508039614611
"What is the name of the project ID used in the example to request an allocation of compute nodes?
","Before you can do anything you need a Project to do your things in. Fortunately, when you get an allocation on one of our clusters a Project is automatically created for you with the same name as the allocation. By using our own distinct Project we are ensuring that we will not interfere with anyone else's work.

NOTE: Everywhere that you see <cluster> replace that with the cluster that you will be running on (marble or onyx).",4.232999241409385
"What is the name of the project ID used in the example to request an allocation of compute nodes?
","The requesting project's allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Summit would be equivalent to using 16,000 Summit node-hours of a project's allocation.



As is the case with many other queuing systems, it is possible to place dependencies on jobs to prevent them from running until other jobs have started/completed/etc. Several possible dependency settings are described in the table below:",4.211028271711991
"What is the name of the project ID used in the example to request an allocation of compute nodes?
","The requesting project’s allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Frontier would be equivalent to using 16,000 Frontier node-hours of a project’s allocation.

Reservations should not be confused with priority requests. If quick turnaround is needed for a few jobs or for a period of time, a priority boost should be requested. A reservation should only be requested if users need to guarantee availability of a set of nodes at a given time, such as for a live demonstration at a conference.",4.2025545380617775
"Where is the project home data stored?
","Open and Moderate Projects are provided with a Project Home storage area in the NFS-mounted filesystem. This area is intended for storage of data, code, and other files that are of interest to all members of a project. Since Project Home is an NFS-mounted filesystem, its performance will not be as high as other filesystems.

Moderate Enhanced projects are not provided with Project Home spaces, just Project Work spaces.

Project Home area is accessible at /ccs/proj/abc123 (where abc123 is your project ID).",4.2857455435470735
"Where is the project home data stored?
","Project Home directories are NFS-mounted on selected OLCF systems and are intended to store long-term, frequently-accessed data that is needed by all collaborating members of a project. Project Home areas are backed up on a daily basis. This file system does not generally provide the input/output (I/O) performance required by most compute jobs, and is not available to compute jobs on most systems. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for more details on applicable quotas, backups, purge, and retention timeframes.",4.2489107680699885
"Where is the project home data stored?
","User Home: Long-term data for routine access that is unrelated to a project. It is mounted on compute nodes of Summit as read only

User Archive: A ""link farm"" with symbolic links to a user's project directories on HPSS. (Previously this was for non-project data on HPSS; such use is now deprecated)

Project Home: Long-term project data for routine access that's shared with other project members. It is mounted on compute nodes of Summit as read only

Member Work: Short-term user data for fast, batch-job access that is not shared with other project members.",4.234769481814127
"What does it mean if the kernel is memory-bound?
","The most important output to look at is the ""GPU Speed of Light"" section, which tells you what fraction of peak memory throughput and what fraction of peak compute throughput you achieved. Typically if you have achieved higher than 60% of the peak of either subsystem, your kernel would be considered memory-bound or compute-bound (respectively), and if you have not achieved 60% of either this is often a latency-bound kernel. (A common cause of latency issues is not exposing enough parallelism to saturate the GPU's compute capacity -- peak GPU performance can only be achieved when there is",4.366149520690165
"What does it mean if the kernel is memory-bound?
","The accessibility of memory from GPU kernels and whether pages may migrate depends three factors: how the memory was allocated; the XNACK operating mode of the GPU; whether the kernel was compiled to support page migration. The latter two factors are intrinsically linked, as the MI250X GPU operating mode restricts the types of kernels which may run.",4.239551955875221
"What does it mean if the kernel is memory-bound?
","The accessibility of memory from GPU kernels and whether pages may migrate depends three factors: how the memory was allocated; the XNACK operating mode of the GPU; whether the kernel was compiled to support page migration. The latter two factors are intrinsically linked, as the MI250X GPU operating mode restricts the types of kernels which may run.",4.239551955875221
"Can I use Globus to transfer files larger than 1MB?
","transfers across all the users. Each user has a limit of 3 active transfers, so it is required to transfer a lot of data on each transfer than less data across many transfers.  If a folder is constituted with mixed files including thousands of small files (less than 1MB each one), it would be better to tar the small files.  Otherwise, if the files are larger, Globus will handle them.",4.418713671134286
"Can I use Globus to transfer files larger than 1MB?
","The OLCF users have access to a new functionality, using Globus to transfer files to HPSS through the endpoint ""OLCF HPSS"". Globus has restriction of 8 active transfers across all the users. Each user has a limit of 3 active transfers, so it is required to transfer a lot of data on each transfer than less data across many transfers. If a folder is constituted with mixed files including thousands of small files (less than 1MB each one), it would be better to tar the small files.  Otherwise, if the files are larger, Globus will handle them. To transfer the files, follow these steps:",4.388798928882588
"Can I use Globus to transfer files larger than 1MB?
","When the installation has finished, click on the Globus icon and select Web: Transfer Files as below



Globus will ask you to login. If your institution does not have an organizational login, you may choose to either Sign in with Google or Sign in with ORCiD iD.



In the main Globus web page, select the two-panel view, then set the source and destination endpoints. (Left/Right order does not matter)



Next, navigate to the appropriate source and destination paths to select the files you want to transfer. Click the ""Start"" button to begin the transfer.",4.375850353384981
"What are some of the benefits of using SPI's CITADEL framework for HPC computing?
","The National Center for Computational Science (NCCS) and the Oak Ridge Leadership Computing Facility (OLCF) have implemented the CITADEL security framework as part of their Scalable Protected Infrastructure (SPI). This infrastructure provides resources and protocols that enable researchers to process protected data at scale. With the CITADEL framework, researchers can use the OLCF’s large HPC resources to compute data containing protected health information (PHI), personally identifiable information (PII), data protected under International Traffic in Arms Regulations, and other types of data",4.467652731934192
"What are some of the benefits of using SPI's CITADEL framework for HPC computing?
","The Citadel framework allows use of the OLCF's existing HPC resources Summit and Frontier for SPI workflows.  Citadel adds measures to ensure separation of SPI and non-SPI workflows and data. This section provides differences when using OLCF resources for SPI and non-SPI workflows.  Because the Citadel framework just adds another security layer to existing HPC resources, many system use methods are the same between SPI and non-SPI workflows.  For example, compiling, batch scheduling, and job layout are the same between the two security enclaves.  Because of this, the existing resource user",4.45540111533876
"What are some of the benefits of using SPI's CITADEL framework for HPC computing?
","With the CITADEL framework, researchers can use the OLCF’s large HPC resources including Frontier and Summit to compute data containing protected health information (PHI), personally identifiable information (PII), data protected under International Traffic in Arms Regulations, and other types of data that require privacy.",4.402887037766935
"Where can users view queue information and submit jobs?
","The most straightforward monitoring is with the bjobs command. This command will show the current queue, including both pending and running jobs. Running bjobs -l will provide much more detail about a job (or group of jobs). For detailed output of a single job, specify the job id after the -l. For example, for detailed output of job 12345, you can run bjobs -l 12345 . Other options to bjobs are shown below. In general, if the command is specified with -u all it will show information for all users/all jobs. Without that option, it only shows your jobs. Note that this is not an exhaustive list.",4.307529861172618
"Where can users view queue information and submit jobs?
","Users can access information about IBM Quantum's systems, view queue information, and submit jobs on their cloud dashboard at https://quantum-computing.ibm.com. The cloud dashboard allows access to IBM Quantum's graphical circuit builder, Quantum Composer, IBM's Quantum Lab, and associated program examples.  A Jupyterlab server is provisioned with IBM Quantum's Qiskit programming framework for job submission.",4.304153704780053
"Where can users view queue information and submit jobs?
","| Task | Slurm | | --- | --- | | View batch queue | squeue | | Submit batch script | sbatch | | Submit interactive batch job | salloc |

| Node Count | Duration | Policy | | --- | --- | --- | | 1-4 Nodes | 0 - 24 hrs | max 1 job running per user |",4.270801039519966
"Can I do analysis or visualization tasks on the login nodes?
","Login nodes should be used for basic tasks such as file editing, code compilation, data backup, and job submission. Login nodes should not be used for memory- or compute-intensive tasks. Users should also limit the number of simultaneous tasks performed on the login resources. For example, a user should not run (10) simultaneous tar processes on a login node.

Compute-intensive, memory-intensive, or otherwise disruptive processes running on login nodes may be killed without warning.",4.38368732888921
"Can I do analysis or visualization tasks on the login nodes?
","Login nodes

Andes features 8 login nodes which are identical to the batch partition compute nodes.  The login nodes provide an environment for editing, compiling, and launching codes onto the compute nodes. All Andes users will access the system through these same login nodes, and as such, any CPU- or memory-intensive tasks on these nodes could interrupt service to other users. As a courtesy, we ask that you refrain from doing any analysis or visualization tasks on the login nodes.",4.378572945690697
"Can I do analysis or visualization tasks on the login nodes?
","Andes features 8 login nodes which are identical to the batch partition compute nodes.  The login nodes provide an environment for editing, compiling, and launching codes onto the compute nodes. All Andes users will access the system through these same login nodes, and as such, any CPU- or memory-intensive tasks on these nodes could interrupt service to other users. As a courtesy, we ask that you refrain from doing any analysis or visualization tasks on the login nodes.",4.3487967444063615
"Can project members request an exception to the data retention policy?
","The project data retention policy exists to reclaim storage space after a project ends. By default, the OLCF will retain data in project-centric storage areas only for a designated amount of time after the project end date. During this time, a project member can request a temporary user account extension for data access. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for details on purge and retention timeframes for each project-centric storage area.",4.348438984536206
"Can project members request an exception to the data retention policy?
","If you need an exception to the limits listed in the table above, such as a higher quota in your User/Project Home or a purge exemption in a Member/Project/World Work area, contact help@olcf.ornl.gov with a summary of the exception that you need.

By default, the OLCF does not guarantee lifetime data retention on any OLCF resources. Following a user account deactivation or project end, user and project data in non-purged areas will be retained for 90 days. After this timeframe, the OLCF retains the right to delete data. Data in purged areas remains subject to normal purge policies.",4.282511141789753
"Can project members request an exception to the data retention policy?
","For sensitive projects only, all data related to the project must be purged from all OLCF computing resources within 30 days of the project’s end or termination date.

Although the Member Work and Member Archive directories are for storage of data a user does not want to make available to other users on the system, files in these directories are still considered project data and can be reassigned to another user at the PI's request.",4.235888816860568
"How do I launch Vampir on Summit?
","For detailed information about using Vampir on Summit and the builds available, please see the Vampir Software Page.",4.492982849421955
"How do I launch Vampir on Summit?
","This method will require you to have a local copy of the Vampir GUI already installed on your machine.  If you do not have a local copy, please reach out to the help desk at help@olcf.ornl.gov for instructions on getting a local copy.

<string>:17: (INFO/1) Duplicate explicit target name: ""summit"".

Similar to the previous methods outlined above, you will start by connecting to Summit. Once connected you will then need to start the https://docs.olcf.ornl.gov/systems/Vampir.html#VampirServer. <vamps>

$ module load vampir

#Start the VampirServer",4.397182703352921
"How do I launch Vampir on Summit?
","After connecting to Summit using X11 forwarding you will need to load the Vampir module and start the https://docs.olcf.ornl.gov/systems/Vampir.html#VampirServer. <vamps>

$ module load vampir

$ vampirserver start -- -P <projectID> -w <walltime> -q <queue>

#Example: vampirserver start -- -P 123456 -w 60 -q debug



Successful VampirServer startup message should appear in terminal window. You will need this information!",4.3848751503967565
"What is the theoretical peak double-precision performance of Summit?
","Summit is an IBM system located at the Oak Ridge Leadership Computing Facility. With a theoretical peak double-precision performance of approximately 200 PF, it is one of the most capable systems in the world for a wide range of traditional computational science applications. It is also one of the ""smartest"" computers in the world for deep learning applications with a mixed-precision capability in excess of 3 EF.



Summit node architecture diagram",4.472798319099402
"What is the theoretical peak double-precision performance of Summit?
","The NVIDIA Tesla V100 GPUs in Summit are capable of over 7TF/s of double-precision and 15 TF/s of single-precision floating point performance. Additionally, the V100 is capable of over 120 TF/s of half-precision floating point performance when using its Tensor Core feature. The Tensor Cores are purpose-built accelerators for half-precision matrix multiplication operations. While they were designed especially to accelerate machine learning workflows, they are exposed through several other APIs that are useful to other HPC applications. This section provides information for using the V100",4.341342616592383
"What is the theoretical peak double-precision performance of Summit?
","Summit node architecture diagram

The basic building block of Summit is the IBM Power System AC922 node. Each of the approximately 4,600 compute nodes on Summit contains two IBM POWER9 processors and six NVIDIA Tesla V100 accelerators and provides a theoretical double-precision capability of approximately 40 TF. Each POWER9 processor is connected via dual NVLINK bricks, each capable of a 25GB/s transfer rate in each direction.",4.329939813721262
"What are the compile/link flags, header files, and libraries required for compiling HIP with the Cray compiler wrappers?
","| Vendor | Module | Language | Compiler | OpenMP flag (GPU) | | --- | --- | --- | --- | --- | | Cray | cce | C C++ |  | -fopenmp | | Fortran | ftn |  | | AMD | rocm |  |  |  |

This section shows how to compile HIP codes using the Cray compiler wrappers and hipcc compiler driver.

Make sure the craype-accel-amd-gfx908 module is loaded when using HIP.

| Compiler | Compile/Link Flags, Header Files, and Libraries | | --- | --- | | CC |  | | hipcc |  |",4.477011897950758
"What are the compile/link flags, header files, and libraries required for compiling HIP with the Cray compiler wrappers?
","Make sure the craype-accel-amd-gfx90a module is loaded when compiling HIP with the Cray compiler wrappers.

| Compiler | Compile/Link Flags, Header Files, and Libraries | | --- | --- | |  |  | | hipcc |  |

hipcc requires the ROCm Toolclain, See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#exposing-the-rocm-toolchain-to-your-programming-environment",4.4379423660375945
"What are the compile/link flags, header files, and libraries required for compiling HIP with the Cray compiler wrappers?
","Make sure the craype-accel-amd-gfx90a module is loaded when compiling HIP with the Cray compiler wrappers.

| Compiler | Compile/Link Flags, Header Files, and Libraries | | --- | --- | |  |  | | hipcc |  |",4.421151585503853
"How do I debug my quantum circuit when running on a high-performance computing cluster?
","A recommended workflow for running on Quantinuum's quantum computers is to utilize the syntax checker first, run on the emulator, then run on one of the quantum computers. This is highlighted in the examples.",4.322192974872737
"How do I debug my quantum circuit when running on a high-performance computing cluster?
","Information on submitting jobs to IonQ systems, system availability, checking job status, and tracking usage can be found via the IonQ Cloud Console.

A recommended workflow for running on IonQ's quantum computers is to utilize the emulator first, then run on one of the quantum computers. This is highlighted in the examples.",4.258716133829519
"How do I debug my quantum circuit when running on a high-performance computing cluster?
","Jobs are compiled and submitted via Qiskit in a Python virtual environment or Jupyter notebook (see https://docs.olcf.ornl.gov/systems/ibm_quantum.html#Cloud Access <ibm-cloud> and https://docs.olcf.ornl.gov/systems/ibm_quantum.html#Local Access <ibm-local> sections above).

Circuit jobs comprise jobs of constructed quantum circuits and algorithms submitted to backends in IBM Quantum fair-share queue.

Program jobs utilize a pre-compiled quantum program utilizing the Qiskit Runtime framework.",4.224434828496383
"How can I access the ROCm 5.5.1 modulefile on Frontier?
","On Tuesday, July 18, 2023, Frontier was upgraded to a new version of the system software stack. During the upgrade, the following changes took place:

The system was upgraded to Cray OS 2.5, Slingshot Host Software 2.0.2-112, and the AMD GPU 6.0.5 device driver (ROCm 5.5.1 release).

ROCm 5.5.1 is now available via the rocm/5.5.1 modulefile.

HPE/Cray Programming Environments (PE) 23.05 is now available via the cpe/23.05 modulefile.

HPE/Cray PE 23.05 introduces support for ROCm 5.5.1. However, due to issues identified during testing, ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.",4.272205603672101
"How can I access the ROCm 5.5.1 modulefile on Frontier?
","hipcc requires the ROCm Toolclain, See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#exposing-the-rocm-toolchain-to-your-programming-environment





Computational work on Frontier is performed by jobs. Jobs typically consist of several componenets:

A batch submission script

A binary executable

A set of input files for the executable

A set of output files created by the executable

In general, the process for running a job is to:

Prepare executables and input files.

Write a batch script.

Submit the batch script to the batch scheduler.",4.244267123064863
"How can I access the ROCm 5.5.1 modulefile on Frontier?
","As mentioned above, HIP can be used on systems running on either the ROCm or CUDA platform, so OLCF users can start preparing their applications for Frontier today on Summit. To use HIP on Summit, you must load the HIP module:

$ module load hip-cuda

CUDA 11.4.0 or later must be loaded in order to load the hip-cuda module. hipBLAS, hipFFT, hipSOLVER, hipSPARSE, and hipRAND have also been added to hip-cuda.",4.190274195514748
"How can I set the number of threads for each task in Summit?
","In addition to specifying the SMT level, you can also control the number of threads per MPI task by exporting the OMP_NUM_THREADS environment variable. If you don't export it yourself, Jsrun will automatically set the number of threads based on the number of cores requested (-c) and the binding (-b) option. It is better to be explicit and set the OMP_NUM_THREADS value yourself rather than relying on Jsrun constructing it for you. Especially when you are using Job Step Viewer which relies on the presence of that environment variable to give you visual thread assignment information.",4.232698406160028
"How can I set the number of threads for each task in Summit?
","summit> setenv OMP_NUM_THREADS 4
summit> jsrun -n12 -a1 -c4 -g1 -b packed:4 -d packed ./a.out
Rank: 0; RankCore: 0; Thread: 0; ThreadCore: 0; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}
Rank: 0; RankCore: 0; Thread: 1; ThreadCore: 4; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}
Rank: 0; RankCore: 0; Thread: 2; ThreadCore: 8; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}
Rank: 0; RankCore: 0; Thread: 3; ThreadCore: 12; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}",4.192795635704626
"How can I set the number of threads for each task in Summit?
","This section provides some of the most commonly used LSF commands as well as some of the most useful options to those commands and information on jsrun, Summit's job launch command. Many commands have much more information than can be easily presented here. More information about these commands is available via the online manual (i.e. man jsrun). Additional LSF information can be found on IBM’s website.

Each physical core on Summit contains 4 hardware threads. The SMT level can be set using LSF flags (the default is smt4):

SMT1",4.17217948358039
"Can you specify the exact command to set the MPI rank distribution in Crusher?
","--distribution=<value>[:<value>][:<value>] specifies the distribution of MPI ranks across compute nodes, sockets (L3 cache regions on Crusher), and cores, respectively. The default values are block:cyclic:cyclic, which is where the cyclic assignment comes from in the previous examples.

In the job step for this example, --distribution=*:block is used, where * represents the default value of block for the distribution of MPI ranks across compute nodes and the distribution of MPI ranks across L3 cache regions has been changed to block from its default value of cyclic.",4.309288068781889
"Can you specify the exact command to set the MPI rank distribution in Crusher?
","The intent with both of the following examples is to launch 8 MPI ranks across the node where each rank is assigned its own logical (and, in this case, physical) core.  Using the -m distribution flag, we will cover two common approaches to assign the MPI ranks -- in a ""round-robin"" (cyclic) configuration and in a ""packed"" (block) configuration. Slurm's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive method was used to request an allocation of 1 compute node for these examples: salloc -A <project_id> -t 30 -p <parition> -N 1",4.267608361400586
"Can you specify the exact command to set the MPI rank distribution in Crusher?
","--distribution=<value>:[<value>]:[<value>] specifies the distribution of MPI ranks across compute nodes, sockets (NUMA domains on Spock), and cores, respectively. The default values are block:cyclic:cyclic, which is where the cyclic assignment comes from in the previous examples.

In the job step for this example, --distribution=*:block is used, where * represents the default value of block for the distribution of MPI ranks across compute nodes and the distribution of MPI ranks across NUMA domains has been changed to block from its default value of cyclic.",4.221335833237597
"How does Frontier's memory management differ from traditional GPU architectures?
","The AMD MI250X and operating system on Frontier supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Frontier. In the following section we'll discuss how the heterogenous memory space on a Frontier node is surfaced within your application.",4.413414436147413
"How does Frontier's memory management differ from traditional GPU architectures?
","The Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.409518029387307
"How does Frontier's memory management differ from traditional GPU architectures?
","GPU when we talk about a GCD) for a total of 8 GPUs per node (compared to Summit's 6 Nvidia V100 GPUs per node). Each pair of GPUs is associated with a particular NUMA domain (see node diagram in https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes section) which might affect how your application should lay out data and computation.  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#amd-gpus section for more information.  Programming Models  Since Frontier uses AMD GPUs, code written in Nvidia's CUDA language will not work as is. They need to be",4.292244760769929
"How can I increase my disk quota?
","You can check your home directory quota with the quota command. If it is over quota, you need to bring usage under the quota and then your jobs should run without encountering the Disk quota exceeded error.

Footnotes

1

This entry is for legacy User Archive directories which contained user data on January 14, 2020.

2

User Archive directories that were created (or had no user data) after January 14, 2020. Settings other than permissions are not applicable because directories are root-owned and contain no user files.

3",4.288762834436485
"How can I increase my disk quota?
","$ quota -Qs
Disk quotas for user usrid (uid 12345):
     Filesystem  blocks   quota   limit   grace   files   quota   limit   grace
nccsfiler1a.ccs.ornl.gov:/vol/home
                  4858M   5000M   5000M           29379   4295m   4295m

Moderate enhanced projects have home directores located in GPFS. There is no enforced quota, but it is recommended that users not exceed 50 TB. These home directories are subject to the 90 day purge",4.251888212380395
"How can I increase my disk quota?
","If your home directory reaches its quota, your batch jobs might fail with the error cat: write error: Disk quota exceeded. This error may not be intuitive, especially if your job exclusively uses work areas that are well under quota. The error is actually related to your home directory quota. Sometimes, batch systems write temporary files to the home directory (for example, on Summit LSF writes temporary data in ~/.lsbatch), so if the home directory is over quota and that file creation fails, the job will fail with the quota error.",4.129406824729783
"How can I generate a profile run of my instrumented code using Score-P?
","To instrument your code, you need to compile the code using the Score-P instrumentation command (scorep), which is added as a prefix to your compile statement. In most cases the Score-P instrumentor is able to automatically detect the programming paradigm from the set of compile and link options given to the compiler. Some cases will, however, require some additional link options within the compile statement e.g. CUDA instrumentation.

Below are some basic examples of the different instrumentation scenarios:",4.494050113716955
"How can I generate a profile run of my instrumented code using Score-P?
","Once the code has been instrumented, it is time to begin the measurement run of the newly compiled code. The measurement calls will gather information during the runtime of the code where this information will be stored for later analysis.

By default Score-P is configured to run with profiling set to true and tracing set to false. Measurement types are configured via environment variables.

##Environment variable setup examples

export SCOREP_ENABLE_TRACING=true

You can check what current Score-P environment variables are set:

$ scorep-info config-vars --full

#Output",4.351501465834143
"How can I generate a profile run of my instrumented code using Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations. It works in combination with Periscope, Scalasca, Vampir, and Tau.

Steps in a typical Score-P workflow to run on https://docs.olcf.ornl.gov/systems/Scorep.html#summit-user-guide:",4.3343095414172845
"How can I request membership on an additional project in myOLCF?
",Join an Additional Project Existing users with RSA SecurID tokens should log in to the myOLCF self-service portal to apply for additional projects. Existing users without RSA SecurID tokens should fill out the Account Application Form that can be found at the top left of the myOLCF login page without needing to sign in. See the section https://docs.olcf.ornl.gov/systems/documents_and_forms.html#applying-for-a-user-account for complete details.,4.461715960334589
"How can I request membership on an additional project in myOLCF?
","If you already have a user account at the OLCF, your existing credentials can be leveraged across multiple projects.

If your user account has an associated RSA SecurID (i.e. you have an ""OLCF Moderate"" account), you gain access to another project by logging in to the myOLCF self-service portal and filling out the application under My Account > Join Another Project. For more information, see the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#myOLCF self-service portal documentation<myolcf-overview>.",4.4530262454555904
"How can I request membership on an additional project in myOLCF?
","As a principal investigator of a project at the OLCF, you must approve (or reject) every potential user that requests membership on your project. myOLCF provides a mechanism for processing these requests via the ""For My Approval"" page.

Click the ""For My Approval"" link in the ""My Account"" top navigation dropdown:

for my approval link

You'll see a list of all pending requests that need your response:

<string>:6: (INFO/1) Enumerated list start value not ordinal-1: ""2"" (ordinal 2)

for my approval link",4.451303345485042
"How can I create a kustomization.yaml file for my Kubernetes resources?
","kustomize Examples

The kustomization.yaml file declares what resource files kustomize should use when generating kubernetes resources. Additionally, the kustomization.yaml file will specify how resources should be modified, if needed. A kustomization.yaml file will contain information that falls typically into four categories:

resources: what existing resource files should be used.

generators: what new resources should be created dynamically.

transformers: what resources should be changed and how to change them.

meta: fields that may influence all of the above.",4.507830877389538
"How can I create a kustomization.yaml file for my Kubernetes resources?
","The structure of the kustomization.yaml file starts off similar to the structure of a kubernetes object: apiVersion, kind, and metadata.name. From there, the file contains resource information and meta information. The resource information is specified in the resources block and lists files that should be included for use by kustomize. In this case, three files are specified: deployment.yaml, service.yaml, and configMap.yaml. Each of these files define a kubernetes resource of the type indicated by the filename. Resource file names are arbitrary, but they must match the name of the file in",4.499689851563523
"How can I create a kustomization.yaml file for my Kubernetes resources?
","Starting with the Hello World example in the prior section, the kustomization.yaml file located in the base directory would look similar to:

apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
metadata:
  name: arbitrary
commonLabels:
  app: hello
resources:
- deployment.yaml
- service.yaml
- configMap.yaml",4.418025913341958
"Can I use the Spack environment files provided in the guide for a different purpose than application development?
","This not intended as a guide for a new Spack user.  Please see the Spack 101 tutorial if you need assistance starting out with Spack.

The provided Spack environment files are intended to assist OLCF users in setup their development environment at the OLCF.  The base environment file includes the compilers and packages that are installed at the system level.",4.394150326580259
"Can I use the Spack environment files provided in the guide for a different purpose than application development?
","Traditionally, the user environment is modified by module files.  For example, a user would add use  module load cmake/3.18.2 to load CMake version 3.18.2 into their environment.  Using a Spack environment, a user can add an OLCF provided package and build against it using Spack without having to load the module file separately.

The information presented here is a subset of what can be found at the Spack documentation site.",4.29593255550841
"Can I use the Spack environment files provided in the guide for a different purpose than application development?
","This guide meant as an example for a user to setup a Spack environment for application development using the OLCF provided files as a template.

The OLCF uses an internal mirror of the Spack repo that is customized for use on OLCF systems.  This results in the hash values generated by another version of Spack to not match.  It is recommended to use the existing module as external packages instead of chaining at this time.

The provided spack.yaml files are templates for a user to use as an example.",4.283728131226346
"How do the GPU_ID and RT_GPU_ID relate to each other?
","The output contains different IDs associated with the GPUs so it is important to first describe these IDs before moving on. GPU_ID is the node-level (or global) GPU ID, which is labeled as one might expect from looking at a node diagram: 0, 1, 2, 3. RT_GPU_ID is the HIP runtime GPU ID, which can be thought of as each MPI rank's local GPU ID numbering (with zero-based indexing). So in the output above, each MPI rank has access to 1 unique GPU - where MPI 000 has access to GPU 0, MPI 001 has access to GPU 1, etc., but all MPI ranks show a HIP runtime GPU ID of 0. The reason is that each MPI",4.351457712066718
"How do the GPU_ID and RT_GPU_ID relate to each other?
","Here is a summary of the different GPU IDs reported by the example program:

GPU_ID is the node-level (or global) GPU ID read from ROCR_VISIBLE_DEVICES. If this environment variable is not set (either by the user or by Slurm), the value of GPU_ID will be set to N/A.

RT_GPU_ID is the HIP runtime GPU ID (as reported from, say hipGetDevice).

Bus_ID is the physical bus ID associated with the GPUs. Comparing the bus IDs is meant to definitively show that different GPUs are being used.",4.349752007808384
"How do the GPU_ID and RT_GPU_ID relate to each other?
","Here is a summary of the different GPU IDs reported by the example program:

GPU_ID is the node-level (or global) GPU ID read from ROCR_VISIBLE_DEVICES. If this environment variable is not set (either by the user or by Slurm), the value of GPU_ID will be set to N/A by this program.

RT_GPU_ID is the HIP runtime GPU ID (as reported from, say hipGetDevice).

Bus_ID is the physical bus ID associated with the GPUs. Comparing the bus IDs is meant to definitively show that different GPUs are being used.",4.346456675210318
"What is the purpose of the Home directories for users on OLCF systems?
","Home directories for each user are NFS-mounted on all OLCF systems and are intended to store long-term, frequently-accessed user data. User Home areas are backed up on a daily basis. This file system does not generally provide the input/output (I/O) performance required by most compute jobs, and is not available to compute jobs on most systems. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for more details on applicable quotas, backups, purge, and retention timeframes.",4.512985185881192
"What is the purpose of the Home directories for users on OLCF systems?
","Project Home directories are NFS-mounted on selected OLCF systems and are intended to store long-term, frequently-accessed data that is needed by all collaborating members of a project. Project Home areas are backed up on a daily basis. This file system does not generally provide the input/output (I/O) performance required by most compute jobs, and is not available to compute jobs on most systems. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for more details on applicable quotas, backups, purge, and retention timeframes.",4.463367658392082
"What is the purpose of the Home directories for users on OLCF systems?
","The OLCF uses a standard file system structure to assist users with data organization on OLCF systems. Complete details about all file systems available to OLCF users can be found in the Data Management Policy section.

Additional file systems and file protections may be employed for sensitive data. If you are a user on a project producing sensitive data, further instructions will be given by the OLCF. The following guidelines apply to sensitive data:

Only store sensitive data in designated locations. Do not store sensitive data in your User Home directory.",4.382152138659071
"Where can I find the left navigation menu in myOLCF?
","At any time, you can view account pages by clicking on the ""My Account"" link in the top navigation menu:

link to my account page

There is only (1) account context in myOLCF: ""you"" as the currently-authenticated user. This account context is linked to the OLCF Moderate account that you used to authenticate to myOLCF.

account page left navigation menu

The left navigation menu also includes an expandable item with links to account-centric pages.",4.42733381476977
"Where can I find the left navigation menu in myOLCF?
","After authenticating, you are redirected to the project pages area of myOLCF.

Every individual page in the project pages area should be interpreted within the context of a single, current project, which is displayed at the top of the left navigation menu (e.g. ""ABC123""):

project pages left navigation menu

The top navigation bar has a dropdown menu that can be used to switch the current project context to any of the projects of which you are a member.

switch projects dropdown menu",4.355595895809902
"Where can I find the left navigation menu in myOLCF?
","Project members

The myOLCF site is provided to aid in the utilization and management of OLCF allocations. See the https://docs.olcf.ornl.gov/services_and_applications/myolcf/index.html for more information.

If you have any questions or have a request for additional data, please contact the OLCF User Assistance Center.",4.134747511719725
"How can I ensure that my plots are saved successfully without encountering the ""Scalable Render Request Failed (VisItException)"" error message?
","Some users have encountered their compute engine exiting abnormally on Andes after VisIt reaches 100% when drawing a plot, resulting in a ""Scalable Render Request Failed (VisItException)"" error message. This message has also been reported when users try to save plots, if VisIt was successfully able to draw. The error seems to more commonly occur for users that are trying to visualize large datasets.",4.410610170576895
"How can I ensure that my plots are saved successfully without encountering the ""Scalable Render Request Failed (VisItException)"" error message?
","VisIt developers have been notified, and at this time the current workaround is to disable Scalable Rendering from being used. To do this, go to Options→Rendering→Advanced and set the ""Use Scalable Rendering"" option to ""Never"".

However, this workaround has been reported to affect VisIt's ability to save images, as scalable rendering is utilized to save plots as image files (which can result in another compute engine crash). To avoid this, screen capture must be enabled. Go to File→""Set save options"" and check the box labeled ""Screen capture"".",4.374339586389786
"How can I ensure that my plots are saved successfully without encountering the ""Scalable Render Request Failed (VisItException)"" error message?
","Using VisIt on Summit is also an option, as the scalable rendering problem is currently not an issue on Summit (as of Sept. 2021).

As of February 2022, this issue on Andes has been fixed (must use VisIt 3.2.2 or higher).",4.086636030691093
"How do I check the status of my job in the queue?
","The most straightforward monitoring is with the bjobs command. This command will show the current queue, including both pending and running jobs. Running bjobs -l will provide much more detail about a job (or group of jobs). For detailed output of a single job, specify the job id after the -l. For example, for detailed output of job 12345, you can run bjobs -l 12345 . Other options to bjobs are shown below. In general, if the command is specified with -u all it will show information for all users/all jobs. Without that option, it only shows your jobs. Note that this is not an exhaustive list.",4.240257884939781
"How do I check the status of my job in the queue?
","The squeue command is used to show the batch queue. You can filter the level of detail through several command-line options. For example:

| squeue -l | Show all jobs currently in the queue | | --- | --- | |  | Show all of your jobs currently in the queue |

The sacct command gives detailed information about jobs currently in the queue and recently-completed jobs. You can also use it to see the various steps within a batch jobs.",4.160197907740364
"How do I check the status of my job in the queue?
","Most users will find batch jobs an easy way to use the system, as they allow you to ""hand off"" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.",4.131221093969883
"What is the label I need to add to my route to expose it externally?
","Under metadata, add a label for ccs.ornl.gov/externalRoute: 'true' as shown below and click the Save button at the bottom of the page.

Route After

After saving, your route will be exposed on two routers, default and external. This means your service is now accessible from outside ORNL. Note that if your project has not yet been approved for external routing, this second router will not expose your route.

Route Exposed",4.43466625554937
"What is the label I need to add to my route to expose it externally?
","Once your project has been approved, you only need to give your route a label to tell the OpenShift router to expose this service externally. You can do this in the CLI or in the web interface.

<string>:249: (INFO/1) Duplicate implicit target name: ""cli"".

On the CLI, run oc label route {ROUTE_NAME} ccs.ornl.gov/externalRoute=true.

In the web interface, from the side menu, select Networking, then Routes.

Routes Menu

This will show a list of your routes. Click the route you want to expose, and click the YAML tab.",4.428794741864303
"What is the label I need to add to my route to expose it externally?
","By default, a route will only expose your https://docs.olcf.ornl.gov/systems/route.html#slate_services to NCCS networks. If you need your service exposed to the world outside ORNL, you will first need to get your project approved for external routes. To do this, submit a systems ticket. In the description, give us your project name and a brief reasoning for why exposing externally is needed.

We will let you know once your project is able to set up external routes.",4.279834236849303
"What is the purpose of the Forest SDK?
","Quil is the Rigetti-developed quantum instruction/assembly language. PyQuil is a Python library for writing and running quantum programs using Quil.

Installing pyQuil requires installing the Forest SDK. To quote Rigetti: ""pyQuil, along with quilc, the QVM, and other libraries, make up what is called the Forest SDK"". Because we don't have Docker functionality and due to normal users not having sudo privileges, this means that you will have to install the SDK via the ""bare-bones"" method. The general info below came from:

https://pyquil-docs.rigetti.com/en/stable/start.html",4.028626029463495
"What is the purpose of the Forest SDK?
",This section of the Summit User Guide is intended to show current OLCF users how to start preparing their applications to run on the upcoming Frontier system. We will continue to add more topics to this section in the coming months. Please see the topics below to get started.,3.953931948398508
"What is the purpose of the Forest SDK?
","Newer: https://github.com/libffi/libffi/releases/

ZMQ download: https://github.com/zeromq/libzmq/releases

Forest SDK download: https://qcs.rigetti.com/sdk-downloads

Below are example instructions for installing the above packages into your $HOME directory. Versions may vary.

Newer versions than those used in the install instructions below are known to work on Andes; however, on Frontier, newer versions of libffi than 3.2.1 are known to cause problems.

Andes

.. code-block:: bash

    $ module load gcc cmake

Frontier

.. code-block:: bash",3.937866172534769
"Where can I find more information about requesting a new project for an SPI workflow?
","Similar to standard OLCF workflows, to run SPI workflows on OLCF resources, you will first need an approved project.  The project will be awarded an allocation(s) that will allow resource access and use.  The first step to requesting an allocation is to complete the project request form.  The form will initiate the process that includes peer review, export control review, agreements, and others.  Once submitted, members of the OLCF accounts team will help walk you through the process.

Please see the OLCF Accounts and Projects section of this site to request a new project.",4.288069196277588
"Where can I find more information about requesting a new project for an SPI workflow?
","Once a project has been approved and created, the next step will be to request user accounts.  A user account will allow an individual to access the project's allocated resources.    This process to request an account is very similar to the process used for non-SPI projects.  One notable difference between SPI and non-SPI accounts: SPI usernames are unique to a project.  SPI usernames use the format: <userid>_<proj>_mde.  If you have access to three SPI projects, you will have three userIDs with three separate home areas.",4.202446352842819
"Where can I find more information about requesting a new project for an SPI workflow?
","https://docs.olcf.ornl.gov/systems/index.html#Request an allocation (project)<spi-allocations-projects>.  All access and resource use occurs within an approved allocation.

https://docs.olcf.ornl.gov/systems/index.html#Request a user account<spi-user-accounts>.  Once an allocation (project) has been approved, each member of the project must request an account to use the project's allocated resources.",4.178680103193128
"Is the ArgoCD server using a wildcard termination?
","In other words, the CD in ArgoCD is for continuous delivery of the application(s).",4.138362718557033
"Is the ArgoCD server using a wildcard termination?
",allow for better control of resources allocated to ArgoCD.,4.1232240738427
"Is the ArgoCD server using a wildcard termination?
","Next Click the ""Create ArgoCD"" button. You will be presented with a form view similar to:

Image of the form view for ArgoCD instance creation.

Starting with the form view of the process, make the following changes allowing for access to the ArgoCD web UI via a route:

Server -> Insecure -> true

Server -> Route -> Enabled -> true

Server -> Route -> Tls -> Termination -> edge

Image of the form view for ArgoCD instance creation with the route settings configured.",4.106934469777286
"How do I apply the settings in VisIt?
","Click ""Apply""

At the top menu click on ""Options""→""Save Settings""



Once you have VisIt installed and set up on your local computer:

Open VisIt on your local computer.

Go to: ""File→Open file"" or click the ""Open"" button on the GUI.

Click the ""Host"" dropdown menu on the ""File open"" window that popped up and choose ""ORNL_Andes"".

This will prompt you for your OLCF password, and connect you to Andes.

Navigate to the appropriate file.",4.349594551673842
"How do I apply the settings in VisIt?
","You can modify settings relevant to this host machine. For example, you can change the ""Username"" field if your OLCF username differs from your local computer username.

Once you have made your changes, press the ""Apply"" button, and then save the settings (Options/Save Settings).

Each host can have several launch profiles. A launch profile specifies how VisIt runs on a given host computer. To make changes to a host's launch profile, do the following:

Go to ""Options→Host Profiles"".

Select the host in the left side of the window.",4.248031668848821
"How do I apply the settings in VisIt?
","- **Use cluster’s graphics cards**: Unchecked

Click “Apply” and make sure to save the settings (Options/Save Settings).
Exit and re-launch VisIt.



See https://docs.olcf.ornl.gov/systems/visit.html#visit-host-profiles section above for creating your initial host profile.

To make changes to an existing host profile, do the following:

Go to ""Options→Host Profiles"".

The window will display the known hosts on the left, with the settings for that host shown on the right in the ""Host Settings"" tab.",4.245225311144321
"How can I search for a specific module named <modulename> in Andes?
","Searching for modules

Modules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the spider sub-command can be used as summarized in the following table.",4.117454693640454
"How can I search for a specific module named <modulename> in Andes?
",For Andes:,4.077282150676304
"How can I search for a specific module named <modulename> in Andes?
","Modules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the spider sub-command can be used as summarized in the following table.",4.073578279744502
"Can you summarize the main differences between the ""killable"" queue policy and the ""batch"" queue policy?
","-------------------------



At the start of a scheduled system outage, a *queue reservation* is used

to ensure that no jobs are running. In the ``batch`` queue, the

scheduler will not start a job if it expects that the job would not

complete (based on the job's user-specified max walltime) before the

reservation's start time. In constrast, the ``killable`` queue allows

the scheduler to start a job even if it will *not* complete before a

scheduled reservation. It enforces the following policies:



-  Jobs will be killed if still running when a system outage begins.",4.290356215243721
"Can you summarize the main differences between the ""killable"" queue policy and the ""batch"" queue policy?
","-  The scheduler will stop scheduling jobs in the ``killable`` queue (1)

hour before a scheduled outage.

-  Maximum-job-per-user limits are the same (i.e., in conjunction with)

the ``batch`` queue.

-  Any killed jobs will be automatically re-queued after a system outage

completes.



``debug`` Queue Policy

----------------------



The ``debug`` queue is intended to provide faster turnaround times for

the code development, testing, and debugging cycle. For example,

interactive parallel work is an ideal use for the debug queue. It

enforces the following policies:",4.238786369969877
"Can you summarize the main differences between the ""killable"" queue policy and the ""batch"" queue policy?
","| Bin | Min Nodes | Max Nodes | Max Walltime (Hours) | Aging Boost (Days) | | --- | --- | --- | --- | --- | | 1 | 5,645 | 9,408 | 12.0 | 8 | | 2 | 1,882 | 5,644 | 12.0 | 4 | | 3 | 184 | 1,881 | 12.0 | 0 | | 4 | 92 | 183 | 6.0 | 0 | | 5 | 1 | 91 | 2.0 | 0 |

The batch queue is the default queue for production work on Frontier. Most work on Frontier is handled through this queue. The following policies are enforced for the batch queue:

Limit of four eligible-to-run jobs per user. (Jobs in excess of this number will be held, but will move to the eligible-to-run state at the appropriate time.)",4.145931363550186
"How can I register a GitLab runner to a group or project on Slate?
","To deploy a GitLab Runner from the Developer Perspective, navigate to ""Topology"" -> ""Helm Chart"" and select ""GitLab Runner vX.X.X Provided by Slate Helm Charts"". From the new window, select ""Install Helm Chart"".

On the resulting screen, one can customize the installation of the GitLab Runner helm chart. The chart has been forked from upstream to allow for customized deployment to Slate. From the Form View, expand the ""GitLab Runner"" fields. Using the registration token retrieved in the prior section, enter token for adding the runner to the GitLab server in the empty field.",4.408581198627034
"How can I register a GitLab runner to a group or project on Slate?
","Prior to installation of the GitLab Runner, a registration token for the runner is needed from the GitLab server. This token will allow a GitLab runner to register to the server in the needed location for running CI/CD jobs. Runners themselves may be registered to either a group as a shared runner or a project as a repository specific runner.",4.400883818581391
"How can I register a GitLab runner to a group or project on Slate?
","If the runner is to be a group shared runner, navigate to the group in GitLab and then go to Settings -> CI/CD. Expand the Runners section of the CI/CD Settings panel. Ensure that the ""Enable shared runners for this group"" toggle is enabled. The registration token should also be available for retrieval from ""Group Runners"" area.",4.356536697104002
"How do I set the environment variables for the hipcc build on Frontier?
","hipcc requires the ROCm Toolclain, See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#exposing-the-rocm-toolchain-to-your-programming-environment





Computational work on Frontier is performed by jobs. Jobs typically consist of several componenets:

A batch submission script

A binary executable

A set of input files for the executable

A set of output files created by the executable

In general, the process for running a job is to:

Prepare executables and input files.

Write a batch script.

Submit the batch script to the batch scheduler.",4.1806690922911125
"How do I set the environment variables for the hipcc build on Frontier?
","To use GPU-aware Cray MPICH with the Cray compiler wrappers, the following environment variables must be set before compiling. These variables are automatically set by the cray-mpich modulefile:

## These must be set before compiling so the executable picks up GTL
PE_MPICH_GTL_DIR_amd_gfx90a=""-L${CRAY_MPICH_ROOTDIR}/gtl/lib""
PE_MPICH_GTL_LIBS_amd_gfx90a=""-lmpi_gtl_hsa""

In addition, the following header files and libraries must be included:

-I${ROCM_PATH}/include
-L${ROCM_PATH}/lib -lamdhip64

where the include path implies that #include <hip/hip_runtime.h> is included in the source file.",4.170851404914583
"How do I set the environment variables for the hipcc build on Frontier?
","First, load the gnu compiler module (most Python packages assume GCC), relevant GPU module (necessary for CuPy), and the python module (allows you to create a new environment):

Summit

.. code-block:: bash

   $ module load gcc/7.5.0 # might work with other GCC versions
   $ module load cuda/11.0.2
   $ module load python

Frontier

.. code-block:: bash

   $ module load PrgEnv-gnu
   $ module load amd-mixed/5.3.0
   $ module load craype-accel-amd-gfx90a
   $ module load cray-python # only if not using Miniconda on Frontier",4.166820715254266
"What is the role of the CRAY_XPMEM_POST_LINK_OPTS environment variable on Crusher?
",The compatibility table below was determined by linker testing with all current combinations of cray-mpich and rocm modules on Crusher.,4.092857360916809
"What is the role of the CRAY_XPMEM_POST_LINK_OPTS environment variable on Crusher?
","| Implementation | Module | Compiler | Header Files & Linking | | --- | --- | --- | --- | | Cray MPICH | cray-mpich | cc, CC, ftn (Cray compiler wrappers) | MPI header files and linking is built into the Cray compiler wrappers | | hipcc |  |

To use GPU-aware Cray MPICH, users must set the following modules and environment variables:

module load craype-accel-amd-gfx90a
module load rocm

export MPICH_GPU_SUPPORT_ENABLED=1

There are extra steps needed to enable GPU-aware MPI on Crusher, which depend on the compiler that is used (see 1. and 2. below).",4.066449294901475
"What is the role of the CRAY_XPMEM_POST_LINK_OPTS environment variable on Crusher?
","module load PrgEnv-amd

This module will setup your programming environment with paths to software and libraries that are compatible with AMD compilers.

Use the -craype-verbose flag to display the full include and link information used by the Cray compiler wrappers. This must be called on a file to see the full output (e.g., CC -craype-verbose test.cpp).



If you need to add the tools and libraries related to ROCm, the framework for targeting AMD GPUs, to your path, you will need to use a version of ROCm that is compatible with your programming environment.",4.037432937151831
"Who is authorized to access information on DOE computers?
","authorized individual or investigative agency is in effect during the period of your access to information on a DOE computer and for a period of three years thereafter. OLCF personnel and users are required to address, safeguard against, and report misuse, abuse and criminal activities. Misuse of OLCF resources can lead to temporary or permanent disabling of accounts, loss of DOE allocations, and administrative or legal actions. Users who have not accessed a OLCF computing resource in at least 6 months will be disabled. They will need to reapply to regain access to their account. All users",4.425194591047418
"Who is authorized to access information on DOE computers?
","Users are advised that there is no expectation of privacy of your activities on any system that is owned by, leased or operated by UT-Battelle on behalf of the U.S. Department of Energy (DOE). The Company retains the right to monitor all activities on these systems, to access any computer files or electronic mail messages, and to disclose all or part of information gained to authorized individuals or investigative agencies, all without prior notice to, or consent from, any user, sender, or addressee. This access to information or a system by an authorized individual or investigative agency is",4.360888981080886
"Who is authorized to access information on DOE computers?
","OLCF resources are federal computer systems, and as such, users should have no explicit or implicit expectation of privacy. OLCF employees and authorized vendor personnel with “root” privileges have access to all data on OLCF systems. Such employees can also login to OLCF systems as other users. As a general rule, OLCF employees will not discuss your data with any unauthorized entities nor grant access to data files to any person other than the UNIX “owner” of the data file, except in the following situations:",4.21380257014752
"What is the difference between a Persistent Volume (PV) and a Persistent Volume Claim (PVC)?
","Once the Persistent Volume is created its allocated memory is available to be claimed by a project. This is done through a Persistent Volume Claim. PVC's are created using a YAML file in the same manner that PV's are created. An example of a PVC that claims the PV created in the previous section follows:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: storage-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

The claim is then placed using:

oc create -f storage-1.yaml",4.435317984375721
"What is the difference between a Persistent Volume (PV) and a Persistent Volume Claim (PVC)?
","PV Settings

This will redirect you to the status page of your new PVC. You should see Bound in the Status field.

PV Status

Once the claim has been granted to the project a pod within that project can access that storage. To do this, edit the YAML of the pod and under the volume portion of the file add a name and the name of the claim. It will look similar to the following.

volumes:
  name: pvol
  persistentVolumeClaim:
    claimName: storage-1

Then all that is left to do is mount the storage into a container:

volumeMounts:
  - mountPath: /tmp/
    name: pvol",4.380948145450778
"What is the difference between a Persistent Volume (PV) and a Persistent Volume Claim (PVC)?
","From the main screen of a project go to the Storage option in the hamburger menu on the left hand side of the screen, then click Persistent Volume Claims

Persistent Volume Claim Menu

In the upper right click the Create Persistent Volume Claim button.

Create Storage

This will take you to a screen that allows you to select what you need for your PVC. Give your claim a name and request an amount of storage and click Create. You can also click Edit YAML to edit the PVC object directly.

PV Settings",4.345977291963564
"How can I change the directory to the user scratch space in the example script?
","The suggested directory structure is to have a outer directory say swift-work that has the swift source and shell scripts. Inside of swift-work create a new directory called data.

Additionally, we will need two terminals open. In the first terminal window, navigate to the swift-work directory and invoke the data generation script like so:

$ ./gendata.sh

In the second terminal, we will run the swift workflow as follows (make sure to change the project name per your allocation):",4.112318257446777
"How can I change the directory to the user scratch space in the example script?
","date

# Change directory to user scratch space (GPFS)
cd /gpfs/alpine/<projid>/scratch/<userid>

echo "" ""
echo ""*****ORIGINAL FILE*****""
cat test.txt
echo ""***********************""

# Move file from GPFS to SSD
mv test.txt /mnt/bb/<userid>

# Edit file from compute node
srun -n1 hostname >> /mnt/bb/<userid>/test.txt

# Move file from SSD back to GPFS
mv /mnt/bb/<userid>/test.txt .

echo "" ""
echo ""*****UPDATED FILE******""
cat test.txt
echo ""***********************""

And here is the output from the script:

$ cat nvme_test-<jobid>.out",4.063478971475399
"How can I change the directory to the user scratch space in the example script?
","Before asking for a compute node, change into your GPFS scratch directory:

$ cd $MEMBERWORK/<YOUR_PROJECT_ID>
$ mkdir cupy_test
$ cd cupy_test

Let's see the boosts explicitly by running the timings.py script. To do so, you must submit submit_timings to the queue:

Summit

.. code-block:: bash

   $ bsub -L $SHELL submit_timings.lsf

Example ""submit_timings"" batch script:

Summit

.. code-block:: bash

   #!/bin/bash
   #BSUB -P <PROJECT_ID>
   #BSUB -W 00:05
   #BSUB -nnodes 1
   #BSUB -J cupy_timings
   #BSUB -o cupy_timings.%J.out
   #BSUB -e cupy_timings.%J.err",4.053383807378063
"How can I set the SSH path in Paraview?
","<Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Windows"">
            <Set name=""TERM_PATH"" value=""cmd"" />
            <Set name=""TERM_ARG1"" value=""/C"" />
            <Set name=""TERM_ARG2"" value=""start"" />
            <Set name=""TERM_ARG3"" value="""" />
            <Set name=""SSH_PATH"" value=""plink.exe"" />
          </Case>
          <Case value=""Unix"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.18639056001136
"How can I set the SSH path in Paraview?
","<Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Windows"">
            <Set name=""TERM_PATH"" value=""cmd"" />
            <Set name=""TERM_ARG1"" value=""/C"" />
            <Set name=""TERM_ARG2"" value=""start"" />
            <Set name=""TERM_ARG3"" value="""" />
            <Set name=""SSH_PATH"" value=""plink.exe"" />
          </Case>
          <Case value=""Unix"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.18639056001136
"How can I set the SSH path in Paraview?
","After installing, you must give ParaView the relevant server information to be able to connect to OLCF systems (comparable to VisIt's system of host profiles). The following provides an example of doing so. Although several methods may be used, the one described should work in most cases.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.

For Windows clients, it is necessary to install PuTTY to create an ssh connection in step 2.

Step 1: Save the following servers.pvsc file to your local computer",4.177514484593649
"What is the purpose of the --stdio_stdout/-o option when using jsrun?
","If JSM or PMIX errors occur as the result of backgrounding many job steps, using the --immediate option to jsrun may help, as shown in the following example.

#!/bin/bash
#BSUB -P ABC123
#BSUB -W 3:00
#BSUB -nnodes 1
#BSUB -J RunSim123
#BSUB -o RunSim123.%J
#BSUB -e RunSim123.%J

cd $MEMBERWORK/abc123
jsrun <options> --immediate ./a.out
jsrun <options> --immediate ./a.out
jsrun <options> --immediate ./a.out

By default, jsrun --immediate does not produce stdout or stderr. To capture stdout and/or stderr when using this option, additionally include --stdio_stdout/-o and/or --stdio_stderr/-k.",4.205278314591959
"What is the purpose of the --stdio_stdout/-o option when using jsrun?
","In addition, to debug slow startup JSM provides the option to create a progress file. The file will show information that can be helpful to pinpoint if a specific node is hanging or slowing down the job step launch. To enable it, you can use: jsrun --progress ./my_progress_file_output.txt.

When using file-based specification of resource sets with jsrun -U, the -a flag (number of tasks per resource set) is ignored. This has been reported to IBM and they are investigating. It is generally recommended to use jsrun explicit resource files (ERF) with --erf_input and --erf_output instead of -U.",4.195246052131262
"What is the purpose of the --stdio_stdout/-o option when using jsrun?
","This section describes tools that users might find helpful to better understand the jsrun job launcher.

hello_jsrun is a ""Hello World""-type program that users can run on Summit nodes to better understand how MPI ranks and OpenMP threads are mapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A screencast showing how to use Hello_jsrun is also available: https://vimeo.com/261038849

Job Step Viewer provides a graphical view of an application's runtime layout on Summit. It allows users to preview and quickly iterate with multiple jsrun options to understand and optimize job launch.",4.117318394491053
"How can users optimize the performance of their codes using floating point atomic operations on memory regions allocated via regular hipMalloc()?
","Users applying floating point atomic operations (e.g., atomicAdd) on memory regions allocated via regular hipMalloc() can safely apply the -munsafe-fp-atomics flags to their codes to get the best possible performance and leverage hardware supported floating point atomics. Atomic operations supported in hardware on non-FP datatypes  (e.g., INT32) will work correctly regardless of the nature of the memory region used.",4.608030069258647
"How can users optimize the performance of their codes using floating point atomic operations on memory regions allocated via regular hipMalloc()?
","Users applying floating point atomic operations (e.g., atomicAdd) on memory regions allocated via regular hipMalloc() can safely apply the -munsafe-fp-atomics flags to their codes to get the best possible performance and leverage hardware supported floating point atomics. Atomic operations supported in hardware on non-FP datatypes  (e.g., INT32) will work correctly regardless of the nature of the memory region used.",4.608030069258647
"How can users optimize the performance of their codes using floating point atomic operations on memory regions allocated via regular hipMalloc()?
","HIP has two kinds of memory allocations, coarse grained and fine grained, with tradeoffs between performance and coherence. Particularly relevant if you want to ues the hardware FP atomic instructions. See more in https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#fp-atomic-ops-coarse-fine-allocations.

FP32 atomicAdd operations on Local Data Store (i.e. block shared memory) can be slower than the equivalent FP64 operations. See more in https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#performance-lds-atomicadd.",4.362168171324415
"What is the name of the service described in the example?
","For example, let's look at service that was created in the https://docs.olcf.ornl.gov/systems/nodeport.html#slate_services document. This document assumes that it was deployed to the my-project project.

If you run oc get services, you should see your service in the list.

$ oc get services
NAME                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)     AGE
my-service            ClusterIP   172.25.170.246   <none>        8080/TCP    8s

Then, get some information about the service with oc describe service my-service.",4.131640993751282
"What is the name of the service described in the example?
","Here is an example service definition:

apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    name: my-app
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080

This definition tells Kubernetes that all pods with the label ""my-app"" are associated with this service. Any traffic to the service should be distributed among these pods.

The port parameter contains what port the service listens on, and the targetPort parameter contains the port to which the service forwards connections.",4.120322331502199
"What is the name of the service described in the example?
","In Kubernetes, a Service is an internal load balancer which identifies a set of pods and can proxy traffic to them. This set of pods is determined by a label selector.

A service is a stable way of accessing a set of pods, which are ephemeral.

When a service is created, it is granted a ClusterIP, which is an IP address internal to the Kubernetes cluster. Other pods can use this ClusterIP to access the service.

Here is an example service definition:",4.042060824277825
"How can I activate an environment named my_env using Conda at OLCF?
","This guide introduces a user to the basic workflow of using conda environments, as well as providing an example of how to create a conda environment that uses a different version of Python than the base environment uses on Summit. Although Summit is being used in this guide, all of the concepts still apply to other OLCF systems. If using Frontier, this assumes you already have installed your own version of conda (e.g., by https://docs.olcf.ornl.gov/software/python/miniconda.html).

OLCF Systems this guide applies to:

Summit

Andes

Frontier (if using conda)",4.408007636969834
"How can I activate an environment named my_env using Conda at OLCF?
","By default this should create the cloned environment in /ccs/home/${USER}/.conda/envs/cloned_env (unless you changed it, as outlined in our https://docs.olcf.ornl.gov/software/python/index.html page).

To activate the new environment you should still load the module first. This will ensure that all of the conda settings remain the same.

$ module load open-ce
(open-ce-1.2.0-py38-X) $ conda activate cloned_env
(cloned_env) $",4.321263355138424
"How can I activate an environment named my_env using Conda at OLCF?
","$ source activate my_env
$ conda env export > environment.yml

You can then email or otherwise provide the environment.yml file to the desired person. The person would then be able to create the environment like so:

$ conda env create -f environment.yml



List environments:

$ conda env list

List installed packages in current environment:

$ conda list

Creating an environment with Python version X.Y:

For a specific path:

$ conda create -p /path/to/your/my_env python=X.Y

For a specific name:

$ conda create -n my_env python=X.Y

Deleting an environment:

For a specific path:",4.287099916208778
"How can I generate a colormap for Process IDs in Paraview?
","The following script renders a 3D sphere colored by the ID (rank) of each MPI task:

# para_example.py:
from paraview.simple import *

# Add a polygonal sphere to the 3D scene
s = Sphere()
s.ThetaResolution = 128                        # Number of theta divisions (longitude lines)
s.PhiResolution = 128                          # Number of phi divisions (latitude lines)

# Convert Proc IDs to scalar values
p = ProcessIdScalars()                         # Apply the ProcessIdScalars filter to the sphere",4.136852479710781
"How can I generate a colormap for Process IDs in Paraview?
","display = Show(p)                              # Show data
curr_view = GetActiveView()                    # Retrieve current view

# Generate a colormap for Proc Id's
cmap = GetColorTransferFunction(""ProcessId"")   # Generate a function based on Proc ID
cmap.ApplyPreset('Viridis (matplotlib)')       # Apply the Viridis preset colors
#print(GetLookupTableNames())                  # Print a list of preset color schemes",4.044368835794502
"How can I generate a colormap for Process IDs in Paraview?
","Navigate to the appropriate file.

Once you choose a file, you will be prompted for the number of nodes and processors you would like to use (remember that each node of Andes contains 32 processors, or 28 if using the high-memory GPU partition) and the Project ID, which VisIt calls a ""Bank"" as shown below.



Once specified, the server side of VisIt will be launched, and you can interact with your data.",3.9600075911609807
"Can I use the open-ce module for other types of workloads besides machine learning?
","Comparing to IBM WML CE, Open-CE no longer has IBM DDL, Caffe(IBM-enhanced), IBM SnapML, Nvidia Rapids, Apex packages, and TensorFlow and PyTorch are not compiled with IBM Large Model Support (LMS). For standalone Keras users using Open-CE version 1.2.0, please pip install keras after module load open-ce.

WML-CE on Summit (slides | recording)  Scaling up deep learning application on Summit (slides | recording)  ML/DL on Summit (slides | recording)",4.258491347707503
"Can I use the open-ce module for other types of workloads besides machine learning?
","IBM Watson Machine Learning Community Edition (ibm-wml-ce) has been replaced by Open-CE. The Open-CE environment is provided on Summit through the module open-ce, which is built based on the Open Cognitive Environment. Open-CE is a Python Anaconda environment that is pre-loaded with many popular machine learning frameworks and tuned to Summit's Power9+NVIDIA Volta hardware.

To access the latest analytics packages use the module load command:

module load open-ce",4.229063965517867
"Can I use the open-ce module for other types of workloads besides machine learning?
","There are several tools that can be used to profile the performance of a deep learning job. Below are links to several tools that are available as part of the open-ce module.

The open-ce module contains the nvprof profiling tool. It can be used to profile work that is running on GPUs. It will give information about when different CUDA kernels are being launched and how long they take to complete. For more information on using the NVIDA profiling tools on Summit, please see these slides.",4.221586980225318
"How can users ensure that they are in compliance with the OLCF Policy?
","The requirements outlined in this document apply to all individuals who have an OLCF account. It is your responsibility to ensure that all individuals have the proper need-to-know before allowing them access to the information on OLCF computing resources. This document will outline the main security concerns.

OLCF computing resources are for business use only. Installation or use of software for personal use is not allowed. Incidents of abuse will result in account termination. Inappropriate uses include, but are not limited to:

Sexually oriented information",4.40628685064359
"How can users ensure that they are in compliance with the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to or use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title:Security Policy Version: 12.10",4.398253508651001
"How can users ensure that they are in compliance with the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to or use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title: Data Management Policy Version: 20.02",4.369260242852618
"How can I increase the memory limit for dask-cuda-workers in Nvidia Rapids?
","echo ""Done!""

Note twelve dask-cuda-workers are executed, one per each available GPU, --memory-limit is set to 82 GB and  --device-memory-limit is set to 16 GB. If using Summit's high-memory nodes --memory-limit can be increased and setting --device-memory-limit to 32 GB  and --rmm-pool-size to 30 GB or so is recommended. Also note it is recommeded to wait some seconds for the dask-scheduler and dask-cuda-workers to start.",4.315375931467177
"How can I increase the memory limit for dask-cuda-workers in Nvidia Rapids?
","Running RAPIDS multi-gpu/multi-node workloads requires a dask-cuda cluster. Setting up a dask-cuda cluster on Summit requires two components:

dask-scheduler.

dask-cuda-workers.

Once the dask-cluster is running, the RAPIDS script should perform four main tasks. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

Reference of multi-gpu/multi-node operation with cuDF, cuML, cuGraph is available in the next links:

10 Minutes to cuDF and Dask-cuDF.

cuML's Multi-Node, Multi-GPU Algorithms.",4.201509737657444
"How can I increase the memory limit for dask-cuda-workers in Nvidia Rapids?
","Large workloads.

Long runtimes on Summit's high memory nodes.

Your Python script has support for multi-gpu/multi-node execution via dask-cuda.

Your Python script is single GPU but requires simultaneous job steps.

RAPIDS is provided in Jupyter following  these instructions.

Note that Python scripts prepared on Jupyter can be deployed on Summit if they use the same RAPIDS version. Use !jupyter nbconvert --to script my_notebook.ipynb to convert notebook files to Python scripts.

RAPIDS is provided on Summit through the module load command:",4.1365254008791
"How can I profile my GPU kernels in detail on Summit?
","There are several tools that can be used to profile the performance of a deep learning job. Below are links to several tools that are available as part of the open-ce module.

The open-ce module contains the nvprof profiling tool. It can be used to profile work that is running on GPUs. It will give information about when different CUDA kernels are being launched and how long they take to complete. For more information on using the NVIDA profiling tools on Summit, please see these slides.",4.354378182149539
"How can I profile my GPU kernels in detail on Summit?
","The first step to GPU profiling is collecting a timeline of your application. (This operation is also sometimes called ""tracing,"" that is, finding the start and stop timestamps of all activities that occurred on the GPU or involved the GPU, such as copying data back and forth.) To do this, we can collect a timeline using the command-line interface, nsys. To use this tool, load the nsight-systems module.

summit> module load nsight-systems

For example, we can profile the vectorAdd CUDA sample (the CUDA samples can be found in $OLCF_CUDA_ROOT/samples if the cuda module is loaded.)",4.282103277795423
"How can I profile my GPU kernels in detail on Summit?
","Prior to Nsight Systems and Nsight Compute, the NVIDIA command line profiling tool was nvprof, which provides both tracing and kernel profiling capabilities. Like with Nsight Systems and Nsight Compute, the profiler data output can be saved and imported into the NVIDIA Visual Profiler for additional graphical analysis. nvprof is in maintenance mode now: it still works on Summit and significant bugs will be fixed, but no new feature development is occurring on this tool.

To use nvprof, the cuda module must be loaded.

summit> module load cuda",4.268403359921459
"What is the default behavior when connecting to Frontier?
","$ ssh <username>@frontier.olcf.ornl.gov

For more information on connecting to OLCF resources, see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#connecting-to-olcf.

By default, connecting to Frontier will automatically place the user on a random login node. If you need to access a specific login node, you will ssh to that node after your intial connection to Frontier.

[<username>@login12.frontier ~]$ ssh <username>@login01.frontier.olcf.ornl.gov

Users can connect to any of the 17 Frontier login nodes by replacing login01 with their login node of choice.",4.106963080130258
"What is the default behavior when connecting to Frontier?
","Recall from the System Overview that Frontier contains two node types: Login and Compute. When you connect to the system, you are placed on a login node. Login nodes are used for tasks such as code editing, compiling, etc. They are shared among all users of the system, so it is not appropriate to run tasks that are long/computationally intensive on login nodes. Users should also limit the number of simultaneous tasks on login nodes (e.g. concurrent tar commands, parallel make",4.075704943080102
"What is the default behavior when connecting to Frontier?
","On Frontier, there are two major types of nodes you will encounter: Login and Compute. While these are similar in terms of hardware (see: https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes), they differ considerably in their intended use.",4.070308580969541
"How can I submit an interactive job on Summit?
","As is the case on other OLCF systems, computational work on Summit is performed within jobs. A typical job consists of several components:

A submission script

An executable

Input files needed by the executable

Output files created by the executable

In general, the process for running a job is to:

Prepare executables and input files

Write the batch script

Submit the batch script

Monitor the job's progress before and during execution",4.32012989972996
"How can I submit an interactive job on Summit?
","To access Summit and Frontier's compute resources for SPI workflows, you must first log into a https://docs.olcf.ornl.gov/systems/index.html#Citadel login node<citadel-login-nodes> and then submit a batch job to one of the SPI specific batch queues.",4.260576689493249
"How can I submit an interactive job on Summit?
","All of the above can also be achieved in an interactive batch job through the use of the salloc command on Andes or the bsub -Is command on Summit. Recall that login nodes should not be used for memory- or compute-intensive tasks, including VisIt.",4.231850839450047
"How can I run a Dask scheduler on Nvidia Rapids?
","Running RAPIDS multi-gpu/multi-node workloads requires a dask-cuda cluster. Setting up a dask-cuda cluster on Summit requires two components:

dask-scheduler.

dask-cuda-workers.

Once the dask-cluster is running, the RAPIDS script should perform four main tasks. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

Reference of multi-gpu/multi-node operation with cuDF, cuML, cuGraph is available in the next links:

10 Minutes to cuDF and Dask-cuDF.

cuML's Multi-Node, Multi-GPU Algorithms.",4.381066435574788
"How can I run a Dask scheduler on Nvidia Rapids?
","Large workloads.

Long runtimes on Summit's high memory nodes.

Your Python script has support for multi-gpu/multi-node execution via dask-cuda.

Your Python script is single GPU but requires simultaneous job steps.

RAPIDS is provided in Jupyter following  these instructions.

Note that Python scripts prepared on Jupyter can be deployed on Summit if they use the same RAPIDS version. Use !jupyter nbconvert --to script my_notebook.ipynb to convert notebook files to Python scripts.

RAPIDS is provided on Summit through the module load command:",4.246457898331434
"How can I run a Dask scheduler on Nvidia Rapids?
","As mentioned earlier, the RAPIDS code should perform four main tasks as shown in the following script. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

import sys
from dask.distributed import Client

def disconnect(client, workers_list):
    client.retire_workers(workers_list, close_workers=True)
    client.shutdown()

if __name__ == '__main__':

    sched_file = str(sys.argv[1]) #scheduler file
    num_workers = int(sys.argv[2]) # number of workers to wait for",4.241208401547596
"How can I simplify support for large task counts in Frontier?
","As a DOE Leadership Computing Facility, OLCF has a mandate that a large portion of Frontier's usage come from large, leadership-class (a.k.a. capability) jobs. To ensure that OLCF complies with this directive, we strongly encourage users to run jobs on Frontier that are as large as their code will allow. To that end, OLCF implements queue policies that enable large jobs to run in a timely fashion.

The OLCF implements queue policies that encourage the submission and timely execution of large, leadership-class jobs on Frontier.",4.173846307052027
"How can I simplify support for large task counts in Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.155071143668053
"How can I simplify support for large task counts in Frontier?
","Because a Frontier compute node has two hardware threads available (2 logical cores per physical core), this enables the possibility of multithreading your application (e.g., with OpenMP threads). Although the additional hardware threads can be assigned to additional MPI tasks, this is not recommended. It is highly recommended to only use 1 MPI task per physical core and to use OpenMP threads instead on any additional logical cores gained when using both hardware threads.",4.129114959268573
"How many GPUs are available to CuPy on Frontier?
","Compute nodes equipped with NVIDIA GPUs will be able to take full advantage of CuPy's capabilities on the system, providing significant speedups over NumPy-written code. CuPy with AMD GPUs is still being explored, and the same performance is not guaranteed (especially with larger data sizes). Instructions for Frontier are available in this guide, but users must note that the CuPy developers have labeled this method as experimental and has limitations.



<string>:60: (INFO/1) Duplicate implicit target name: ""installing cupy"".",4.423904666746571
"How many GPUs are available to CuPy on Frontier?
","Summit

Frontier

Andes

GPU computing has become a big part of the data science landscape, as array operations with NVIDIA GPUs can provide considerable speedups over CPU computing. Although GPU computing on Summit is often utilized in codes that are written in Fortran and C, GPU-related Python packages are quickly becoming popular in the data science community. One of these packages is CuPy, a NumPy/SciPy-compatible array library accelerated with NVIDIA CUDA.",4.350551550801151
"How many GPUs are available to CuPy on Frontier?
","As noted in AMD+CuPy limitations, data sizes explored here hang. So, this section currently does not apply to Frontier.

Now that you know how to use CuPy, time to see the actual benefits that CuPy provides for large datasets. More specifically, let's see how much faster CuPy can be than NumPy on Summit. You won't need to fix any errors; this is mainly a demonstration on what CuPy is capable of.

There are a few things to consider when running on GPUs, which also apply to using CuPy:

Higher precision means higher cost (time and space)

The structuring of your data is important",4.315545042562583
"What is the maximum number of CPU cores available on Summit?
","Summit node architecture diagram

The basic building block of Summit is the IBM Power System AC922 node. Each of the approximately 4,600 compute nodes on Summit contains two IBM POWER9 processors and six NVIDIA Tesla V100 accelerators and provides a theoretical double-precision capability of approximately 40 TF. Each POWER9 processor is connected via dual NVLINK bricks, each capable of a 25GB/s transfer rate in each direction.",4.321154070295483
"What is the maximum number of CPU cores available on Summit?
","Summit is an IBM system located at the Oak Ridge Leadership Computing Facility. With a theoretical peak double-precision performance of approximately 200 PF, it is one of the most capable systems in the world for a wide range of traditional computational science applications. It is also one of the ""smartest"" computers in the world for deep learning applications with a mixed-precision capability in excess of 3 EF.



Summit node architecture diagram",4.308938899695715
"What is the maximum number of CPU cores available on Summit?
","Login nodes have (2) 16-core Power9 CPUs and (4) V100 GPUs. Compute nodes have (2) 22-core Power9 CPUs and (6) V100 GPUs.

Summit nodes are connected to a dual-rail EDR InfiniBand network providing a node injection bandwidth of 23 GB/s. Nodes are interconnected in a Non-blocking Fat Tree topology. This interconnect is a three-level tree implemented by a switch to connect nodes within each cabinet (first level) along with Director switches (second and third level) that connect cabinets together.",4.301808686221238
"Are there any specific requirements for SPI project usernames?
","In order to help ensure data separation, each SPI user is given a unique userid for each project. SPI user names use the format: <userid>_<proj>_mde . For example: usera_prj123_mde.

SPI usernames will not overlap with usernames used in the non-SPI enclaves. Unlike non-SPI usernames, SPI usernames only exist in a single project. Users on multiple SPI projects will have a unique username for each SPI project.  You must specify your unique SPI username matching the target project when connecting.",4.463251683785151
"Are there any specific requirements for SPI project usernames?
","Once a project has been approved and created, the next step will be to request user accounts.  A user account will allow an individual to access the project's allocated resources.    This process to request an account is very similar to the process used for non-SPI projects.  One notable difference between SPI and non-SPI accounts: SPI usernames are unique to a project.  SPI usernames use the format: <userid>_<proj>_mde.  If you have access to three SPI projects, you will have three userIDs with three separate home areas.",4.450853752402708
"Are there any specific requirements for SPI project usernames?
","In order to help ensure data separation, each SPI user is given a unique userID for each project. SPI userIDs use the format: <userid>_<proj>_mde . For example: userx_abc123_mde. SPI usernames will not overlap with usernames used in the non-SPI enclaves. Unlike non-SPI usernames, SPI usernames only exist in a single project. Users on multiple SPI projects will have a unique username for each SPI project.",4.436555514307922
"What is the difference between frontier-simple and frontier-mapping job layouts?
","node layout <frontier-simple> should also be considered when selecting job layout.  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-srun section for more srun information, and see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-mapping for srun examples on Frontier.  OLCF Support  If you encounter any issues or have questions, please contact the OLCF via the following:  Email us at help@olcf.ornl.gov  Contact your OLCF liaison  Sign-up to attend OLCF Office Hours",4.224029519120009
"What is the difference between frontier-simple and frontier-mapping job layouts?
","different commands.  Notable are the separation in batch script submission (sbatch) and interactive batch submission (salloc).  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-slurm section for more infomation including a LSF to Slurm command comparison.  Srun job launcher  Frontier uses Slurm's job launcher, srun, instead of Summit's jsrun to launch parallel jobs within a batch script.  Overall functionality is similar, but commands are notably different. Frontier's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#compute node layout <frontier-simple>",4.197185324354381
"What is the difference between frontier-simple and frontier-mapping job layouts?
","To override this default layout (not recommended), set -S 0 at job allocation.



Frontier uses SchedMD's Slurm Workload Manager for scheduling and managing jobs. Slurm maintains similar functionality to other schedulers such as IBM's LSF, but provides unique control of Frontier's resources through custom commands and options specific to Slurm. A few important commands can be found in the conversion table below, but please visit SchedMD's Rosetta Stone of Workload Managers for a more complete conversion reference.",4.193261531434477
"What is the difference between ""DNSPolicy: ClusterFirst"" and ""DNSPolicy: Default""?
","Network policies are specifications of how groups of pods are allowed to communicate with each other and other network endpoints. A pod is selected in a Network Policy based on a label so the Network Policy can define rules to specify traffic to that pod.

When you create a namespace in Slate with the Quota Dashboard some Network Policies are added to your newly created namespace. This means that pods inside your project are isolated from connections not defined in these Network Policies.",4.02770772097989
"What is the difference between ""DNSPolicy: ClusterFirst"" and ""DNSPolicy: Default""?
","Network Policies do not conflict, they are additive. This means that if two policies match a pod the pod will be restricted to what is allowed by the union of those policies' ingress/egress rules.

To create a Network policy using the GUI click the Networking tab followed by the Network Policy tab:

Creating Network Policies

This will place you in an editor with some boiler plate YAML. From here you can define the network policy that you need for your namespace. Below is an example Network Policy that you would use to allow all external traffic to access a nodePort in your namespace.",4.007947953250152
"What is the difference between ""DNSPolicy: ClusterFirst"" and ""DNSPolicy: Default""?
","The key value pair, or label, under spec.podSelector.matchLabels will need to match exactly to the pod in your namespace that the policy is for example the above NetworkPolicy would match pods with these labels set:  apiVersion: v1 Kind: Pod metadata:   labels:     key: value ...

To view the Network policies in your namespace you can run:

oc get networkpolicy -n YOUR_NAMESPACE

to get the name of the network policy and then:

oc get networkpolicy NETWORKPOLICY_NAME -o yaml

to view object's YAML.",3.93008729455447
"Can I request a reservation for a specific node on Summit?
","The requesting project's allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Summit would be equivalent to using 16,000 Summit node-hours of a project's allocation.



As is the case with many other queuing systems, it is possible to place dependencies on jobs to prevent them from running until other jobs have started/completed/etc. Several possible dependency settings are described in the table below:",4.296893647139727
"Can I request a reservation for a specific node on Summit?
","The requesting project’s allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Frontier would be equivalent to using 16,000 Frontier node-hours of a project’s allocation.

Reservations should not be confused with priority requests. If quick turnaround is needed for a few jobs or for a period of time, a priority boost should be requested. A reservation should only be requested if users need to guarantee availability of a set of nodes at a given time, such as for a live demonstration at a conference.",4.2529957923359065
"Can I request a reservation for a specific node on Summit?
",For Summit:,4.204228426426005
"How do I define a user region in Score-P?
","In this case, ""my_region"" is the handle name of the region which has to be defined with SCOREP_USER_REGION_DEFINE. Additionally, ""foo"" is the string containing the region's unique name (this is the name that will show up in Vampir) and SCOREP_USER_REGION_TYPE_COMMON identifies the type of the region. Make note of the header files seen in the above example that are needed to include the Score-P macros. See the Score-P User Adapter page for more user configuration options.

Below are some examples of manually instrumented regions using phase and loop types:

#include <scorep/SCOREP_User.h>",4.4123905819343525
"How do I define a user region in Score-P?
",".. code::

   #include <scorep/SCOREP_User.inc>

   subroutine foo
      SCOREP_USER_REGION_DEFINE(my_region)
      SCOREP_USER_REGION_BEGIN(my_region, ""foo"", SCOREP_USER_REGION_TYPE_COMMON)
      ! do something
      SCOREP_USER_REGION_END(my_region)
   end subroutine foo",4.304544517907768
"How do I define a user region in Score-P?
","The regions ""sum"" and ""my_calculations"" in the above examples would then be included in the profiling and tracing runs and can be analysed with Vampir. For more details, refer to the Advanced Score-P training in the https://docs.olcf.ornl.gov/systems/Scorep.html#training-archive.

Please see the provided video below to watch a brief demo of using Score-P provided by TU-Dresden and presented by Ronny Brendel.",4.269080244759477
"How do I get more information about my job's performance on Summit?
",For Summit:,4.239607452124859
"How do I get more information about my job's performance on Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.193169555796888
"How do I get more information about my job's performance on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.177633418430028
"Can I use Ubuntu as a base image for containerization on Summit?
","Users will be building and running containers on Summit without root permissions i.e. containers on Summit are rootless.  This means users can get the benefits of containers without needing additional privileges. This is necessary for a shared system like Summit. And this is part of the reason why Docker doesn't work on Summit. Podman and Singularity provides rootless support but to different extents hence why users need to use a combination of both.

Users will need to set up a file in their home directory /ccs/home/<username>/.config/containers/storage.conf with the following content:",4.288926963773021
"Can I use Ubuntu as a base image for containerization on Summit?
","You can view the Dockerfiles used to build the MPI base image at the code.ornl.gov repository. These Dockerfiles are buildable on Summit yourself by cloning the repository and running the ./build in the individual directories in the repository. This allows you the freedom to modify these base images to your own needs if you don't need all the components in the base images. You may run into the cgroup memory limit when building so kill the podman process, log out, and try running the build again if that happens when building.",4.286801685076014
"Can I use Ubuntu as a base image for containerization on Summit?
","Podman is a container framework from Red Hat that functions as a drop in replacement for Docker. On Summit, we will use Podman for building container images and converting them into tar files for Singularity to use. Podman makes use of Dockerfiles to describe the images to be built. We cannot use Podman to run containers as it doesn't properly support MPI on Summit, and Podman does not support storing its container images on GPFS or NFS.",4.272159338721467
"What is the disadvantage of installing the OpenShift CLI tool from source?
","It is a single binary that can be downloaded from a number of places (the choice is yours):

Direct from the cluster (preferred):

Marble Command Line Tools

Onyx Command Line Tools

Homebrew on MacOS (need Homebrew setup first):

The Homebrew package is not always kept up to date with the latest version of OpenShift so some client features may not be available

$ brew install openshift-cli

RHEL/CentOS (requires openshift-origin repo):

$ yum install origin-clients

From Source

https://github.com/openshift/oc",4.254060836194164
"What is the disadvantage of installing the OpenShift CLI tool from source?
","Source to image is a tool to build docker formatted container images. This is accomplished by injecting source code into a container image and then building a new image. The new image will contain the source code ready to run, via the $ docker run command inside the newly built image.

Using the S2I model of building images, it is possible to have your source hosted on a git repository and then pulled and built by Openshift.

Click on the newly created project and then on the blue Add to Project button on the center of the page.",4.0547324289165765
"What is the disadvantage of installing the OpenShift CLI tool from source?
","Openshift will automatically pull and build your source. If you make a change and need an updated build simply navigate to the build option on the menu to the left and select your project and click Start Build in the upper right.

The first step to creating the the build is to create a build config. This is done in one of three ways depending on how your code is structured. If you have a git repository already configured and it is public then the command

oc new-build .

will create your build config from that repository.",4.052044917347553
"Can I use a NodePort to access a service that is not exposed by a service?
","Note that a NodePort value will automatically be given by the service controller.

Your service can then be accessed by the scheme apps.{cluster}.ccs.ornl.gov:{nodePort}.

In this example, if the service was running on marble, I could access it with apps.marble.ccs.ornl.gov:30298",4.376451695768541
"Can I use a NodePort to access a service that is not exposed by a service?
","In general, using FQDNs to access a service is more convenient. If your service communicates over HTTP or HTTPS, you can set up a https://docs.olcf.ornl.gov/systems/services.html#slate_routes to achieve this. If your service uses another protocol, you can use https://docs.olcf.ornl.gov/systems/services.html#slate_nodeports.

NodePort is a type of Service that reserves a port across the cluster that can route traffic to your pods. This is more flexible than a Route and can handle any TCP or UDP traffic.",4.34225712230044
"Can I use a NodePort to access a service that is not exposed by a service?
","A NodePort reserves a port across all nodes of the cluster. This port routes traffic to a service, which points to the pods that match the service's label selector.

NodePorts are given in the 30000-32767 range. These are ports you can use from outside the cluster to access resources inside of OpenShift.

For the Openshift clusters you will additionally need to create a https://docs.olcf.ornl.gov/systems/nodeport.html#network policy <slate_network_policies> file to allow external traffic into your namespace.",4.331676675527194
"What is the command to create a NodePort for the MongoDB service?
","$ oc patch service mongo -p '{""spec"":{""type"":""NodePort""}}'
$ oc get service mongo
NAME    TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE
mongo   NodePort   172.25.233.185   <none>        27017:32093/TCP   13s

In this example, the NodePort that was automatically assigned was 32093 which is routing traffic to 27017 on the Service.

We will also need to add a network rule to allow ingress traffic.",4.420993441561431
"What is the command to create a NodePort for the MongoDB service?
","Steps to configure mongoku

Navigate to http://localhost:3100

Add Server -> ""admin:password@mongo:27017""

Click ""mongo""

We could use the port forwarding technique but that uses a connection that goes through the API server for the cluster which is not very performant. We will change the Service/mongo object so that it creates a NodePort that we can access from outside of the cluster.",4.4195254240842345
"What is the command to create a NodePort for the MongoDB service?
","For instance, if you have a mongoDB instance running on port 27017 with a deployment named mongodb, you could run oc port-forward deployment/mongodb 7777:27017. Now you can simply run mongo --port 7777 (assuming you have the mongo client installed on your local machine) and have access to your mongodb instance in the cluster, as if it were running on your local machine.",4.305297845804913
"How do I ensure the feasibility of my project's success using QCUP resources?
","A QCUP proposal describes the nature, methodology, and merits of the project, explains why it requires access to QCUP resources, and outlines any other essential information that might be needed for its consideration. Project applications are submitted using the Project Application Form. Select ""OLCF Quantum Computing User Program"" from the dropdown menu.

For QCUP Projects, all proposed work must be open, fundamental research and no Export Control, PHI, or other controlled data can be used.",4.346631200790299
"How do I ensure the feasibility of my project's success using QCUP resources?
","Once submitted, you will receive email notification of successful proposal submission.  The proposal is then reviewed by the Quantum Resource Utilization Council (QRUC), as well as independent referees for merit and to ensure the feasibility of project success using the resources available to the QCUP. You will be notified of the QRUC decision via email.

Once a project request is approved by the QRUC, an OLCF Accounts Manager will communicate with the project’s PI to finalize activation and request a signed Principal Investigator’s PI Agreement to be submitted.",4.339545779736215
"How do I ensure the feasibility of my project's success using QCUP resources?
","Welcome! The information below introduces how we structure projects and user accounts for access to the Quantum resources within the QCUP program. In general, OLCF QCUP resources are granted to projects, which are in turn made available to the approved users associated with each project.",4.2924258095358745
"What is the oldest version of Horovod available in the open-ce/1.5.0 environment?
",| PyTorch 1.10.2 https://github.com/open-ce/pytorch-feedstock | | Horovod 0.21.0 (NCCL Backend) https://github.com/horovod/horovod | Horovod 0.22.1 (NCCL Backend) https://github.com/horovod/horovod | Horovod 0.23.0 (NCCL Backend) https://github.com/horovod/horovod | Horovod 0.23.0 (NCCL Backend) https://github.com/horovod/horovod | | Complete List | 1.2.0 Software Packages https://github.com/open-ce/open-ce/releases/tag/open-ce-v1.2.0 | 1.4.0 Software Packages https://github.com/open-ce/open-ce/releases/tag/open-ce-v1.4.0 | 1.5.0 Software Packages,4.172943804449572
"What is the oldest version of Horovod available in the open-ce/1.5.0 environment?
","Horovod comes with a tool called Timeline which can help analyze the performance of Horovod. This is particularly useful when trying to scale a deep learning job to many nodes. The Timeline tool can help pick various options that can improve the performance of distributed deep learning jobs that are using Horovod. For more information, please see Horovod's documentation.",4.062319223942556
"What is the oldest version of Horovod available in the open-ce/1.5.0 environment?
","On Tuesday, July 18, 2023, Frontier was upgraded to a new version of the system software stack. During the upgrade, the following changes took place:

The system was upgraded to Cray OS 2.5, Slingshot Host Software 2.0.2-112, and the AMD GPU 6.0.5 device driver (ROCm 5.5.1 release).

ROCm 5.5.1 is now available via the rocm/5.5.1 modulefile.

HPE/Cray Programming Environments (PE) 23.05 is now available via the cpe/23.05 modulefile.

HPE/Cray PE 23.05 introduces support for ROCm 5.5.1. However, due to issues identified during testing, ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.",3.9918457589830334
"How can I improve the performance of my deep learning model on Summit?
",| | 2020-02-10 | NCCL on Summit | Sylvain Jeaugey (NVIDIA) | Scaling Up Deep Learning Applications on Summit https://www.olcf.ornl.gov/calendar/scaling-up-deep-learning-applications-on-summit/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/12/Summit-NCCL.pdf https://vimeo.com/391520479 | | 2020-02-10 | Introduction to Watson Machine Learning CE | Brad Nemanich & Bryant Nelson (IBM) | Scaling Up Deep Learning Applications on Summit https://www.olcf.ornl.gov/calendar/scaling-up-deep-learning-applications-on-summit/ | (slides | recording),4.233322412730628
"How can I improve the performance of my deep learning model on Summit?
","Large workloads.

Long runtimes on Summit's high memory nodes.

Your Python script has support for multi-gpu/multi-node execution via dask-cuda.

Your Python script is single GPU but requires simultaneous job steps.

RAPIDS is provided in Jupyter following  these instructions.

Note that Python scripts prepared on Jupyter can be deployed on Summit if they use the same RAPIDS version. Use !jupyter nbconvert --to script my_notebook.ipynb to convert notebook files to Python scripts.

RAPIDS is provided on Summit through the module load command:",4.229821727471875
"How can I improve the performance of my deep learning model on Summit?
","Summit

Andes

Frontier

Scientific simulations generate large amounts of data on Summit (about 100 Terabytes per day for some applications). Because of how large some datafiles may be, it is important that writing and reading these files is done as fast as possible. Less time spent doing input/output (I/O) leaves more time for advancing a simulation or analyzing data.",4.210885853145789
"What is the purpose of the jsrun command in TAU?
","This section describes tools that users might find helpful to better understand the jsrun job launcher.

hello_jsrun is a ""Hello World""-type program that users can run on Summit nodes to better understand how MPI ranks and OpenMP threads are mapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A screencast showing how to use Hello_jsrun is also available: https://vimeo.com/261038849

Job Step Viewer provides a graphical view of an application's runtime layout on Summit. It allows users to preview and quickly iterate with multiple jsrun options to understand and optimize job launch.",4.29912911685285
"What is the purpose of the jsrun command in TAU?
","For Fortran: replace the compiler with the TAU wrapper tau_f90.sh / tau_f77.sh

Even if you don't compile your application with a TAU wrapper, you can profile some basic functionalities with tau_exec, for example:

jsrun -n 4 –r 4 –a 1 –c 1 tau_exec -T mpi ./test

The above command profiles MPI for the binary test, which was not compiled with the TAU wrapper.

The following TAU environment variables may be useful in job submission scripts.",4.261656447471461
"What is the purpose of the jsrun command in TAU?
",Below is a comparison table between srun and jsrun.,4.225177365031183
"How can I set the username for the ORNL Summit server in Paraview?
","After setting up and installing ParaView, you can connect to OLCF systems remotely to visualize your data interactively through ParaView's GUI. To do so, go to File→Connect and select either ORNL Andes or ORNL Summit (provided they were successfully imported -- as outlined in https://docs.olcf.ornl.gov/systems/paraview.html#paraview-install-setup). Next, click on Connect and change the values in the Connection Options box.",4.258163132201728
"How can I set the username for the ORNL Summit server in Paraview?
","Summit

**For Summit:**

- **Host nickname**: ``Summit`` (this is arbitrary)
- **Remote hostname**: ``summit.olcf.ornl.gov`` (required)
- **Host name aliases**: ``login#`` (required)
- **Maximum Nodes**: Unchecked
- **Maximum processors**: Unchecked (arbitrary)
- **Path to VisIt Installation**: ``/sw/summit/visit`` (required)
- **Username**: Your OLCF Username (required)
- **Tunnel data connections through SSH**: Checked (required)

Under the “Launch Profiles” tab create a launch profile. Most of these values
are arbitrary",4.214520275795243
"How can I set the username for the ORNL Summit server in Paraview?
","After installing, you must give ParaView the relevant server information to be able to connect to OLCF systems (comparable to VisIt's system of host profiles). The following provides an example of doing so. Although several methods may be used, the one described should work in most cases.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.

For Windows clients, it is necessary to install PuTTY to create an ssh connection in step 2.

Step 1: Save the following servers.pvsc file to your local computer",4.206856995691645
"How is the forfeited amount calculated?
","| Date | Utilization to-Date | Forfeited Amount | | --- | --- | --- | | May 1 | < 10% | Up to 30% of remaining allocation | | < 15% | Up to 15% of remaining allocation | | September 1 | < 10% | Up to 75% of remaining allocation | | < 33% | Up to 50% of remaining allocation | | < 50% | Up to 33% of remaining allocation |

For example, a 1,000,000 core-hour INCITE project that has utilized only 50,000 core-hours (5% of the allocation) on May 1st would forfeit (0.30 * 950,000) = 285,000 core-hours from their remaining allocation.",4.185018001966047
"How is the forfeited amount calculated?
",| % Of Allocation Used | Priority Reduction | | --- | --- | | < 100% | 0 days | | 100% to 125% | 30 days | | > 125% | 365 days |,3.93713181146429
"How is the forfeited amount calculated?
","Once submitted, you will receive email notification of successful proposal submission.  The proposal is then reviewed by the Quantum Resource Utilization Council (QRUC), as well as independent referees for merit and to ensure the feasibility of project success using the resources available to the QCUP. You will be notified of the QRUC decision via email.

Once a project request is approved by the QRUC, an OLCF Accounts Manager will communicate with the project’s PI to finalize activation and request a signed Principal Investigator’s PI Agreement to be submitted.",3.8852133321371736
"Can I use h5py to manipulate the shape of an HDF5 dataset in parallel?
","There are various tools that allow users to interact with HDF5 data, but we will be focusing on h5py -- a Python interface to the HDF5 library. h5py provides a simple interface to exploring and manipulating HDF5 data as if they were Python dictionaries or NumPy arrays. For example, you can extract specific variables through slicing, manipulate the shapes of datasets, and even write completely new datasets from external NumPy arrays.",4.383649272090315
"Can I use h5py to manipulate the shape of an HDF5 dataset in parallel?
","Both HDF5 and h5py can be compiled with MPI support, which allows you to optimize your HDF5 I/O in parallel. MPI support in Python is accomplished through the mpi4py package, which provides complete Python bindings for MPI. Building h5py against mpi4py allows you to write to an HDF5 file using multiple parallel processes, which can be helpful for users handling large datasets in Python. h5Py is available after loading the default Python module on either Summit or Andes, but it has not been built with parallel support.",4.363081957323321
"Can I use h5py to manipulate the shape of an HDF5 dataset in parallel?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.

This guide teaches you how to build a personal, parallel-enabled version of h5py and how to write an HDF5 file in parallel using mpi4py and h5py.

In this guide, you will:

Learn how to install mpi4py

Learn how to install parallel h5py

Test your build with Python scripts

OLCF Systems this guide applies to:

Summit

Andes

Frontier",4.197452928133068
"What is the command to install MinIO using Helm?
","It is assumed you have already gone through https://docs.olcf.ornl.gov/systems/minio.html#slate_getting_started and established the https://docs.olcf.ornl.gov/systems/minio.html#helm_prerequisite. Please do that before attempting to deploy anything on Slate's clusters.

This example uses Helm version 3 to deploy a MinIO standalone Helm chart on Slate's Marble Cluster. This is the cluster in OLCF's Moderate enclave, the same enclave as Summit.

To start, clone the slate helm examples repository , containing the MinIO standalone Helm chart, and navigate into the charts directory:",4.326204848584359
"What is the command to install MinIO using Helm?
","You have configured your values.yaml file.

You have created your MinIO Application's Secret Tokens and applied them to the Marble project you are logged into.

<string>:5: (INFO/1) Duplicate explicit target name: ""slate helm examples repository"".

You are in the slate_helm_examples/charts directory, within your local copy of the slate helm examples repository.

If you checked the above off, you can install the MinIO chart, into your Marble project, with this command:

$ helm install <your application name> minio-standalone/ --namespace <your marble project namespace>",4.3202130329228
"What is the command to install MinIO using Helm?
","These values are picked up as environment variables from the templates/minio-standalone-deployment.yaml file.

It is recommended to keep the secret-token.yaml file safe, locally, and not in a repository if unencrypted.

At this point we are ready to install our minio-standalone chart in our Marble project namespace.

To list your available project spaces run this command:

$ oc projects

Check list:

You have the OC CLI Tool

You have Helm version 3

You are logged into Marble, with the OC CLI Tool, and in the correct Marble project.

You have configured your values.yaml file.",4.263060001257016
"How can I build a package from source in my Conda environment?
","One way to install packages into your conda environment is to build packages from source using pip. This approach is useful if a specific package or package version is not available in the conda repository, or if the pre-compiled binaries don't work on the HPC resources (which is common). However, building from source means you need to take care of some of the dependencies yourself, especially for optimization. Pip is available to use after installing Python into your conda environment, which you have already done.",4.435948167627943
"How can I build a package from source in my Conda environment?
","Because issues can arise when using conda and pip together (see link in https://docs.olcf.ornl.gov/systems/conda_basics.html#conda-refs), it is recommended to do this only if absolutely necessary.

To build a package from source, use pip install --no-binary=<package_name> <package_name>:

$ CC=gcc pip install --no-binary=numpy numpy

The CC=gcc flag will ensure that you are using the proper compiler and wrapper. Building from source results in a longer installation time for packages, so you may need to wait a few minutes for the install to finish.",4.344416997423773
"How can I build a package from source in my Conda environment?
","Cloning the base environment:

It is not recommended to try to install new packages into the base environment. Instead, you can clone the base environment for yourself and install packages into the clone. To clone an environment, you must use the --clone <env_to_clone> flag when creating a new conda environment. An example for cloning the base environment into your Project Home directory on Summit is provided below:",4.305113468372426
"How do I specify the configuration file for my Paraview session?
","Click ""Apply""

At the top menu click on ""Options""→""Save Settings""



Once you have VisIt installed and set up on your local computer:

Open VisIt on your local computer.

Go to: ""File→Open file"" or click the ""Open"" button on the GUI.

Click the ""Host"" dropdown menu on the ""File open"" window that popped up and choose ""ORNL_Andes"".

This will prompt you for your OLCF password, and connect you to Andes.

Navigate to the appropriate file.",4.176550156202731
"How do I specify the configuration file for my Paraview session?
","After installing, you must give ParaView the relevant server information to be able to connect to OLCF systems (comparable to VisIt's system of host profiles). The following provides an example of doing so. Although several methods may be used, the one described should work in most cases.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.

For Windows clients, it is necessary to install PuTTY to create an ssh connection in step 2.

Step 1: Save the following servers.pvsc file to your local computer",4.1635386502563065
"How do I specify the configuration file for my Paraview session?
","Although they can be separate files, both Andes and Summit server configurations can be combined and saved into one file following the hierarchy <Servers><Server name= >...<\Server><Server name= >...<\Server><\Servers>.

Step 2: Launch ParaView on your Desktop and Click on File -> Connect

Start ParaView and then select File/Connect to begin.



Step 3: Import Servers

Click Load Servers button and find the servers.pvsc file",4.158666344518264
"What is the link to the OLCF SPI request form?
","More information on the account process and a link to the request form can be found in the https://docs.olcf.ornl.gov/systems/index.html#OLCF Applying for a User Account<applying-for-a-user-account> section.

The OLCF SPI provides compute, filesystem, and data transfer resources.

<string>:98: (WARNING/2) Title underline too short.

`https://docs.olcf.ornl.gov/systems/index.html#Compute<spi-compute-citadel>`
-------------------------------------",4.360738134251121
"What is the link to the OLCF SPI request form?
","Similar to standard OLCF workflows, to run SPI workflows on OLCF resources, you will first need an approved project.  The project will be awarded an allocation(s) that will allow resource access and use.  The first step to requesting an allocation is to complete the project request form.  The form will initiate the process that includes peer review, export control review, agreements, and others.  Once submitted, members of the OLCF accounts team will help walk you through the process.

Please see the OLCF Accounts and Projects section of this site to request a new project.",4.354496638125133
"What is the link to the OLCF SPI request form?
","CITADEL, having undergone comprehensive technical-, legal-, and policy-oriented reviews and received third-party accreditation, has presented new possibilities for research projects that previously could not access utilize OLCF systems due to the nature of their data.

If you are new to the OLCF or the OLCF's SPI, this page can help get you started.  Below are some of the high-level steps needed to begin use of the OLCF's SPI:",4.318469266880344
"How do I invoke the ""gendata.sh"" script?
","The suggested directory structure is to have a outer directory say swift-work that has the swift source and shell scripts. Inside of swift-work create a new directory called data.

Additionally, we will need two terminals open. In the first terminal window, navigate to the swift-work directory and invoke the data generation script like so:

$ ./gendata.sh

In the second terminal, we will run the swift workflow as follows (make sure to change the project name per your allocation):",4.236619036406598
"How do I invoke the ""gendata.sh"" script?
","In order to demonstrate the data generation, we have a script that downloads image data from the NOAA website periodically. The image is a geographical image showing current cloud cover over south-east US. The code gendata.sh looks like so:

#!/bin/bash
set -eu

function cleanup() {
  \rm -f ./data/earth*.jpg
}

while true
do
  uid=$(uuidgen | awk -F- '{print $1}')
  wget -q https://cdn.star.nesdis.noaa.gov/GOES16/ABI/SECTOR/se/GEOCOLOR/1200x1200.jpg -O ./data/earth${uid}.jpg
  sleep 5
  trap cleanup EXIT
done",4.123018104533391
"How do I invoke the ""gendata.sh"" script?
","Next, we have the data processing script called processdata.sh that looks as follows:

#!/bin/bash
set -eu

TASK=convert
DATA=$1
echo ""\nProcessing ${DATA}\n""
${TASK} ${DATA} -fuzz 10% -fill white -opaque white -fill black +opaque white -format ""%[fx:100*mean]"" info:
sleep 5

The above script computes the cloud cover percentage by looking at the amount of white pixels in the image. Note that it uses ImageMagick's convert utility.",4.078668111744744
"How long does the job request to run for?
","*apparent* time a job has been waiting. These factors include:



-  The number of nodes requested by the job.

-  The queue to which the job is submitted.

-  The 8-week history of usage for the project associated with the job.

-  The 8-week history of usage for the user associated with the job.



If your jobs require resources outside these queue policies, please complete the

relevant request form on the `Special Requests

<https://www.olcf.ornl.gov/support/getting-started/special-request-form/>`__

page. If you have any questions or comments on the queue policies below, please",4.245793050939378
"How long does the job request to run for?
","same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. Additionally, projects that are at 125% of their allocated time will be limited to only 3 running jobs at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.171895980079147
"How long does the job request to run for?
","same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. Additionally, projects that are at 125% of their allocated time will be limited to only 3 running jobs at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.171895980079147
"What is the advantage of using Lmod to modify my shell environment?
","The modules software package allows you to dynamically modify your user environment by using pre-written modulefiles. Environment modules are provided through Lmod, a Lua-based module system for dynamically altering shell environments.  By managing changes to the shell’s environment variables (such as path, ld_library_path, and pkg_config_path), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.",4.508787783394365
"What is the advantage of using Lmod to modify my shell environment?
","Environment management with lmod

The modules software package allows you to dynamically modify your user environment by using pre-written modulefiles. Environment modules are provided through Lmod, a Lua-based module system for dynamically altering shell environments.  By managing changes to the shell’s environment variables (such as path, ld_library_path, and pkg_config_path), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.",4.499212244240728
"What is the advantage of using Lmod to modify my shell environment?
","Environment modules are provided through Lmod, a Lua-based module system for dynamically altering shell environments. By managing changes to the shell’s environment variables (such as PATH, LD_LIBRARY_PATH, and PKG_CONFIG_PATH), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.

The interface to Lmod is provided by the module command:",4.473764976919429
"What is the role of the GigaThread Engine on Summit?
","Each Summit Compute node has 6 NVIDIA V100 GPUs.  The NVIDIA Tesla V100 accelerator has a peak performance of 7.8 TFLOP/s (double-precision) and contributes to a majority of the computational work performed on Summit. Each V100 contains 80 streaming multiprocessors (SMs), 16 GB (32 GB on high-memory nodes) of high-bandwidth memory (HBM2), and a 6 MB L2 cache that is available to the SMs. The GigaThread Engine is responsible for distributing work among the SMs and (8) 512-bit memory controllers control access to the 16 GB (32 GB on high-memory nodes) of HBM2 memory. The V100 uses NVIDIA's",4.270131856685913
"What is the role of the GigaThread Engine on Summit?
","The NVIDIA Tesla V100 accelerator has a peak performance of 7.8 TFLOP/s (double-precision) and contributes to a majority of the computational work performed on Summit. Each V100 contains 80 streaming multiprocessors (SMs), 16 GB (32 GB on high-memory nodes) of high-bandwidth memory (HBM2), and a 6 MB L2 cache that is available to the SMs. The GigaThread Engine is responsible for distributing work among the SMs and (8) 512-bit memory controllers control access to the 16 GB (32 GB on high-memory nodes) of HBM2 memory. The V100 uses NVIDIA's NVLink interconnect to pass data between GPUs as well",4.200510130167213
"What is the role of the GigaThread Engine on Summit?
",For Summit:,4.176291670822951
"What is the server authentication window pop-up?
","When the server authentication window pops up, you will need to enter your USERID & the https://docs.olcf.ornl.gov/systems/Vampir.html#VampirServer password <vampserpw> that was printed on the terminal screen. Once authenticated, you will be able to navigate through the filesystem to your .otf2 files















































This connection method is more complex than the other 2 methods, however it also can provide a more optimal experience for very large trace files.",4.128788229280415
"What is the server authentication window pop-up?
","Launch the Vampir GUI on your local machine

Similar to how we have connected Vampir to the VampirServer in the https://docs.olcf.ornl.gov/systems/Vampir.html#previous section, <vampauth> you will follow the same steps except you will use localhost for the server name and your local machine port number you selected. Press 'Connect' and this should open the authentication window where you will enter your UserID and the https://docs.olcf.ornl.gov/systems/Vampir.html#VampirServer password <vampserpw> printed after a successful connection.",4.056670631291645
"What is the server authentication window pop-up?
","If the pop-up box called ""metadata server launch progress"" never goes away after entering your passcode, you may need to check if you have enough storage space available in your home directory (/ccs/home/[user id]). When connecting to OLCF systems, VisIt creates some small temporary files in your home directory that are unable to be created if you are over your quota (50 GB is the default quota limit).",4.016372764473681
"What is the purpose of the `-o` flag in the job submission script?
","cd $SLURM_SUBMIT_DIR
date
srun -n 8 ./a.out

This batch script shows examples of the three sections outlined above:

<string>:509: (INFO/1) Duplicate implicit target name: ""interpreter line"".

1: This line is optional and can be used to specify a shell to interpret the script. In this example, the bash shell will be used.

2: The job will be charged to the “XXXYYY” project.

3: The job will be named test.

4: The job will request (2) nodes.

5: The job will request (1) hour walltime.

<string>:526: (INFO/1) Duplicate implicit target name: ""shell commands"".",4.178959126872161
"What is the purpose of the `-o` flag in the job submission script?
",is in the format HH:MM:SS. | | -p | #SBATCH -p <partition_name> | Allocates resources on specified partition. | | -o | #SBATCH -o <filename> | Writes standard output to <name> instead of <job_script>.o$SLURM_JOB_UID. $SLURM_JOB_UID is an environment variable created by Slurm that contains the batch job identifier. | | -e | #SBATCH -e <filename> | Writes standard error to <name> instead of <job_script>.e$SLURM_JOB_UID. | | --mail-type | #SBATCH --mail-type=FAIL | Sends email to the submitter when the job fails. | |  | #SBATCH --mail-type=BEGIN | Sends email to the submitter when the job,4.168583374723754
"What is the purpose of the `-o` flag in the job submission script?
",is in the format HH:MM:SS. | | -p | #SBATCH -p <partition_name> | Allocates resources on specified partition. | | -o | #SBATCH -o <filename> | Writes standard output to <name> instead of <job_script>.o$SLURM_JOB_UID. $SLURM_JOB_UID is an environment variable created by Slurm that contains the batch job identifier. | | -e | #SBATCH -e <filename> | Writes standard error to <name> instead of <job_script>.e$SLURM_JOB_UID. | | --mail-type | #SBATCH --mail-type=FAIL | Sends email to the submitter when the job fails. | |  | #SBATCH --mail-type=BEGIN | Sends email to the submitter when the job,4.168583374723754
"What is the difference between running my application with jsrun and running it with mpirun or mpiexec on Summit?
","This section describes tools that users might find helpful to better understand the jsrun job launcher.

hello_jsrun is a ""Hello World""-type program that users can run on Summit nodes to better understand how MPI ranks and OpenMP threads are mapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A screencast showing how to use Hello_jsrun is also available: https://vimeo.com/261038849

Job Step Viewer provides a graphical view of an application's runtime layout on Summit. It allows users to preview and quickly iterate with multiple jsrun options to understand and optimize job launch.",4.366650547248075
"What is the difference between running my application with jsrun and running it with mpirun or mpiexec on Summit?
",Below is a comparison table between srun and jsrun.,4.282271008182644
"What is the difference between running my application with jsrun and running it with mpirun or mpiexec on Summit?
","While jsrun performs similar job launching functions as aprun and mpirun, its syntax is very different. A large reason for syntax differences is the introduction of the resource set concept. Through resource sets, jsrun can control how a node appears to each job. Users can, through jsrun command line flags, control which resources on a node are visible to a job. Resource sets also allow the ability to run multiple jsruns simultaneously within a node. Under the covers, a resource set is a cgroup.

At a high level, a resource set allows users to configure what a node look like to their job.",4.260400251134746
"What is the period of access to information on a DOE computer?
","authorized individual or investigative agency is in effect during the period of your access to information on a DOE computer and for a period of three years thereafter. OLCF personnel and users are required to address, safeguard against, and report misuse, abuse and criminal activities. Misuse of OLCF resources can lead to temporary or permanent disabling of accounts, loss of DOE allocations, and administrative or legal actions. Users who have not accessed a OLCF computing resource in at least 6 months will be disabled. They will need to reapply to regain access to their account. All users",4.43226491449644
"What is the period of access to information on a DOE computer?
","Users are advised that there is no expectation of privacy of your activities on any system that is owned by, leased or operated by UT-Battelle on behalf of the U.S. Department of Energy (DOE). The Company retains the right to monitor all activities on these systems, to access any computer files or electronic mail messages, and to disclose all or part of information gained to authorized individuals or investigative agencies, all without prior notice to, or consent from, any user, sender, or addressee. This access to information or a system by an authorized individual or investigative agency is",4.292778349655895
"What is the period of access to information on a DOE computer?
","The Spock Early Access System was decommissioned on March 15, 2023. The file systems that were available on Spock are still accessible from the Home server and the Data Transfer Nodes (DTN), so all your data will remain accessible. If you do not have access to other OLCF systems, your project will move to data-only for 30-days. If you have any questions, please contact help@olcf.ornl.gov.",4.214351354363713
"What is the maximum execution time specified for the job in the sbcast file?
","same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. Additionally, projects that are at 125% of their allocated time will be limited to only 3 running jobs at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.139085937409483
"What is the maximum execution time specified for the job in the sbcast file?
","same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. Additionally, projects that are at 125% of their allocated time will be limited to only 3 running jobs at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.139085937409483
"What is the maximum execution time specified for the job in the sbcast file?
","Use the sbatch --test-only command to see when a job of a specific size could be scheduled. For example, the snapshot below shows that a (2) node job would start at 10:54.

$ sbatch --test-only -N2 -t1:00:00 batch-script.slurm

  sbatch: Job 1375 to start at 2019-08-06T10:54:01 using 64 processors on nodes andes[499-500] in partition batch

The queue is fluid, the given time is an estimate made from the current queue state and load. Future job submissions and job completions will alter the estimate.



The following table summarizes frequently-used options to Slurm:",4.1188585715739725
"What is the advantage of using round-robin GPU sharing in my MPI application?
","The output shows the round-robin (cyclic) distribution of MPI ranks to GPUs. In fact, it is a round-robin distribution of MPI ranks to L3 cache regions (the default distribution). The GPU mapping is a consequence of where the MPI ranks are distributed; --gpu-bind=closest simply maps the GPU in an L3 cache region to the MPI ranks in the same L3 region.

Example 6: 32 MPI ranks - where 2 ranks share a GPU (round-robin, multi-node)

This example is an extension of Example 5 to run on 2 nodes.",4.3675555927637415
"What is the advantage of using round-robin GPU sharing in my MPI application?
","The output shows the round-robin (cyclic) distribution of MPI ranks to GPUs. In fact, it is a round-robin distribution of MPI ranks to NUMA domains (the default distribution). The GPU mapping is a consequence of where the MPI ranks are distributed; --gpu-bind=closest simply maps the GPU in a NUMA domain to the MPI ranks in the same NUMA domain.

Example 6: 16 MPI ranks - where 2 ranks share a GPU (round-robin, multi-node)

This example is an extension of Example 5 to run on 2 nodes.",4.342733702770532
"What is the advantage of using round-robin GPU sharing in my MPI application?
","$ OMP_NUM_THREADS=1 srun -N1 -n16 -c1 --ntasks-per-gpu=2 --gpu-bind=closest ./hello_jobstep | sort

The output shows the round-robin (cyclic) distribution of MPI ranks to GPUs. In fact, it is a round-robin distribution of MPI ranks to L3 cache regions (the default distribution). The GPU mapping is a consequence of where the MPI ranks are distributed; --gpu-bind=closest simply maps the GPU in an L3 cache region to the MPI ranks in the same L3 region.

Example 5: 32 MPI ranks - where 2 ranks share a GPU (round-robin, multi-node)

This example is an extension of Example 4 to run on 2 nodes.",4.281897004306863
"How do I specify the walltime for my job submission script on Summit?
","cd $SLURM_SUBMIT_DIR
date
srun -n 8 ./a.out

This batch script shows examples of the three sections outlined above:

<string>:509: (INFO/1) Duplicate implicit target name: ""interpreter line"".

1: This line is optional and can be used to specify a shell to interpret the script. In this example, the bash shell will be used.

2: The job will be charged to the “XXXYYY” project.

3: The job will be named test.

4: The job will request (2) nodes.

5: The job will request (1) hour walltime.

<string>:526: (INFO/1) Duplicate implicit target name: ""shell commands"".",4.321514326682686
"How do I specify the walltime for my job submission script on Summit?
","cd $SLURM_SUBMIT_DIR
date
srun -n 8 ./a.out

This batch script shows examples of the three sections outlined above:

Interpreter Line

<string>:509: (INFO/1) Duplicate implicit target name: ""interpreter line"".

1: This line is optional and can be used to specify a shell to interpret the script. In this example, the bash shell will be used.

Slurm Options

2: The job will be charged to the “XXXYYY” project.

3: The job will be named test.

4: The job will request (2) nodes.

5: The job will request (1) hour walltime.

Shell Commands",4.319951479135504
"How do I specify the walltime for my job submission script on Summit?
","This method on Summit requires the user to be present until the job completes. For users who have long scripts or are unable to monitor the job, you can submit the above lines in a batch script. However, you will wait in the queue twice, so this is not recommended. Alternatively, one can use Andes.

For Andes/Frontier (Slurm Script):

Andes

.. code-block:: bash


   #!/bin/bash
   #SBATCH -A XXXYYY
   #SBATCH -J visit_test
   #SBATCH -N 1
   #SBATCH -p gpu
   #SBATCH -t 0:05:00

   cd $SLURM_SUBMIT_DIR
   date

   module load visit",4.303241854317276
"How do I update a service in Kubernetes?
","Here is an example service definition:

apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    name: my-app
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080

This definition tells Kubernetes that all pods with the label ""my-app"" are associated with this service. Any traffic to the service should be distributed among these pods.

The port parameter contains what port the service listens on, and the targetPort parameter contains the port to which the service forwards connections.",4.244626806420525
"How do I update a service in Kubernetes?
","In Kubernetes, a Service is an internal load balancer which identifies a set of pods and can proxy traffic to them. This set of pods is determined by a label selector.

A service is a stable way of accessing a set of pods, which are ephemeral.

When a service is created, it is granted a ClusterIP, which is an IP address internal to the Kubernetes cluster. Other pods can use this ClusterIP to access the service.

Here is an example service definition:",4.21976557049398
"How do I update a service in Kubernetes?
","On the command line, services can be created with the command oc create. Assuming our YAML file from above is in the file my-service.yaml, you can create the service with

$ oc create -f my-service.yaml

Then, you can run oc describe service my-service to see some information about it.",4.2132961171343375
"How do I request an allocation on Summit?
","The requesting project's allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Summit would be equivalent to using 16,000 Summit node-hours of a project's allocation.



As is the case with many other queuing systems, it is possible to place dependencies on jobs to prevent them from running until other jobs have started/completed/etc. Several possible dependency settings are described in the table below:",4.278192496974334
"How do I request an allocation on Summit?
",For Summit:,4.271352998818722
"How do I request an allocation on Summit?
","SummitPLUS is one of the new allocation programs that will be used to allocate a significant portion of the system for 2024. The program is open to researchers from academia, government laboratories, federal agencies, and industry. We welcome proposals for computationally ready projects from investigators who are new to Summit, as well as from previous INCITE, ALCC, DD, ECP awardees and projects. We encourage proposals on emerging paradigms for computational campaigns including data-intensive science and AI/ML.",4.230452108384588
"How do I create a VolumeSnapshot?
","Now, if the data in the PersistentVolume becomes corrupted or lost in some way, we can create a new PersistentVolume that is identical to the original PersistentVolume at the time the VolumeSnapshot was created.

To do this, create a PersistentVolumeClaim that contains a dataSource field in the Pod's spec. An example follows:",4.248125006923825
"How do I create a VolumeSnapshot?
","apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: snapshot-pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Gi

We can now create a VolumeSnapshot, which will capture the data in the PersistentVolume that was provisioned by the named PersistentVolumeClaim at the time the VolumeSnapshot is created, to backup the data.

apiVersion: snapshot.storage.k8s.io/v1beta1
kind: VolumeSnapshot
metadata:
  # Name of VolumeSnapshot
  name: pvc-snap
spec:
  source:
    # Name of persistentVolumeClaim to snapshot
    persistentVolumeClaimName: snapshot-pvc",4.212800444524777
"How do I create a VolumeSnapshot?
","Add Storage Menu

You should see a green popup appear in the upper right saying that the storage was added. This should additionally trigger a new deployment. To make sure a new deployment happened look at the Created time of the top most deployment.

A PersistentVolume is backed up by creating a VolumeSnapshot object. The VolumeSnapshot will create a point in time backup of your PersistentVolume and is something that you would likely do before an upgrade.

A Volume Snapshot will bind to a PersistentVolumeClaim. An example PersistentVolumeClaim to bind a VolumeSnapshot to follows:",4.211825094304514
"What is the benefit of using the ROCBLAS_TENSILE_LIBPATH environment variable on Frontier?
","# RocBLAS has over 1,000 device libraries that may be `dlopen`'d by RocBLAS during a run.
# It's impractical to SBCAST all of these, so you can set this path instead, if you use RocBLAS:
#export ROCBLAS_TENSILE_LIBPATH=${ROCM_PATH}/lib/rocblas/library

# You may notice that some libraries are still linked from /sw/crusher, even after SBCASTing.
# This is because the Spack-build modules use RPATH to find their dependencies. This behavior cannot be changed.
echo ""*****ldd /mnt/bb/$USER/${exe}*****""
ldd /mnt/bb/$USER/${exe}
echo ""*************************************""",4.117163870354649
"What is the benefit of using the ROCBLAS_TENSILE_LIBPATH environment variable on Frontier?
","# RocBLAS has over 1,000 device libraries that may be `dlopen`'d by RocBLAS during a run.
# It's impractical to SBCAST all of these, so you can set this path instead, if you use RocBLAS:
#export ROCBLAS_TENSILE_LIBPATH=${ROCM_PATH}/lib/rocblas/library

# You may notice that some libraries are still linked from /sw/crusher, even after SBCASTing.
# This is because the Spack-build modules use RPATH to find their dependencies. This behavior cannot be changed.
echo ""*****ldd /mnt/bb/$USER/${exe}*****""
ldd /mnt/bb/$USER/${exe}
echo ""*************************************""",4.117163870354649
"What is the benefit of using the ROCBLAS_TENSILE_LIBPATH environment variable on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.111046726639654
"What might happen if I don't use the --smpiargs=""-gpu"" flag when running a GPU-aware MPI job on Summit?
","jsrun --smpiargs=""-gpu"" ...

Not using the --smpiargs=""-gpu"" flag might result in confusing segmentation faults. If you see a segmentation fault when trying to do GPU aware MPI, check to see if you have the flag set correctly.

LSF provides several utilities with which you can monitor jobs. These include monitoring the queue, getting details about a particular job, viewing STDOUT/STDERR of running jobs, and more.",4.408655596380664
"What might happen if I don't use the --smpiargs=""-gpu"" flag when running a GPU-aware MPI job on Summit?
","While this level of control over mapping MPI ranks to GPUs might be useful for some applications, it is always important to consider the implication of the mapping. For example, if the order of the GPU IDs in the map_gpu option is reversed, the MPI ranks and the GPUs they are mapped to would be in different NUMA domains, which could potentially lead to poorer performance.

$ export OMP_NUM_THREADS=2
$ srun -N1 -n4 -c2 --gpus-per-task=1 --gpu-bind=map_gpu:3,2,1,0 ./hello_jobstep | sort",4.304437375077229
"What might happen if I don't use the --smpiargs=""-gpu"" flag when running a GPU-aware MPI job on Summit?
","--smpiargs=""-disable_gpu_hooks""

as an argument to jsrun. Note that this is not compatible with the -gpu argument to --smpiargs, since that is what enables CUDA-aware MPI and the CUDA-aware MPI functionality depends on the CUDA hook.",4.291393970641476
"What is the impact of contention on the performance of FP atomicAdd() operations in Frontier?
","In cases when contention is very low, a FP32 CAS loop implementing an atomicAdd() operation could be faster than an hardware FP32 LDS atomicAdd(). Applications using single precision FP atomicAdd() are encouraged to experiment with the use of double precision to evaluate the trade-off between high atomicAdd() performance vs. potential lower occupancy due to higher LDS usage.",4.360124119345871
"What is the impact of contention on the performance of FP atomicAdd() operations in Frontier?
","In cases when contention is very low, a FP32 CAS loop implementing an atomicAdd() operation could be faster than an hardware FP32 LDS atomicAdd(). Applications using single precision FP atomicAdd() are encouraged to experiment with the use of double precision to evaluate the trade-off between high atomicAdd() performance vs. potential lower occupancy due to higher LDS usage.",4.360124119345871
"What is the impact of contention on the performance of FP atomicAdd() operations in Frontier?
","Hardware FP atomic operations performed in LDS memory are usually always faster than an equivalent CAS loop, in particular when contention on LDS memory locations is high. Because of a hardware design choice, FP32 LDS atomicAdd() operations can be slower than equivalent FP64 LDS atomicAdd(), in particular when contention on memory locations is low (e.g. random access pattern). The aforementioned behavior is only true for FP atomicAdd() operations. Hardware atomic operations for CAS/Min/Max on FP32 are usually faster than the FP64 counterparts. In cases when contention is very low, a FP32 CAS",4.312765343446591
"What is the name of the library that provides support for PMI2?
","Users will be advised to do a module load <UMS project>, which will expose modules for the individual products associated with that project, accessed by a second module load <product>.

Project PI must ensure that support is provided for users of the product, as documented in the statement of support.

Project PI must ensure that the product is updated in response to changes in the system software environment (e.g. updates to OS, compiler, library, or other key tools) and in a timely fashion.  If this commitment is not met, the OLCF may remove the software from UMS.",4.010804282749177
"What is the name of the library that provides support for PMI2?
","Products must provide a statement of support, to be displayed via the module system and in other appropriate contexts/locations.

The statement should clearly indicate that the product is not supported or maintained by the OLCF, but is supported by the UMS project applicant and/or the UMS project team.

The statement should clearly indicate the organization that is providing support and maintenance, and clearly indicate the preferred method(s) of reporting issues or requesting support.

Product modules will be grouped under project-level modules.",3.948356319734274
"What is the name of the library that provides support for PMI2?
","Project PI must ensure that installations are tested to ensure basic functionality before being released to users. These are expected to be at minimum basic function/unit tests to ensure that the build/install was successful.

The resources provided by the OLCF for UMS shall not be used for software development or for routine testing purposes beyond the installation testing as described above.

Products may be removed from UMS at the request of the Project PI by notifying the OLCF (help@olcf.ornl.gov) of their intent and cleaning up their directory space.",3.909665592105539
"How do I monitor the performance of my application when using NVMe support on Summit?
","Each compute node on Summit has a 1.6TB Non-Volatile Memory (NVMe) storage device (high-memory nodes have a 6.4TB NVMe storage device), colloquially known as a ""Burst Buffer"" with theoretical performance peak of 2.1 GB/s for writing and 5.5 GB/s for reading. The NVMes could be used to reduce the time that applications wait for I/O.  More information can be found later in the Burst Buffer section.



<string>:208: (INFO/1) Duplicate implicit target name: ""software"".",4.228501561586812
"How do I monitor the performance of my application when using NVMe support on Summit?
","If you want to collect information on just a specific performance measurement, for example the number of bytes written to DRAM, you can do so with the --metrics option:

summit> jsrun -n1 -a1 -g1 nv-nsight-cu-cli -k vectorAdd --metrics dram__bytes_write.sum ./vectorAdd

The list of available metrics can be obtained with nv-nsight-cu-cli --query-metrics. Most metrics have both a base name and suffix. Together these  make up the full metric name to pass to nv-nsight-cu-cli. To list the full names for a collection of metrics, use --query-metrics-mode suffix --metrics <metrics list>.",4.192598473584286
"How do I monitor the performance of my application when using NVMe support on Summit?
","Remember that by default NVMe support one file per MPI process up to one file per compute node. If users desire a single file as output from data staged on the NVMe they will need to construct it.  Tools to save automatically checkpoint files from NVMe to GPFS as also methods that allow automatic n to 1 file writing with NVMe staging are under development.   Tutorials about NVME:   Burst Buffer on Summit (slides, video) Summit Burst Buffer Libraries (slides, video).",4.177603083782413
"What is the benefit of using the latest version of ROCm on Frontier?
","On Tuesday, July 18, 2023, Frontier was upgraded to a new version of the system software stack. During the upgrade, the following changes took place:

The system was upgraded to Cray OS 2.5, Slingshot Host Software 2.0.2-112, and the AMD GPU 6.0.5 device driver (ROCm 5.5.1 release).

ROCm 5.5.1 is now available via the rocm/5.5.1 modulefile.

HPE/Cray Programming Environments (PE) 23.05 is now available via the cpe/23.05 modulefile.

HPE/Cray PE 23.05 introduces support for ROCm 5.5.1. However, due to issues identified during testing, ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.",4.30714802217676
"What is the benefit of using the latest version of ROCm on Frontier?
","This page lists significant changes to software provided on OLCF systems. The most recent changes are listed first.



<p style=""font-size:20px""><b>Frontier and Crusher: System Software Upgrade (July 18, 2023)</b></p>

The Crusher TDS and Frontier systems were upgraded to a new version of the system software stack. This stack introduces ROCm 5.5.1 and HPE/Cray Programming Environment 23.05. For more information, please see:

Crusher System Updates.

Frontier System Updates.

Please contact help@olcf.ornl.gov with any issues or questions.",4.254794547670584
"What is the benefit of using the latest version of ROCm on Frontier?
","As mentioned above, HIP can be used on systems running on either the ROCm or CUDA platform, so OLCF users can start preparing their applications for Frontier today on Summit. To use HIP on Summit, you must load the HIP module:

$ module load hip-cuda

CUDA 11.4.0 or later must be loaded in order to load the hip-cuda module. hipBLAS, hipFFT, hipSOLVER, hipSPARSE, and hipRAND have also been added to hip-cuda.",4.235096348732973
"What is the difference between a round-robin and a packed distribution of MPI ranks in Frontier?
","The intent with both of the following examples is to launch 8 MPI ranks across the node where each rank is assigned its own logical (and, in this case, physical) core.  Using the -m distribution flag, we will cover two common approaches to assign the MPI ranks -- in a ""round-robin"" (cyclic) configuration and in a ""packed"" (block) configuration. Slurm's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive method was used to request an allocation of 1 compute node for these examples: salloc -A <project_id> -t 30 -p <parition> -N 1",4.410954507949735
"What is the difference between a round-robin and a packed distribution of MPI ranks in Frontier?
","There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.

Assigning MPI ranks in a ""round-robin"" (cyclic) manner across L3 cache regions (sockets) is the default behavior on Frontier. This mode will assign consecutive MPI tasks to different sockets before it tries to ""fill up"" a socket.",4.34911956407918
"What is the difference between a round-robin and a packed distribution of MPI ranks in Frontier?
","Instead, you can assign MPI ranks so that the L3 regions are filled in a ""packed"" (block) manner.  This mode will assign consecutive MPI tasks to the same L3 region (socket) until it is ""filled up"" or ""packed"" before assigning a task to a different socket.

Recall that the -m flag behaves like: -m <node distribution>:<socket distribution>.  Hence, the key setting to achieving the round-robin nature is the -m block:block flag, specifically the block setting provided for the ""socket distribution"". This ensures that the MPI tasks will be distributed in a packed manner.",4.313607776347582
"How does Frontier's performance compare to other high-performance computing systems?
","Frontier is a HPE Cray EX supercomputer located at the Oak Ridge Leadership Computing Facility. With a theoretical peak double-precision performance of approximately 2 exaflops (2 quintillion calculations per second), it is the fastest system in the world for a wide range of traditional computational science applications. The system has 74 Olympus rack HPE cabinets, each with 128 AMD compute nodes, and a total of 9,408 AMD compute nodes.",4.422781641175683
"How does Frontier's performance compare to other high-performance computing systems?
","Each Frontier compute node contains 4 AMD MI250X. The AMD MI250X has a peak performance of 53 TFLOPS in double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 26.5 TFLOPS (double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in each direction simultaneously).

To connect to Frontier, ssh to frontier.olcf.ornl.gov. For example:

$ ssh <username>@frontier.olcf.ornl.gov",4.361551326102338
"How does Frontier's performance compare to other high-performance computing systems?
","Frontier connects to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#dtn-user-guide to move data to/from HPSS.

Frontier is running Cray OS 2.4 based on SUSE Linux Enterprise Server (SLES) version 15.4.",4.361169655886224
"What is the command to measure Tensor Core utilization using nvprof?
","When using NVIDIA’s nvprof profiler, one should add the -m tensor_precision_fu_utilization option to measure Tensor Core utilization. Below is the output from measuring this metric on one of the example programs.

$ nvprof -m tensor_precision_fu_utilization ./simpleCUBLAS
==43727== NVPROF is profiling process 43727, command: ./simpleCUBLAS
GPU Device 0: ""Tesla V100-SXM2-16GB"" with compute capability 7.0",4.573748956050632
"What is the command to measure Tensor Core utilization using nvprof?
","When attempting to use Tensor Cores it is useful to measure and confirm that the Tensor Cores are being used within your code. For implicit use via a library like cuBLAS, the Tensor Cores will only be used above a certain threshold, so Tensor Core use should not be assumed. The NVIDIA Tools provide a performance metric to measure Tensor Core utilization on a scale from 0 (Idle) to 10 (Max) utilization.",4.432206880153877
"What is the command to measure Tensor Core utilization using nvprof?
","NVIDIA’s Nsight Compute may also be used to measure tensor core utilization via the sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_active metric, as follows:

$ nv-nsight-cu-cli --metrics sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_active ./cudaTensorCoreGemm",4.4315570467283445
"How does the increased number of MPS clients in Volta GPUs improve performance?
","Volta GPUs improve MPS with new capabilities. For instance, each Volta MPS client (MPI rank) is assigned a ""subcontext"" that has its own GPU address space, instead of sharing the address space with other clients. This isolation helps protect MPI ranks from out-of-range reads/writes performed by other ranks within CUDA kernels. Because each subcontext manages its own GPU resources, it can submit work directly to the GPU without the need to first pass through the MPS server. In addition, Volta GPUs support up to 48 MPS clients (up from 16 MPS clients on Pascal).",4.581729402820438
"How does the increased number of MPS clients in Volta GPUs improve performance?
","The Multi-Process Service (MPS) enables multiple processes (e.g. MPI ranks) to concurrently share the resources on a single GPU. This is accomplished by starting an MPS server process, which funnels the work from multiple CUDA contexts (e.g. from multiple MPI ranks) into a single CUDA context. In some cases, this can increase performance due to better utilization of the resources. The figure below illustrates MPS on a pre-Volta GPU.",4.425735171388055
"How does the increased number of MPS clients in Volta GPUs improve performance?
","The Multi-Process Service (MPS) enables multiple processes (e.g. MPI ranks) to concurrently share the resources on a single GPU. This is accomplished by starting an MPS server process, which funnels the work from multiple CUDA contexts (e.g. from multiple MPI ranks) into a single CUDA context. In some cases, this can increase performance due to better utilization of the resources. As mentioned in the Common bsub Options section above, MPS can be enabled with the -alloc_flags ""gpumps"" option to bsub. The following screencast shows an example of how to start an MPS server process for a job:",4.2617372977365
"What is the name of the Linux kernel module used in Frontier?
","The Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.208991984002556
"What is the name of the Linux kernel module used in Frontier?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers. Please see the following https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more detailed inforation on how to compile using these modules.",4.170194944342804
"What is the name of the Linux kernel module used in Frontier?
","<p style=""font-size:20px""><b>Frontier: Darshan Runtime 3.4.0 (May 10, 2023)</b></p>

The Darshan Runtime modulefile darshan-runtime/3.4.0 on Frontier is now loaded by default. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>.

Unloading darshan-runtime modulefile is recommended for users profiling their applications with other profilers to prevent conflicts.",4.166562863454973
"How can I open a tunneling connection with Andes and port 5901?
","In a new terminal, open a tunneling connection with andes-gpu5.olcf.ornl.gov and                                                                              port 5901
example:
     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov
     andes: ssh -4L 5901:localhost:5901 andes-gpu5

**************************************************************************",4.528906738734256
"How can I open a tunneling connection with Andes and port 5901?
","In a new terminal, open a tunneling connection with andes-gpu5.olcf.ornl.gov and                                                                              port 5901
example:
     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov
     andes: ssh -4L 5901:localhost:5901 andes-gpu5

**************************************************************************",4.528906738734256
"How can I open a tunneling connection with Andes and port 5901?
","In a new terminal, open a tunneling connection with andes79.olcf.ornl.gov and port 5901
example:
     localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov
     andes: ssh -4L 5901:localhost:5901 andes79

**************************************************************************

MATLAB is selecting SOFTWARE OPENGL rendering.

In a second terminal on your local system open a tunneling connection following the instructions given by the vnc start-up script:

localsystem: ssh -L 5901:localhost:5901 username@andes.olcf.ornl.gov

andes: ssh -4L 5901:localhost:5901 andes79",4.4632911099937855
"What is the name of the library that provides support for Cray's Environment Management System?
","Cray provides PrgEnv-<compiler> modules (e.g., PrgEnv-cray) that load compatible components of a specific compiler toolchain. The components include the specified compiler as well as MPI, LibSci, and other libraries. Loading the PrgEnv-<compiler> modules also defines a set of compiler wrappers for that compiler toolchain that automatically add include paths and link in libraries for Cray software. Compiler wrappers are provided for C (cc), C++ (CC), and Fortran (ftn).",4.289675452802368
"What is the name of the library that provides support for Cray's Environment Management System?
","Cray provides PrgEnv-<compiler> modules (e.g., PrgEnv-cray) that load compatible components of a specific compiler toolchain. The components include the specified compiler as well as MPI, LibSci, and other libraries. Loading the PrgEnv-<compiler> modules also defines a set of compiler wrappers for that compiler toolchain that automatically add include paths and link in libraries for Cray software. Compiler wrappers are provided for C (cc), C++ (CC), and Fortran (ftn).",4.289675452802368
"What is the name of the library that provides support for Cray's Environment Management System?
","Cray provides PrgEnv-<compiler> modules (e.g., PrgEnv-cray) that load compatible components of a specific compiler toolchain. The components include the specified compiler as well as MPI, LibSci, and other libraries. Loading the PrgEnv-<compiler> modules also defines a set of compiler wrappers for that compiler toolchain that automatically add include paths and link in libraries for Cray software. Compiler wrappers are provided for C (cc), C++ (CC), and Fortran (ftn).",4.289675452802368
"What is the difference in data type between NumPy and CuPy?
","As is the standard with NumPy being imported as ""np"", CuPy is often imported in a similar fashion:

>>> import numpy as np
>>> import cupy as cp

Similar to NumPy arrays, CuPy arrays can be declared with the cupy.ndarray class. NumPy arrays will be created on the CPU (the ""host""), while CuPy arrays will be created on the GPU (the ""device""):

>>> x_cpu = np.array([1,2,3])
>>> x_gpu = cp.array([1,2,3])

Manipulating a CuPy array can also be done in the same way as manipulating NumPy arrays:",4.358668387055848
"What is the difference in data type between NumPy and CuPy?
","CuPy is a library that implements NumPy arrays on NVIDIA GPUs by utilizing CUDA Toolkit libraries like cuBLAS, cuRAND, cuSOLVER, cuSPARSE, cuFFT, cuDNN and NCCL. Although optimized NumPy is a significant step up from Python in terms of speed, performance is still limited by the CPU (especially at larger data sizes) -- this is where CuPy comes in. Because CuPy's interface is nearly a mirror of NumPy, it acts as a replacement to run existing NumPy/SciPy code on NVIDIA CUDA platforms, which helps speed up calculations further. CuPy supports most of the array operations that NumPy provides,",4.349234275394397
"What is the difference in data type between NumPy and CuPy?
","The exact numbers may be slightly different, but you should see a speedup factor of approximately 2 or better when comparing ""GPU time"" to ""CPU time"". Switching to float32 was easier on memory for the GPU, which improved the time further. Things are even better when you look at ""GPU float32 restructured time"", which represents an additional factor of 4 speedup when compared to ""GPU float32 time"". Overall, using CuPy and restructuring the data led to a speedup factor of >20 when compared to traditional NumPy! This factor would diminish with smaller datasets, but represents what CuPy is capable",4.340076373131164
"How does the high-bandwidth memory (HBM2) on Summit compare to traditional GPU memory in terms of access speed?
","Most Summit nodes contain 512 GB of DDR4 memory for use by the POWER9 processors, 96 GB of High Bandwidth Memory (HBM2) for use by the accelerators, and 1.6TB of non-volatile memory that can be used as a burst buffer. A small number of nodes (54) are configured as ""high memory"" nodes. These nodes contain 2TB of DDR4 memory, 192GB of HBM2, and 6.4TB of non-volatile memory.",4.460468342136529
"How does the high-bandwidth memory (HBM2) on Summit compare to traditional GPU memory in terms of access speed?
","Each V100 has access to 16 GB (32GB for high-memory nodes) of high-bandwidth memory (HBM2), which can be accessed at speeds of up to 900 GB/s. Access to this memory is controlled by (8) 512-bit memory controllers, and all accesses to the high-bandwidth memory go through the 6 MB L2 cache.

The processors within a node are connected by NVIDIA's NVLink interconnect. Each link has a peak bandwidth of 25 GB/s (in each direction), and since there are 2 links between processors, data can be transferred from GPU-to-GPU and CPU-to-GPU at a peak rate of 50 GB/s.",4.376791523571032
"How does the high-bandwidth memory (HBM2) on Summit compare to traditional GPU memory in terms of access speed?
",nodes) of HBM2 memory. The V100 uses NVIDIA's NVLink interconnect to pass data between GPUs as well as from CPU-to-GPU. We provide a more in-depth look into the NVIDIA Tesla V100 later in the Summit Guide.,4.360557535228084
"What is the IonQ User Interface and how do I access it?
","After submitting the OLCF quantum account application and receiving approval, you will receive an email from IonQ inviting you to create your quantum account. Once logged in, users will have access to IonQ's User Interface, https://cloud.ionq.com/, their online platform for managing jobs and accessing the available quantum systems, including the Harmony and Aria-1 systems, as well as the simulator, via the cloud. From the UI, users can view system status and upcoming system availability, as well as monitor batch submissions and job history.",4.342670815759656
"What is the IonQ User Interface and how do I access it?
","IonQ backends are available via the IonQ cloud interface via the API and also via many quantum Software Development Kits (SDK’s)



Users can access information about IonQ’s systems, view submitted jobs, look up machine availability, and update job notification preferences via the IonQ Cloud Console.

Jupyter at OLCF: Access to the IonQ queues can also be obtained via OLCF JupyterHub, a web-based interactive computing environment. See examples of common use case notebooks at IonQ Notebook Samples.",4.331751698263917
"What is the IonQ User Interface and how do I access it?
","Information on submitting jobs to IonQ systems, system availability, checking job status, and tracking usage can be found via the IonQ Cloud Console.

A recommended workflow for running on IonQ's quantum computers is to utilize the emulator first, then run on one of the quantum computers. This is highlighted in the examples.",4.269525880059564
"What is the benefit of using Globus to transfer data to and from SPI resources?
","The SPI provides separate https://docs.olcf.ornl.gov/systems/index.html#Data Transfer Nodes<spi-data-transfer> configured specifically for SPI workflows.  The nodes are not directly accessible for login but are accessible through the Globus tool.  The SPI DTNs mount the same Arx filesystem available on the SPI compute resources.  Globus is the preferred method to transfer data into and out of the SPI resources.

Please see the https://docs.olcf.ornl.gov/systems/index.html#Data Transfer Nodes<spi-data-transfer> section for more details.",4.39211856361148
"What is the benefit of using Globus to transfer data to and from SPI resources?
","The SPI Data Transfer Nodes are not directly accessible, but can be used through Globus to transfer data.

A simple example using the CLI:

myproxy-logon -T -b -l usera_prj123_mde
globus-url-copy -cred /gpfs/arx/prj123_mde/home/usera_prj123_mde/dataA -dcpriv -list",4.309523890659576
"What is the benefit of using Globus to transfer data to and from SPI resources?
",The following example is intended to help users who are making the transition from Summit to Frontier to move their data between Alpine GPFS and Orion Lustre. We strongly recommend using Globus for this transfer as it is the method that is most efficient for users and that causes the least contention on filesystems and data transfer nodes.,4.288312781039607
"How do I train a quantum circuit using PennyLane?
","PennyLane is a cross-platform Python library for programming quantum computers.  Its differentiable programming paradigm enables the execution and training of quantum programs on various backends.

General information of how to install and use PennyLane can be found here:

https://docs.pennylane.ai/en/stable/introduction/pennylane.html

https://pennylane.ai/qml/demos_getting-started.html

https://pennylane.ai/install.html

On our systems, the install method is relatively simple:

Andes

.. code-block:: bash",4.365959955455547
"How do I train a quantum circuit using PennyLane?
","Laser based quantum gates

Linear trap Quantum Charge-Coupled Device (QCCD) architecture with three or more parallel gate zones

Mid-circuit measurement conditioned circuit branching

Qubit reuse after mid-circuit measurement

Native gate set: single-qubit rotations, two-qubit ZZ-gates



Users can access information about Quantinuum's systems, view submitted jobs, look up machine availability, and update job notification preferences on the cloud dashboard on the Quantinuum User Portal.",4.185179802793077
"How do I train a quantum circuit using PennyLane?
","the quantum resources via Jupyter notebooks is available in the UI via the “Examples” tab. Quantinuum’s systems feature mid-circuit measurement and qubit reuse, and are compatible with a variety of software frameworks.",4.142248072419961
"How do I create an OpenShift ImageStream?
","First, we will log into the cluster using the oc CLI tool

oc login https://api.<cluster>.ccs.ornl.gov

Next we will create the ImageStream that the BuildConfig will push the completed image to. The ImageStream is a direct mapping to the image stored in the OpenShift integrated registry.

oc create imagestream local-image

Next, we will create the BuildConfig object",4.432897493821558
"How do I create an OpenShift ImageStream?
","When tagging an image, you must use the format registry.apps.<cluster>.ccs.ornl.gov/<namespace>/<image> where:

Cluster is the name of the OpenShift cluster

Namespace is the name of the Kubernetes namespace you are using (Use oc status to see what OpenShift Project/Kubernetes Namespace you are currently in)

Image is the name of the image you want to push

Once you push the image into the registry, a OpenShift ImageStream will be automatically created",4.3593673598772895
"How do I create an OpenShift ImageStream?
","$ oc get imagestream local-image
NAME          DOCKER REPO                                                   TAGS     UPDATED
local-image   image-registry.openshift-image-registry.svc:5000/stf002platform/local-image   latest   5 minutes ago

Now that we have built a container image we can deploy it with a Deployment object. Using the Docker Repo specified in the ImageStream we can create our deployment:",4.358809329904607
"What is the benefit of using a consistent Clang version with CCE and ROCm compilers on Frontier?
","Both the CCE and ROCm compilers are Clang-based, so please be sure to use consistent (major) Clang versions when using them together. You can check which version of Clang is being used with CCE and ROCm by giving the --version flag to CC and hipcc, respectively.

<string>:653: (INFO/1) Duplicate implicit target name: ""mpi"".

The MPI implementation available on Frontier is Cray's MPICH, which is ""GPU-aware"" so GPU buffers can be passed directly to MPI calls.",4.352307159508515
"What is the benefit of using a consistent Clang version with CCE and ROCm compilers on Frontier?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers. Please see the following https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more detailed inforation on how to compile using these modules.",4.255614942841851
"What is the benefit of using a consistent Clang version with CCE and ROCm compilers on Frontier?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers.

It is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) whenever possible. See the next section for more details.",4.2357967084623525
"Can I use the `srun` command to run an application on multiple nodes on Frontier?
","different commands.  Notable are the separation in batch script submission (sbatch) and interactive batch submission (salloc).  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-slurm section for more infomation including a LSF to Slurm command comparison.  Srun job launcher  Frontier uses Slurm's job launcher, srun, instead of Summit's jsrun to launch parallel jobs within a batch script.  Overall functionality is similar, but commands are notably different. Frontier's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#compute node layout <frontier-simple>",4.3693298272742425
"Can I use the `srun` command to run an application on multiple nodes on Frontier?
","Unlike Summit and Titan, there are no launch/batch nodes on Frontier. This means your batch script runs on a node allocated to you rather than a shared node. You still must use the job launcher (srun) to run parallel jobs across all of your nodes, but serial tasks need not be launched with srun.



To easily visualize job examples (see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-mapping further below), the compute node diagram has been simplified to the picture shown below.

Simplified Frontier node architecture diagram",4.362056295184819
"Can I use the `srun` command to run an application on multiple nodes on Frontier?
","Using srun

By default, commands will be executed on the job’s primary compute node, sometimes referred to as the job’s head node. The srun command is used to execute an MPI binary on one or more compute nodes in parallel.

srun accepts the following common options:

| -N | Minimum number of nodes | | --- | --- | | -n | Total number of MPI tasks | | --cpu-bind=no | Allow code to control thread affinity | | -c | Cores per MPI task | | --cpu-bind=cores | Bind to cores |

If you do not specify the number of MPI tasks to srun via -n, the system will default to using only one task per node.",4.322144533611486
"How can I shutdown the dask-cuda-cluster in Nvidia Rapids?
","# 5. Shutting down the dask-cuda-cluster
    print(""Shutting down the cluster"")
    workers_list = list(workers_info)
    disconnect (client, workers_list)

Consult this example for single gpu usage. Then, follow RAPIDS' basic or simultaneous execution LFS scripts.",4.332565131640484
"How can I shutdown the dask-cuda-cluster in Nvidia Rapids?
","# 3. Do computation
    # ...
    # ...

    # 4. Shutting down the dask-cuda-cluster
    print(""Shutting down the cluster"")
    workers_list = list(workers_info)
    disconnect (client, workers_list)

The RAPIDS environment is read-only. Therefore, users cannot install any additional packages that may be needed. If users need any additional conda or pip packages, they can clone the RAPIDS environment into their preferred directory and then add any packages they need.

Cloning the RAPIDS environment can be done with the next commands:",4.316960413553308
"How can I shutdown the dask-cuda-cluster in Nvidia Rapids?
","Running RAPIDS multi-gpu/multi-node workloads requires a dask-cuda cluster. Setting up a dask-cuda cluster on Summit requires two components:

dask-scheduler.

dask-cuda-workers.

Once the dask-cluster is running, the RAPIDS script should perform four main tasks. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

Reference of multi-gpu/multi-node operation with cuDF, cuML, cuGraph is available in the next links:

10 Minutes to cuDF and Dask-cuDF.

cuML's Multi-Node, Multi-GPU Algorithms.",4.24736691654973
"How do I create a secure route in Slate?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.273882756629736
"How do I create a secure route in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.257273741373281
"How do I create a secure route in Slate?
","By default, a route will only expose your https://docs.olcf.ornl.gov/systems/route.html#slate_services to NCCS networks. If you need your service exposed to the world outside ORNL, you will first need to get your project approved for external routes. To do this, submit a systems ticket. In the description, give us your project name and a brief reasoning for why exposing externally is needed.

We will let you know once your project is able to set up external routes.",4.200946709450263
"How do you access the MinIO application instance after it has been deployed using the values.yaml file?
","Paths to each in the GUI panel:

Workloads->Pods

Workloads->Deployments

Workloads->Secrets

Networking->Services

Networking->Routes

Storage->Persistent Volume Claims (only applicable if you disabled use_olcf_fs in values.yaml)

After a few minutes, the URL to your MinIO server will become available.

You can reach it by going to the URL you put for the host value in your values.yaml file.

<string>:5: (INFO/1) Duplicate explicit target name: ""marble gui"".",4.287410770105804
"How do you access the MinIO application instance after it has been deployed using the values.yaml file?
","You can also go to it by logging into the Marble GUI. Once logged in, go to Networking->Routes and click the URL in the ""Location"" column of your MinIO applications row.

You will be greeted with the NCCS SSO page. Continue through that with your normal NCCS login credentials.

After the NCCS login, you will be greeted with MinIO's login page. Here you will enter the access-key and secret-key you created with the secret-token.yaml file.

At this point, you should be inside the MinIO Browser.",4.285751876436495
"How do you access the MinIO application instance after it has been deployed using the values.yaml file?
","Where you cloned the slate_helm_examples repository, in the 'slate_helm_examples/charts/minio-standalone` directory, you will see a values.yaml file. This file containes variables for the Helm chart deployment.

This is how we configure your instance of the MinIO application. All of these changes will be to your local copy of values.yaml.

Here is what it looks like:",4.2761133819704
"How can I ensure that the GPUs are evenly distributed among the resource sets?
","Based on how your code expects to interact with the system, you can create resource sets containing the needed GPU and core resources. If a code expects to utilize one GPU per task, a resource set would contain one core and one GPU. If a code expects to pass work to a single GPU from two tasks, a resource set would contain two cores and one GPU.

Decide on the number of resource sets needed

Once you understand tasks, threads, and GPUs in a resource set, you simply need to decide the number of resource sets needed.",4.3090608862882105
"How can I ensure that the GPUs are evenly distributed among the resource sets?
","The following example will create 12 resource sets each with 1 task, 4 threads, and 1 GPU. Each MPI task will start 4 threads and have access to 1 GPU. Rank 0 will have access to GPU 0 and start 4 threads on the first socket of the first node ( red resource set). Rank 2 will have access to GPU 1 and start 4 threads on the second socket of the first node ( green resource set). This pattern will continue until 12 resource sets have been created. The following jsrun command will create 12 resource sets (-n12). Each resource set will contain 1 MPI task (-a1), 1 GPU (-g1), and 4 cores (-c4).",4.261561557339228
"How can I ensure that the GPUs are evenly distributed among the resource sets?
","The first step to creating resource sets is understanding how a code would like the node to appear. For example, the number of tasks/threads per GPU. Once this is understood, the next step is to simply calculate the number of resource sets that can fit on a node. From here, the number of needed nodes can be calculated and passed to the batch job request.

The basic steps to creating resource sets:

Understand how your code expects to interact with the system.

How many tasks/threads per GPU?",4.251027194784874
"What is the reason for the error according to the error message?
","During handling of the above exception, another exception occurred:",3.939423358785813
"What is the reason for the error according to the error message?
","If you see an error similar to ""The metadata server on host andes.olcf.ornl.gov could not be launched or it could not connect back to your local computer"" with the specific error listed as ""The reason for the exception was not described"", double check your host profiles. This bug may occur when you have two or more host profiles that represent the same system (e.g., if you have two host profiles that connect to andes.olcf.ornl.gov, but may have different settings / usernames for both). This bug can affect both Summit and Andes.",3.928855284616986
"What is the reason for the error according to the error message?
",for more information.,3.9075210529946247
"What is the name of the simulator function used in the simulation?
","Simulator backends currently available: https://quantum-computing.ibm.com/services?services=simulators

IBM's Documentation

IBM Quantum Insider",4.007445237906207
"What is the name of the simulator function used in the simulation?
","Users select or supply generator and simulator functions to express their ensembles; the generator typically steers the ensemble based on prior simulator results. Such functions can also launch and monitor external executables at any scale.

Begin by loading the python module:

$ module load cray-python

libEnsemble is available on PyPI, conda-forge, the xSDK, and E4S. Most users install libEnsemble via pip:

$ pip install libensemble",3.9848143842530286
"What is the name of the simulator function used in the simulation?
","sim_specs = {
    ""sim_f"": sim_find_sine,  # Our simulator function
    ""in"": [""x""],  # Input field names. 'x' from gen_f output
    ""out"": [(""y"", float)],  # sim_f output. 'y' = sine('x')
}

persis_info = add_unique_random_streams({}, 5)  # Initialize manager/workers random streams

exit_criteria = {""sim_max"": 80}  # Stop libEnsemble after 80 simulations

H, persis_info, flag = libE(sim_specs, gen_specs, exit_criteria, persis_info, libE_specs=libE_specs)",3.9690423070953047
"What is the difference between CuPy v13.0.0 and earlier versions?
","CuPy v13.0.0 removed support for CUDA 10.2, 11.0, and 11.1. Please try installing CuPy<13.0.0 if you run into issues with older CUDA versions. See CuPy Release Notes for more details and other compatibility changes.

Summit

.. code-block:: bash

   $ CC=gcc NVCC=nvcc pip install --no-cache-dir --no-binary=cupy cupy

Frontier

.. code-block:: bash

   $ export CUPY_INSTALL_USE_HIP=1
   $ export ROCM_HOME=/opt/rocm-5.3.0
   $ export HCC_AMDGPU_TARGET=gfx90a
   $ CC=gcc pip install --no-cache-dir --no-binary=cupy cupy

Andes

.. code-block:: bash",4.34289464992095
"What is the difference between CuPy v13.0.0 and earlier versions?
","If you wish to use CuPy within a clone of the OpenCE environment, the installation process is very similar to what we do in the regular CuPy installation we saw above.

The open-ce/1.2.0-pyXY-0 (which is the current default) will not support this. So make sure you are using open-ce/1.5.0-pyXY-0 or higher.

The contents of the open-ce module cannot be modified so you need to make your own clone of the open-ce environment.",4.20754300662804
"What is the difference between CuPy v13.0.0 and earlier versions?
","You have now discovered what CuPy can provide! Now you can try speeding up your own codes by swapping CuPy and NumPy where you can.

CuPy User Guide

CuPy Website

CuPy API Reference

CuPy Release Notes",4.1779239805394575
"How can I create a new container using podman on Summit?
","Podman is a container framework from Red Hat that functions as a drop in replacement for Docker. On Summit, we will use Podman for building container images and converting them into tar files for Singularity to use. Podman makes use of Dockerfiles to describe the images to be built. We cannot use Podman to run containers as it doesn't properly support MPI on Summit, and Podman does not support storing its container images on GPFS or NFS.",4.468166880259282
"How can I create a new container using podman on Summit?
","Users can build container images with Podman, convert it to a SIF file, and run the container using the Singularity runtime. This page is intended for users with some familiarity with building and running containers.

Users will make use of two applications on Summit - Podman and Singularity - in their container build and run workflow. Both are available without needing to load any modules.",4.425539031431448
"How can I create a new container using podman on Summit?
","Users will be building and running containers on Summit without root permissions i.e. containers on Summit are rootless.  This means users can get the benefits of containers without needing additional privileges. This is necessary for a shared system like Summit. And this is part of the reason why Docker doesn't work on Summit. Podman and Singularity provides rootless support but to different extents hence why users need to use a combination of both.

Users will need to set up a file in their home directory /ccs/home/<username>/.config/containers/storage.conf with the following content:",4.419928750249429
"How can I optimize my transfers in Globus?
","When the installation has finished, click on the Globus icon and select Web: Transfer Files as below



Globus will ask you to login. If your institution does not have an organizational login, you may choose to either Sign in with Google or Sign in with ORCiD iD.



In the main Globus web page, select the two-panel view, then set the source and destination endpoints. (Left/Right order does not matter)



Next, navigate to the appropriate source and destination paths to select the files you want to transfer. Click the ""Start"" button to begin the transfer.",4.370680351509325
"How can I optimize my transfers in Globus?
","transfers across all the users. Each user has a limit of 3 active transfers, so it is required to transfer a lot of data on each transfer than less data across many transfers.  If a folder is constituted with mixed files including thousands of small files (less than 1MB each one), it would be better to tar the small files.  Otherwise, if the files are larger, Globus will handle them.",4.35914860103438
"How can I optimize my transfers in Globus?
",The following example is intended to help users who are making the transition from Summit to Frontier to move their data between Alpine GPFS and Orion Lustre. We strongly recommend using Globus for this transfer as it is the method that is most efficient for users and that causes the least contention on filesystems and data transfer nodes.,4.322168998622499
"How can I convert a tar file into a Singularity sif file?
","You can now create a Singularity sif file with singularity build --disable-cache --docker-login simple.sif docker://docker.io/<username>/simple.

This will ask you to enter your Docker username and password again for Singularity to download the image from Dockerhub and convert it to a sif file.",4.366637059876382
"How can I convert a tar file into a Singularity sif file?
","Singularity is a container framework from Sylabs. On Summit, we will use Singularity solely as the runtime. We will convert the tar files of the container images Podman creates into sif files, store those sif files on GPFS, and run them with Jsrun. Singularity also allows building images but ordinary users cannot utilize that on Summit due to additional permissions not allowed for regular users.",4.309521451303052
"How can I convert a tar file into a Singularity sif file?
","Users can build container images with Podman, convert it to a SIF file, and run the container using the Singularity runtime. This page is intended for users with some familiarity with building and running containers.

Users will make use of two applications on Summit - Podman and Singularity - in their container build and run workflow. Both are available without needing to load any modules.",4.254028328206823
"How can I migrate memory regions between the host DDR4 and GPU HBM in Frontier?
","The AMD MI250X and operating system on Frontier supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Frontier. In the following section we'll discuss how the heterogenous memory space on a Frontier node is surfaced within your application.",4.326817766651056
"How can I migrate memory regions between the host DDR4 and GPU HBM in Frontier?
","The page migration behavior summarized by the following tables represents the current, observable behavior. Said behavior will likely change in the near future.  CPU accesses to migratable memory may behave differently than other platforms you're used to. On Frontier, pages will not migrate from GPU HBM to CPU DDR4 based on access patterns alone. Once a page has migrated to GPU HBM it will remain there even if the CPU accesses it, and all accesses which do not resolve in the CPU cache will occur over the Infinity Fabric between the AMD ""Optimized 3rd Gen EPYC"" CPU and AMD MI250X GPU. Pages",4.319295384077052
"How can I migrate memory regions between the host DDR4 and GPU HBM in Frontier?
","The Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.292149454843793
"What is the benefit of using the TCC_EA_WRREQ_64B and TCC_EA_RDREQ_32B metrics on Crusher?
","BytesMoved = BytesWritten + BytesRead

where

BytesWritten = 64 * TCC\_EA\_WRREQ\_64B\_sum + 32 * (TCC\_EA\_WRREQ\_sum - TCC\_EA\_WRREQ\_64B\_sum)

BytesRead = 32 * TCC\_EA\_RDREQ\_32B\_sum + 64 * (TCC\_EA\_RDREQ\_sum - TCC\_EA\_RDREQ\_32B\_sum)



This section details 'tips and tricks' and information of interest to users when porting from Summit to Crusher.",4.42758050342719
"What is the benefit of using the TCC_EA_WRREQ_64B and TCC_EA_RDREQ_32B metrics on Crusher?
","BytesMoved = BytesWritten + BytesRead

where

BytesWritten = 64 * TCC\_EA\_WRREQ\_64B\_sum + 32 * (TCC\_EA\_WRREQ\_sum - TCC\_EA\_WRREQ\_64B\_sum)

BytesRead = 32 * TCC\_EA\_RDREQ\_32B\_sum + 64 * (TCC\_EA\_RDREQ\_sum - TCC\_EA\_RDREQ\_32B\_sum)





This section details 'tips and tricks' and information of interest to users when porting from Summit to Frontier.",4.22561334111608
"What is the benefit of using the TCC_EA_WRREQ_64B and TCC_EA_RDREQ_32B metrics on Crusher?
","Arithmetic intensity calculates the ratio of FLOPS to bytes moved between HBM and L2 cache. We calculated FLOPS above (FP64_FLOPS). We can calculate the number of bytes moved using the rocprof metrics TCC_EA_WRREQ_64B, TCC_EA_WRREQ_sum, TCC_EA_RDREQ_32B, and TCC_EA_RDREQ_sum. TCC refers to the L2 cache, and EA is the interface between L2 and HBM. WRREQ and RDREQ are write-requests and read-requests, respectively. Each of these requests is either 32 bytes or 64 bytes. So we calculate the number of bytes traveling over the EA interface as:

BytesMoved = BytesWritten + BytesRead

where",4.10519810063492
"How can I use cuda-gdb to set breakpoints inside CUDA kernels on Summit?
",cuda-gdb allows for breakpoints to be set inside CUDA kernels to inspect the program state on the GPU. This can be a valuable debugging tool but breaking inside kernels does incur significant overhead that should be included in your expected runtime.,4.525776195626914
"How can I use cuda-gdb to set breakpoints inside CUDA kernels on Summit?
","The time required to hit a breakpoint inside a CUDA kernel depends on how many CUDA threads are used to execute the kernel. It may take several seconds to stop at kernel breakpoints for very large numbers of threads. For this reason, it is recommended to choose breakpoints judiciously, especially when running the debugger in ""batch"" or ""offline"" mode where this overhead may be misperceived as the code hanging. If possible, debugging a smaller problem size with fewer active threads can be more pleasant.",4.350919802623204
"How can I use cuda-gdb to set breakpoints inside CUDA kernels on Summit?
","Using the hip-cuda/5.1.0 module on Summit requires CUDA v11.4.0 or later. However, only CUDA v11.0.3 and earlier currently support GPU-aware MPI, so GPU-aware MPI is currently not available when using HIP on Summit.

Any codes compiled with post 11.0.3 CUDA (cuda/11.0.3) will not work with MPS enabled (-alloc_flags ""gpumps"") on Summit. The code will hang indefinitely. CUDA v11.0.3 is the latest version that is officially supported by IBM's software stack installed on Summit. We are continuing to look into this issue.",4.223494930476044
"How do I activate a Conda environment on Frontier?
","Currently, Crusher and Frontier do NOT have Anaconda/Conda modules. If your workflow better suits conda environments, you can install your own Miniconda on Frontier.

The install process is rather simple (with a few notable warnings, see https://docs.olcf.ornl.gov/systems/miniconda.html#Cautionary Notes <miniconda-notes> further below):

mkdir miniconda_frontier/
cd miniconda_frontier/
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
chmod u+x Miniconda3-latest-Linux-x86_64.sh
./Miniconda3-latest-Linux-x86_64.sh -u -p ~/miniconda_frontier",4.422651840714438
"How do I activate a Conda environment on Frontier?
","Currently, Frontier does NOT have an Anaconda/Conda module.  To use conda, you will have to download and install Miniconda on your own (see our https://docs.olcf.ornl.gov/software/python/miniconda.html). Alternatively, you can use Python's native virtual environments venv feature with the cray-python module (as we will explore in the guides below).  For more details on venv, see Python's Official Documentation.  Contact help@olcf.ornl.gov if conda is required for your workflow, or if you have any issues.",4.400597220246053
"How do I activate a Conda environment on Frontier?
","This guide introduces a user to the basic workflow of using conda environments, as well as providing an example of how to create a conda environment that uses a different version of Python than the base environment uses on Summit. Although Summit is being used in this guide, all of the concepts still apply to other OLCF systems. If using Frontier, this assumes you already have installed your own version of conda (e.g., by https://docs.olcf.ornl.gov/software/python/miniconda.html).

OLCF Systems this guide applies to:

Summit

Andes

Frontier (if using conda)",4.390172834651693
"How can I check the status of my batch job in Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.264620203320453
"How can I check the status of my batch job in Andes?
","Batch Queues on Andes

The compute nodes on Andes are separated into two partitions the ""batch partition"" and the ""GPU partition"" as described in the https://docs.olcf.ornl.gov/systems/your_file.html#andes-compute-nodes section. The scheduling policies for the individual partitions are as follows:

Batch Partition Policy (default)

Jobs that do not specify a partition will run in the 704 node batch partition:",4.200717051361939
"How can I check the status of my batch job in Andes?
","If VisIt will not connect to Andes or Summit when you try to draw an image, you should login to the system and check if a job is in the queue. To do this on Andes, enter squeue from the command line. To do this on Summit, enter bjobs from the command line. Your VisIt job should appear in the queue. If you see it in a state marked ""PD"" or ""PEND"" you should wait a bit longer to see if it will start. If you do not see your job listed in the queue, check to make sure your project ID is entered in your VisIt host profile. See the https://docs.olcf.ornl.gov/systems/visit.html#visit-modify-host",4.153185632800781
"How do I return the RSA SecurID Token to the OLCF Accounts Team?
","You will also be sent a request to complete identity verification. When your account is approved, your RSA SecurID token will also be enabled. Please refer to our https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#system-user-guides for more information on host access. DO NOT share your PIN or RSA SecurID token with anyone. Sharing of accounts will result in termination. If your SecurID token is stolen or misplaced, contact the OLCF immediately and report the missing token. Upon termination of your account access, return the token to the OLCF in person or via mail.",4.5157325696680575
"How do I return the RSA SecurID Token to the OLCF Accounts Team?
","If your project is still active and you require continued access to OLCF, you'll need to request a replacement fob. To do so, contact either the User Assistance Team (help@olcf.ornl.gov) or the Accounts Team (accounts@olcf.ornl.gov). You do not need to return the broken/expired RSA token to OLCF. Disposal and recycling information can be found in the vendor's disposal statement.



When submitting a ticket to help@olcf.ornl.gov requesting help, you will likely get faster resolution by supporting a few best practices:",4.450386845143171
"How do I return the RSA SecurID Token to the OLCF Accounts Team?
",Join an Additional Project Existing users with RSA SecurID tokens should log in to the myOLCF self-service portal to apply for additional projects. Existing users without RSA SecurID tokens should fill out the Account Application Form that can be found at the top left of the myOLCF login page without needing to sign in. See the section https://docs.olcf.ornl.gov/systems/documents_and_forms.html#applying-for-a-user-account for complete details.,4.375034828558703
"What is the name of the dynamic linker used by Frontier?
","<p style=""font-size:20px""><b>Frontier: Darshan Runtime 3.4.0 (May 10, 2023)</b></p>

The Darshan Runtime modulefile darshan-runtime/3.4.0 on Frontier is now loaded by default. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>.

Unloading darshan-runtime modulefile is recommended for users profiling their applications with other profilers to prevent conflicts.",4.160153270233814
"What is the name of the dynamic linker used by Frontier?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers. Please see the following https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more detailed inforation on how to compile using these modules.",4.129586914335432
"What is the name of the dynamic linker used by Frontier?
","Frontier connects to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#dtn-user-guide to move data to/from HPSS.

Frontier is running Cray OS 2.4 based on SUSE Linux Enterprise Server (SLES) version 15.4.",4.107616425500619
"How do I request a new project in the SPI enclave?
","Once a project has been approved and created, the next step will be to request user accounts.  A user account will allow an individual to access the project's allocated resources.    This process to request an account is very similar to the process used for non-SPI projects.  One notable difference between SPI and non-SPI accounts: SPI usernames are unique to a project.  SPI usernames use the format: <userid>_<proj>_mde.  If you have access to three SPI projects, you will have three userIDs with three separate home areas.",4.24930221715898
"How do I request a new project in the SPI enclave?
","https://docs.olcf.ornl.gov/systems/index.html#Request an allocation (project)<spi-allocations-projects>.  All access and resource use occurs within an approved allocation.

https://docs.olcf.ornl.gov/systems/index.html#Request a user account<spi-user-accounts>.  Once an allocation (project) has been approved, each member of the project must request an account to use the project's allocated resources.",4.211276680817383
"How do I request a new project in the SPI enclave?
","Similar to standard OLCF workflows, to run SPI workflows on OLCF resources, you will first need an approved project.  The project will be awarded an allocation(s) that will allow resource access and use.  The first step to requesting an allocation is to complete the project request form.  The form will initiate the process that includes peer review, export control review, agreements, and others.  Once submitted, members of the OLCF accounts team will help walk you through the process.

Please see the OLCF Accounts and Projects section of this site to request a new project.",4.194620032485624
"How can I use a container registry like DockerHub to save my Podman container images?
","If you are familiar with using a container registry like DockerHub, you can use that to save your Podman container images and use Singularity to pull from the registry and build the sif file. Below, we will use DockerHub as the example but there are many other container registries that you can use.

Using the simple example from the previous section, build the container image with podman build -t docker.io/<username>/simple -f simple.dockerfile . where <username> is your user on DockerHub.

podman push uses the URL in the container image's name to push to the appropriate registry.",4.499740906629507
"How can I use a container registry like DockerHub to save my Podman container images?
","Check if your image is created

$ podman image ls
REPOSITORY                         TAG      IMAGE ID      CREATED      SIZE
docker.io/subilabrahamornl/simple  latest   e47dbfde3e99  3 hours ago  687 MB
localhost/simple                   latest   e47dbfde3e99  3 hours ago  687 MB
quay.io/centos/centos              stream8  ad6f8b5e7f64  8 days ago   497 MB

Run podman login docker.io and enter your account's username and password so that Podman is logged in to the container registry before pushing.

Push the container image to the registry with podman push docker.io/<username>/simple.",4.265893857439028
"How can I use a container registry like DockerHub to save my Podman container images?
","Podman is a container framework from Red Hat that functions as a drop in replacement for Docker. On Summit, we will use Podman for building container images and converting them into tar files for Singularity to use. Podman makes use of Dockerfiles to describe the images to be built. We cannot use Podman to run containers as it doesn't properly support MPI on Summit, and Podman does not support storing its container images on GPFS or NFS.",4.242819823807889
"How can I add a path to the modulefile search cache and MODULESPATH in Andes?
",| Adds <path> to the modulefile search cache and MODULESPATH | | module unuse <path> | Removes <path> from the modulefile search cache and MODULESPATH | | module purge | Unloads all modules | | module reset | Resets loaded modules to system defaults | | module update | Reloads all currently loaded modules |,4.194355983613151
"How can I add a path to the modulefile search cache and MODULESPATH in Andes?
",| Adds <path> to the modulefile search cache and MODULESPATH | | module unuse <path> | Removes <path> from the modulefile search cache and MODULESPATH | | module purge | Unloads all modules | | module reset | Resets loaded modules to system defaults | | module update | Reloads all currently loaded modules |,4.194355983613151
"How can I add a path to the modulefile search cache and MODULESPATH in Andes?
",| Adds <path> to the modulefile search cache and MODULESPATH | | module unuse <path> | Removes <path> from the modulefile search cache and MODULESPATH | | module purge | Unloads all modules | | module reset | Resets loaded modules to system defaults | | module update | Reloads all currently loaded modules |,4.194355983613151
"What is the default distribution layout for MPI tasks with the srun command?
","The below srun command will achieve the intended 7 MPI ""packed"" layout:

$ export OMP_NUM_THREADS=1
$ srun -N1 -n7 -c1 --cpu-bind=threads --threads-per-core=1 -m block:block ./hello_mpi_omp | sort



Breaking down the srun command, the only difference than the previous example is:

-m block:block: distribute the tasks in a block layout across nodes (default), and in a block (packed) socket layout",4.347609954936367
"What is the default distribution layout for MPI tasks with the srun command?
","The intent with both of the following examples is to launch 8 MPI ranks across the node where each rank is assigned its own logical (and, in this case, physical) core.  Using the -m distribution flag, we will cover two common approaches to assign the MPI ranks -- in a ""round-robin"" (cyclic) configuration and in a ""packed"" (block) configuration. Slurm's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive method was used to request an allocation of 1 compute node for these examples: salloc -A <project_id> -t 30 -p <parition> -N 1",4.3436081417155945
"What is the default distribution layout for MPI tasks with the srun command?
","Using srun

By default, commands will be executed on the job’s primary compute node, sometimes referred to as the job’s head node. The srun command is used to execute an MPI binary on one or more compute nodes in parallel.

srun accepts the following common options:

| -N | Minimum number of nodes | | --- | --- | | -n | Total number of MPI tasks | | --cpu-bind=no | Allow code to control thread affinity | | -c | Cores per MPI task | | --cpu-bind=cores | Bind to cores |

If you do not specify the number of MPI tasks to srun via -n, the system will default to using only one task per node.",4.314672933166345
"How can I reset my container storage area on Summit?
","Users will be building and running containers on Summit without root permissions i.e. containers on Summit are rootless.  This means users can get the benefits of containers without needing additional privileges. This is necessary for a shared system like Summit. And this is part of the reason why Docker doesn't work on Summit. Podman and Singularity provides rootless support but to different extents hence why users need to use a combination of both.

Users will need to set up a file in their home directory /ccs/home/<username>/.config/containers/storage.conf with the following content:",4.225622747467409
"How can I reset my container storage area on Summit?
",For Summit:,4.084170038972409
"How can I reset my container storage area on Summit?
","Summit is connected to an IBM Spectrum Scale™ filesystem providing 250PB of storage capacity with a peak write speed of 2.5 TB/s. Summit also has access to the center-wide NFS-based filesystem (which provides user and project home areas) and has access to the center’s High Performance Storage System (HPSS) for user and project archival storage.

Summit is running Red Hat Enterprise Linux (RHEL) version 8.2.",4.060548150091665
"How do I create a new image in Openshift using the S2I model?
","Source to image is a tool to build docker formatted container images. This is accomplished by injecting source code into a container image and then building a new image. The new image will contain the source code ready to run, via the $ docker run command inside the newly built image.

Using the S2I model of building images, it is possible to have your source hosted on a git repository and then pulled and built by Openshift.

Click on the newly created project and then on the blue Add to Project button on the center of the page.",4.452977894849084
"How do I create a new image in Openshift using the S2I model?
","Building an image in Openshift is the act of transferring a set of input parameters into an object. That object is typically an image. Openshift contains all of the necessary components to build a Docker image or a piece of source code an image that will run in a container.

A Docker build is achieved by linking to a source repository that contains a docker image and all of the necessary Docker artifacts. A build is then triggered through the standard $ docker build command. More specific documentation on docker builds can be found here.",4.261723202951427
"How do I create a new image in Openshift using the S2I model?
","Openshift will automatically pull and build your source. If you make a change and need an updated build simply navigate to the build option on the menu to the left and select your project and click Start Build in the upper right.

The first step to creating the the build is to create a build config. This is done in one of three ways depending on how your code is structured. If you have a git repository already configured and it is public then the command

oc new-build .

will create your build config from that repository.",4.242238952208713
"What is the recommended way to launch and run jobs on Frontier?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.36554967749876
"What is the recommended way to launch and run jobs on Frontier?
","Unlike Summit and Titan, there are no launch/batch nodes on Frontier. This means your batch script runs on a node allocated to you rather than a shared node. You still must use the job launcher (srun) to run parallel jobs across all of your nodes, but serial tasks need not be launched with srun.



To easily visualize job examples (see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-mapping further below), the compute node diagram has been simplified to the picture shown below.

Simplified Frontier node architecture diagram",4.321148966043802
"What is the recommended way to launch and run jobs on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.317679059936883
"How can I unload all modules in Frontier?
","You will need to unload the darshan-runtime module. In some instances you may need to unload the xalt and xl modules.  $ module unload darshan-runtime

Serial

..  C

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load scorep
        $ module load gcc
        $ scorep gcc -c test.c
        $ scorep gcc -o test test.o

..  C++

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load scorep
        $ module load gcc
        $ scorep g++ -c test.cpp main.cpp
        $ scorep g++ -o test test.o main.o

..  Fortran",4.116874165537726
"How can I unload all modules in Frontier?
","<p style=""font-size:20px""><b>Frontier: Darshan Runtime 3.4.0 (May 10, 2023)</b></p>

The Darshan Runtime modulefile darshan-runtime/3.4.0 on Frontier is now loaded by default. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>.

Unloading darshan-runtime modulefile is recommended for users profiling their applications with other profilers to prevent conflicts.",4.082684533694366
"How can I unload all modules in Frontier?
","First, load the gnu compiler module (most Python packages assume GCC), relevant GPU module (necessary for CuPy), and the python module (allows you to create a new environment):

Summit

.. code-block:: bash

   $ module load gcc/7.5.0 # might work with other GCC versions
   $ module load cuda/11.0.2
   $ module load python

Frontier

.. code-block:: bash

   $ module load PrgEnv-gnu
   $ module load amd-mixed/5.3.0
   $ module load craype-accel-amd-gfx90a
   $ module load cray-python # only if not using Miniconda on Frontier",4.074338368598378
"How can I ensure that all necessary libraries are included when running an executable on a high-performance computing cluster?
","Some libraries still resolved to paths outside of /mnt/bb, and the reason for that is that the executable may have several paths in RPATH.



Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-user-guide.",4.191563690115208
"How can I ensure that all necessary libraries are included when running an executable on a high-performance computing cluster?
","The OLCF provides hundreds of pre-installed software packages and scientific libraries for your use, in addition to taking software installation requests. See the software page for complete details on existing installs.

Compiling code on andes is typical of commodity or Beowulf-style HPC Linux clusters.

The following compilers are available on Andes:

intel, intel composer xe (default)

pgi, the portland group compilar suite

gcc, the gnu compiler collection",4.164219508958
"How can I ensure that all necessary libraries are included when running an executable on a high-performance computing cluster?
","Installed Software

The OLCF provides hundreds of pre-installed software packages and scientific libraries for your use, in addition to taking software installation requests. See the software page for complete details on existing installs.

Compiling

Compiling code on andes is typical of commodity or Beowulf-style HPC Linux clusters.

Available compilers

The following compilers are available on Andes:

intel, intel composer xe (default)

pgi, the portland group compilar suite

gcc, the gnu compiler collection",4.151295158466265
"How do you specify the block size when using --distribution=*:block?
","In the job step for this example, --distribution=*:block is used, where * represents the default value of block for the distribution of MPI ranks across compute nodes and the distribution of MPI ranks across L3 cache regions has been changed to block from its default value of cyclic.",4.448946454890908
"How do you specify the block size when using --distribution=*:block?
","--distribution=<value>[:<value>][:<value>] specifies the distribution of MPI ranks across compute nodes, sockets (L3 cache regions on Crusher), and cores, respectively. The default values are block:cyclic:cyclic, which is where the cyclic assignment comes from in the previous examples.

In the job step for this example, --distribution=*:block is used, where * represents the default value of block for the distribution of MPI ranks across compute nodes and the distribution of MPI ranks across L3 cache regions has been changed to block from its default value of cyclic.",4.323165283394237
"How do you specify the block size when using --distribution=*:block?
","--distribution=<value>:[<value>]:[<value>] specifies the distribution of MPI ranks across compute nodes, sockets (NUMA domains on Spock), and cores, respectively. The default values are block:cyclic:cyclic, which is where the cyclic assignment comes from in the previous examples.

In the job step for this example, --distribution=*:block is used, where * represents the default value of block for the distribution of MPI ranks across compute nodes and the distribution of MPI ranks across NUMA domains has been changed to block from its default value of cyclic.",4.2984570685873305
"What is the potential drawback of using the maximizegpfs flag?
","By default, GPFS system service tasks are forced onto only the isolated cores. This can be overridden at the batch job level using the maximizegpfs argument to LSF's alloc_flags. For example:

#BSUB -alloc_flags maximizegpfs

The maximizegpfs flag will allow GPFS tasks to utilize any core on the compute node. This may be beneficial because it provides more resources for GPFS service tasks, but it may also cause resource contention for the jsrun compute job.",4.329670755951232
"What is the potential drawback of using the maximizegpfs flag?
","If your application occupies up to two compute nodes and it requires a significant number of I/O operations, you could try to add the following flag in your job script  file and investigate if the total execution time is decreased. This flag could cause worse results, it depends on the application.

#BSUB -alloc_flags maximizegpfs

The file systems have many technical differences, but we will mention only what a user needs to be familiar with:",4.31709778761134
"What is the potential drawback of using the maximizegpfs flag?
","export GPFSMPIO_COMM=1

This command will use non-blocking MPI calls and not MPI_Alltoallv for exchange of data between the MPI I/O aggregators which requires significant more amount of memory.



The following issues were resolved with the May 21, 2019 upgrade:

Some users have reported an internal compiler error when compiling their code with XL with the `-g` flag. This has been reported to IBM and they are investigating.

This bug was fixed in xl/16.1.1-3",4.013819947785793
"How many replicas are specified in the Deployment?
","pods, so you may have to adjust the number of pod replicas for each version to deal with the increased/decreased load.",4.239503059610327
"How many replicas are specified in the Deployment?
","Below is an example Deployment:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 5
  selector:
    matchLabels:
      name: my-app
  template: { ... }
  strategy:
    type: RollingUpdate
  revisionHistoryLimit: 2
  minReadySeconds: 0
  paused: false

Let's look at the individual parts of this definition, under spec.

replicas - the number of replicas to be passed down to the ReplicaSet

selector - the selector to determine which pods are managed by the ReplicaSet.",4.239337790445909
"How many replicas are specified in the Deployment?
","A recreate deployment scales the previous deployment down to 0 before starting the new deployment. This is best used when a downtime is acceptable, and your application cannot handle having the old and new versions running at the same time.

Here is an example recreate deployment:

strategy:
  type: Recreate

The recreate strategy follows this sequence:

Scale down the old deployment to 0 replicas.

Scale up the new deployment to the number of desired replicas.",4.181930944185618
"What is the advantage of oversubscribing the system memory in Summit?
","Most Summit nodes contain 512 GB of DDR4 memory for use by the POWER9 processors, 96 GB of High Bandwidth Memory (HBM2) for use by the accelerators, and 1.6TB of non-volatile memory that can be used as a burst buffer. A small number of nodes (54) are configured as ""high memory"" nodes. These nodes contain 2TB of DDR4 memory, 192GB of HBM2, and 6.4TB of non-volatile memory.",4.191669622735973
"What is the advantage of oversubscribing the system memory in Summit?
","Summit is connected to an IBM Spectrum Scale™ filesystem providing 250PB of storage capacity with a peak write speed of 2.5 TB/s. Summit also has access to the center-wide NFS-based filesystem (which provides user and project home areas) and has access to the center’s High Performance Storage System (HPSS) for user and project archival storage.

Summit is running Red Hat Enterprise Linux (RHEL) version 8.2.",4.167249928242043
"What is the advantage of oversubscribing the system memory in Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.109932175369387
"What is the purpose of the cp.cuda.Stream.null.synchronize() function?
","This implementation allows threads to diverge and synchronize at the sub-warp level using the __syncwarp() function. The independent thread scheduling enables the thread scheduler to stall execution of any thread, allowing other threads in the warp to execute different statements. This means that threads in one branch can stall at a sync point and wait for the threads in the other branch to reach their sync point.",3.9413253819088503
"What is the purpose of the cp.cuda.Stream.null.synchronize() function?
",https://vimeo.com/582093007 | | 2021-07-16 | CUDA Multithreading with Streams | Robert Searles (NVIDIA) | CUDA Training Series https://www.olcf.ornl.gov/calendar/cuda-multithreading/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/05/10-Multithreading-and-CUDA-Concurrency.pdf https://vimeo.com/575930839 | | 2021-05-26 | ROCgdb and HIP Math Libraries | Justin Chang (AMD) | HIP Training Workshop https://www.olcf.ornl.gov/calendar/2021hip/ | (slides | exercises | recording) https://www.olcf.ornl.gov/wp-content/uploads/2021/04/rocgdb_hipmath_ornl_2021_v2.pdf,3.93948187713291
"What is the purpose of the cp.cuda.Stream.null.synchronize() function?
",new CUDA API functions introduced in CUDA8 allow users to fine tune the use of unified memory.,3.939070649579822
"Which pod is responsible for server functionality in Slate?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.197509867728735
"Which pod is responsible for server functionality in Slate?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",4.185348633078872
"Which pod is responsible for server functionality in Slate?
","Jenkins

OpenShift Pipelines

GitLab Runners

Deploying a GitLab Runner into a Slate project may be found in the https://docs.olcf.ornl.gov/systems/overview.html#slate_gitlab_runners document which leverages a localized Slate Helm Chart. The GitLab Pipelines documentation as well as GitLab CI/CD Examples provide more details on GitLab Runner capabilities and usage.

Jenkins

For Jenkins deployment, a Slate Helm Chart is planned to be released. Documentation concerning Jenkins Pipelines as well as Jenkins Pipeline Tutorials are available.

OpenShift Pipelines",4.062948286340613
"What is the purpose of the page https://docs.olcf.ornl.gov/systems/user_centric.html#data-storage-and-transfers?
","For more information about using the data storage archiving systems, please refer to the pages on https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#data-storage-and-transfers.",4.608601385614653
"What is the purpose of the page https://docs.olcf.ornl.gov/systems/user_centric.html#data-storage-and-transfers?
","This page has been deprecated, and relevant information is now available on https://docs.olcf.ornl.gov/systems/policies.html#data-storage-and-transfers. Please update any bookmarks to use that page.

A brief description of each area and basic guidelines to follow are provided in the table below:",4.507565902613506
"What is the purpose of the page https://docs.olcf.ornl.gov/systems/user_centric.html#data-storage-and-transfers?
","This page has been deprecated, and relevant information is now available on https://docs.olcf.ornl.gov/systems/user_centric.html#data-storage-and-transfers. Please update any bookmarks to use that page.

The following table summarizes user-centric storage areas available on OLCF resources and lists relevant polices.

User-Centric Storage Areas",4.499941692926331
"How can I ensure that my application's data is persisted even if the pods are ephemeral?
","By design pods are ephemeral. They are meant to be something that can be killed with relatively little impact to the application. Since containers are ephemeral, we need a way to consume storage that is persistent and can hold data for our applications. One option that Kubernetes has for storing data is with Persistent Volumes. PersistentVolume objects are created by the cluster administrator and reference storage that is on a resilient storage platform such as NetApp. A user can request storage by creating a PersistentVolumeClaim which requests a desired size for a PersistentVolume. The",4.34054439914667
"How can I ensure that my application's data is persisted even if the pods are ephemeral?
",If the application needs access to a temporary space for doing something like generating a configuration file on launch you can mount a EmptyDir volume in the PodSpec which will ensure that whatever user the container is running as will have access to write to that directory.,4.210827937886508
"How can I ensure that my application's data is persisted even if the pods are ephemeral?
","Now, if the data in the PersistentVolume becomes corrupted or lost in some way, we can create a new PersistentVolume that is identical to the original PersistentVolume at the time the VolumeSnapshot was created.

To do this, create a PersistentVolumeClaim that contains a dataSource field in the Pod's spec. An example follows:",4.178771337419435
"Is there a way to set the environment variable OMPI_MCA_io permanently?
","| Package | Current Default | New Default | | --- | --- | --- | | spectrum-mpi | unset | 10.2.0.11-20190201 | | xl | 16.1.1-1 | 16.1.1-2 | | pgi | unset | 18.10 |

In addition, the following default Spectrum MPI settings will be changed to address issues resolved with the February 19, 2019 software upgrade:

| Environment Variable | Current Default | New Default | | --- | --- | --- | | OMP_MCA_io | romio314 | romio321 | | OMPI_MCA_coll_ibm_xml_disable_cache | 1 | unset | | PAMI_PMIX_USE_OLD_MAPCACHE | 1 | unset |",4.05042188302095
"Is there a way to set the environment variable OMPI_MCA_io permanently?
","Unload the darshan-runtime modulefile.

Alternatively, set export OMPI_MCA_io=romio314 in your environment to use the previous version of ROMIO. Please note that this version has known performance issues with parallel HDF5 (see ""Slow performance using parallel HDF5"" issue below).



The following issues were resolved with the February 19, 2019 upgrade:",4.005566696468463
"Is there a way to set the environment variable OMPI_MCA_io permanently?
","$ jsrun --nrs 41 -c 21 -a 1 --bind rs ./a.out
[a03n07:74208] *** Process received signal ***
[a03n07:74208] Signal: Segmentation fault (11)
[a03n07:74208] Signal code: Address not mapped (1)
[a03n07:74208] Failing at address: (nil)
...

As a workaround, two environment variables are set as default in the user environment PAMI_PMIX_USE_OLD_MAPCACHE=1 and OMPI_MCA_coll_ibm_xml_disable_cache=1.",3.969873030215792
"Can I run a job step on a single node in Frontier?
","Unlike Summit and Titan, there are no launch/batch nodes on Frontier. This means your batch script runs on a node allocated to you rather than a shared node. You still must use the job launcher (srun) to run parallel jobs across all of your nodes, but serial tasks need not be launched with srun.



To easily visualize job examples (see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-mapping further below), the compute node diagram has been simplified to the picture shown below.

Simplified Frontier node architecture diagram",4.343230414144544
"Can I run a job step on a single node in Frontier?
","Due to the unique architecture of Frontier compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.

Before jumping into the examples, it is helpful to understand the output from the hello_jobstep program:",4.274357281681234
"Can I run a job step on a single node in Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.270624853499279
"What is the benefit of using Tensor Cores in Summit for iterative refinement techniques?
","Tensor Cores provide the potential for an enormous performance boost over full-precision operations, but when their use is appropriate is highly application and even problem independent. Iterative Refinement techniques can suffer from slow or possible a complete lack of convergence if the condition number of the matrix is very large. By using Tensor Cores, which support 32-bit accumulation, rather than strict 16-bit math operations, iterative refinement becomes a viable option in a much larger number of cases, so it should be attempted when an application is already using a supported solver.",4.553948344587809
"What is the benefit of using Tensor Cores in Summit for iterative refinement techniques?
",| (recording) https://vimeo.com/306440151 | | 2018-12-04 | Using V100 Tensor Cores | Jeff Larkin (NVIDIA) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Tensor-Cores.pdf https://vimeo.com/306437682 | | 2018-12-04 | NVIDIA Profilers | Jeff Larkin (NVIDIA) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Profilers.pdf,4.31821195441597
"What is the benefit of using Tensor Cores in Summit for iterative refinement techniques?
",| (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Tensor-Cores.pdf https://vimeo.com/346452359 | | 2019-02-12 | NVIDIA Profilers | Jeff Larkin (NVIDIA) | Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Libraries.pdf https://vimeo.com/346452291 | | 2019-02-12 | GPU-Accelerated Libraries | Jeff Larkin (NVIDIA) | Summit Training Workshop (February 2019),4.281987253775305
"What is the advantage of using -c 1 instead of -S 8?
","An alternative solution to Example 6 and 7's -S 8 issue is to use -c 1 instead.  There is no problem when running with 1 core per MPI rank (i.e., 7 ranks per GPU) because the task can’t span multiple L3s.

$ OMP_NUM_THREADS=1 srun -N1 -n56 -c1 --ntasks-per-gpu=7 --gpu-bind=closest --distribution=*:block ./hello_jobstep | sort

Example 9: 8 independent and simultaneous job steps running on a single node",4.14901902366081
"What is the advantage of using -c 1 instead of -S 8?
","Frontier uses low-noise mode and core specialization (-S flag at job allocation, e.g., sbatch).  Low-noise mode constrains all system processes to core 0.  Core specialization (by default, -S 8) reserves the first core in each L3 region.  This prevents the user running on the core that system processes are constrained to.  This also means that there are only 56 allocatable cores by default instead of 64. Therefore, this modifies the simplified node layout to:

Simplified Frontier node architecture diagram (low-noise mode)",4.105874242931865
"What is the advantage of using -c 1 instead of -S 8?
","Slurm's core specialization default will change: Slurm --core-spec or -S value will be set to 8. This will provide a symmetric distribution of cores per GCD to the application and will reserve one core per L3 cache region. After the outage, the default number of cores available to each GCD on a node will be 7. To change from the new default value, you can set --core-spec or -S in your job submission.

The default NIC mapping will be updated to MPICH_OFI_NIC_POLICY=NUMA to address known issues described in OLCFDEV-192 and OLCFDEV-1366.",4.084969850115635
"What is the recommended way to specify the number of resource sets in a jsrun command?
","Resource sets allow each jsrun to control how the node appears to a code. This method is unique to jsrun, and requires thinking of each job launch differently than aprun or mpirun. While the method is unique, the method is not complicated and can be reasoned in a few basic steps.",4.4414929010595054
"What is the recommended way to specify the number of resource sets in a jsrun command?
","It's recommended to explicitly specify jsrun options and not rely on the default values. This most often includes --nrs,--cpu_per_rs, --gpu_per_rs, --tasks_per_rs, --bind, and --launch_distribution.

The below examples were launched in the following 2 node interactive batch job:

summit> bsub -nnodes 2 -Pprj123 -W02:00 -Is $SHELL

The following example will create 12 resource sets each with 1 MPI task and 1 GPU. Each MPI task will have access to a single GPU.",4.402306561497123
"What is the recommended way to specify the number of resource sets in a jsrun command?
","The following table provides a quick reference for creating resource sets of various common use cases. The -n flag can be altered to specify the number of resource sets needed.

| Resource Sets | MPI Tasks | Threads | Physical Cores | GPUs | jsrun Command | | --- | --- | --- | --- | --- | --- | | 1 | 42 | 0 | 42 | 0 | jsrun -n1 -a42 -c42 -g0 | | 1 | 1 | 0 | 1 | 1 | jsrun -n1 -a1 -c1 -g1 | | 1 | 2 | 0 | 2 | 1 | jsrun -n1 -a2 -c2 -g1 | | 1 | 1 | 0 | 1 | 2 | jsrun -n1 -a1 -c1 -g2 | | 1 | 1 | 21 | 21 | 3 | jsrun -n1 -a1 -c21 -g3 -bpacked:21 |",4.394040076426206
"How do I know if my Summit application requires CUDA-aware MPI?
","Using the hip-cuda/5.1.0 module on Summit requires CUDA v11.4.0 or later. However, only CUDA v11.0.3 and earlier currently support GPU-aware MPI, so GPU-aware MPI is currently not available when using HIP on Summit.

Any codes compiled with post 11.0.3 CUDA (cuda/11.0.3) will not work with MPS enabled (-alloc_flags ""gpumps"") on Summit. The code will hang indefinitely. CUDA v11.0.3 is the latest version that is officially supported by IBM's software stack installed on Summit. We are continuing to look into this issue.",4.347686227274757
"How do I know if my Summit application requires CUDA-aware MPI?
","Although there are newer CUDA modules on Summit, cuda/11.0.3 is the latest version that is officially supported by the version of IBM's software stack installed on Summit. When loading the newer CUDA modules, a message is printed to the screen stating that the module is for “testing purposes only”. These newer unsupported CUDA versions might work with some users’ applications, but importantly, they are known not to work with GPU-aware MPI.",4.2977472832043855
"How do I know if my Summit application requires CUDA-aware MPI?
","If you do need CUDA-aware MPI functionality, then the only known working solution to this problem is to refactor your code so that no CUDA calls occur before MPI_Init(). (This includes any libraries or programming models such as OpenACC or OpenMP that would use CUDA behind the scenes.) While it is not explicitly codified in the standard, it is worth noting that the major MPI implementations all recommend doing as little as possible before MPI_Init(), and this recommendation is consistent with that.",4.292180643999607
"What is the purpose of the --immediate option in jsrun?
","If JSM or PMIX errors occur as the result of backgrounding many job steps, using the --immediate option to jsrun may help, as shown in the following example.

#!/bin/bash
#BSUB -P ABC123
#BSUB -W 3:00
#BSUB -nnodes 1
#BSUB -J RunSim123
#BSUB -o RunSim123.%J
#BSUB -e RunSim123.%J

cd $MEMBERWORK/abc123
jsrun <options> --immediate ./a.out
jsrun <options> --immediate ./a.out
jsrun <options> --immediate ./a.out

By default, jsrun --immediate does not produce stdout or stderr. To capture stdout and/or stderr when using this option, additionally include --stdio_stdout/-o and/or --stdio_stderr/-k.",4.349233215939017
"What is the purpose of the --immediate option in jsrun?
","In addition, to debug slow startup JSM provides the option to create a progress file. The file will show information that can be helpful to pinpoint if a specific node is hanging or slowing down the job step launch. To enable it, you can use: jsrun --progress ./my_progress_file_output.txt.

When using file-based specification of resource sets with jsrun -U, the -a flag (number of tasks per resource set) is ignored. This has been reported to IBM and they are investigating. It is generally recommended to use jsrun explicit resource files (ERF) with --erf_input and --erf_output instead of -U.",4.168259421664251
"What is the purpose of the --immediate option in jsrun?
","This section describes tools that users might find helpful to better understand the jsrun job launcher.

hello_jsrun is a ""Hello World""-type program that users can run on Summit nodes to better understand how MPI ranks and OpenMP threads are mapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A screencast showing how to use Hello_jsrun is also available: https://vimeo.com/261038849

Job Step Viewer provides a graphical view of an application's runtime layout on Summit. It allows users to preview and quickly iterate with multiple jsrun options to understand and optimize job launch.",4.123465551090517
"How can I analyze the results of my jobstep on Frontier?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.134406140619338
"How can I analyze the results of my jobstep on Frontier?
","Due to the unique architecture of Frontier compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.

Before jumping into the examples, it is helpful to understand the output from the hello_jobstep program:",4.107714165301583
"How can I analyze the results of my jobstep on Frontier?
","To access Summit and Frontier's compute resources for SPI workflows, you must first log into a https://docs.olcf.ornl.gov/systems/index.html#Citadel login node<citadel-login-nodes> and then submit a batch job to one of the SPI specific batch queues.",4.079033564659228
"Can I use pmake with other packages installed in the virtual environment?
","pmake is a standard python package.  It is recommended to install it in a virtual environment.  One easy way to create a virtual environment is to load an available python module, and then put a new environment into /ccs/proj/<projid>/<systemname>.  This way, the project can share environments, and each system gets its own install location.

$ module load python/3.8-anaconda3
$ python -m venv /path/to/new-venv
$ source /path/to/new-venv/bin/activate

On subsequent logins, remember to load the same python module and run the source /path/to/new-venv/bin/activate command again.",4.401766778204328
"Can I use pmake with other packages installed in the virtual environment?
","Once you have entered the virtual environment, pmake can be installed with:

$ python -m pip install git+https://code.ornl.gov/99R/pmake.git@latest

Run the following command to verify that pmake is available:

$ pmake --help

To run a pmake demo on Summit, you will create a pmake-example directory with its preferred file layout, then submit a batch job to LSF from a Summit login node.

First, create the directories,

$ mkdir -p pmake-example/simulation

Next, create pmake's two configuration files, rules.yaml and targets.yaml:

# pmake-example/targets.yaml",4.382357089535257
"Can I use pmake with other packages installed in the virtual environment?
","#... additional rules here

To check the syntax of your files, cd into the pmake-example directory and run pmake --test. It should show the commands that would be run if pmake were being executed inside a job-script.

Finally, create an LSF batch script called pmake.lsf, fix the python module and virtual environment path to match your installation above, and change abc123 to match your own project identifier:

#BSUB -P abc123
#BSUB -W 10
#BSUB -nnodes 1

#BSUB -J pmake_demo
#BSUB -o pmake.o%J
#BSUB -e pmake.e%J

module load python/3.8-anaconda3
source /path/to/new-venv/bin/activate",4.113142121648384
"How can I link a pre-installed library into my Fortran program using pgfortran on Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.26392094264194
"How can I link a pre-installed library into my Fortran program using pgfortran on Summit?
","$ module load tau
$ export TAU_MAKEFILE=/sw/summit/tau/tau2/ibm64linux/lib/Makefile.tau-pgi-papi-mpi-cupti-pdt-pgi
$ export TAU_OPTIONS='-optLinking=-lpnetcdf -optVerbose'
$ ./cmake_summit_pgi.sh
$ make serial

If there were no MPI headers, you should select the makefile /sw/summit/tau/tau2//ibm64linux/lib/Makefile.tau-pgi-papi-pdt-pgi or if you don't want PDT support, /sw/summit/tau/tau2//ibm64linux/lib/Makefile.tau-pgi-papi-pgi Add to your submission script the TAU variables that you want to use (or uncomment them below). By default the TAU will apply profiling, and not apply tracing.",4.173369487856275
"How can I link a pre-installed library into my Fortran program using pgfortran on Summit?
","mpif77 or mpif90 to invoke appropriate versions of the fortran compiler

These wrapper programs are cognizant of your currently loaded modules, and will ensure that your code links against our openmpi installation.  more information about using openmpi at our center can be found in our software documentation.

When building threaded codes, compiler-specific flags must be included to ensure a proper build.

For pgi, add ""-mp"" to the build line.

$ mpicc -mp test.c -o test.x
$ export OMP_NUM_THREADS=2

For gnu, add ""-fopenmp"" to the build line.",4.151947404228155
"What is the difference between cublasGemmEx and cublasGemm in Summit?
","If it is, the cublasGemmEx API allows the programmer to control when the conversion to 16-bit occurs, which may result in higher throughput than allowing the cuBLAS library to do the conversion at call time.",4.284537447218795
"What is the difference between cublasGemmEx and cublasGemm in Summit?
",HGEMM using Tensor Cores. For either of these two methods the cublasSetMathMode function must be used to change from CUBLAS_DEFAULT_MATH to CUBLAS_TENSOR_OP_MATH mode.,4.184933182638488
"What is the difference between cublasGemmEx and cublasGemm in Summit?
","The cuBLAS libraries provides access to the TensorCores using 3 different routines, depending on the application needs. The cublasHgemm routine performs a general matrix multiplication of half-precision matrices. The numerical operands to this routine must be of type half and math mode must be set to CUBLAS_TENSOR_OP_MATH to enable Tensor Core use. Additionally, if the cublasSgemm routine will down-convert from single precision to half precision when the math mode is set to CUBLAS_TENSOR_OP_MATH, enabling simple conversion from SGEMM to HGEMM using Tensor Cores. For either of these two",4.155449713888937
"How should sensitive or controlled information be transferred according to the OLCF Policy?
","Never allow access to your sensitive data to anyone outside of your group.

Transfer of sensitive data must be through the use encrypted methods (scp, sftp, etc).

All sensitive data must be removed from all OLCF resources when your project has concluded.",4.42747665308112
"How should sensitive or controlled information be transferred according to the OLCF Policy?
","3. Prior to your project being initialized, we must have source IP addresses for devices in your organization that are authorized to transfer sensitive/controlled information to/from your organization, or for use in accessing that information. Encryption is necessary for transferring sensitive and/or controlled information to and from the OLCF.",4.397965225621305
"How should sensitive or controlled information be transferred according to the OLCF Policy?
","biological, or chemical weapons or any weapons of mass destruction. Authors/generators/owners of information are responsible for its correct categorization as sensitive or non-sensitive. Owners of sensitive information are responsible for its secure handling, transmission, processing, storage, and disposal on OLCF systems. Principal investigators, users, or project delegates that use OLCF resources, or are responsible for overseeing projects that use OLCF resources, are strictly responsible for knowing whether their project generates any of these prohibited data types or information that",4.371390073858578
"What is the difference between the -c and -n options in srun?
",The following srun options will be used in the examples below. See man srun for a complete list of options and more information.,4.33049727060139
"What is the difference between the -c and -n options in srun?
",The following srun options will be used in the examples below. See man srun for a complete list of options and more information.,4.33049727060139
"What is the difference between the -c and -n options in srun?
","srun accepts the following common options:

| -N | Minimum number of nodes | | --- | --- | | -n | Total number of MPI tasks | | --cpu-bind=no | Allow code to control thread affinity | | -c | Cores per MPI task | | --cpu-bind=cores | Bind to cores |

If you do not specify the number of MPI tasks to srun via -n, the system will default to using only one task per node.

Each compute node on Andes contains two sockets each with 16 cores.  Depending on your job, it may be useful to control task layout within and across nodes.",4.278188077478206
"Can I run MPI jobs on Summit?
","If you want to do GPU computing on Summit with R, we would love to collaborate with you (see contact details at the bottom of this document).

For more information about using R and pbdR effectively in an HPC environment such as Summit, please see the R and pbdR articles. These are long-form articles that introduce HPC concepts like MPI programming in much more detail than we do here.

Also, if you want to use R and/or pbdR on Summit, please feel free to contact us directly:

Mike Matheson - mathesonma AT ornl DOT gov

George Ostrouchov - ostrouchovg AT ornl DOT gov",4.322965027822263
"Can I run MPI jobs on Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.321643365693609
"Can I run MPI jobs on Summit?
","Using the hip-cuda/5.1.0 module on Summit requires CUDA v11.4.0 or later. However, only CUDA v11.0.3 and earlier currently support GPU-aware MPI, so GPU-aware MPI is currently not available when using HIP on Summit.

Any codes compiled with post 11.0.3 CUDA (cuda/11.0.3) will not work with MPS enabled (-alloc_flags ""gpumps"") on Summit. The code will hang indefinitely. CUDA v11.0.3 is the latest version that is officially supported by IBM's software stack installed on Summit. We are continuing to look into this issue.",4.318068674956868
"How can I use the GCC compiler on Frontier?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers. Please see the following https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more detailed inforation on how to compile using these modules.",4.46664131343046
"How can I use the GCC compiler on Frontier?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers.

It is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) whenever possible. See the next section for more details.",4.4021141801156345
"How can I use the GCC compiler on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.373320666529716
"How can I load a module into the current environment in Frontier?
","First, load the gnu compiler module (most Python packages assume GCC), relevant GPU module (necessary for CuPy), and the python module (allows you to create a new environment):

Summit

.. code-block:: bash

   $ module load gcc/7.5.0 # might work with other GCC versions
   $ module load cuda/11.0.2
   $ module load python

Frontier

.. code-block:: bash

   $ module load PrgEnv-gnu
   $ module load amd-mixed/5.3.0
   $ module load craype-accel-amd-gfx90a
   $ module load cray-python # only if not using Miniconda on Frontier",4.236992456390979
"How can I load a module into the current environment in Frontier?
","Summit

Andes

Frontier (if using conda)

First, load the python module and the gnu compiler module on Summit (most Python packages assume use of GCC)

$ module load gcc
$ module load python

The above module load python does not apply to Frontier (since you will be using a personal Miniconda instead).

This puts you in the ""base"" conda environment, which is the default Python environment after loading the module. To see a list of environments, use the command conda env list:

$ conda env list

# conda environments:
#
base                  *  /sw/summit/python/3.8/anaconda3/2020.07-rhel8",4.171142639481603
"How can I load a module into the current environment in Frontier?
","<p style=""font-size:20px""><b>Frontier: Darshan Runtime 3.4.0 (May 10, 2023)</b></p>

The Darshan Runtime modulefile darshan-runtime/3.4.0 on Frontier is now loaded by default. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>.

Unloading darshan-runtime modulefile is recommended for users profiling their applications with other profilers to prevent conflicts.",4.165520103719466
"How do I navigate through the filesystem to my .otf2 files after authentication?
","Once the authentication step is complete, it should open up the remote filesystem for you to navigate to and load your .otf2 trace file.

Please see the provided video below to get a brief demo of the Vampir GUI provided by TU-Dresden and presented by Ronny Brendel.

You can skip ahead to around the 22 minute mark!",4.429138937932357
"How do I navigate through the filesystem to my .otf2 files after authentication?
","When the server authentication window pops up, you will need to enter your USERID & the https://docs.olcf.ornl.gov/systems/Vampir.html#VampirServer password <vampserpw> that was printed on the terminal screen. Once authenticated, you will be able to navigate through the filesystem to your .otf2 files















































This connection method is more complex than the other 2 methods, however it also can provide a more optimal experience for very large trace files.",4.37813078356513
"How do I navigate through the filesystem to my .otf2 files after authentication?
","When prompted, authenticate into the OLCF DTN endpoint using your OLCF username and PIN followed by your RSA passcode.

Click in the left side “Path” box in the File Manager and enter the path to your data on Alpine. For example, /gpfs/alpine/stf007/proj-shared/my_alpine_data. You should see a list of your files and folders under the left “Path” Box.

Click on all files or folders that you want to transfer in the list. This will highlight them.

Click on the right side “Collection” box in the File Manager and type “OLCF DTN”",4.167108849139737
"How do I create a directory for my MLflow project on Summit?
","MLflow is an open source platform to manage the Machine Learning (ML) lifecycle, including experimentation, reproducibility, deployment, and a central model registry. To learn more about MLflow, please refer to its documentation.

In order to use MLflow on Summit, load the module as shown below:

$ module load workflows
$ module load mlflow/1.22.0

Run the following command to verify that MLflow is available:

$ mlflow --version
mlflow, version 1.22.0

To run this MLflow demo on Summit, you will create a directory with two files and then submit a batch job to LSF from a Summit login node.",4.379078974080996
"How do I create a directory for my MLflow project on Summit?
","It is highly recommended to create new environments in the ""Project Home"" directory (on Summit, this is /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>). This space avoids purges, allows for potential collaboration within your project, and works better with the compute nodes. It is also recommended, for convenience, that you use environment names that indicate the hostname, as virtual environments created on one system will not necessarily work on others.",4.214325609526642
"How do I create a directory for my MLflow project on Summit?
","First, create a directory mlflow-example to contain two files. The first will be named MLproject:

name: demo

entry_points:
  main:
    command: ""python3 demo.py""

The second will be named demo.py:

import mlflow

print(""MLflow Version:"", mlflow.version.VERSION)
print(""Tracking URI:"", mlflow.tracking.get_tracking_uri())

with mlflow.start_run() as run:
    print(""Run ID:"", run.info.run_id)
    print(""Artifact URI:"", mlflow.get_artifact_uri())
    with open(""hello.txt"", ""w"") as f:
        f.write(""Hello world!"")
        mlflow.log_artifact(""hello.txt"")",4.195484649254278
"What is the name of the container in the Deployment?
","name: the-container
        ports:
        - containerPort: 8080",4.201461550924717
"What is the name of the container in the Deployment?
","We will be deploying MongoDB with a StatefulSet. This is a special kind of deployment controller that is different from a normal Deployment in a few distinct ways and is primarily meant for for applications that rely on well known names for each pod.

We will create a Service with the StatefulSet because the StatefulSet controller requires a headless service in order to provide well-known DNS identifiers.",4.133984484497685
"What is the name of the container in the Deployment?
","deployment: hello
  template:
    metadata:
      labels:
        app: hello
        deployment: hello
    spec:
      containers:
      - command:
        - /hello
        - --port=8080
        - --enableRiskyFeature=$(ENABLE_RISKY)
        env:
        - name: ALT_GREETING
          valueFrom:
            configMapKeyRef:
              key: altGreeting
              name: the-map
        - name: ENABLE_RISKY
          valueFrom:
            configMapKeyRef:
              key: enableRisky
              name: the-map
        image: monopole/hello:1
        name: the-container",4.113346650394218
"What is the maximum quota for storage on Lustre Orion?
","Frontier mounts Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Frontier will not mount Alpine when Orion is in production.",4.365173652351265
"What is the maximum quota for storage on Lustre Orion?
","Frontier is connected to Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Data will not be automatically transferred from Alpine to Orion. Frontier also has access to the center-wide NFS-based filesystem (which provides user and project home areas). Each compute node has two 1.92TB Non-Volatile Memory storage devices. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-data-storage for more",4.310131172541541
"What is the maximum quota for storage on Lustre Orion?
","Each user-centric and project-centric storage area has an associated quota, which could be a hard (systematically-enforceable) quota or a soft (policy-enforceable) quota. Storage usage will be monitored continually. When a user or project exceeds a soft quota for a storage area, the user or project PI will be contacted and will be asked if at all possible to purge data from the offending area. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for details on quotas for each storage area.",4.254532531484803
"How can I specify the volume mounts for the MongoDB container in Slate?
","image: mongo
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          value: admin
        - name: MONGO_INITDB_ROOT_PASSWORD
          value: password
        ports:
        - containerPort: 27017
          name: mongo
        volumeMounts:
        - name: mongo-store
          mountPath: /data/db
  volumeClaimTemplates:
  - metadata:
      name: mongo-store
    spec:
      accessModes: [""ReadWriteOnce""]
      resources:
        requests:
          storage: 1Gi",4.112423908599303
"How can I specify the volume mounts for the MongoDB container in Slate?
","OLCF shared filesystems can be mounted into a container running in Slate. The mountpoints will be the same as a cluster node. The Kubernetes object will need to be annotated in order to get the necessary configuration injected into the container at runtime.

| Cluster | Annotation | Value | Mounts | | --- | --- | --- | --- | | Marble | ccs.ornl.gov/fs | olcf | /ccs/sw, /ccs/home, /ccs/sys, /ccs/proj, /gpfs/alpine | | Onyx | ccs.ornl.gov/fs | ccsopen | /ccsopen/sw, /ccsopen/home, /ccsopen/proj, /gpfs/wolf |

If you already have a Deployment running you can add the annotation with the client",4.105942719982245
"How can I specify the volume mounts for the MongoDB container in Slate?
","MongoDB is a common ""NoSQL"" database. We will be creating a Deployment to run the MongoDB service and expose it external to the cluster after setting up authentication. We will also be deploying a management Web UI for viewing queries.

Access to an allocation in Slate, the NCCS Kubernetes service

oc client installed

CLI client is logged into the cluster (oc login https://api.<cluster>.ccs.ornl.gov)

<string>:18: (INFO/1) Duplicate implicit target name: ""deploy mongodb"".",4.098360539719028
"How can I install ArgoCD on Slate?
","The Red Hat OpenShift GitOps operator is already installed onto each of the Slate clusters. However, to use ArgoCD an instance will need to be added into a project namespace. If one is to use ArgoCD to manage resources in a single project, then the ArgoCD instance could be located in the same project as the resources being managed. However, if the application being managed is resource (CPU and memory) sensitive or if resources in multiple projects are to be managed, it may be better to have the ArgoCD instance deployed into a separate project to allow for better control of resources allocated",4.212659599023873
"How can I install ArgoCD on Slate?
",Image of ArgoCD new application general settings.,4.142307464452945
"How can I install ArgoCD on Slate?
","Next Click the ""Create ArgoCD"" button. You will be presented with a form view similar to:

Image of the form view for ArgoCD instance creation.

Starting with the form view of the process, make the following changes allowing for access to the ArgoCD web UI via a route:

Server -> Insecure -> true

Server -> Route -> Enabled -> true

Server -> Route -> Tls -> Termination -> edge

Image of the form view for ArgoCD instance creation with the route settings configured.",4.137856131594903
"How can I expose my service to the world outside ORNL?
","By default, a route will only expose your https://docs.olcf.ornl.gov/systems/route.html#slate_services to NCCS networks. If you need your service exposed to the world outside ORNL, you will first need to get your project approved for external routes. To do this, submit a systems ticket. In the description, give us your project name and a brief reasoning for why exposing externally is needed.

We will let you know once your project is able to set up external routes.",4.388990424830773
"How can I expose my service to the world outside ORNL?
","I will use best efforts to publish the results from my use of the ORNL User Facility in an open scientific journal or significant industry technical journal or conference proceedings. I will acknowledge use of the ORNL User Facility in the publication and notify the ORNL User Facility of any publications that result from my use of the facility.

I will comply with all U.S. Export Control laws and regulations and be responsible for the appropriate handling and transfer of any export controlled information, which may require advance U.S. Government authorization.",4.280373855626096
"How can I expose my service to the world outside ORNL?
","Under metadata, add a label for ccs.ornl.gov/externalRoute: 'true' as shown below and click the Save button at the bottom of the page.

Route After

After saving, your route will be exposed on two routers, default and external. This means your service is now accessible from outside ORNL. Note that if your project has not yet been approved for external routing, this second router will not expose your route.

Route Exposed",4.196308296917401
"How much high-bandwidth memory (HBM2) does each GPU have access to?
","Each V100 has access to 16 GB (32GB for high-memory nodes) of high-bandwidth memory (HBM2), which can be accessed at speeds of up to 900 GB/s. Access to this memory is controlled by (8) 512-bit memory controllers, and all accesses to the high-bandwidth memory go through the 6 MB L2 cache.

The processors within a node are connected by NVIDIA's NVLink interconnect. Each link has a peak bandwidth of 25 GB/s (in each direction), and since there are 2 links between processors, data can be transferred from GPU-to-GPU and CPU-to-GPU at a peak rate of 50 GB/s.",4.487443247242912
"How much high-bandwidth memory (HBM2) does each GPU have access to?
","Most Summit nodes contain 512 GB of DDR4 memory for use by the POWER9 processors, 96 GB of High Bandwidth Memory (HBM2) for use by the accelerators, and 1.6TB of non-volatile memory that can be used as a burst buffer. A small number of nodes (54) are configured as ""high memory"" nodes. These nodes contain 2TB of DDR4 memory, 192GB of HBM2, and 6.4TB of non-volatile memory.",4.436293541829671
"How much high-bandwidth memory (HBM2) does each GPU have access to?
","The 110 CUs in each GPU deliver peak performance of 26.5 TFLOPS in double precision. Also, each GPU contains 64 GB of high-bandwidth memory (HBM2) accessible at a peak bandwidth of 1.6 TB/s. The 2 GPUs in an MI250X are connected with [4x] GPU-to-GPU Infinity Fabric links providing 200+200 GB/s of bandwidth. (Consult the diagram in the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes section for information on how the accelerators are connected to each other, to the CPU, and to the network.",4.406295912680792
"How do I apply to be a mentor?
","If you would like more information about how you can volunteer to mentor a team at an upcoming Open/GPU hackathon, please visit our Become a Mentor page.

<p style=""font-size:20px""><b>Who can I contact with questions?</b></p>

If you have any questions about the OLCF GPU Hackathon Series, please contact OLCF Help at (help@olcf.ornl.gov).",4.235844800379568
"How do I apply to be a mentor?
","First, you must decide which event you'd like to attend (use link below to find a hackathon whose dates make sense for your team), and then submit a short proposal form describing your application and team. The organizing committee will then review all proposals after the call for that event closes and select the teams they believe are best suited for the event.",4.12624145014756
"How do I apply to be a mentor?
","Please visit openhackathons.org/s/events-overview to see the current list of events (new ones added throughout the year) and their proposal deadlines. To submit a proposal, click on the event you'd like to attend and submit the form.

The OLCF-supported events are a subset of a larger number of Open hackathons organized around the world. Look for hackathons with the OLCF logo to find events supported by OLCF.

<p style=""font-size:20px""><b>Want to be a mentor?</b></p>",4.042010594034704
"Can I request a specific resource for my job in Andes?
",For Andes:,4.212528601699684
"Can I request a specific resource for my job in Andes?
","Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-user-guide.

For a full list of software available at the OLCF, please see the Software section (coming soon).",4.12164324170547
"Can I request a specific resource for my job in Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.047120654440559
"What is the role of the OLCF in maintaining and supporting user-managed software installations?
","Users should acknowledge the OLCF in all publications and presentations that speak to work performed on OLCF resources:

This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725.



To request software installation, please contact help@olcf.ornl.gov with a description of the software, including the following details:

Software name.

Description of the software and its purpose. Is it export controlled?",4.4210108679396445
"What is the role of the OLCF in maintaining and supporting user-managed software installations?
","The OLCF provides a comprehensive suite of hardware and software resources for the creation, manipulation, and retention of scientific data. This document comprises guidelines for acceptable use of those resources. It is an official policy of the OLCF, and as such, must be agreed to by relevant parties as a condition of access to and use of OLCF computational resources.",4.388526206650758
"What is the role of the OLCF in maintaining and supporting user-managed software installations?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators of UMS Projects  Points of Contact for UMS Projects  Title: UMS Project Policies Version: 21.08

This document is intended to describe the agreement between the OLCF and the providers of user-managed software installations. User-managed software is built, maintained, and supported by OLCF users rather than as official offerings of the OLCF, but is exposed to all users through the module system.",4.388347839896062
"Can I run my own quantum algorithms on Rigetti's systems?
","Rigetti currently offers access to their systems via their Quantum Cloud Services (QCS).  With QCS, Rigetti's quantum processors (QPUs) are tightly integrated with classical computing infrastructure and made available to you over the cloud. Rigetti also provides users with quantum computing example algorithms for optimization, quantum system profiling, and other applications.

A list of available Rigetti systems/QPUs, along with their performance statistics, can be found on the Rigetti Systems Page.",4.427184569891903
"Can I run my own quantum algorithms on Rigetti's systems?
","accessing the hybrid infrastructure of available quantum processors and classical computational framework via the cloud. From the QCS, users can view system status and availability, initiate and manage quantum infrastructure reservations (either executing programs manually or adding them to the queue). Information on using this resource is available on the Rigetti's Documentation or our https://docs.olcf.ornl.gov/quantum/quantum_systems/rigetti.html.",4.371767314935665
"Can I run my own quantum algorithms on Rigetti's systems?
","Users are able to install Rigetti software locally for the purpose of development using a provided Quantum Virtual Machine, or QVM, an implementation of a quantum computer simulator that can run Rigetti's Quil programs.  This can be done via two methods:

Installing manually: https://docs.rigetti.com/qcs/getting-started/installing-locally

Docker: https://hub.docker.com/r/rigetti/forest",4.320028829534673
"What is the name of the tutorial that this Slate tutorial is a continuation of?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.205740369563003
"What is the name of the tutorial that this Slate tutorial is a continuation of?
","Before we dive in there are some terms that need to be understood. This will be a basic set of terms and a copy and paste from our https://docs.olcf.ornl.gov/systems/guided_tutorial.html#slate_glossary, so we recommend reading that document and even keeping it handy until you are familiar with all of the definitions there. On that note, another good piece of reference documentation the https://docs.olcf.ornl.gov/systems/guided_tutorial.html#slate_examples document. There you can find basic YAML definitions for the most common objects in Kubernetes.",4.040390787426097
"What is the name of the tutorial that this Slate tutorial is a continuation of?
","Jenkins

OpenShift Pipelines

GitLab Runners

Deploying a GitLab Runner into a Slate project may be found in the https://docs.olcf.ornl.gov/systems/overview.html#slate_gitlab_runners document which leverages a localized Slate Helm Chart. The GitLab Pipelines documentation as well as GitLab CI/CD Examples provide more details on GitLab Runner capabilities and usage.

Jenkins

For Jenkins deployment, a Slate Helm Chart is planned to be released. Documentation concerning Jenkins Pipelines as well as Jenkins Pipeline Tutorials are available.

OpenShift Pipelines",4.016128922035642
"How can I update my bookmarks to access the archiving information?
","This page has been deprecated, and relevant information is now available on https://docs.olcf.ornl.gov/systems/archiving.html#data-storage-and-transfers. Please update any bookmarks to use that page.",4.057731383130182
"How can I update my bookmarks to access the archiving information?
","World Archive Directory: 775

For example, if you have data that must be restricted only to yourself, keep them in your Member Archive directory for that project (and leave the default permissions unchanged). If you have data that you intend to share with researchers within your project, keep them in the project’s Project Archive directory. If you have data that you intend to share with researchers outside of a project, keep them in the project’s World Archive directory.",3.978326977842881
"How can I update my bookmarks to access the archiving information?
","For more information about using the data storage archiving systems, please refer to the pages on https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#data-storage-and-transfers.",3.976588446612783
"What is the purpose of the pat_report command?
","pat_build -g hip,io,mpi -w -f <executable>

The pat_build command in the above examples generates an instrumented executable with +pat appended to the executable name (e.g., hello_jobstep+pat).

When run, the instrumented executable will trace HIP, I/O, MPI, and all user functions and generate a folder of results (e.g., hello_jobstep+pat+39545-2t).

To analyze these results, use the pat_report command, e.g.:

pat_report hello_jobstep+pat+39545-2t",4.103661255248479
"What is the purpose of the pat_report command?
","pat_build -g hip,io,mpi -w -f <executable>

The pat_build command in the above examples generates an instrumented executable with +pat appended to the executable name (e.g., hello_jobstep+pat).

When run, the instrumented executable will trace HIP, I/O, MPI, and all user functions and generate a folder of results (e.g., hello_jobstep+pat+39545-2t).

To analyze these results, use the pat_report command, e.g.:

pat_report hello_jobstep+pat+39545-2t",4.103661255248479
"What is the purpose of the pat_report command?
","module load valgrind4hpc

Additional information about Valgrind4hpc usage can be found on the HPE Cray Programming Environment User Guide Page.

The Performance Analysis Tools (PAT), formerly CrayPAT, are a suite of utilities that enable users to capture and analyze performance data generated during program execution. These tools provide an integrated infrastructure for measurement, analysis, and visualization of computation, communication, I/O, and memory utilization to help users optimize programs for faster execution and more efficient computing resource usage.",4.102273798274466
"Can you give me an example of how to enable Horizontal Pod Autoscaling for my application on Slate?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",4.196206095346902
"Can you give me an example of how to enable Horizontal Pod Autoscaling for my application on Slate?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.155931592378626
"Can you give me an example of how to enable Horizontal Pod Autoscaling for my application on Slate?
","It is assumed you have already gone through https://docs.olcf.ornl.gov/systems/minio.html#slate_getting_started and established the https://docs.olcf.ornl.gov/systems/minio.html#helm_prerequisite. Please do that before attempting to deploy anything on Slate's clusters.

This example uses Helm version 3 to deploy a MinIO standalone Helm chart on Slate's Marble Cluster. This is the cluster in OLCF's Moderate enclave, the same enclave as Summit.

To start, clone the slate helm examples repository , containing the MinIO standalone Helm chart, and navigate into the charts directory:",4.153502304049904
"Can I run threaded jobs on a login node on Frontier?
","Recall from the System Overview that Frontier contains two node types: Login and Compute. When you connect to the system, you are placed on a login node. Login nodes are used for tasks such as code editing, compiling, etc. They are shared among all users of the system, so it is not appropriate to run tasks that are long/computationally intensive on login nodes. Users should also limit the number of simultaneous tasks on login nodes (e.g. concurrent tar commands, parallel make",4.345882221892229
"Can I run threaded jobs on a login node on Frontier?
","| Node Type | Description | | --- | --- | | Login | When you connect to Frontier, you're placed on a login node. This is the place to write/edit/compile your code, manage data, submit jobs, etc. You should never launch parallel jobs from a login node nor should you run threaded jobs on a login node. Login nodes are shared resources that are in use by many users simultaneously. | | Compute | Most of the nodes on Frontier are compute nodes. These are where your parallel job executes. They're accessed via the srun command. |",4.342145064116908
"Can I run threaded jobs on a login node on Frontier?
","Because a Frontier compute node has two hardware threads available (2 logical cores per physical core), this enables the possibility of multithreading your application (e.g., with OpenMP threads). Although the additional hardware threads can be assigned to additional MPI tasks, this is not recommended. It is highly recommended to only use 1 MPI task per physical core and to use OpenMP threads instead on any additional logical cores gained when using both hardware threads.",4.283762385033578
"Are there any other programming models that can be used on Frontier?
",Programming Environment  Frontier utilizes the Cray Programming Environment.  Many aspects including LMOD are similar to Summit's environment.  But Cray's compiler wrappers and the Cray and AMD compilers are worth noting.  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more information.  AMD GPUs  Each frontier node has 4 AMD MI250X accelerators with two Graphic Compute Dies (GCDs) in each accelerator. The system identifies each GCD as an independent device (so for simplicity we use the term GPU when we talk about a GCD) for a total of 8,4.242953959752112
"Are there any other programming models that can be used on Frontier?
","There are a variety of programming models available to program GPUs (e.g. CUDA, OpenACC, OpenMP offloading, etc.) and you are welcome to use any of them at these events.

<p style=""font-size:20px""><b>Why participate?</b></p>",4.180492046662192
"Are there any other programming models that can be used on Frontier?
","| (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/Data-and-Storage-areas-3.pdf https://vimeo.com/803622140 | | 2023-02-15 | Using the Frontier Programming Environment | Matt Belhorn, HPC Engineer, ORNL | Frontier Training Workshop https://www.olcf.ornl.gov/calendar/frontier-training-workshop-february-2023/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/frontier_programming_environment_20230215.pdf https://vimeo.com/803621185 | | 2023-02-15 | Frontier Programming Environment | Wael Elwasif, Computer Scientist, ORNL | Frontier Training Workshop",4.170354356121749
"How do I access my NGINX web server in Slate?
","You can also go to it by logging into the Marble GUI. Once logged in, go to Networking->Routes and click the URL in the ""Location"" column of your MinIO applications row.

You will be greeted with the NCCS SSO page. Continue through that with your normal NCCS login credentials.

After the NCCS login, you will be greeted with MinIO's login page. Here you will enter the access-key and secret-key you created with the secret-token.yaml file.

At this point, you should be inside the MinIO Browser.",4.104242506866166
"How do I access my NGINX web server in Slate?
","In general, using FQDNs to access a service is more convenient. If your service communicates over HTTP or HTTPS, you can set up a https://docs.olcf.ornl.gov/systems/services.html#slate_routes to achieve this. If your service uses another protocol, you can use https://docs.olcf.ornl.gov/systems/services.html#slate_nodeports.

NodePort is a type of Service that reserves a port across the cluster that can route traffic to your pods. This is more flexible than a Route and can handle any TCP or UDP traffic.",4.1012792666873015
"How do I access my NGINX web server in Slate?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",4.098572385672423
"How can I determine the offset and size of a ROC bundle?
","The AMD tool `roc-obj-ls` will let you see what code objects are in a binary.

.. code::
    $ hipcc --amdgpu-target=gfx90a:xnack+ square.hipref.cpp -o xnack_plus.exe
    $ roc-obj-ls -v xnack_plus.exe
    Bundle# Entry ID:                                                              URI:
    1       host-x86_64-unknown-linux                                           file://xnack_plus.exe#offset=8192&size=0
    1       hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+                              file://xnack_plus.exe#offset=8192&size=9752",4.042570257115378
"How can I determine the offset and size of a ROC bundle?
","The AMD tool `roc-obj-ls` will let you see what code objects are in a binary.

.. code::
    $ hipcc --amdgpu-target=gfx90a:xnack+ square.hipref.cpp -o xnack_plus.exe
    $ roc-obj-ls -v xnack_plus.exe
    Bundle# Entry ID:                                                              URI:
    1       host-x86_64-unknown-linux                                           file://xnack_plus.exe#offset=8192&size=0
    1       hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+                              file://xnack_plus.exe#offset=8192&size=9752",4.042570257115378
"How can I determine the offset and size of a ROC bundle?
","This provides the minimum set of metrics used to construct a roofline model, in the minimum number of passes. Each line that begins with pmc indicates that the application will be re-run, and the metrics in that line will be collected. rocprof can collect up to 8 counters from each block (SQ, TCC) in each application re-run. To gather metrics across multiple MPI ranks, you will need to use a command that redirects the output of rocprof to a unique file for each task. For example:",3.9342935870571454
"How can I specify the number of MPI tasks for Paraview?
","Submitting one of the above scripts will submit a job to the batch partition for five minutes using 28 MPI tasks across 1 node. As rendering speeds and memory issues widely vary for different datasets and MPI tasks, users are encouraged to find the optimal amount of MPI tasks to use for their data. Users with large datasets may also find a slight increase in performance by using the gpu partition on Andes, or by utilizing the GPUs on Summit. Once the batch job makes its way through the queue, the script will launch the loaded ParaView module (specified with module load) and execute a python",4.291491529433503
"How can I specify the number of MPI tasks for Paraview?
","1 MPI task (-a1), 1 GPU (-g1), and 4 cores (-c4). Notice that more cores are requested than MPI tasks; the extra cores will be needed to place threads. Without requesting additional cores, threads will be placed on a single core.",4.204502847831076
"How can I specify the number of MPI tasks for Paraview?
","In addition to specifying the SMT level, you can also control the number of threads per MPI task by exporting the OMP_NUM_THREADS environment variable. If you don't export it yourself, Jsrun will automatically set the number of threads based on the number of cores requested (-c) and the binding (-b) option. It is better to be explicit and set the OMP_NUM_THREADS value yourself rather than relying on Jsrun constructing it for you. Especially when you are using Job Step Viewer which relies on the presence of that environment variable to give you visual thread assignment information.",4.177099480266074
"How can I programmatically map GPUs to MPI ranks on Frontier?
","As has been pointed out previously in the Frontier documentation, notice that GPUs are NOT mapped to MPI ranks in sequential order (e.g., MPI rank 0 is mapped to physical CPU cores 1-7 and GPU 4, MPI rank 1 is mapped to physical CPU cores 9-15 and GPU 5), but this IS expected behavior. It is simply a consequence of the Frontier node architectures as shown in the Frontier Node Diagram and subsequent https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Note on NUMA domains <numa-note>.

Example 2: 1 MPI rank with 7 CPU cores and 1 GPU (single-node)",4.4641598601315655
"How can I programmatically map GPUs to MPI ranks on Frontier?
","This section describes how to map processes (e.g., MPI ranks) and process threads (e.g., OpenMP threads) to the CPUs, GPUs, and NICs on Frontier.

Users are highly encouraged to use the CPU- and GPU-mapping programs used in the following sections to check their understanding of the job steps (i.e., srun commands) they intend to use in their actual jobs.

For the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-cpu-map and https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-multi-map sections:",4.425155582048832
"How can I programmatically map GPUs to MPI ranks on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.414803648678067
"Are there any restrictions on the use of OLCF QCUP resources?
","Welcome! The information below introduces how we structure projects and user accounts for access to the Quantum resources within the QCUP program. In general, OLCF QCUP resources are granted to projects, which are in turn made available to the approved users associated with each project.",4.434675660516817
"Are there any restrictions on the use of OLCF QCUP resources?
","Access to OLCF resources is limited to approved projects and their users. The type of project (INCITE, ALCC, or Director's Discretion) will determine the application and review procedures. *Quarterly reports are required from industrial Director's Discretion projects only.",4.352865657933311
"Are there any restrictions on the use of OLCF QCUP resources?
","The OLCF will then establish a QCUP project and notify the PI of its creation along with the 6-character OLCF QCUP Project ID and resources allocation details. At this time project participants may proceed with applying for their individual user accounts.

QCUP Projects have a finite duration; when starting, projects get however many months are left in that allocation period and then must be renewed for subsequent 6 month intervals. Projects can be renewed by filling out a renewal form (:download:`Accounts Renewal Form <Quantum-Renewal-Form.docx>`) and emailing it to accounts@ccs.ornl.gov.",4.352502171650424
"What is the difference between hipMallocManaged() and hipMallocManaged() with hipMemAdvise(hipMemAdviseSetCoarseGrain)?
","Finally, the following table summarizes the nature of the memory returned based on the flag passed as argument to hipMallocManaged() and the use of CPU regular malloc() routine with the possible use of hipMemAdvise().

| API | MemAdvice | Result | | --- | --- | --- | | hipMallocManaged() |  | Fine grained | | hipMallocManaged() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained | | malloc() |  | Fine grained | | malloc() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained |",4.608181035828158
"What is the difference between hipMallocManaged() and hipMallocManaged() with hipMemAdvise(hipMemAdviseSetCoarseGrain)?
","Finally, the following table summarizes the nature of the memory returned based on the flag passed as argument to hipMallocManaged() and the use of CPU regular malloc() routine with the possible use of hipMemAdvise().

| API | MemAdvice | Result | | --- | --- | --- | | hipMallocManaged() |  | Fine grained | | hipMallocManaged() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained | | malloc() |  | Fine grained | | malloc() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained |",4.608181035828158
"What is the difference between hipMallocManaged() and hipMallocManaged() with hipMemAdvise(hipMemAdviseSetCoarseGrain)?
","| API | Flag | Results | | --- | --- | --- | | hipHostMalloc() | hipHostMallocDefault | Fine grained | | hipHostMalloc() | hipHostMallocNonCoherent | Coarse grained |

The following table shows the nature of the memory returned based on the flag passed as argument to hipExtMallocWithFlags().

| API | Flag | Result | | --- | --- | --- | | hipExtMallocWithFlags() | hipDeviceMallocDefault | Coarse grained | | hipExtMallocWithFlags() | hipDeviceMallocFinegrained | Fine grained |",4.327875548472059
"How do you specify the number of CPUs to use with pbdR?
","If you want to do GPU computing on Summit with R, we would love to collaborate with you (see contact details at the bottom of this document).

For more information about using R and pbdR effectively in an HPC environment such as Summit, please see the R and pbdR articles. These are long-form articles that introduce HPC concepts like MPI programming in much more detail than we do here.

Also, if you want to use R and/or pbdR on Summit, please feel free to contact us directly:

Mike Matheson - mathesonma AT ornl DOT gov

George Ostrouchov - ostrouchovg AT ornl DOT gov",4.13198709165644
"How do you specify the number of CPUs to use with pbdR?
","For parallelism, you should use pbdR packages, Rmpi directly, or an interface which can use Rmpi as a backend. We address GPUs specifically next.

There are some R packages which can use GPUs, such as xgboost. There is also the gpuR series of packages. Several pbdR packages support GPU computing. It is also possible to offload some linear algebra computations (specifically matrix-matrix products, and methods which are computationally dominated by them) to the GPU using NVIDIA’s NVBLAS.",4.127807611436911
"How do you specify the number of CPUs to use with pbdR?
","In addition to specifying the SMT level, you can also control the number of threads per MPI task by exporting the OMP_NUM_THREADS environment variable. If you don't export it yourself, Jsrun will automatically set the number of threads based on the number of cores requested (-c) and the binding (-b) option. It is better to be explicit and set the OMP_NUM_THREADS value yourself rather than relying on Jsrun constructing it for you. Especially when you are using Job Step Viewer which relies on the presence of that environment variable to give you visual thread assignment information.",4.077163022405662
"How can I use the ""h5py"" module to write data to an HDF5 file in parallel?
","Both HDF5 and h5py can be compiled with MPI support, which allows you to optimize your HDF5 I/O in parallel. MPI support in Python is accomplished through the mpi4py package, which provides complete Python bindings for MPI. Building h5py against mpi4py allows you to write to an HDF5 file using multiple parallel processes, which can be helpful for users handling large datasets in Python. h5Py is available after loading the default Python module on either Summit or Andes, but it has not been built with parallel support.",4.48047825277474
"How can I use the ""h5py"" module to write data to an HDF5 file in parallel?
","The guide is designed to be followed from start to finish, as certain steps must be completed in the correct order before some commands work properly.

This guide teaches you how to build a personal, parallel-enabled version of h5py and how to write an HDF5 file in parallel using mpi4py and h5py.

In this guide, you will:

Learn how to install mpi4py

Learn how to install parallel h5py

Test your build with Python scripts

OLCF Systems this guide applies to:

Summit

Andes

Frontier",4.347985139941375
"How can I use the ""h5py"" module to write data to an HDF5 file in parallel?
","There are various tools that allow users to interact with HDF5 data, but we will be focusing on h5py -- a Python interface to the HDF5 library. h5py provides a simple interface to exploring and manipulating HDF5 data as if they were Python dictionaries or NumPy arrays. For example, you can extract specific variables through slicing, manipulate the shapes of datasets, and even write completely new datasets from external NumPy arrays.",4.312514748946324
"How can I run multiple jobs on the same node with different GPU bindings?
","This example shows how to run multiple independent, simultaneous job steps on a single compute node. Specifically, it shows how to run 8 independent hello_jobstep programs running on their own CPU core and GPU.

Submission script:

#!/bin/bash

#SBATCH -A stf016_frontier
#SBATCH -N 1
#SBATCH -t 5

for idx in {1..8};

    do
        date

        OMP_NUM_THREADS=1 srun -u --gpus-per-task=1 --gpu-bind=closest -N1 -n1 -c1 ./hello_jobstep &

        sleep 1
    done

wait

Output:",4.342412089481497
"How can I run multiple jobs on the same node with different GPU bindings?
","So the job step (i.e., srun command) used above gave the desired output. Each MPI rank spawned 2 OpenMP threads and had access to a unique GPU. The --gpus-per-task=1 allocated 1 GPU for each MPI rank and the --gpu-bind=closest ensured that the closest GPU to each rank was the one used.

Example 2: 8 MPI ranks - each with 2 OpenMP threads and 1 GPU (multi-node)

This example will extend Example 1 to run on 2 nodes. As the output shows, it is a very straightforward exercise of changing the number of nodes to 2 (-N2) and the number of MPI ranks to 8 (-n8).",4.2898300679916925
"How can I run multiple jobs on the same node with different GPU bindings?
","Due to the unique architecture of Frontier compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.

Before jumping into the examples, it is helpful to understand the output from the hello_jobstep program:",4.288293971590226
"How does ArgoCD in Slate differ from other deployment tools?
",allow for better control of resources allocated to ArgoCD.,4.309715939222441
"How does ArgoCD in Slate differ from other deployment tools?
","In other words, the CD in ArgoCD is for continuous delivery of the application(s).",4.263453817200363
"How does ArgoCD in Slate differ from other deployment tools?
","The Red Hat OpenShift GitOps operator is already installed onto each of the Slate clusters. However, to use ArgoCD an instance will need to be added into a project namespace. If one is to use ArgoCD to manage resources in a single project, then the ArgoCD instance could be located in the same project as the resources being managed. However, if the application being managed is resource (CPU and memory) sensitive or if resources in multiple projects are to be managed, it may be better to have the ArgoCD instance deployed into a separate project to allow for better control of resources allocated",4.254452363331833
"How can I run multiple job steps simultaneously in a single allocation using Crusher?
","This example shows how to run multiple job steps simultaneously in a single allocation. The example below demonstrates running 4 independent, single rank MPI executions on a single node, however the example could be extrapolated to more complex invocations using the above examples.

Submission script:

#!/bin/bash
#SBATCH -A <projid>
#SBATCH -N 1
#SBATCH -t 10",4.3024667790710405
"How can I run multiple job steps simultaneously in a single allocation using Crusher?
","Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.",4.293893683241415
"How can I run multiple job steps simultaneously in a single allocation using Crusher?
","To execute multiple job steps concurrently, standard UNIX process backgrounding can be used by adding a & at the end of the command. This will return control to the job script and execute the next command immediately, allowing multiple job launches to start at the same time. The jsruns will not share core/gpu resources in this configuration. The batch node allocation is equal to the sum of those of each jsrun, and the total walltime must be equal to or greater than that of the longest running jsrun task.",4.285848860779658
"What is the purpose of the PMIX_MCA_ptl_base_max_msg_size parameter?
","If you get an error message that looks like:

A received msg header indicates a size that is too large:
 Requested size: 25836785
 Size limit: 16777216
If you believe this msg is legitimate, please increase the
max msg size via the ptl_base_max_msg_size parameter.

This can be resolved by setting export PMIX_MCA_ptl_base_max_msg_size=18 where the value is size in MB. Setting it to 18 or higher usually works. The default if its not explicitly set is around 16 MB.

Adding FAULT_TOLERANCE=1 in your individual ~/.jsm.conf file, will result in LSF jobs failing to successfully start.",4.276892460901392
"What is the purpose of the PMIX_MCA_ptl_base_max_msg_size parameter?
","755          8          8          8          0  Message size for all-reduce
       302  2.621E+05          4  1.302E+05  1.311E+05  Message size for broadcast
---------------------------------------------------------------------------------------",3.917775240262964
"What is the purpose of the PMIX_MCA_ptl_base_max_msg_size parameter?
","The reason this occurs is that the PAMI messaging backend, used by Spectrum MPI by default, has a ""CUDA hook"" that records GPU memory allocations. This record is used later during CUDA-aware MPI calls to efficiently detect whether a given message is sent from the CPU or the GPU. This is done by design in the IBM implementation and is unlikely to be changed.

There are two main ways to work around this problem. If CUDA-aware MPI is not a relevant factor for your work (which is naturally true for serial applications) then you can simply disable the CUDA hook with:",3.873263163142111
"Can I prevent a preempted job from being automatically requeued?
","The killable queue is a preemptable queue that allows jobs in bins 4 and 5 to request walltimes up to 24 hours. Jobs submitted to the killable queue will be preemptable once the job reaches the guaranteed runtime limit as shown in the table below. For example, a job in bin 5 submitted to the killable queue can request a walltime of 24 hours. The job will be preemptable after two hours of run time. Similarly, a job in bin 4 will be preemptable after six hours of run time. Once a job is preempted, the job will be resubmitted by default with the original limits as requested in the job script and",4.145909336470362
"Can I prevent a preempted job from being automatically requeued?
","-------------------------



At the start of a scheduled system outage, a *queue reservation* is used

to ensure that no jobs are running. In the ``batch`` queue, the

scheduler will not start a job if it expects that the job would not

complete (based on the job's user-specified max walltime) before the

reservation's start time. In constrast, the ``killable`` queue allows

the scheduler to start a job even if it will *not* complete before a

scheduled reservation. It enforces the following policies:



-  Jobs will be killed if still running when a system outage begins.",4.134466797555396
"Can I prevent a preempted job from being automatically requeued?
","To submit a job to the killable queue, add the -q killable option to your bsub command or #BSUB -q killable to your job script.

To prevent a preempted job from being automatically requeued, the BSUB -rn flag can be used at submit time.",4.131089060097455
"What is the name of the library that provides the gssapi_krb5 function?
","libgssapi_krb5.so.2 => /usr/lib64/libgssapi_krb5.so.2 (0x00007ffef6ff8000)
    libldap_r-2.4.so.2 => /usr/lib64/libldap_r-2.4.so.2 (0x00007ffef6da4000)
    liblber-2.4.so.2 => /usr/lib64/liblber-2.4.so.2 (0x00007ffef6b95000)
    libzstd.so.1 => /usr/lib64/libzstd.so.1 (0x00007ffef6865000)
    libbrotlidec.so.1 => /usr/lib64/libbrotlidec.so.1 (0x00007ffef6659000)
    libunistring.so.2 => /usr/lib64/libunistring.so.2 (0x00007ffef62d6000)
    libjitterentropy.so.3 => /usr/lib64/libjitterentropy.so.3 (0x00007ffef60cf000)
    libkrb5.so.3 => /usr/lib64/libkrb5.so.3 (0x00007ffef5df6000)",4.223146227935462
"What is the name of the library that provides the gssapi_krb5 function?
","libgssapi_krb5.so.2 => /usr/lib64/libgssapi_krb5.so.2 (0x00007ffef6ff8000)
    libldap_r-2.4.so.2 => /usr/lib64/libldap_r-2.4.so.2 (0x00007ffef6da4000)
    liblber-2.4.so.2 => /usr/lib64/liblber-2.4.so.2 (0x00007ffef6b95000)
    libzstd.so.1 => /usr/lib64/libzstd.so.1 (0x00007ffef6865000)
    libbrotlidec.so.1 => /usr/lib64/libbrotlidec.so.1 (0x00007ffef6659000)
    libunistring.so.2 => /usr/lib64/libunistring.so.2 (0x00007ffef62d6000)
    libjitterentropy.so.3 => /usr/lib64/libjitterentropy.so.3 (0x00007ffef60cf000)
    libkrb5.so.3 => /usr/lib64/libkrb5.so.3 (0x00007ffef5df6000)",4.223146227935462
"What is the name of the library that provides the gssapi_krb5 function?
","libgssapi_krb5.so.2 => /usr/lib64/libgssapi_krb5.so.2 (0x00007ffef6ff8000)
    libldap_r-2.4.so.2 => /usr/lib64/libldap_r-2.4.so.2 (0x00007ffef6da4000)
    liblber-2.4.so.2 => /usr/lib64/liblber-2.4.so.2 (0x00007ffef6b95000)
    libzstd.so.1 => /usr/lib64/libzstd.so.1 (0x00007ffef6865000)
    libbrotlidec.so.1 => /usr/lib64/libbrotlidec.so.1 (0x00007ffef6659000)
    libunistring.so.2 => /usr/lib64/libunistring.so.2 (0x00007ffef62d6000)
    libjitterentropy.so.3 => /usr/lib64/libjitterentropy.so.3 (0x00007ffef60cf000)
    libkrb5.so.3 => /usr/lib64/libkrb5.so.3 (0x00007ffef5df6000)",4.223146227935462
"What are the two environment variables set as default in the user environment to work around the issue?
",Environment variables to be used during compilation through the environment variable TAU_OPTIONS,4.094601868618925
"What are the two environment variables set as default in the user environment to work around the issue?
","It is highly recommended to create new environments in the ""Project Home"" directory (on Summit, this is /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>). This space avoids purges, allows for potential collaboration within your project, and works better with the compute nodes. It is also recommended, for convenience, that you use environment names that indicate the hostname, as virtual environments created on one system will not necessarily work on others.",4.040528932495811
"What are the two environment variables set as default in the user environment to work around the issue?
","Because the login nodes differ from the service nodes, using the –get-user-env option is not recommended. Users should create the needed environment within the batch job.

Further details and other Slurm options may be found through the sbatch man page.



Slurm sets multiple environment variables at submission time. The following Slurm variables are useful within batch scripts:",4.026744143352068
"What is the purpose of the –get-user-env option in Andes?
","Because the login nodes differ from the service nodes, using the –get-user-env option is not recommended. Users should create the needed environment within the batch job.

Further details and other Slurm options may be found through the sbatch man page.



Batch Environment Variables

Slurm sets multiple environment variables at submission time. The following Slurm variables are useful within batch scripts:",4.155150137030882
"What is the purpose of the –get-user-env option in Andes?
","Because the login nodes differ from the service nodes, using the –get-user-env option is not recommended. Users should create the needed environment within the batch job.

Further details and other Slurm options may be found through the sbatch man page.



Slurm sets multiple environment variables at submission time. The following Slurm variables are useful within batch scripts:",4.155136898810396
"What is the purpose of the –get-user-env option in Andes?
","Andes User Guide



System Overview

Andes is a 704-compute node commodity-type linux cluster. The primary purpose of Andes is to provide a conduit for large-scale scientific discovery via pre/post processing and analysis of simulation data generated on Summit.



Compute nodes

Andes contains 704 compute nodes and 9 GPU nodes. Andes has two partitions:",4.114250894518293
"Can HPCToolkit handle large parallel systems like Frontier?
","HPCToolkit is an integrated suite of tools for measurement and analysis of program performance on computers ranging from multicore desktop systems to the nation's largest supercomputers. HPCToolkit provides accurate measurements of a program's work, resource consumption, and inefficiency, correlates these metrics with the program's source code, works with multilingual, fully optimized binaries, has very low measurement overhead, and scales to large parallel systems. HPCToolkit's measurements provide support for analyzing a program execution cost, inefficiency, and scaling characteristics both",4.229622226431233
"Can HPCToolkit handle large parallel systems like Frontier?
","HPCToolkit is an integrated suite of tools for measurement and analysis of program performance on computers ranging from multicore desktop systems to the nation's largest supercomputers. HPCToolkit provides accurate measurements of a program's work, resource consumption, and inefficiency, correlates these metrics with the program's source code, works with multilingual, fully optimized binaries, has very low measurement overhead, and scales to large parallel systems. HPCToolkit's measurements provide support for analyzing a program execution cost, inefficiency, and scaling characteristics both",4.229622226431233
"Can HPCToolkit handle large parallel systems like Frontier?
","HPCToolkit is an integrated suite of tools for measurement and analysis of program performance on computers ranging from multicore desktop systems to the nation’s largest supercomputers. HPCToolkit provides accurate measurements of a program’s work, resource consumption, and inefficiency, correlates these metrics with the program’s source code, works with multilingual, fully optimized binaries, has very low measurement overhead, and scales to large parallel systems. HPCToolkit’s measurements provide support for analyzing a program execution cost, inefficiency, and scaling characteristics both",4.22053222037026
"What are some common issues that users encounter when using ParaView on high performance computing clusters?
","Although in a single machine setup both the ParaView client and server run on the same host, this need not be the case. It is possible to run a local ParaView client to display and interact with your data while the ParaView server runs in an Andes or Summit batch job, allowing interactive analysis of very large data sets.",4.275936353561013
"What are some common issues that users encounter when using ParaView on high performance computing clusters?
","ParaView was developed to analyze extremely large datasets using distributed memory computing resources. The OLCF provides ParaView server installs on Andes and Summit to facilitate large scale distributed visualizations. The ParaView server running on Andes and Summit may be used in a headless batch processing mode or be used to drive a ParaView GUI client running on your local machine.

For a tutorial of how to get started with ParaView on Andes, see our ParaView at OLCF Tutorial.",4.269171399051258
"What are some common issues that users encounter when using ParaView on high performance computing clusters?
","You will obtain the best performance by running the ParaView client on your local computer and running the server on OLCF resources with the same version of ParaView. It is highly recommended to check the available ParaView versions using module avail paraview on the system you plan to connect ParaView to. Precompiled ParaView binaries for Windows, macOS, and Linux can be downloaded from Kitware.

Recommended ParaView versions on our systems:

Summit: ParaView 5.9.1, 5.10.0, 5.11.0

Andes: ParaView 5.9.1, 5.10.0, 5.11.0",4.222406547856313
"How can I submit a job to the batch-spi queue on Citadel?
","There are special queue names when submitting jobs to citadel.ccs.ornl.gov (the Moderate Enhanced version of Summit). These queues are: batch-spi, batch-hm-spi, and debug-spi.  For example, to submit a job to the batch-spi queue on Citadel, you would need -q batch-spi when using the bsub command or #BSUB -q batch-spi when using a job script.

Except for the enhanced security policies for jobs in these queues, all other queue properties are the same as the respective Summit queues described above, such as maximum walltime and number of eligible running jobs.",4.510142176610722
"How can I submit a job to the batch-spi queue on Citadel?
","To access Summit and Frontier's compute resources for SPI workflows, you must first log into a https://docs.olcf.ornl.gov/systems/index.html#Citadel login node<citadel-login-nodes> and then submit a batch job to one of the SPI specific batch queues.",4.381114810563796
"How can I submit a job to the batch-spi queue on Citadel?
","batch-spi

The batch queues mirror the purpose of the similarly named Summit/Frontier queues. Details on each queue can be found in the https://docs.olcf.ornl.gov/systems/index.html#Summit User Guide<summit-user-guide> and https://docs.olcf.ornl.gov/systems/index.html#Frontier User Guide<frontier-user-guide>. The SPI queues must be used to launch batch jobs from the https://docs.olcf.ornl.gov/systems/index.html#Citadel login nodes<citadel-login-nodes> and can not be used directly from the non-SPI Summit/Frontier login nodes.",4.377598319070589
"Can I use the existing resource user guides for information on how to use Summit and Frontier?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.475237908654818
"Can I use the existing resource user guides for information on how to use Summit and Frontier?
",This section of the Summit User Guide is intended to show current OLCF users how to start preparing their applications to run on the upcoming Frontier system. We will continue to add more topics to this section in the coming months. Please see the topics below to get started.,4.357096110318102
"Can I use the existing resource user guides for information on how to use Summit and Frontier?
",User Guide<summit-user-guide> and https://docs.olcf.ornl.gov/systems/index.html#Frontier User Guide<frontier-user-guide> can be used when building workflows for the non-SPI as well as the Citadel framework.,4.355319655400324
"Can I request an increase in my Member Work directory quota?
","Users interested in sharing files publicly via the World Wide Web can request a user website directory be created for their account. User website directories (~/www) have a 5GB storage quota and allow access to files at http://users.nccs.gov/~user (where user is your userid). If you are interested in having a user website directory created, please contact the User Assistance Center at help@olcf.ornl.gov.",4.193073139012296
"Can I request an increase in my Member Work directory quota?
","3

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

4

Retention is not applicable as files will follow purge cycle.",4.149573817032052
"Can I request an increase in my Member Work directory quota?
","3

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

4

Retention is not applicable as files will follow purge cycle.",4.149573817032052
"What is the benefit of compressing files before transferring them to the OLCF?
","This page has been deprecated, and relevant information is now available on https://docs.olcf.ornl.gov/systems/transferring.html#data-storage-and-transfers. Please update any bookmarks to use that page.

In general, when transferring data into or out of the OLCF from the command line, it's best to initiate the transfer from outside the OLCF. If moving many small files, it can be beneficial to compress them into a single archive file, then transfer just the one archive file.

scp and rsync are available for remote transfers.

scp - secure copy (remote file copy program)

Sending a file to OLCF",4.242831877618452
"What is the benefit of compressing files before transferring them to the OLCF?
","The OLCF users have access to a new functionality, using Globus to transfer files to HPSS through the endpoint ""OLCF HPSS"". Globus has restriction of 8 active transfers across all the users. Each user has a limit of 3 active transfers, so it is required to transfer a lot of data on each transfer than less data across many transfers. If a folder is constituted with mixed files including thousands of small files (less than 1MB each one), it would be better to tar the small files.  Otherwise, if the files are larger, Globus will handle them. To transfer the files, follow these steps:",4.20960232060829
"What is the benefit of compressing files before transferring them to the OLCF?
","and files are automatically purged on a regular basis. Files should not be retained in this file system for long, but rather should be migrated to Project Home or Project Archive space as soon as the files are not actively being used. If a file system associated with your Member Work directory is nearing capacity, the OLCF may contact you to request that you reduce the size of your Member Work directory. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for more details on applicable quotas, backups, purge, and retention timeframes.",4.161354334413387
"How many rack cabinets does Frontier have?
","Frontier is connected to Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Data will not be automatically transferred from Alpine to Orion. Frontier also has access to the center-wide NFS-based filesystem (which provides user and project home areas). Each compute node has two 1.92TB Non-Volatile Memory storage devices. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-data-storage for more",4.191064574784008
"How many rack cabinets does Frontier have?
","Frontier is a HPE Cray EX supercomputer located at the Oak Ridge Leadership Computing Facility. With a theoretical peak double-precision performance of approximately 2 exaflops (2 quintillion calculations per second), it is the fastest system in the world for a wide range of traditional computational science applications. The system has 74 Olympus rack HPE cabinets, each with 128 AMD compute nodes, and a total of 9,408 AMD compute nodes.",4.146885548644484
"How many rack cabinets does Frontier have?
","Frontier mounts Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Frontier will not mount Alpine when Orion is in production.",4.113061503286497
"Can I use nvprof to profile GPU kernels on Summit?
","Prior to Nsight Systems and Nsight Compute, the NVIDIA command line profiling tool was nvprof, which provides both tracing and kernel profiling capabilities. Like with Nsight Systems and Nsight Compute, the profiler data output can be saved and imported into the NVIDIA Visual Profiler for additional graphical analysis. nvprof is in maintenance mode now: it still works on Summit and significant bugs will be fixed, but no new feature development is occurring on this tool.

To use nvprof, the cuda module must be loaded.

summit> module load cuda",4.428216275343172
"Can I use nvprof to profile GPU kernels on Summit?
","There are several tools that can be used to profile the performance of a deep learning job. Below are links to several tools that are available as part of the open-ce module.

The open-ce module contains the nvprof profiling tool. It can be used to profile work that is running on GPUs. It will give information about when different CUDA kernels are being launched and how long they take to complete. For more information on using the NVIDA profiling tools on Summit, please see these slides.",4.378737770594196
"Can I use nvprof to profile GPU kernels on Summit?
","summit> module load cuda

A simple ""Hello, World!"" run using nvprof can be done by adding ""nvprof"" to the jsrun (see: https://docs.olcf.ornl.gov/systems/summit_user_guide.html#job-launcher-jsrun) line in your batch script (see https://docs.olcf.ornl.gov/systems/summit_user_guide.html#batch-scripts).

...
jsrun -n1 -a1 -g1 nvprof ./hello_world_gpu
...

Although nvprof doesn't provide aggregated MPI data, the %h and %p output file modifiers can be used to create separate output files for each host and process.

...
jsrun -n1 -a1 -g1 nvprof -o output.%h.%p ./hello_world_gpu
...",4.369781607885389
"How are tasks triggered in CI?
","Taking the GitLab CI/CD concepts documentation as a start point, Continuous Integration (CI) completes tasks necessary to test and build software resulting in a container image. Example tasks performed could be code linting, test coverage, unit testing, functional testing, code compiling or integration testing. Tasks would be triggered whenever code is pushed into a repository.",4.358876215916992
"How are tasks triggered in CI?
","CD could be either Continuous Delivery or Continuous Deployment. Both take an application following CI and make it available for use. In Continuous Delivery, an application deployment is triggered manually whereas in Continuous Deployment the process occurs automatically without the involvement of a person.

A more in depth discussion may be found with Martin Fowler's Continuous Integration article.

On Slate, there are three primary CI/CD style tools in use:

GitLab Runners

Jenkins

OpenShift Pipelines

GitLab Runners",4.12863589967275
"How are tasks triggered in CI?
","If the runner is to be registered to a specific project, first ensure that the project is enabled for pipelines by navigating to the project in GitLab. In the Settings for the project, select General. Expand the ""Visibility, project features, and permissions"" section and locate the ""Pipelines"" option. If it is currently disabled, enable the ""Pipelines"" option and then ""Save Changes"". Once saved, refresh the project General Settings page, and locate the newly available ""CI/CD Settings"" option. Select ""CI/CD"", and expand the ""Runners"" section of the CI/CD settings. In the ""Specific Runners""",4.0559978968032215
"What is the estimated maximum buffer size required for the trace?
","Estimated aggregate size of event trace:                   40GB
Estimated requirements for largest trace buffer (max_buf): 10GB
Estimated memory requirements (SCOREP_TOTAL_MEMORY):       10GB
(warning: The memory requirements can not be satisfied by Score-P to avoid
intermediate flushes when tracing. Set SCOREP_TOTAL_MEMORY=4G to get the
maximum supported memory or reduce requirements using USR regions filters.)",4.499005680057069
"What is the estimated maximum buffer size required for the trace?
","The first line of the output gives an estimation of the total size of the trace, aggregated over all processes. This information is useful for estimating the space required on disk. In the given example, the estimated total size of the event trace is 40GB. The second line prints an estimation of the memory space required by a single process for the trace. Since flushes heavily disturb measurements, the memory space that Score-P reserves on each process at application start must be large enough to hold the process’ trace in memory in order to avoid flushes during runtime.",4.350304098646734
"What is the estimated maximum buffer size required for the trace?
","In addition to the trace, Score-P requires some additional memory to maintain internal data structures. Thus, it provides also an estimation for the total amount of required memory on each process. The memory size per process that Score-P reserves is set via the environment variable SCOREP_TOTAL_MEMORY. In the given example the per process memory is about 10GB. When defining a filter, it is recommended to exclude short, frequently called functions from measurement since they require a lot of buffer space (represented by a high value under max_tbc) but incur a high measurement overhead. MPI",4.152164273287471
"How much data can Crusher write per second?
",Crusher is connected to the center-wide IBM Spectrum Scale™ filesystem providing 250 PB of storage capacity with a peak write speed of 2.5 TB/s. Crusher also has access to the center-wide NFS-based filesystem (which provides user and project home areas). While Crusher does not have direct access to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#dtn-user-guide to move data to/from HPSS.,4.3755153048198325
"How much data can Crusher write per second?
","Each Crusher compute node has [2x] 1.92 TB NVMe devices (SSDs) with a peak sequential performance of 5500 MB/s (read) and 2000 MB/s (write). To use the NVMe, users must request access during job allocation using the -C nvme option to sbatch, salloc, or srun. Once the devices have been granted to a job, users can access them at /mnt/bb/<userid>. Users are responsible for moving data to/from the NVMe before/after their jobs. Here is a simple example script:",4.259881147512383
"How much data can Crusher write per second?
",The Crusher nodes are connected with [4x] HPE Slingshot 200 Gbps (25 GB/s) NICs providing a node-injection bandwidth of 800 Gbps (100 GB/s).,4.205220466315312
"Can pmake create a target that depends on a directory?
","<string>:5: (INFO/1) Duplicate implicit target name: ""pmake"".

pmake is a parallel make developed for use within batch jobs.  A rules.yaml file specifies extended make-rules with:

multiple input and multiple output files

a resource-set specification

a multi-line shell script that can use variable substitution (e.g. {mpirun} expands to {jsrun -g -c ...} on summit).

Full documentation and examples are available in https://code.ornl.gov/99R/pmake.",4.1651362269273005
"Can pmake create a target that depends on a directory?
","Inside the simulation directory, you should see 3 new files, simulate.sh, which contains the shell script pmake built from the simulate rule, simulate.log, containing the log output from running simulate.sh, and run.log, the file written during rule execution.

Extending pmake using your own rules is straightforward. pmake acts like make, running rules to create output files (that do not yet exist) from input files (that must exist before the rule is run).

Unlike make, pmake does not run a rule unless its output is requested by some target.",4.107846979002581
"Can pmake create a target that depends on a directory?
","Once you have entered the virtual environment, pmake can be installed with:

$ python -m pip install git+https://code.ornl.gov/99R/pmake.git@latest

Run the following command to verify that pmake is available:

$ pmake --help

To run a pmake demo on Summit, you will create a pmake-example directory with its preferred file layout, then submit a batch job to LSF from a Summit login node.

First, create the directories,

$ mkdir -p pmake-example/simulation

Next, create pmake's two configuration files, rules.yaml and targets.yaml:

# pmake-example/targets.yaml",4.106583880121136
"What is the differentiable programming paradigm in PennyLane?
","PennyLane is a cross-platform Python library for programming quantum computers.  Its differentiable programming paradigm enables the execution and training of quantum programs on various backends.

General information of how to install and use PennyLane can be found here:

https://docs.pennylane.ai/en/stable/introduction/pennylane.html

https://pennylane.ai/qml/demos_getting-started.html

https://pennylane.ai/install.html

On our systems, the install method is relatively simple:

Andes

.. code-block:: bash",4.35601482168021
"What is the differentiable programming paradigm in PennyLane?
","There are a variety of programming models available to program GPUs (e.g. CUDA, OpenACC, OpenMP offloading, etc.) and you are welcome to use any of them at these events.

<p style=""font-size:20px""><b>Why participate?</b></p>",4.063611264143241
"What is the differentiable programming paradigm in PennyLane?
","This section details a variety of high and low-level Tensor Core programming models. Which programming model is appropriate to a given application is highly situational, so this document will present multiple programming models to allow the reader to evaluate each for their merits within the needs of the application.",4.045964993869817
"How can I view the details of an application on ArgoCD?
",Image of ArgoCD new application general settings.,4.377817322048101
"How can I view the details of an application on ArgoCD?
","From the ArgoCD Applications screen, click the Create Application button. In the General application settings, give the application deployment a name for ArgoCD to refer to in the display. For Project usually the ArgoCD default project created during the ArgoCD instance installation is sufficient. However, your workload may benefit from a different logical grouping by using multiple ArgoCD projects. If it is desired to have ArgoCD automatically deploy resources to an OpenShift namespace, change the Sync Policy to Automatic. Check the Prune Resources to automatically remove objects when they",4.354994419450133
"How can I view the details of an application on ArgoCD?
","Image of the ArgoCD applications tab.

Prior to using ArgoCD, a git repository containing Kubernetes custom resources needs to be added for use.

The git repository containing Kubernetes custom resources is typically not the same git repository used to build the application.

There are three ways for ArgoCD to connect to a git repository:

connect using ssh

connect using https

connect using GitHub App",4.305979002477696
"Are there any specific libraries or frameworks that are optimized for the Burst Buffer on Summit?
","Each compute node on Summit has a 1.6TB Non-Volatile Memory (NVMe) storage device (high-memory nodes have a 6.4TB NVMe storage device), colloquially known as a ""Burst Buffer"" with theoretical performance peak of 2.1 GB/s for writing and 5.5 GB/s for reading. The NVMes could be used to reduce the time that applications wait for I/O.  More information can be found later in the Burst Buffer section.



<string>:208: (INFO/1) Duplicate implicit target name: ""software"".",4.174717443271639
"Are there any specific libraries or frameworks that are optimized for the Burst Buffer on Summit?
","The https://docs.olcf.ornl.gov/systems/summit_user_guide.html#OLCF Training Archive<training-archive> provides a list of previous training events, including multi-day Summit Workshops. Some examples of topics addressed during these workshops include using Summit's NVME burst buffers, CUDA-aware MPI, advanced networking and MPI, and multiple ways of programming multiple GPUs per node. You can also find simple tutorials and code examples for some common programming and running tasks in our Github tutorial page .",4.123805501899682
"Are there any specific libraries or frameworks that are optimized for the Burst Buffer on Summit?
",https://vimeo.com/306437439 | | 2018-12-04 | GPU-Accelerated Libraries | Jeff Larkin (NVIDIA) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Libraries.pdf https://vimeo.com/306437127 | | 2018-12-04 | Targeting Summit's Multi-GPU Nodes | Steve Abbott (NVIDIA) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_MultiGPU-nodes.pdf,4.119441797468875
"Why is -nnodes not compatible with expert mode?
","In easy mode, the system software converts options such as -nnodes in a batch script into the resource string needed by the scheduling system. In expert mode, the user is responsible for creating this string and options such as -nnodes cannot be used. In easy mode, you will not be able to use bsub -R to create resource strings. The system will automatically create the resource string based on your other bsub options. In expert mode, you will be able to use -R, but you will not be able to use the following options to bsub: -ln_slots, -ln_mem, -cn_cu, or -nnodes.",4.222837824447657
"Why is -nnodes not compatible with expert mode?
","Most users will want to use easy mode. However, if you need precise control over your job’s resources, such as placement on (or avoidance of) specific nodes, you will need to use expert mode. To use expert mode, add #BSUB -csm y to your batch script (or -csm y to your bsub command line).",4.050593072124735
"Why is -nnodes not compatible with expert mode?
","Frontier uses low-noise mode and core specialization (-S flag at job allocation, e.g., sbatch).  Low-noise mode constrains all system processes to core 0.  Core specialization (by default, -S 8) reserves the first core in each L3 region.  This prevents the user running on the core that system processes are constrained to.  This also means that there are only 56 allocatable cores by default instead of 64. Therefore, this modifies the simplified node layout to:

Simplified Frontier node architecture diagram (low-noise mode)",3.978937580821245
"How can I run rocprof with the gpu: filter in Frontier?
","Integer instructions and cache levels are currently not documented here.

To get started, you will need to make an input file for rocprof, to be passed in through rocprof -i <input_file> --timestamp on -o my_output.csv <my_exe>. Below is an example, and contains the information needed to roofline profile GPU 0, as seen by each rank:",4.27330619985919
"How can I run rocprof with the gpu: filter in Frontier?
","Integer instructions and cache levels are currently not documented here.

To get started, you will need to make an input file for rocprof, to be passed in through rocprof -i <input_file> --timestamp on -o my_output.csv <my_exe>. Below is an example, and contains the information needed to roofline profile GPU 0, as seen by each rank:",4.27330619985919
"How can I run rocprof with the gpu: filter in Frontier?
","srun -N 2 -n 16 --ntasks-per-node=8 --gpus-per-node=8 --gpu-bind=closest bash -c 'rocprof -o ${SLURM_JOBID}_${SLURM_PROCID}.csv -i <input_file> --timestamp on <exe>'

The gpu: filter in the rocprof input file identifies GPUs by the number the MPI rank would see them as. In the srun example above, each MPI rank only has 1 GPU, so each rank sees its GPU as GPU 0.

The theoretical (not attainable) peak roofline constructs a theoretical maximum performance for each operational intensity.",4.2314024030432575
"What is the recommended way to run Spectrum MPI with multiple resource sets per host?
","Spectrum MPI relies on CUDA Inter-process Communication (CUDA IPC) to provide fast on-node between GPUs. At present this capability cannot function with more than one resource set per node.

Set the environment variable PAMI_DISABLE_IPC=1 to force Spectrum MPI to not use fast GPU Peer-to-peer communication. This option will allow your code to run with more than one resource set per host, but you may see slower GPU to GPU communication.

Run in a single resource set per host, i.e. with jsrun --gpu_per_rs 6",4.411197425717339
"What is the recommended way to run Spectrum MPI with multiple resource sets per host?
","The following example will create 12 resource sets each with 1 task, 4 threads, and 1 GPU. Each MPI task will start 4 threads and have access to 1 GPU. Rank 0 will have access to GPU 0 and start 4 threads on the first socket of the first node ( red resource set). Rank 2 will have access to GPU 1 and start 4 threads on the second socket of the first node ( green resource set). This pattern will continue until 12 resource sets have been created. The following jsrun command will create 12 resource sets (-n12). Each resource set will contain 1 MPI task (-a1), 1 GPU (-g1), and 4 cores (-c4).",4.274884808408068
"What is the recommended way to run Spectrum MPI with multiple resource sets per host?
","If on-node MPI communication between GPUs is critical to your application performance, option B is recommended but you’ll need to set the GPU affinity manually. This could be done with an API call in your code (e.g. cudaSetDevice), or by using a wrapper script.

We have seen occasional errors from batch jobs with multiple simultaneous backgrounded jsrun commands. Jobs may see pmix errors during the noted failures.



The following issue was resolved with the software default changes from March 12, 2019 that set Spectrum MPI 10.2.0.11 (20190201) as default and moved ROMIO to version 3.2.1:",4.264790272234762
"How do I optimize the usage of my project's allocations?
","Before you can do anything you need a Project to do your things in. Fortunately, when you get an allocation on one of our clusters a Project is automatically created for you with the same name as the allocation. By using our own distinct Project we are ensuring that we will not interfere with anyone else's work.

NOTE: Everywhere that you see <cluster> replace that with the cluster that you will be running on (marble or onyx).",4.233804266654461
"How do I optimize the usage of my project's allocations?
","The requesting project's allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Summit would be equivalent to using 16,000 Summit node-hours of a project's allocation.



As is the case with many other queuing systems, it is possible to place dependencies on jobs to prevent them from running until other jobs have started/completed/etc. Several possible dependency settings are described in the table below:",4.1881745336458405
"How do I optimize the usage of my project's allocations?
","current project allocations | | Allocation Usage | Usage metrics and graphs, per-allocation | | Usage | Usage metrics and graphs, queryable by resource and timespan |",4.142390792896642
"How can I test whether processes and threads are running where intended on Frontier?
","This section describes how to map processes (e.g., MPI ranks) and process threads (e.g., OpenMP threads) to the CPUs, GPUs, and NICs on Frontier.

Users are highly encouraged to use the CPU- and GPU-mapping programs used in the following sections to check their understanding of the job steps (i.e., srun commands) they intend to use in their actual jobs.

For the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-cpu-map and https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-multi-map sections:",4.178550220053107
"How can I test whether processes and threads are running where intended on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.133595968996965
"How can I test whether processes and threads are running where intended on Frontier?
","Because a Frontier compute node has two hardware threads available (2 logical cores per physical core), this enables the possibility of multithreading your application (e.g., with OpenMP threads). Although the additional hardware threads can be assigned to additional MPI tasks, this is not recommended. It is highly recommended to only use 1 MPI task per physical core and to use OpenMP threads instead on any additional logical cores gained when using both hardware threads.",4.119874986454633
"How do I authenticate with Helm?
","One nice feature of helm is that it uses the underlying authentication credentials to kubernetes, so once you login with oc login, the helm client will authenticate automatically.

By default, helm doesn't have any chart repositories, so let's add the upstream stable repository.

helm repo add stable https://charts.helm.sh/stable

Now you can install helm charts with helm install stable/<package_name>. You can think of this command as a parallel to running yum install on a RHEL/CentOS-based system, or apt install on a debian-based system.",4.282459546923328
"How do I authenticate with Helm?
","$ brew install helm

Or can be pulled from the Helm Release Page. If downloading from the GitHub release page, you can copy this executable into /usr/local/bin to add it to $PATH.

NOTE: One nice feature of Helm is that it uses the underlying authentication credentials used with oc, so once you login with oc login, the helm client will authenticate automatically.

Once oc and helm are setup and you are logged in with oc login, test Helm:

$ helm ls",4.219286077953898
"How do I authenticate with Helm?
","The following items need to be established before deploying applications on Slate systems (Marble or Onyx OpenShift clusters).

Follow https://docs.olcf.ornl.gov/systems/helm_prerequisite.html#slate_getting_started if you do not already have a project/namespace established on Onyx or Marble.

It is recommended to use Helm version 3.

Helm enables application deployment via Helm Charts. The high level Helm Architecture docs are also a great reference for understanding Helm.

Like oc, Helm is a single binary executable.

This can be installed on macOS with Homebrew :

$ brew install helm",4.142224315124913
"How can I remove the resources that were created for this demo?
","When clicking the uninstall, it may appear that the UI hangs and nothing is happening. It may take some time to remove all of the resources.

Once the installation is complete, the UI will refresh and the deployment will no longer be listed.

Verify that the runner has been unregistered from the GitLab project (GitLab->Settings->CI/CD->Runners). One could also check to ensure that all the pods were deleted by changing over to the ""Administrator"" perspective and selecting Workloads -> Pods from the navigation.",4.015715936665209
"How can I remove the resources that were created for this demo?
","from the repository, and takes corrective to add/change/remove application resources |",3.943058853251798
"How can I remove the resources that were created for this demo?
","Resource sets allow each jsrun to control how the node appears to a code. This method is unique to jsrun, and requires thinking of each job launch differently than aprun or mpirun. While the method is unique, the method is not complicated and can be reasoned in a few basic steps.",3.92630812116292
"Why would I want to place a job on hold?
","Sometimes you may need to place a hold on a job to keep it from starting. For example, you may have submitted it assuming some needed data was in place but later realized that data is not yet available. This can be done with the scontrol hold command. Later, when the data is ready, you can release the job (i.e. tell the system that it's now OK to run the job) with the scontrol release command. For example:

| scontrol hold 12345 | Place job 12345 on hold | | --- | --- | | scontrol release 12345 | Release job 12345 (i.e. tell the system it's OK to run it) |",4.185704297360872
"Why would I want to place a job on hold?
",holding up its resources). And the same argument can be made for the other job steps.,4.098603222343164
"Why would I want to place a job on hold?
","Most users will find batch jobs an easy way to use the system, as they allow you to ""hand off"" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.",4.085856786011864
"What is the name of the OpenShift ImageStream that will be created?
","When tagging an image, you must use the format registry.apps.<cluster>.ccs.ornl.gov/<namespace>/<image> where:

Cluster is the name of the OpenShift cluster

Namespace is the name of the Kubernetes namespace you are using (Use oc status to see what OpenShift Project/Kubernetes Namespace you are currently in)

Image is the name of the image you want to push

Once you push the image into the registry, a OpenShift ImageStream will be automatically created",4.279279762359277
"What is the name of the OpenShift ImageStream that will be created?
","First, we will log into the cluster using the oc CLI tool

oc login https://api.<cluster>.ccs.ornl.gov

Next we will create the ImageStream that the BuildConfig will push the completed image to. The ImageStream is a direct mapping to the image stored in the OpenShift integrated registry.

oc create imagestream local-image

Next, we will create the BuildConfig object",4.270529753365744
"What is the name of the OpenShift ImageStream that will be created?
","$ oc get imagestream local-image
NAME          DOCKER REPO                                                   TAGS     UPDATED
local-image   image-registry.openshift-image-registry.svc:5000/stf002platform/local-image   latest   5 minutes ago

Now that we have built a container image we can deploy it with a Deployment object. Using the Docker Repo specified in the ImageStream we can create our deployment:",4.254010004121664
"What is the requirement for using the Vampir GUI connection method?
","Launch the Vampir GUI on your local machine

Similar to how we have connected Vampir to the VampirServer in the https://docs.olcf.ornl.gov/systems/Vampir.html#previous section, <vampauth> you will follow the same steps except you will use localhost for the server name and your local machine port number you selected. Press 'Connect' and this should open the authentication window where you will enter your UserID and the https://docs.olcf.ornl.gov/systems/Vampir.html#VampirServer password <vampserpw> printed after a successful connection.",4.350362798746107
"What is the requirement for using the Vampir GUI connection method?
","$ vampir &



Once the GUI has opened, you will need to connect to the https://docs.olcf.ornl.gov/systems/Vampir.html#VampirServer <vamps> using the Remote File option as shown below. If there is a 'recent files' window open, select 'open other'. Enter the node ID and the port number and press 'Connect'. Also, you will need to select Encrypted password from the Authentication dropdown option.",4.338114912460284
"What is the requirement for using the Vampir GUI connection method?
","VampirServer does not take advantage of GPU components

The following sections will cover the 3 different methods for using Vampir on Summit. For each method, you will need to enable X11 forwarding when logging in to Summit to allow for launching a GUI from Summit. To do so, you can use the ssh option -X as shown below

$ ssh -X <USERID>@summit.olcf.ornl.gov

Please visit this link if you need more information for logging onto Summit













Do not run Vampir on a login node for trace files > 1 GB! Please see the next 2 sections for running larger trace files.",4.236812850815925
"Are there any performance considerations when using scratch?
","Each project has a World Work directory that resides in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. World Work areas can be accessed by all users of the system and are intended for sharing of data between projects. World Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but rather should be",4.027932687589032
"Are there any performance considerations when using scratch?
",performance can only be achieved when there is enough work to hide the latency of memory accesses and to keep all compute pipelines busy.),4.000458711356077
"Are there any performance considerations when using scratch?
","Each project is granted a Project Work directory; these reside in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. Project Work directories can be accessed by all members of a project and are intended for sharing data within a project. Project Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but",3.989842465831937
"How do I submit a job to the batch queue for parallel h5py?
","Time to execute ""hdf5_parallel.py"" by submitting ""submit_h5py"" to the batch queue:

Summit

.. code-block:: bash

   $ bsub -L $SHELL submit_h5py.lsf

Andes

.. code-block:: bash

   $ sbatch --export=NONE submit_h5py.sl

Frontier

.. code-block:: bash

   $ sbatch --export=NONE submit_h5py.sl

Example ""submit_h5py"" batch script:

Summit

.. code-block:: bash

   #!/bin/bash
   #BSUB -P <PROJECT_ID>
   #BSUB -W 00:05
   #BSUB -nnodes 1
   #BSUB -J h5py
   #BSUB -o h5py.%J.out
   #BSUB -e h5py.%J.err

   cd $LSB_OUTDIR
   date

   module load gcc
   module load hdf5
   module load python",4.439562747589058
"How do I submit a job to the batch queue for parallel h5py?
","Submitting one of the above scripts will submit a job to the batch partition for five minutes using 28 MPI tasks across 1 node. As rendering speeds and memory issues widely vary for different datasets and MPI tasks, users are encouraged to find the optimal amount of MPI tasks to use for their data. Users with large datasets may also find a slight increase in performance by using the gpu partition on Andes, or by utilizing the GPUs on Summit. Once the batch job makes its way through the queue, the script will launch the loaded ParaView module (specified with module load) and execute a python",4.236821164351387
"How do I submit a job to the batch queue for parallel h5py?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.220230920726477
"What is the difference between CPU mapping and GPU mapping in Frontier?
","This section describes how to map processes (e.g., MPI ranks) and process threads (e.g., OpenMP threads) to the CPUs, GPUs, and NICs on Frontier.

Users are highly encouraged to use the CPU- and GPU-mapping programs used in the following sections to check their understanding of the job steps (i.e., srun commands) they intend to use in their actual jobs.

For the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-cpu-map and https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-multi-map sections:",4.320139428612462
"What is the difference between CPU mapping and GPU mapping in Frontier?
","As has been pointed out previously in the Frontier documentation, notice that GPUs are NOT mapped to MPI ranks in sequential order (e.g., MPI rank 0 is mapped to physical CPU cores 1-7 and GPU 4, MPI rank 1 is mapped to physical CPU cores 9-15 and GPU 5), but this IS expected behavior. It is simply a consequence of the Frontier node architectures as shown in the Frontier Node Diagram and subsequent https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Note on NUMA domains <numa-note>.

Example 2: 1 MPI rank with 7 CPU cores and 1 GPU (single-node)",4.259010767740486
"What is the difference between CPU mapping and GPU mapping in Frontier?
","Also, recall that the CPU cores in a given L3 cache region are connected to a specific GPU (see the Frontier Node Diagram and subsequent https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Note on NUMA domains <numa-note> for more information). In the examples below, knowledge of these details will be assumed.

There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_jobstep program and test whether processes and threads are mapped to the CPU cores and GPUs as intended..",4.231483639994381
"How can I set the DISPLAY environment variable on the Andes cluster?
","localsytem: ssh -X username@andes.olcf.ornl.gov

andes: salloc -A abc123 -N 1 -t 1:00:00 --x11=batch

andes: ./matlab-vnc.sh

$ ./matlab-vnc.sh

Starting vncserver

Desktop 'TurboVNC: andes79.olcf.ornl.gov:1 (userA)' started on display andes79.olcf.ornl.gov:1

Starting applications specified in /ccs/home/userA/.vnc/xstartup.turbovnc
Log file is /ccs/home/userA/.vnc/andes79.olcf.ornl.gov:1.log

**************************************************************************
Instructions",4.235071316493186
"How can I set the DISPLAY environment variable on the Andes cluster?
","localsytem: ssh -X username@andes.olcf.ornl.gov

andes: salloc -A abc123 -N 1 -t 1:00:00 --x11=batch

andes: ./matlab-vnc.sh

$ ./matlab-vnc.sh

Starting vncserver

Desktop 'TurboVNC: andes79.olcf.ornl.gov:1 (userA)' started on display andes79.olcf.ornl.gov:1

Starting applications specified in /ccs/home/userA/.vnc/xstartup.turbovnc
Log file is /ccs/home/userA/.vnc/andes79.olcf.ornl.gov:1.log

**************************************************************************
Instructions",4.226578400569568
"How can I set the DISPLAY environment variable on the Andes cluster?
","Nice DCV is currently undergoing maintenance. Instead, please use the VNC options detailed above.

Launch an interactive job:

localsytem: ssh username@andes.olcf.ornl.gov
andes: salloc -A PROJECT_ID -p gpu -N 1 -t 60:00 -M andes --constraint=DCV

Run the following commands:

$ xinit &
$ export DISPLAY=:0
$ dcv create-session --gl-display :0 mySessionName
$ hostname  // will be used to open a tunneling connection with this node
$ andes-gpuN

Open a tunneling connection with gpu node N, given by hostname:

localsystem: ssh username@andes.olcf.ornl.gov -L 8443:andes-gpuN:8443",4.189034033848558
"How do I create a circuit for quantum computation on IBM Quantum Services?
","IBM Quantum provides Qiskit (Quantum Information Software Kit for Quantum Computation) for working with OpenQASM and the IBM Q quantum processors. Qiskit allows users to build quantum circuits, compile them for a particular backend, and run the compiled circuits as jobs. Additional information on using Qiskit is available at https://qiskit.org/learn/ and in our https://docs.olcf.ornl.gov/systems/ibm_quantum.html#Software Section <ibm-soft> below.",4.453977485785587
"How do I create a circuit for quantum computation on IBM Quantum Services?
","Jobs are compiled and submitted via Qiskit in a Python virtual environment or Jupyter notebook (see https://docs.olcf.ornl.gov/systems/ibm_quantum.html#Cloud Access <ibm-cloud> and https://docs.olcf.ornl.gov/systems/ibm_quantum.html#Local Access <ibm-local> sections above).

Circuit jobs comprise jobs of constructed quantum circuits and algorithms submitted to backends in IBM Quantum fair-share queue.

Program jobs utilize a pre-compiled quantum program utilizing the Qiskit Runtime framework.",4.428390423791375
"How do I create a circuit for quantum computation on IBM Quantum Services?
","IBM Quantum Services provides access to more than 20 currently available quantum systems (known as backends).  IBM's quantum processors are made up of superconducting transmon qubits, and users can utilize these systems via the universal, gate-based, circuit model of quantum computation.  Additionally, users have access to 5 different types of simulators, simulating from 32 up to 5000 qubits to represent different aspects of the quantum backends.",4.400555774809563
"How do I log into OpenShift?
","In order for us to maintain our existing security posture, only users who are on a project will be able to access to services that the project runs. This means that when a user accesses a route they will first be prompted to log in to OpenShift and once they are authenticated they will be able to access the service.

login prompt

All routes require authentication

HTTPS is required on routes for authentication so that sensitive cookie information is not leaked.

The authenticated user must use their NCCS Username and RSA PASSCODE to log in to OpenShift",4.349452222177409
"How do I log into OpenShift?
","$ oc login <URL>

After entering the login command above, oc will ask you to obtain an API token and will provide a URL like the following: https://oauth-openshift.apps.<CLUSTER>.ccs.ornl.gov/oauth/token/request.

You will need to go to the given URL in your browser, log in with NCCS, click the Display Token link, copy the command under Log in with this token and enter it into your terminal.

(NOTE: Marble authentication uses NCCS Usernames and RSA passcodes. Onyx uses XCAMS usernames and passwords).

Once you login, the output will tell you what projects/namespaces you have access to.",4.329611267759328
"How do I log into OpenShift?
","This tutorial is meant to be followed step by step so you can get a basic understanding of Openshift and Openshift objects. This will only cover a very surface level knowledge of all the things you can accomplish with Openshift but will hopefully get you familiar with the foundational concepts.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

If you have not already done so, you will need to get an allocation on either our Marble or Onyx  Cluster. To do this contact help@olcf.ornl.gov.",4.292216333011102
"How do I copy the input file for my job into place?
","When the installation has finished, click on the Globus icon and select Web: Transfer Files as below



Globus will ask you to login. If your institution does not have an organizational login, you may choose to either Sign in with Google or Sign in with ORCiD iD.



In the main Globus web page, select the two-panel view, then set the source and destination endpoints. (Left/Right order does not matter)



Next, navigate to the appropriate source and destination paths to select the files you want to transfer. Click the ""Start"" button to begin the transfer.",4.054400172157944
"How do I copy the input file for my job into place?
","We need to save this to a file, say job.bs. We submit the job to the queue via bsub job.bs. Once we do, we have to wait for the job to start, then to run. After however long that takes, I get the output file rhw.679095. If I cat that file, I see:

[1] ""Hello from rank 0 (local rank 0) of 4""
[1] ""Hello from rank 1 (local rank 1) of 4""
[1] ""Hello from rank 2 (local rank 0) of 4""
[1] ""Hello from rank 3 (local rank 1) of 4""

------------------------------------------------------------
(additional output excluded for brevity's sake)",4.048306110767349
"How do I copy the input file for my job into place?
","If you’ve previously used LSF, you’re probably used to submitting a job with input redirection (i.e. bsub < myjob.lsf). This is not needed (and will not work) on Summit.

As an example, consider the following batch script:

#!/bin/bash
# Begin LSF Directives
#BSUB -P ABC123
#BSUB -W 3:00
#BSUB -nnodes 2048
#BSUB -alloc_flags gpumps
#BSUB -J RunSim123
#BSUB -o RunSim123.%J
#BSUB -e RunSim123.%J

cd $MEMBERWORK/abc123
cp $PROJWORK/abc123/RunData/Input.123 ./Input.123
date
jsrun -n 4092 -r 2 -a 12 -g 3 ./a.out
cp my_output_file /ccs/proj/abc123/Output.123",4.046008213547973
"What is the next step after filling in the personal information?
","The next screen will show you some information about the project, you don't need to change anything, just click ""Next"".

Fill in your personal information and then click ""Next"".

Fill in your shipping information and then click ""Next"".",4.437748636135014
"What is the next step after filling in the personal information?
","Fill in your Employment/Institution Information. If you are student please use your school affiliation for both ""Employer"" and ""Funding Source"". If you are a student and you do not see your school listed, choose ""other"" for both ""Employer"" and ""Funding Source"" and then manually enter your school affiliation in the adjacent fields.  Click “Next” when you are done.

On the Project information screen fill the ""Proposed Contribution to Project"" with ""Participating in OLCF training."" Leave all the questions about the project set to ""no"" and click ""Next"".",4.219368585534352
"What is the next step after filling in the personal information?
","On the ""Policies & Agreements"" page click the links to read the polices and then answer ""Yes"" to affirm you have read each. Certify your application by selecting ""Yes"" as well. Then Click ""Submit.""

<string>:5: (INFO/1) Enumerated list start value not ordinal-1: ""8"" (ordinal 8)

After submitting your application, it will need to pass through the approval process. Depending on when you submit, approval might not occur until the next business day.",4.101533377272474
"How can I analyze performance data using Perftools?
","There are three programming interfaces available: (1) Perftools-lite, (2) Perftools, and (3) Perftools-preload.

Below are two examples that generate an instrumented executable using Perftools, which is an advanced interface that provides full-featured data collection and analysis capability, including full traces with timeline displays.

The first example generates an instrumented executable using a PrgEnv-amd build:

module load PrgEnv-amd
module load craype-accel-amd-gfx90a
module load rocm
module load perftools",4.384119535765966
"How can I analyze performance data using Perftools?
","There are three programming interfaces available: (1) Perftools-lite, (2) Perftools, and (3) Perftools-preload.

Below are two examples that generate an instrumented executable using Perftools, which is an advanced interface that provides full-featured data collection and analysis capability, including full traces with timeline displays.

The first example generates an instrumented executable using a PrgEnv-amd build:

module load PrgEnv-amd
module load craype-accel-amd-gfx90a
module load rocm
module load perftools",4.384119535765966
"How can I analyze performance data using Perftools?
","pat_report hello_jobstep+pat+39545-2t

The resulting report includes profiles of functions, profiles of maximum function times, details on load imbalance, details on program energy and power usages, details on memory high water mark, and more.

More detailed information on the HPE Performance Analysis Tools can be found in the HPE Performance Analysis Tools User Guide.

When using perftools-lite-gpu, there is a known issue causing ld.lld not to be found. A workaround this issue can be found here.",4.223174089004212
"How do you scale a pod in Kubernetes?
","Pod Actions

Kubernetes Pod Overview",4.297529602014901
"How do you scale a pod in Kubernetes?
","pods, so you may have to adjust the number of pod replicas for each version to deal with the increased/decreased load.",4.264210573758896
"How do you scale a pod in Kubernetes?
","A pod is the smallest unit in Kubernetes, it is a grouping of containers that will be scheduled together onto a node in the cluster. usually it will just be one container but it could be a group of processes that make up an application.

Pods have a lifecycle: they are defined, scheduled onto a node, and then they run until their containers exit or the pod is removed from the node for some reason. Pods are immutable and changes to a pod are not persisted between restarts.

A pod does not:

have state (data should be stored in persistent volumes)

move nodes once scheduled onto a node",4.2574407484672285
"How does RAPIDS utilize NVIDIA CUDA primitives for low-level compute optimization?
","RAPIDS is a suite of libraries to execute end-to-end data science and analytics pipelines on GPUs. RAPIDS utilizes NVIDIA CUDA primitives for low-level compute optimization through user-friendly Python interfaces. An overview of the RAPIDS libraries available at OLCF is given next.

cuDF is a Python GPU DataFrame library (built on the Apache Arrow columnar memory format) for loading, joining, aggregating, filtering, and otherwise manipulating data.",4.398096788683788
"How does RAPIDS utilize NVIDIA CUDA primitives for low-level compute optimization?
","Note the ""RAPIDS basic execution"" option is for illustrative purposes and not recommended to run RAPIDS on Summit since it underutilizes resources. If your RAPIDS code is single GPU, consider Jupyter or the concurrent job steps option.

<string>:3: (INFO/1) Duplicate explicit target name: ""simultaneous job steps"".

In cases when a set of time steps need to be processed by single-GPU RAPIDS codes and each time step fits comfortably in GPU memory, it is recommended to execute simultaneous job steps.

The following script provides a general pattern to run job steps simultaneously with RAPIDS:",4.1862356148371465
"How does RAPIDS utilize NVIDIA CUDA primitives for low-level compute optimization?
","Running RAPIDS multi-gpu/multi-node workloads requires a dask-cuda cluster. Setting up a dask-cuda cluster on Summit requires two components:

dask-scheduler.

dask-cuda-workers.

Once the dask-cluster is running, the RAPIDS script should perform four main tasks. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

Reference of multi-gpu/multi-node operation with cuDF, cuML, cuGraph is available in the next links:

10 Minutes to cuDF and Dask-cuDF.

cuML's Multi-Node, Multi-GPU Algorithms.",4.175336143185651
"Why do I need to install NumPy and SciPy before installing CuPy?
","Andes

.. code-block:: bash

   $ conda install -c defaults --override-channels numpy scipy

After following the prompts, NumPy and its linear algebra dependencies should successfully install. SciPy is an optional dependency, but it would allow you to use the additional SciPy-based routines in CuPy:

Finally, install CuPy from source into your environment. To make sure that you are building from source, and not a pre-compiled binary, use pip:",4.268139577022944
"Why do I need to install NumPy and SciPy before installing CuPy?
","CuPy is a library that implements NumPy arrays on NVIDIA GPUs by utilizing CUDA Toolkit libraries like cuBLAS, cuRAND, cuSOLVER, cuSPARSE, cuFFT, cuDNN and NCCL. Although optimized NumPy is a significant step up from Python in terms of speed, performance is still limited by the CPU (especially at larger data sizes) -- this is where CuPy comes in. Because CuPy's interface is nearly a mirror of NumPy, it acts as a replacement to run existing NumPy/SciPy code on NVIDIA CUDA platforms, which helps speed up calculations further. CuPy supports most of the array operations that NumPy provides,",4.219230431310541
"Why do I need to install NumPy and SciPy before installing CuPy?
","You have now discovered what CuPy can provide! Now you can try speeding up your own codes by swapping CuPy and NumPy where you can.

CuPy User Guide

CuPy Website

CuPy API Reference

CuPy Release Notes",4.188415790928693
"How do I resume a paused RollingUpdate deployment in Slate?
","A rolling deployment slowly replaces instances of the previous version with instances of the new version. This deployment waits for new pods to become ready before scaling down the old replication controller. This strategy is easily aborted and reverted.

A rolling deployment is best used when you want to take no downtime during an update, but you know your application can support having old and new code running at the same time.

Here is an example Rolling deployment:

strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: ""20%""
    maxUnavailable: ""10%""",4.0957188721677005
"How do I resume a paused RollingUpdate deployment in Slate?
","To roll back a deployment, run

oc rollout undo deploy/{NAME}

When using the web interface, you can view and edit a Deployment, from the sidebar, go to Applications, then Deployments.

Deployment Menu

You can get info on any deployment by clicking on it.

To edit this configuration, click Actions in the upper right hand corner, then Edit on whatever you wish to edit, or Edit Deployment if you'd rather edit the YAML directly.

Edit Deployment Config",4.076932020598109
"How do I resume a paused RollingUpdate deployment in Slate?
","A RollingUpdate strategy follows this sequence:

Scale up the new ReplicaSet based on maxSurge.

Scale down the old ReplicaSet based on maxUnavailable.

Repeat the scaling until the new replication controller has the desired replica count and the old replication controller has 0.

During the scale down, the strategy waits for pods to become ""ready"" to determine if scaling down more will affect availability. If the new pods don't become ""ready"", the deployment will eventually time out and revert to the old deployment.",4.061423541035871
"How can I optimize my ParaView visualization for large-scale data?
","ParaView was developed to analyze extremely large datasets using distributed memory computing resources. The OLCF provides ParaView server installs on Andes and Summit to facilitate large scale distributed visualizations. The ParaView server running on Andes and Summit may be used in a headless batch processing mode or be used to drive a ParaView GUI client running on your local machine.

For a tutorial of how to get started with ParaView on Andes, see our ParaView at OLCF Tutorial.",4.331126222452749
"How can I optimize my ParaView visualization for large-scale data?
","Although in a single machine setup both the ParaView client and server run on the same host, this need not be the case. It is possible to run a local ParaView client to display and interact with your data while the ParaView server runs in an Andes or Summit batch job, allowing interactive analysis of very large data sets.",4.321474611941808
"How can I optimize my ParaView visualization for large-scale data?
","ParaView is an open-source, multi-platform data analysis and visualization application. ParaView users can quickly build visualizations to analyze their data using qualitative and quantitative techniques. The data exploration can be done interactively in 3D or programmatically using ParaView’s batch processing capabilities. Further information regarding ParaView can be found at the links provided in the https://docs.olcf.ornl.gov/systems/paraview.html#paraview-resources section.",4.278687110000355
"What is the purpose of the ""nvprof"" command in the given scenario?
","While using nvprof on the command-line is a quick way to gain insight into your CUDA application, a full visual profile is often even more useful. For information on how to view the output of nvprof in the NVIDIA Visual Profiler, see the NVIDIA Documentation.",4.411708690474427
"What is the purpose of the ""nvprof"" command in the given scenario?
","Prior to Nsight Systems and Nsight Compute, the NVIDIA command line profiling tool was nvprof, which provides both tracing and kernel profiling capabilities. Like with Nsight Systems and Nsight Compute, the profiler data output can be saved and imported into the NVIDIA Visual Profiler for additional graphical analysis. nvprof is in maintenance mode now: it still works on Summit and significant bugs will be fixed, but no new feature development is occurring on this tool.

To use nvprof, the cuda module must be loaded.

summit> module load cuda",4.363993239060823
"What is the purpose of the ""nvprof"" command in the given scenario?
","There are many various metrics and events that the profiler can capture. For example, to output the number of double-precision FLOPS, you may use the following:

...
jsrun -n1 -a1 -g1 nvprof --metrics flops_dp -o output.%h.%p ./hello_world_gpu
...

To see a list of all available metrics and events, use the following:

summit> nvprof --query-metrics
summit> nvprof --query-events",4.224563456400499
"How do I compile a Swift program using Swift/T?
","Swift/T is a completely new implementation of the Swift language for high-performance computing which translates Swift scripts into MPI programs that use the Turbine (hence, /T) and ADLB runtime libraries. This tutorial shows how to get up and running with Swift/T on Summit specifically. For more information about Swift/T, please refer to its documentation.

Swift/T is available as a module on Summit, and it can be loaded as follows:

$ module load workflows
$ module load swift/1.5.0

You will also need to set the PROJECT environment variable:

$ export PROJECT=""ABC123""",4.3925247236123965
"How do I compile a Swift program using Swift/T?
","$ export PROJECT=""ABC123""

To run an example ""Hello world"" program with Swift/T on Summit, create a file called hello.swift with the following contents:

trace(""Hello world!"");

Now, run the program from a shell or script:

$ swift-t -m lsf hello.swift

The output should look something like the following:",4.358293006459655
"How do I compile a Swift program using Swift/T?
","$ module load imagemagick # for convert utility
$ export WALLTIME=00:10:00
$ export PROJECT=STF019
$ export TURBINE_OUTPUT=/gpfs/alpine/scratch/ketan2/stf019/swift-work/cross-facility/data
$ swift-t -O0 -m lsf workflow.swift

If all goes well, and when the job starts running, the output will be produced in the data directory output.txt file.",4.086699126254774
"What is the purpose of the MLflow platform?
","MLflow is an open source platform to manage the Machine Learning (ML) lifecycle, including experimentation, reproducibility, deployment, and a central model registry. To learn more about MLflow, please refer to its documentation.

In order to use MLflow on Summit, load the module as shown below:

$ module load workflows
$ module load mlflow/1.22.0

Run the following command to verify that MLflow is available:

$ mlflow --version
mlflow, version 1.22.0

To run this MLflow demo on Summit, you will create a directory with two files and then submit a batch job to LSF from a Summit login node.",4.334803322932263
"What is the purpose of the MLflow platform?
","First, create a directory mlflow-example to contain two files. The first will be named MLproject:

name: demo

entry_points:
  main:
    command: ""python3 demo.py""

The second will be named demo.py:

import mlflow

print(""MLflow Version:"", mlflow.version.VERSION)
print(""Tracking URI:"", mlflow.tracking.get_tracking_uri())

with mlflow.start_run() as run:
    print(""Run ID:"", run.info.run_id)
    print(""Artifact URI:"", mlflow.get_artifact_uri())
    with open(""hello.txt"", ""w"") as f:
        f.write(""Hello world!"")
        mlflow.log_artifact(""hello.txt"")",3.9722807418992656
"What is the purpose of the MLflow platform?
",| (Overview slides | Workflow slides | DL slides | recording ) https://www.olcf.ornl.gov/wp-content/uploads/Jupyter_Overview.pdf https://www.olcf.ornl.gov/wp-content/uploads/Jupyter_Analysis_Workflow.pdf https://www.olcf.ornl.gov/wp-content/uploads/Jupyter_DL_Workflow.pdf https://vimeo.com/730396217 | | 2022-07-14 | Introduction to HIP Programming | Tom Papatheodore (OLCF) | Introduction to HIP Programming https://www.olcf.ornl.gov/calendar/introduction-to-hip-programming/ | (slides | recording | repo ) https://www.olcf.ornl.gov/wp-content/uploads/intro_to_hip.pdf https://vimeo.com/736962754,3.939188385456948
"How can I compile with OpenMP offload using the Fortran compiler?
","This section shows how to compile with OpenMP Offload using the different compilers covered above.

Make sure the craype-accel-amd-gfx90a module is loaded when using OpenMP offload.

| Vendor | Module | Language | Compiler | OpenMP flag (GPU) | | --- | --- | --- | --- | --- | | Cray | cce | C C++ |  | -fopenmp | | Fortran | ftn (wraps crayftn) |  | | AMD | amd |  |  | -fopenmp |",4.429560564123206
"How can I compile with OpenMP offload using the Fortran compiler?
","| Vendor | Module | Language | Compiler | OpenMP flag (CPU thread) | | --- | --- | --- | --- | --- | | Cray | cce | C, C++ |  | -fopenmp | | Fortran | ftn (wraps crayftn) |  | | AMD | rocm |  |  | -fopenmp | | GCC | gcc |  |  | -fopenmp |

This section shows how to compile with OpenMP Offload using the different compilers covered above.

Make sure the craype-accel-amd-gfx90a module is loaded when using OpenMP offload.",4.42186541868571
"How can I compile with OpenMP offload using the Fortran compiler?
","mpif77 or mpif90 to invoke appropriate versions of the fortran compiler

These wrapper programs are cognizant of your currently loaded modules, and will ensure that your code links against our openmpi installation.  more information about using openmpi at our center can be found in our software documentation.

Compiling threaded codes

When building threaded codes, compiler-specific flags must be included to ensure a proper build.

Openmp

For pgi, add ""-mp"" to the build line.

$ mpicc -mp test.c -o test.x
$ export OMP_NUM_THREADS=2

For gnu, add ""-fopenmp"" to the build line.",4.399774453202207
"How do I optimize my application's performance on Frontier?
",This section of the Summit User Guide is intended to show current OLCF users how to start preparing their applications to run on the upcoming Frontier system. We will continue to add more topics to this section in the coming months. Please see the topics below to get started.,4.253841369816517
"How do I optimize my application's performance on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.21653517160257
"How do I optimize my application's performance on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.179972738811269
"What is the benefit of complying with OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to or use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title:Security Policy Version: 12.10",4.322456793270974
"What is the benefit of complying with OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  Title: Project Reporting Policy Version: 12.10",4.320517463859929
"What is the benefit of complying with OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to or use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title: Data Management Policy Version: 20.02",4.298386222831255
"How do I offload computation to the GPU using Crusher?
","Crusher contains a total of 768 AMD MI250X. The AMD MI250X has a peak performance of 53 TFLOPS in double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 26.5 TFLOPS (double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in both directions simultaneously).



To connect to Crusher, ssh to crusher.olcf.ornl.gov. For example:

$ ssh <username>@crusher.olcf.ornl.gov",4.249232741276624
"How do I offload computation to the GPU using Crusher?
","If CAAR or ECP teams require a temporary exception to this policy, please email help@olcf.ornl.gov with your request and it will be given to the OLCF Resource Utilization Council (RUC) for review.

This section describes how to map processes (e.g., MPI ranks) and process threads (e.g., OpenMP threads) to the CPUs and GPUs on Crusher. The https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#crusher-compute-nodes diagram will be helpful when reading this section to understand which physical CPU cores (and hardware threads) your processes and threads run on.",4.236608662735505
"How do I offload computation to the GPU using Crusher?
","Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.",4.219955137576649
"How can I optimize my application's performance on the IBM Spectrum Scale filesystem?
","For best performance on the IBM Spectrum Scale filesystem, use large page aligned I/O and asynchronous reads and writes. The filesystem blocksize is 16MB, the minimum fragment size is 16K so when a file under 16K is stored, it will still use 16K of the disk. Writing files of 16 MB or larger, will achieve better performance. All files are striped across LUNs which are distributed across all IO servers.",4.399879019968534
"How can I optimize my application's performance on the IBM Spectrum Scale filesystem?
","Summit mounts a POSIX-based IBM Spectrum Scale parallel filesystem called Alpine. Alpine's maximum capacity is 250 PB. It is consisted of 77 IBM Elastic Storage Server (ESS) GL4 nodes running IBM Spectrum Scale 5.x which are called Network Shared Disk (NSD) servers. Each IBM ESS GL4 node, is a scalable storage unit (SSU), constituted by two dual-socket IBM POWER9 storage servers, and a 4X EDR InfiniBand network for up to 100Gbit/sec of networking bandwidth.  The maximum performance of the final production system will be about 2.5 TB/s for sequential I/O and 2.2 TB/s for random I/O under FPP",4.301560057071137
"How can I optimize my application's performance on the IBM Spectrum Scale filesystem?
",Spock is connected to an IBM Spectrum Scale™ filesystem providing 250 PB of storage capacity with a peak write speed of 2.5 TB/s. Spock also has access to the center-wide NFS-based filesystem (which provides user and project home areas). While Spock does not have direct access to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the https://docs.olcf.ornl.gov/systems/spock_quick_start_guide.html#dtn-user-guide to move data to/from HPSS.,4.288930079436514
"What is the purpose of the Linux kernel's HMM support when using Crusher?
","The Crusher system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.327474036047852
"What is the purpose of the Linux kernel's HMM support when using Crusher?
","The Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.137796284164052
"What is the purpose of the Linux kernel's HMM support when using Crusher?
","Crusher users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.096137245529931
"Where are the container images built using Podman on Summit stored?
","Podman is a container framework from Red Hat that functions as a drop in replacement for Docker. On Summit, we will use Podman for building container images and converting them into tar files for Singularity to use. Podman makes use of Dockerfiles to describe the images to be built. We cannot use Podman to run containers as it doesn't properly support MPI on Summit, and Podman does not support storing its container images on GPFS or NFS.",4.439366887824684
"Where are the container images built using Podman on Summit stored?
","Users will be building and running containers on Summit without root permissions i.e. containers on Summit are rootless.  This means users can get the benefits of containers without needing additional privileges. This is necessary for a shared system like Summit. And this is part of the reason why Docker doesn't work on Summit. Podman and Singularity provides rootless support but to different extents hence why users need to use a combination of both.

Users will need to set up a file in their home directory /ccs/home/<username>/.config/containers/storage.conf with the following content:",4.36581230255763
"Where are the container images built using Podman on Summit stored?
","You can view the Dockerfiles used to build the MPI base image at the code.ornl.gov repository. These Dockerfiles are buildable on Summit yourself by cloning the repository and running the ./build in the individual directories in the repository. This allows you the freedom to modify these base images to your own needs if you don't need all the components in the base images. You may run into the cgroup memory limit when building so kill the podman process, log out, and try running the build again if that happens when building.",4.354801275942565
"What is the difference between the `host-x86_64-unknown-linux` and `hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+` URIs in the `roc-obj-ls` output?
","The AMD tool `roc-obj-ls` will let you see what code objects are in a binary.

.. code::
    $ hipcc --amdgpu-target=gfx90a:xnack+ square.hipref.cpp -o xnack_plus.exe
    $ roc-obj-ls -v xnack_plus.exe
    Bundle# Entry ID:                                                              URI:
    1       host-x86_64-unknown-linux                                           file://xnack_plus.exe#offset=8192&size=0
    1       hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+                              file://xnack_plus.exe#offset=8192&size=9752",4.314871893753028
"What is the difference between the `host-x86_64-unknown-linux` and `hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+` URIs in the `roc-obj-ls` output?
","The AMD tool `roc-obj-ls` will let you see what code objects are in a binary.

.. code::
    $ hipcc --amdgpu-target=gfx90a:xnack+ square.hipref.cpp -o xnack_plus.exe
    $ roc-obj-ls -v xnack_plus.exe
    Bundle# Entry ID:                                                              URI:
    1       host-x86_64-unknown-linux                                           file://xnack_plus.exe#offset=8192&size=0
    1       hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+                              file://xnack_plus.exe#offset=8192&size=9752",4.314871893753028
"What is the difference between the `host-x86_64-unknown-linux` and `hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+` URIs in the `roc-obj-ls` output?
","The above log messages indicate the type of image required by each device, given its current mode (amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack-) and the images found in the binary (hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+).",4.23585205620059
"How do I interact with my data in VisIt?
","For sample data and additional examples, explore the VisIt Data Archive and various VisIt Tutorials. Supplementary test data can be found in your local installation in the data directory:

Linux: /path/to/visit/data

macOS: /path/to/VisIt.app/Contents/Resources/data

Windows: C:\path\to\LLNL\VisIt x.y.z\data

Additionally, check out our beginner friendly OLCF VisIt Tutorial which uses Andes to visualize example datasets.",4.3284001856428524
"How do I interact with my data in VisIt?
","VisIt is now available on Frontier through the UMS022 module.

VisIt 3.3.3 is now available on Andes.

VisIt is an interactive, parallel analysis and visualization tool for scientific data. VisIt contains a rich set of visualization features so you can view your data in a variety of ways. It can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes. Further information regarding VisIt can be found at the links provided in the https://docs.olcf.ornl.gov/systems/visit.html#visit-resources section.",4.279543865103727
"How do I interact with my data in VisIt?
","Navigate to the appropriate file.

Once you choose a file, you will be prompted for the number of nodes and processors you would like to use (remember that each node of Andes contains 32 processors, or 28 if using the high-memory GPU partition) and the Project ID, which VisIt calls a ""Bank"" as shown below.



Once specified, the server side of VisIt will be launched, and you can interact with your data.",4.278289171542633
"Can I send an email when my job completes?
","When all of the above steps are completed, your user account will be created and you will be notified by email. Now that you have a user account and it has been associated with a project, you're ready to get to work.",4.133021925028104
"Can I send an email when my job completes?
","An activity report will appear, and you can click on it to see the status of the transfer.



Various information about the transfer is shown in the activity report. You will receive an email once the transfer is finished, including if it fails for any reason.",4.110869263599763
"Can I send an email when my job completes?
","Most users will find batch jobs an easy way to use the system, as they allow you to ""hand off"" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.",4.018188277170676
"How can I run my program with TAU tracing enabled?
","TAU is a portable profiling and tracing toolkit that supports many programming languages. The instrumentation can be done by inserting in the source code using an automatic tool based on the Program Database Toolkit (PDT), on the compiler instrumentation, or manually using the instrumentation API.

Webpage: https://www.cs.uoregon.edu/research/tau/home.php",4.3313201754572255
"How can I run my program with TAU tracing enabled?
","TAU is installed with Program Database Toolkit (PDT) on Summit. PDT is a framework for analyzing source code written in several programming languages. Moreover, Performance Application Programming Interface (PAPI) is supported. PAPI counters are used to assess the CPU performance. In this section, some approaches for profiling and tracing will be presented.

In most cases, we need to use wrappers to recompile the application:

For C: replace the compiler with the TAU wrapper tau_cc.sh

For C++: replace the compiler with the TAU wrapper tau_cxx.sh",4.318805768282524
"How can I run my program with TAU tracing enabled?
","For Fortran: replace the compiler with the TAU wrapper tau_f90.sh / tau_f77.sh

Even if you don't compile your application with a TAU wrapper, you can profile some basic functionalities with tau_exec, for example:

jsrun -n 4 –r 4 –a 1 –c 1 tau_exec -T mpi ./test

The above command profiles MPI for the binary test, which was not compiled with the TAU wrapper.

The following TAU environment variables may be useful in job submission scripts.",4.317347536303887
"What is the purpose of the `echo ""*****ldd ./${exe}*****""` command?
","# You may notice that some libraries are still linked from /sw/frontier, even after SBCASTing.
# This is because the Spack-build modules use RPATH to find their dependencies.
echo ""*****ldd /mnt/bb/$USER/${exe}*****""
ldd /mnt/bb/$USER/${exe}
echo ""*************************************""

and here is the output from that script:",4.14625242429362
"What is the purpose of the `echo ""*****ldd ./${exe}*****""` command?
","# You may notice that some libraries are still linked from /sw/frontier, even after SBCASTing.
# This is because the Spack-build modules use RPATH to find their dependencies.
echo ""*****ldd /mnt/bb/$USER/${exe}*****""
ldd /mnt/bb/$USER/${exe}
echo ""*************************************""

and here is the output from that script:",4.14625242429362
"What is the purpose of the `echo ""*****ldd ./${exe}*****""` command?
","# Check to see if file exists
echo ""*****ls -lh /mnt/bb/$USER*****""
ls -lh /mnt/bb/$USER/
echo ""*****ls -lh /mnt/bb/$USER/${exe}_libs*****""
ls -lh /mnt/bb/$USER/${exe}_libs

# SBCAST sends all libraries detected by `ld` (minus any excluded), and stores them in the same directory in each node's node-local storage
# Any libraries opened by `dlopen` are NOT sent, since they are not known by the linker at run-time.

# All required libraries now reside in /mnt/bb/$USER/${exe}_libs
export LD_LIBRARY_PATH=""/mnt/bb/$USER/${exe}_libs""",4.133271740764591
"How many CPUs are available per RS in Paraview?
","module load paraview/5.11.0-osmesa

   # Set up flags for jsrun
   export NNODES=$(($(cat $LSB_DJOB_HOSTFILE | uniq | wc -l)-1))
   export NCORES_PER_NODE=28
   export NGPU_PER_NODE=0
   export NRS_PER_NODE=1
   export NMPI_PER_RS=28
   export NCORES_PER_RS=$(($NCORES_PER_NODE/$NRS_PER_NODE))
   export NGPU_PER_RS=$(($NGPU_PER_NODE/$NRS_PER_NODE))
   export NRS=$(($NNODES*$NRS_PER_NODE))

   jsrun -n ${NRS} -r ${NRS_PER_NODE} -a ${NMPI_PER_RS} -g ${NGPU_PER_RS} -c ${NCORES_PER_RS} pvbatch para_example.py",4.133060698801038
"How many CPUs are available per RS in Paraview?
","You will obtain the best performance by running the ParaView client on your local computer and running the server on OLCF resources with the same version of ParaView. It is highly recommended to check the available ParaView versions using module avail paraview on the system you plan to connect ParaView to. Precompiled ParaView binaries for Windows, macOS, and Linux can be downloaded from Kitware.

Recommended ParaView versions on our systems:

Summit: ParaView 5.9.1, 5.10.0, 5.11.0

Andes: ParaView 5.9.1, 5.10.0, 5.11.0",4.078209608079607
"How many CPUs are available per RS in Paraview?
","Although in a single machine setup both the ParaView client and server run on the same host, this need not be the case. It is possible to run a local ParaView client to display and interact with your data while the ParaView server runs in an Andes or Summit batch job, allowing interactive analysis of very large data sets.",4.077212278321499
"How do I specify the name of my job on Summit?
",For Summit:,4.2203726997215805
"How do I specify the name of my job on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.159767133891488
"How do I specify the name of my job on Summit?
","below), you can choose between the rendering options by loading its corresponding module on the system you're connected to. For example, to see these modules on Summit:",4.1072994128891285
"Where can I download precompiled ParaView binaries?
","You will obtain the best performance by running the ParaView client on your local computer and running the server on OLCF resources with the same version of ParaView. It is highly recommended to check the available ParaView versions using module avail paraview on the system you plan to connect ParaView to. Precompiled ParaView binaries for Windows, macOS, and Linux can be downloaded from Kitware.

Recommended ParaView versions on our systems:

Summit: ParaView 5.9.1, 5.10.0, 5.11.0

Andes: ParaView 5.9.1, 5.10.0, 5.11.0",4.254281363966806
"Where can I download precompiled ParaView binaries?
","Alternatively, the ParaView installations in /sw/andes/paraview (i.e., the paraview/5.9.1-egl and paraview/5.9.1-osmesa modules) can also be loaded to avoid this issue.



The ParaView at OLCF Tutorial highlights how to get started on Andes with example datasets.

The Official ParaView User's Guide and the Python API Documentation contain all information regarding the GUI and Python interfaces.

A full list of ParaView Documentation can be found on ParaView's website.

The ParaView Wiki contains extensive information about all things ParaView.",4.198474706841287
"Where can I download precompiled ParaView binaries?
","Tutorials can be found on the ParaView Wiki at The ParaView Tutorial and SNL ParaView Tutorials.

Sample Data not pre-packaged with ParaView can be found on the ParaView download page under the Data section.

Specific ParaView Versions and their Release Notes can be found on the ParaView website and ParaView Wiki, respectively.

Non-ORNL related bugs and issues in ParaView can be found and reported on Discourse.",4.184024578738851
"Who can access World Work directories in the OLCF Policy?
","this file system for long, but rather should be migrated to Project Home or Project Archive space as soon as the files are not actively being used. If a file system associated with World Work storage is nearing capacity, the OLCF may contact the PI of the project to request that he or she reduce the size of the World Work directory. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for more details on applicable quotas, backups, purge, and retention timeframes.",4.400084628621748
"Who can access World Work directories in the OLCF Policy?
","To facilitate collaboration among researchers, the OLCF provides (3) distinct types of project-centric work storage areas: Member Work directories, Project Work directories, and World Work directories. Each directory should be used for storing files generated by computationally-intensive HPC jobs related to a project.

Moderate enhanced projects do not have World Work directories and the filesystem is called ""arx"" rather than ""alpine""",4.363331517084255
"Who can access World Work directories in the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to or use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title:Security Policy Version: 12.10",4.318286937456247
"How can I access Alpine during the read-only period?
","Summit will accept batch jobs prior to 08:00 on December 18, but only batch jobs that will complete prior to 08:00 Dec 18 will run.  All batch jobs remaining in the queue at 08:00, Dec 18 will be deleted.



Alpine will be unmounted from Andes on December 19.  Jobs must be modified to use Orion as their scratch filesystem prior to this day.



In preparation for Alpine's decommission on January 01, Alpine will become read-only from all OLCF systems on December 19.",4.162890494693063
"How can I access Alpine during the read-only period?
","| Alpine becomes read-only and available only from DTNs. Please do not wait, begin transferring your Lustre data now. | | https://docs.olcf.ornl.gov/systems/2023_olcf_system_changes.html#Jan 01<alpine_decom> | Alpine decommissioned.  ALL REMAINING DATA WILL BE PERMANENTLY DELETED | | https://docs.olcf.ornl.gov/systems/2023_olcf_system_changes.html#Early 2024<summit_return_to_service> | Summit available for projects with 2024 allocation. | | https://docs.olcf.ornl.gov/systems/2023_olcf_system_changes.html#Early 2024<alpine_ii_available> | Alpine II filesystem available. |",4.144931245743495
"How can I access Alpine during the read-only period?
","To assist you with moving your data off of Alpine, the DTN's mount the new Orion filesystem and all projects with access to Alpine have now been granted access to the Orion filesystem.

Please do not wait to migrate needed data, begin migrating all needed data now.",4.142943122685924
"Can I use TAU with other performance analysis tools?
","TAU is a portable profiling and tracing toolkit that supports many programming languages. The instrumentation can be done by inserting in the source code using an automatic tool based on the Program Database Toolkit (PDT), on the compiler instrumentation, or manually using the instrumentation API.

Webpage: https://www.cs.uoregon.edu/research/tau/home.php",4.412073611328295
"Can I use TAU with other performance analysis tools?
","TAU is installed with Program Database Toolkit (PDT) on Summit. PDT is a framework for analyzing source code written in several programming languages. Moreover, Performance Application Programming Interface (PAPI) is supported. PAPI counters are used to assess the CPU performance. In this section, some approaches for profiling and tracing will be presented.

In most cases, we need to use wrappers to recompile the application:

For C: replace the compiler with the TAU wrapper tau_cc.sh

For C++: replace the compiler with the TAU wrapper tau_cxx.sh",4.361331160689141
"Can I use TAU with other performance analysis tools?
","A similar approach for other metrics, not all of them can be used. TAU provides a tool called tau_cupti_avail, where we can see the list of available metrics, then we have to figure out which CUPTI metrics use these ones.

Activate tracing and declare the data format to OTF2. OTF2 format is supported only by MPI and OpenSHMEM applications.

$ export TAU_TRACE=1
$ export TAU_TRACE_FORMAT=otf2

Use Vampir for visualization.

For example, do not instrument routine sort*(int *)

Create a file select.tau

BEGIN_EXCLUDE_LIST
void sort_#(int *)
END_EXCLUDE_LIST

Declare the TAU_OPTIONS variable",4.320272589026432
"How can I inspect the output of the kustomize build command in Slate?
","For this example to work, it is required to have a project ""automation user"" setup for the NCCS filesystem integration. Please contact User Assistance by submitting a help ticket if you are unsure about the automation user setup for your project.

This is not meant to be a production deployment, but a way for users to gain familiarity with building an application targeting Slate.",3.9968869444306088
"How can I inspect the output of the kustomize build command in Slate?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",3.966295612334384
"How can I inspect the output of the kustomize build command in Slate?
","Jenkins

OpenShift Pipelines

GitLab Runners

Deploying a GitLab Runner into a Slate project may be found in the https://docs.olcf.ornl.gov/systems/overview.html#slate_gitlab_runners document which leverages a localized Slate Helm Chart. The GitLab Pipelines documentation as well as GitLab CI/CD Examples provide more details on GitLab Runner capabilities and usage.

Jenkins

For Jenkins deployment, a Slate Helm Chart is planned to be released. Documentation concerning Jenkins Pipelines as well as Jenkins Pipeline Tutorials are available.

OpenShift Pipelines",3.9625419625039218
"Can I use Score-P with Fortran?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi-processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations.

For detailed information about using Score-P on Summit and the builds available, please see the Score-P Software Page.",4.353158329551939
"Can I use Score-P with Fortran?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations. It works in combination with Periscope, Scalasca, Vampir, and Tau.

Steps in a typical Score-P workflow to run on https://docs.olcf.ornl.gov/systems/Scorep.html#summit-user-guide:",4.335788962576525
"Can I use Score-P with Fortran?
","..  Fortran

    .. code-block:: bash

          $ module unload darshan-runtime
          $ module load scorep
          $ module load gcc
          $ scorep mpifort -pthread -fopenmp -c test.f90
          $ scorep mpifort -pthread -fopenmp -o test test.o

CUDA

In some cases e.g. **CUDA** applications, Score-P needs to be made aware of the programming paradigm in order to do the correct instrumentation.

.. code-block:: bash",4.304466650966228
"What is the purpose of the OLCF Policy?
","The OLCF provides a comprehensive suite of hardware and software resources for the creation, manipulation, and retention of scientific data. This document comprises guidelines for acceptable use of those resources. It is an official policy of the OLCF, and as such, must be agreed to by relevant parties as a condition of access to and use of OLCF computational resources.",4.350186529787915
"What is the purpose of the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to or use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  All Users  Title:Security Policy Version: 12.10",4.342313307372409
"What is the purpose of the OLCF Policy?
","This details an official policy of the OLCF, and must be agreed to by the following persons as a condition of access to and use of OLCF computational resources:  Principal Investigators (Non-Profit)  Principal Investigators (Industry)  Title: Project Reporting Policy Version: 12.10",4.326688663483319
"How does Crusher's memory management handle page faults?
","Most applications that use ""managed"" or ""unified"" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Crusher. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.",4.294719674644044
"How does Crusher's memory management handle page faults?
","The AMD MI250X and operating system on Crusher supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Crusher. In the following section we'll discuss how the heterogenous memory space on a Crusher node is surfaced within your application.",4.262742777339784
"How does Crusher's memory management handle page faults?
","On Pascal-generation GPUs and later, this automatic migration is enhanced with hardware support. A page migration engine enables GPU page faulting, which allows the desired pages to be migrated to the GPU ""on demand"" instead of the entire ""managed"" allocation. In addition, 49-bit virtual addressing allows programs using unified memory to access the full system memory size. The combination of GPU page faulting and larger virtual addressing allows programs to oversubscribe the system memory, so very large data sets can be processed. In addition, new CUDA API functions introduced in CUDA8 allow",4.252097749314236
"How can I create a route that points to the old service in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.296258778265677
"How can I create a route that points to the old service in Slate?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.226211102698501
"How can I create a route that points to the old service in Slate?
","By default, a route will only expose your https://docs.olcf.ornl.gov/systems/route.html#slate_services to NCCS networks. If you need your service exposed to the world outside ORNL, you will first need to get your project approved for external routes. To do this, submit a systems ticket. In the description, give us your project name and a brief reasoning for why exposing externally is needed.

We will let you know once your project is able to set up external routes.",4.190249598017437
"How do I integrate heterogeneous resources with libEnsemble?
","libEnsemble is a complete https://docs.olcf.ornl.gov/systems/libensemble.html#Python<py-index> toolkit for steering dynamic ensembles of calculations. Workflows are highly portable and detect/integrate heterogeneous resources with little effort. For instance, libEnsemble can automatically detect, assign, and reassign allocated processors and GPUs to ensemble members.",4.382336878800992
"How do I integrate heterogeneous resources with libEnsemble?
","Users select or supply generator and simulator functions to express their ensembles; the generator typically steers the ensemble based on prior simulator results. Such functions can also launch and monitor external executables at any scale.

Begin by loading the python module:

$ module load cray-python

libEnsemble is available on PyPI, conda-forge, the xSDK, and E4S. Most users install libEnsemble via pip:

$ pip install libensemble",4.172586059095012
"How do I integrate heterogeneous resources with libEnsemble?
","Upon initialization, libEnsemble will detect available nodes and GPUs from the Slurm environment, and allocate those resources towards application-launches.

Start an interactive session:

$ salloc --nodes=2 -A <project_id> --time=00:10:00

Within the session (multiprocessing comms, all processes on first node):

$ python my_libensemble_script.py --comms local --nworkers 8",4.152088274269611
"What is the minimum percentage of allocation that a project must have available to run a job?
","For example, consider that job1 is submitted at the same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. In addition, projects that are at 125% of their allocated time will be limited to only one running job at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.377465020147042
"What is the minimum percentage of allocation that a project must have available to run a job?
","For example, consider that job1 is submitted at the same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. In addition, projects that are at 125% of their allocated time will be limited to only one running job at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:",4.377465020147042
"What is the minimum percentage of allocation that a project must have available to run a job?
","consider ``job2`` to have been waiting for a longer time than ``job1``.

Also projects that are at 125% of their allocated time will be limited

to only one running job at a time. The adjustment to the apparent submit

time depends upon the percentage that the project is over its

allocation, as shown in the table below:



+------------------------+----------------------+--------------------------+------------------+

| % Of Allocation Used   | Priority Reduction   | Number eligible-to-run   | Number running   |",4.358400409045201
"What is the peak performance of each AMD MI250X in Crusher?
","Crusher contains a total of 768 AMD MI250X. The AMD MI250X has a peak performance of 53 TFLOPS in double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 26.5 TFLOPS (double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in both directions simultaneously).



To connect to Crusher, ssh to crusher.olcf.ornl.gov. For example:

$ ssh <username>@crusher.olcf.ornl.gov",4.526632436900778
"What is the peak performance of each AMD MI250X in Crusher?
","The Crusher system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities. The AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs. This new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.",4.334428595305544
"What is the peak performance of each AMD MI250X in Crusher?
","Each Crusher compute node consists of [1x] 64-core AMD EPYC 7A53 ""Optimized 3rd Gen EPYC"" CPU (with 2 hardware threads per physical core) with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X, each with 2 Graphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can think of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth memory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU, allowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of 36+36 GB/s. The 2 GCDs on the same MI250X are connected with",4.312745739426616
"What is the purpose of the trident.netapp.io/cloneFromPVC annotation?
","Cloning a persistent volume is just as easy as implementing a snapshot. First, find a Persistent Volume Claim in the same namespace that you would like to clone for your new persistent volume. Then it's as simple as adding the trident.netapp.io/cloneFromPVC annotation with a value of the name of the Persistent Volume Claim you would like to clone.

In the below example, we clone a persistent volume named source-clone-pvc into a new volume called destination-clone-pvc",4.286181674368857
"What is the purpose of the trident.netapp.io/cloneFromPVC annotation?
","apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    trident.netapp.io/cloneFromPVC: ""source-clone-pvc""
    volume.beta.kubernetes.io/storage-class: ""basic""
    trident.netapp.io/splitOnClone: ""true""
  name: destination-clone-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

Cloning has applications outside of backups such as testing changes on a new Persistent Volume.",4.218883773022551
"What is the purpose of the trident.netapp.io/cloneFromPVC annotation?
","oc annotate deployment web ccs.ornl.gov/fs=olcf

You cannot annotate an existing pod because the injection happens at pod creation time

Annotating a pod not managed by a deployment with oc annotate pod test ccs.ornl.gov/fs=olcf will not work. Instead delete the pod and add the annotation to the metadata and recreate it.

You can also add the annotations to any workload object's YAML such as a Pod, Deployment, or DeploymentConfig.",3.9695058365573614
"What should I do if I have questions about a parameter or capability of ArgoCD?
","The initial resources set by OpenShift GitOps should be sufficient to start working with ArgoCD. If needed, these may be increased should performance issues occur. On the right hand side is the schema for the ArgoCD custom resource listing all of the available parameters that could be used. At this point, no other parameters are needed to create an instance. However, if there are questions over a parameter or capability, please contact the Platforms Group.",4.206367184824393
"What should I do if I have questions about a parameter or capability of ArgoCD?
",allow for better control of resources allocated to ArgoCD.,4.183290131858538
"What should I do if I have questions about a parameter or capability of ArgoCD?
","If a sub project is being setup under the main project for deployment of ArgoCD, allocate at least 6 CPU, 5 Gi memory, and 5 Gi storage resources. Once deployed, these may need to be adjusted higher depending on observed performance.

Once a project is identified for deployment of ArgoCD, navigate to the project using the Administrator view of the console, Select Operators -> Installed Operators -> Red Hat OpenShift GitOps -> ""Argo CD"" tab

Image of OpenShift UI for creating an ArgoCD instance.

If the project already has an ArgoCD instance, do not install another into the same project.",4.1350911375581045
"What is the purpose of the nginx.conf file?
","# Load modular configuration files from the /etc/nginx/conf.d directory.
    # See http://nginx.org/en/docs/ngx_core_module.html#include
    # for more information.
    include /etc/nginx/conf.d/*.conf;

    server {
        listen       8080 default_server;
        listen       [::]:8080 default_server;
        server_name  _;
        root         /usr/share/nginx/html;

        # Load configuration files for the default server block.
        include /etc/nginx/default.d/*.conf;

        location / {
        }",4.279703237805086
"What is the purpose of the nginx.conf file?
","<h1>Hello, World!</h1>

user nginx;
worker_processes auto;
error_log /tmp/error.log;
pid /tmp/nginx.pid;

# Load dynamic modules. See /usr/share/nginx/README.dynamic.
include /usr/share/nginx/modules/*.conf;

events {
    worker_connections 1024;
}

http {
    log_format  main  '$remote_addr - $remote_user [$time_local] ""$request"" '
                      '$status $body_bytes_sent ""$http_referer"" '
                      '""$http_user_agent"" ""$http_x_forwarded_for""';

    access_log  /tmp/access.log  main;",4.2039484322922585
"What is the purpose of the nginx.conf file?
","location / {
        }

        error_page 404 /404.html;
            location = /40x.html {
        }

        error_page 500 502 503 504 /50x.html;
            location = /50x.html {
        }
    }
}

The NGINX configuration file is completely standard except I changed the listen port to be from 80 to 8080 since the server will be running as a non-root user. The Route that we will add later on will redirect traffic coming in on port 80 to our server running on port 8080.

The BuildConfig, the following should be placed inside a buildconfig.yaml file:",4.178802348473576
"What is the purpose of the --nv flag when running a container on Summit?
","The --nv flag is needed to tell Singularity to make use of the GPU.

You can run containers with CUDA-aware MPI as well. CUDA-aware MPI allows transferring GPU data with MPI without needing to copy the data over to CPU memory first. Read more https://docs.olcf.ornl.gov/systems/containers_on_summit.html#CUDA-Aware MPI.

Let's build and run a container that will demonstrate CUDA-aware MPI.

Create a new directory cudaawarempiexample.

Run the below wget commands to obtain the example code and Makefile from the OLCF tutorial example page.",4.37213711649341
"What is the purpose of the --nv flag when running a container on Summit?
","The Nsight Compute command-line interface, nv-nsight-cu-cli, can be prefixed to your application to collect a report.

summit> module load nsight-compute

summit> jsrun -n1 -a1 -g1 nv-nsight-cu-cli ./vectorAdd

Similar to Nsight Systems, Nsight Compute will create a temporary report file, even when -o is not specified.",4.1391614640040615
"What is the purpose of the --nv flag when running a container on Summit?
","The reason we include the --disable-cache flag is because Singularity's caching can fill up your home directory without you realizing it. And if the home directory is full, Singularity builds will fail. If you wish to make use of the cache, you can set the environment variable SINGULARITY_CACHEDIR=/tmp/containers/<user>/singularitycache or something like that so that the NVMe storage is used as the cache.

As a simple example, we will run hostname with the Singularity container.

Create a file submit.lsf with the contents below.",4.132484755294708
"Can CPUs on different sockets access GPUs on the same socket in Summit?
","In this case, all 6 CPUs can see 3 GPUs. Code must manage CPU -> GPU communication. CPUs on socket0 can not access GPUs or Memory on socket1.

Single resource set per node: 6 GPUs, 12 cores



In this case, all 12 CPUs can see all node’s 6 GPUs. Code must manage CPU to GPU communication. CPUs on socket0 can access GPUs and Memory on socket1. Code must manage cross socket communication.",4.340239149086172
"Can CPUs on different sockets access GPUs on the same socket in Summit?
",| (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_atchley.pdf https://vimeo.com/306002085 | | 2018-11-05 | Programming Methods for Summit's Multi-GPU Nodes | Jeff Larkin & Steve Abbott (NVIDIA) | Programming Methods for Summit's Multi-GPU Nodes https://www.olcf.ornl.gov/calendar/programming-methods-for-summits-multi-gpu-nodes/ | (slides | recording 1 recording 2) https://www.olcf.ornl.gov/wp-content/uploads/2018/11/multi-gpu-workshop.pdf https://vimeo.com/308290719 https://vimeo.com/308290811 | | 2018-06-28 | Intro to OpenACC | Steve Abbott (NVIDIA) |,4.247899335430851
"Can CPUs on different sockets access GPUs on the same socket in Summit?
","Login nodes have (2) 16-core Power9 CPUs and (4) V100 GPUs. Compute nodes have (2) 22-core Power9 CPUs and (6) V100 GPUs.

Summit nodes are connected to a dual-rail EDR InfiniBand network providing a node injection bandwidth of 23 GB/s. Nodes are interconnected in a Non-blocking Fat Tree topology. This interconnect is a three-level tree implemented by a switch to connect nodes within each cabinet (first level) along with Director switches (second and third level) that connect cabinets together.",4.235547481747271
"Is it possible to use only one core on each node for my job on Summit?
","Jobs on Summit are scheduled in full node increments; a node's cores cannot be allocated to multiple jobs. Because the OLCF charges based on what a job makes unavailable to other users, a job is charged for an entire node even if it uses only one core on a node. To simplify the process, users request and are allocated multiples of entire nodes through LSF.

Allocations on Summit are separate from those on Andes and other OLCF resources.

The node-hour charge for each batch job will be calculated as follows:

node-hours = nodes requested * ( batch job endtime - batch job starttime )",4.343878428688367
"Is it possible to use only one core on each node for my job on Summit?
","Login nodes have (2) 16-core Power9 CPUs and (4) V100 GPUs. Compute nodes have (2) 22-core Power9 CPUs and (6) V100 GPUs.

Summit nodes are connected to a dual-rail EDR InfiniBand network providing a node injection bandwidth of 23 GB/s. Nodes are interconnected in a Non-blocking Fat Tree topology. This interconnect is a three-level tree implemented by a switch to connect nodes within each cabinet (first level) along with Director switches (second and third level) that connect cabinets together.",4.241224301672294
"Is it possible to use only one core on each node for my job on Summit?
","Summit node architecture diagram

The basic building block of Summit is the IBM Power System AC922 node. Each of the approximately 4,600 compute nodes on Summit contains two IBM POWER9 processors and six NVIDIA Tesla V100 accelerators and provides a theoretical double-precision capability of approximately 40 TF. Each POWER9 processor is connected via dual NVLINK bricks, each capable of a 25GB/s transfer rate in each direction.",4.232238495047574
"How does the Infinity Fabric affect CPU accesses to migratable memory on Frontier?
","The page migration behavior summarized by the following tables represents the current, observable behavior. Said behavior will likely change in the near future.  CPU accesses to migratable memory may behave differently than other platforms you're used to. On Frontier, pages will not migrate from GPU HBM to CPU DDR4 based on access patterns alone. Once a page has migrated to GPU HBM it will remain there even if the CPU accesses it, and all accesses which do not resolve in the CPU cache will occur over the Infinity Fabric between the AMD ""Optimized 3rd Gen EPYC"" CPU and AMD MI250X GPU. Pages",4.403915218660093
"How does the Infinity Fabric affect CPU accesses to migratable memory on Frontier?
","on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The GCDs on different MI250X are connected with Infinity Fabric GPU-GPU in the arrangement shown in the Frontier Node Diagram below, where the peak bandwidth ranges from 50-100 GB/s based on the number of Infinity Fabric connections between individual GCDs.",4.293117308122128
"How does the Infinity Fabric affect CPU accesses to migratable memory on Frontier?
","The AMD MI250X and operating system on Frontier supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Frontier. In the following section we'll discuss how the heterogenous memory space on a Frontier node is surfaced within your application.",4.280110395100493
"What is the advantage of using pbdR packages over other parallel computing packages in R?
","For parallelism, you should use pbdR packages, Rmpi directly, or an interface which can use Rmpi as a backend. We address GPUs specifically next.

There are some R packages which can use GPUs, such as xgboost. There is also the gpuR series of packages. Several pbdR packages support GPU computing. It is also possible to offload some linear algebra computations (specifically matrix-matrix products, and methods which are computationally dominated by them) to the GPU using NVIDIA’s NVBLAS.",4.509315138032884
"What is the advantage of using pbdR packages over other parallel computing packages in R?
","There are many R packages for parallel computing. Some popular ones include the core parallel package, as well as high-level interface packages like future and foreach. Many of these will use the OS fork mechanism to launch additional R processes. This mechanism generally does not behave well with MPI, which you must use (in the form of jsrun) to push your task out to the compute node(s). We highly recommend you avoid these packages if at all possible, unless they are a frontend to Rmpi.",4.248284028169378
"What is the advantage of using pbdR packages over other parallel computing packages in R?
","If you want to do GPU computing on Summit with R, we would love to collaborate with you (see contact details at the bottom of this document).

For more information about using R and pbdR effectively in an HPC environment such as Summit, please see the R and pbdR articles. These are long-form articles that introduce HPC concepts like MPI programming in much more detail than we do here.

Also, if you want to use R and/or pbdR on Summit, please feel free to contact us directly:

Mike Matheson - mathesonma AT ornl DOT gov

George Ostrouchov - ostrouchovg AT ornl DOT gov",4.16273905215633
"Can you provide an example of how to configure my application to use NVMe support on Summit?
","Each compute node on Summit has a 1.6TB Non-Volatile Memory (NVMe) storage device (high-memory nodes have a 6.4TB NVMe storage device), colloquially known as a ""Burst Buffer"" with theoretical performance peak of 2.1 GB/s for writing and 5.5 GB/s for reading. The NVMes could be used to reduce the time that applications wait for I/O.  More information can be found later in the Burst Buffer section.



<string>:208: (INFO/1) Duplicate implicit target name: ""software"".",4.201866319025847
"Can you provide an example of how to configure my application to use NVMe support on Summit?
","Remember that by default NVMe support one file per MPI process up to one file per compute node. If users desire a single file as output from data staged on the NVMe they will need to construct it.  Tools to save automatically checkpoint files from NVMe to GPFS as also methods that allow automatic n to 1 file writing with NVMe staging are under development.   Tutorials about NVME:   Burst Buffer on Summit (slides, video) Summit Burst Buffer Libraries (slides, video).",4.199410798346014
"Can you provide an example of how to configure my application to use NVMe support on Summit?
","The https://docs.olcf.ornl.gov/systems/summit_user_guide.html#OLCF Training Archive<training-archive> provides a list of previous training events, including multi-day Summit Workshops. Some examples of topics addressed during these workshops include using Summit's NVME burst buffers, CUDA-aware MPI, advanced networking and MPI, and multiple ways of programming multiple GPUs per node. You can also find simple tutorials and code examples for some common programming and running tasks in our Github tutorial page .",4.196048328195349
"How can I ensure that my singularity builds are running efficiently on Summit?
","Singularity is a container framework from Sylabs. On Summit, we will use Singularity solely as the runtime. We will convert the tar files of the container images Podman creates into sif files, store those sif files on GPFS, and run them with Jsrun. Singularity also allows building images but ordinary users cannot utilize that on Summit due to additional permissions not allowed for regular users.",4.283579859458563
"How can I ensure that my singularity builds are running efficiently on Summit?
","Users can build container images with Podman, convert it to a SIF file, and run the container using the Singularity runtime. This page is intended for users with some familiarity with building and running containers.

Users will make use of two applications on Summit - Podman and Singularity - in their container build and run workflow. Both are available without needing to load any modules.",4.184116706839138
"How can I ensure that my singularity builds are running efficiently on Summit?
","To access Summit and Frontier's compute resources for SPI workflows, you must first log into a https://docs.olcf.ornl.gov/systems/index.html#Citadel login node<citadel-login-nodes> and then submit a batch job to one of the SPI specific batch queues.",4.176376800097096
"Can users access IBM Quantum's systems information on the cloud dashboard?
","Users can access information about IBM Quantum's systems, view queue information, and submit jobs on their cloud dashboard at https://quantum-computing.ibm.com. The cloud dashboard allows access to IBM Quantum's graphical circuit builder, Quantum Composer, IBM's Quantum Lab, and associated program examples.  A Jupyterlab server is provisioned with IBM Quantum's Qiskit programming framework for job submission.",4.593575938188055
"Can users access IBM Quantum's systems information on the cloud dashboard?
","Access to the IBM Quantum Computing queues, reservations, and simulators can be obtained via multiple methods -- either through the https://docs.olcf.ornl.gov/systems/ibm_quantum.html#cloud <ibm-cloud> or https://docs.olcf.ornl.gov/systems/ibm_quantum.html#locally <ibm-local>.",4.3658879754639015
"Can users access IBM Quantum's systems information on the cloud dashboard?
","accessing the hybrid infrastructure of available quantum processors and classical computational framework via the cloud. From the QCS, users can view system status and availability, initiate and manage quantum infrastructure reservations (either executing programs manually or adding them to the queue). Information on using this resource is available on the Rigetti's Documentation or our https://docs.olcf.ornl.gov/quantum/quantum_systems/rigetti.html.",4.360071233261725
"Can I run multiple jobs at the same time on Frontier?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.258796033939874
"Can I run multiple jobs at the same time on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.251537221776084
"Can I run multiple jobs at the same time on Frontier?
","Users may have only 100 jobs queued in the batch queue at any time (this includes jobs in all states). Additional jobs will be rejected at submit time.

The debug quality of service (QOS) class can be used to access Frontier's compute resources for short non-production debug tasks. The QOS provides a higher priority compare to jobs of the same job size bin in production queues. Production work and job chaining using the debug QOS is prohibited. Each user is limited to one job in any state at any one point. Attempts to submit multiple jobs to this QOS will be rejected upon job submission.",4.233181271278562
"Can I use the oc CLI client to create objects inside my project?
","Now that you have a project you can create objects inside that project. We will be doing this with the Openshift Web GUI and the oc CLI client so you can use whichever interface you are more comfortable with in this tutorial. If you are more comfortable using the command line than you are using a GUI you can now https://docs.olcf.ornl.gov/systems/guided_tutorial.html#jump to the oc portion of this document<slate_guided_tutorial_cli>. Otherwise, continue with the GUI based tutorial below.

Go to https://console-openshift-console.apps.<cluster>.ccs.ornl.gov/k8s/cluster/projects",4.369613049080677
"Can I use the oc CLI client to create objects inside my project?
","Once the oc client has been installed and is logged into the cluster you need to switch to your Project. Switching to a Project allows the oc client to assume that the commands it is running should be executed inside of the  Project that you switch to. You could alternatively not switch to a project and append the -n flag to each command you run followed by the name of the project you wish to run your command in. That being said, switch to your project:

oc project <PROJECT_NAME>",4.26222086418806
"Can I use the oc CLI client to create objects inside my project?
","On the command line, services can be created with the command oc create. Assuming our YAML file from above is in the file my-service.yaml, you can create the service with

$ oc create -f my-service.yaml

Then, you can run oc describe service my-service to see some information about it.",4.244261734226946
"What is the benefit of using a scratch directory for the kernel code cache?
","When a kernel call is required in CuPy, it compiles a kernel code optimized for the shapes and data types of given arguments, sends it to the GPU device, and executes the kernel. Due to this, CuPy runs slower on its initial execution. This slowdown will be resolved at the second execution because CuPy caches the kernel code sent to GPU device. By default, the compiled code is cached to the $HOME/.cupy/kernel_cache directory, which the compute nodes will not be able to access. It is good practice to change it to your scratch directory:",4.139289522703375
"What is the benefit of using a scratch directory for the kernel code cache?
","Each project is granted a Project Work directory; these reside in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. Project Work directories can be accessed by all members of a project and are intended for sharing data within a project. Project Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but",4.077358841934146
"What is the benefit of using a scratch directory for the kernel code cache?
","Each project has a World Work directory that resides in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. World Work areas can be accessed by all users of the system and are intended for sharing of data between projects. World Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but rather should be",4.067005668320965
"What is the benefit of sourcing additional files in /etc in Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.240149713199608
"What is the benefit of sourcing additional files in /etc in Summit?
","Summit is connected to an IBM Spectrum Scale™ filesystem providing 250PB of storage capacity with a peak write speed of 2.5 TB/s. Summit also has access to the center-wide NFS-based filesystem (which provides user and project home areas) and has access to the center’s High Performance Storage System (HPSS) for user and project archival storage.

Summit is running Red Hat Enterprise Linux (RHEL) version 8.2.",4.135056436705007
"What is the benefit of sourcing additional files in /etc in Summit?
","Users will be building and running containers on Summit without root permissions i.e. containers on Summit are rootless.  This means users can get the benefits of containers without needing additional privileges. This is necessary for a shared system like Summit. And this is part of the reason why Docker doesn't work on Summit. Podman and Singularity provides rootless support but to different extents hence why users need to use a combination of both.

Users will need to set up a file in their home directory /ccs/home/<username>/.config/containers/storage.conf with the following content:",4.103149521264514
"How does Vampir handle large amounts of performance data?
","Vampir is a software performance visualizer focused on highly parallel applications. It presents a unified view on an application run including the use of programming paradigms like MPI, OpenMP, PThreads, CUDA, OpenCL and OpenACC. It also incorporates file I/O, hardware performance counters and other performance data sources. Various interactive displays offer detailed insight into the performance behavior of the analyzed application. Vampir's scalable analysis server and visualization engine enable interactive navigation of large amounts of performance data. Score-P and TAU generate OTF2",4.400897849038854
"How does Vampir handle large amounts of performance data?
","Vampir is a software performance visualizer focused on highly parallel applications. It presents a unified view on an application run including the use of programming paradigms like MPI, OpenMP, PThreads, CUDA, OpenCL and OpenACC. It also incorporates file I/O, hardware performance counters and other performance data sources. Various interactive displays offer detailed insight into the performance behavior of the analyzed application. Vampir’s scalable analysis server and visualization engine enable interactive navigation of large amounts of performance data. Score-P and TAU generate OTF2",4.398937765366895
"How does Vampir handle large amounts of performance data?
","Vampir can be run a few different ways depending on a couple of factors. If you have a large trace file it would benefit from utilizing VampirServer to process the trace file with the system's compute power while reverse connected to the Vampir GUI. If the trace file is small enough (< ~1 GB), it would do just fine viewing on a login node.



vampirserver is the backend software component that can run across multiple compute nodes taking advantage of the machine's memory, this in turn provides an increase in performance for viewing large trace files i.e. >1GB.",4.297101781880938
"How can I re-create the index file for an existing tar archive using htar?
","To extract all files from the project1/src directory in the archive file called project1.tar, and use the time of extraction as the modification time, use the following command:

htar -xm -f  /hpss/prod/[projid]/users/[userid]/project1.tar project1/src

The htar utility has several limitations.

You cannot add or append files to an existing archive.

File path names within an htar archive of the form prefix/name are limited to 154 characters for the prefix and 99 characters for the file name. Link names cannot exceed 99 characters.",4.357114771116634
"How can I re-create the index file for an existing tar archive using htar?
","As with the standard Unix tar utility the -c, -x, and -t options, respectively, function to create, extract, and list tar archive files. The -K option verifies an existing tarfile in HPSS and the -X option can be used to re-create the index file for an existing archive. For example, to store all files in the directory dir1 to a file named /hpss/prod/[projid]/users/[userid]/allfiles.tar on HPSS, use the command:

htar -cvf /hpss/prod/[projid]/users/[userid]/allfiles.tar dir1/*

To retrieve these files:

htar -xvf  /hpss/prod/[projid]/users/[userid]/allfiles.tar",4.3374971025614775
"How can I re-create the index file for an existing tar archive using htar?
","htar will overwrite files of the same name in the target directory.  When possible, extract only the files you need from large archives. To display the names of the files in the project1.tar archive file within the HPSS home directory:

htar -vtf  /hpss/prod/[projid]/users/[userid]/project1.tar

To extract only one file, executable.out, from the project1 directory in the Archive file called `` /hpss/prod/[projid]/users/[userid]/project1.tar``:

htar -xm -f project1.tar project1/ executable.out",4.316274344899119
"Why is the `-C nvme` option required for the `sbcast` command?
","# SBCAST executable from Orion to NVMe -- NOTE: ``-C nvme`` is needed in SBATCH headers to use the NVMe drive
# NOTE: dlopen'd files will NOT be picked up by sbcast
# SBCAST automatically excludes several directories: /lib,/usr/lib,/lib64,/usr/lib64,/opt
#   - These directories are node-local and are very fast to read from, so SBCASTing them isn't critical
#   - see ``$ scontrol show config | grep BcastExclude`` for current list
#   - OLCF-provided libraries in ``/sw`` are not on the exclusion list. ``/sw`` is an NFS shared file system
#   - To override, add ``--exclude=NONE`` to arguments",4.304706107535655
"Why is the `-C nvme` option required for the `sbcast` command?
","# SBCAST executable from Orion to NVMe -- NOTE: ``-C nvme`` is needed in SBATCH headers to use the NVMe drive
# NOTE: dlopen'd files will NOT be picked up by sbcast
# SBCAST automatically excludes several directories: /lib,/usr/lib,/lib64,/usr/lib64,/opt
#   - These directories are node-local and are very fast to read from, so SBCASTing them isn't critical
#   - see ``$ scontrol show config | grep BcastExclude`` for current list
#   - OLCF-provided libraries in ``/sw`` are not on the exclusion list. ``/sw`` is an NFS shared file system
#   - To override, add ``--exclude=NONE`` to arguments",4.304706107535655
"Why is the `-C nvme` option required for the `sbcast` command?
","# SBCAST file from Orion to NVMe -- NOTE: ``-C nvme`` is required to use the NVMe drive
sbcast -pf test.txt /mnt/bb/$USER/test.txt
if [ ! ""$?"" == ""0"" ]; then
    # CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,
    # your application may pick up partially complete shared library files, which would give you confusing errors.
    echo ""SBCAST failed!""
    exit 1
fi",4.256847731242032
"What is the penalty for failing to submit a quarterly progress report for three months?
","Principal Investigators of current OLCF projects must submit a quarterly progress report. The quarterly reports are essential as the OLCF must diligently track the use of the center's resources. In keeping with this, the OLCF (and DOE Leadership Computing Facilities in general) imposes the following penalties for late submission:

| Timeframe | Penalty | | --- | --- | | 1 Month Late | Job submissions against offending project will be suspended. | | 3 Months Late | Login privileges will be suspended for all OLCF resources for all users associated with offending project. |",4.356369894642885
"What is the penalty for failing to submit a quarterly progress report for three months?
","Increased disk quota

Purge exemption for User/Group/World Work areas

Software requests



Closeout Report Template Use this template if you have been asked to submit a closeout report for your project.  Note this form does not apply to INCITE projects.  If you have been provided a template via email, only that template applies.

Industry Quarterly Report Template Use this template if you have an industry project to submit a quarterly report.



Director's Discretion Review Form For internal use only.",3.9916945417335103
"What is the penalty for failing to submit a quarterly progress report for three months?
","Once submitted, you will receive email notification of successful proposal submission.  The proposal is then reviewed by the Quantum Resource Utilization Council (QRUC), as well as independent referees for merit and to ensure the feasibility of project success using the resources available to the QCUP. You will be notified of the QRUC decision via email.

Once a project request is approved by the QRUC, an OLCF Accounts Manager will communicate with the project’s PI to finalize activation and request a signed Principal Investigator’s PI Agreement to be submitted.",3.9818990898165736
"How do I ensure data separation between my SPI and non-SPI projects?
","In order to help ensure data separation, each SPI user is given a unique userid for each project. SPI user names use the format: <userid>_<proj>_mde . For example: usera_prj123_mde.

SPI usernames will not overlap with usernames used in the non-SPI enclaves. Unlike non-SPI usernames, SPI usernames only exist in a single project. Users on multiple SPI projects will have a unique username for each SPI project.  You must specify your unique SPI username matching the target project when connecting.",4.20000903032946
"How do I ensure data separation between my SPI and non-SPI projects?
","In order to help ensure data separation, each SPI user is given a unique userID for each project. SPI userIDs use the format: <userid>_<proj>_mde . For example: userx_abc123_mde. SPI usernames will not overlap with usernames used in the non-SPI enclaves. Unlike non-SPI usernames, SPI usernames only exist in a single project. Users on multiple SPI projects will have a unique username for each SPI project.",4.186545785701655
"How do I ensure data separation between my SPI and non-SPI projects?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.  This page lists the differences you may see when using OLCF resources to execute SPI workflows.  The page will also point to sections within the site where more information on standard non-SPI use can be found.",4.141772709111841
"What is the benefit of using the WMMA API in CUDA?
",and CUDA Fortran. The image below demonstrates the general pattern for WMMA usage.,4.396112515958838
"What is the benefit of using the WMMA API in CUDA?
","The Warp Matrix Multiply and Accumulate (WMMA) API was introduced in CUDA 9 explicitly for programming the Tesla V100 Tensor Cores. This is a low-level API that supports loading matrix data into fragments within the threads of a warp, applying a Tensor Core multiplication on that data, and then restoring it to the main GPU memory. This API is called within CUDA kernels and all WMMA operations are warp-synchronous, meaning the threads in a warp will leave the operation synchronously. Examples are available for using the WMMA instructions in C++ and CUDA Fortran. The image below demonstrates",4.281218450279987
"What is the benefit of using the WMMA API in CUDA?
","The example above performs a 16-bit accumulate operation, but 32-bit is also supported. Please see the provided samples and the WMMA documentation for more details.

CUDA 10 introduced a lower-level alternative to WMMA with the mma.sync() instruction. This is a very low-level instruction that requires the programmer handle the data movement provided by WMMA explicitly, but is capable of higher performance. Details of mma.sync can be found in the PTX documentation and examples for using this feature via CUTLASS cane be found in the second half of this GTC presentation.",4.244279175756473
"What is the purpose of the PE_MPICH_GTL_DIR_amd_gfx90a environment variable?
","To use GPU-aware Cray MPICH with the Cray compiler wrappers, the following environment variables must be set before compiling. These variables are automatically set by the cray-mpich modulefile:

## These must be set before compiling so the executable picks up GTL
PE_MPICH_GTL_DIR_amd_gfx90a=""-L${CRAY_MPICH_ROOTDIR}/gtl/lib""
PE_MPICH_GTL_LIBS_amd_gfx90a=""-lmpi_gtl_hsa""

In addition, the following header files and libraries must be included:

-I${ROCM_PATH}/include
-L${ROCM_PATH}/lib -lamdhip64

where the include path implies that #include <hip/hip_runtime.h> is included in the source file.",4.241387294224472
"What is the purpose of the PE_MPICH_GTL_DIR_amd_gfx90a environment variable?
","module load craype-accel-amd-gfx90a

export MPICH_GPU_SUPPORT_ENABLED=1

If using PrgEnv-cray:

module load craype-accel-amd-gfx90a
module load amd-mixed

export MPICH_GPU_SUPPORT_ENABLED=1

There are extra steps needed to enable GPU-aware MPI on Frontier, which depend on the compiler that is used (see 1. and 2. below).

When using GPU-aware Cray MPICH with the Cray compiler wrappers, most of the needed libraries are automatically linked through the environment variables.

Though, the following header files and libraries must be included explicitly:",4.171484195639791
"What is the purpose of the PE_MPICH_GTL_DIR_amd_gfx90a environment variable?
","The above log messages indicate the type of image required by each device, given its current mode (amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack-) and the images found in the binary (hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+).",4.155568096782335
"What is the retention period for files in user home directories on Crusher?
","Retention - Period of time, post-account-deactivation or post-project-end, after which data will be marked as eligible for permanent deletion.

Important! Files within ""Work"" directories (i.e., Member Work, Project Work, World Work) are not backed up and are purged on a regular basis according to the timeframes listed above.

Footnotes

1

This entry is for legacy User Archive directories which contained user data on January 14, 2020. There is also a quota/limit of 2,000 files on this directory.

2",4.28963573242664
"What is the retention period for files in user home directories on Crusher?
","Home directories for each user are NFS-mounted on all OLCF systems and are intended to store long-term, frequently-accessed user data. User Home areas are backed up on a daily basis. This file system does not generally provide the input/output (I/O) performance required by most compute jobs, and is not available to compute jobs on most systems. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for more details on applicable quotas, backups, purge, and retention timeframes.",4.176768086492201
"What is the retention period for files in user home directories on Crusher?
","The user data retention policy exists to reclaim storage space after a user account is deactivated, e.g., after the user’s involvement on all OLCF projects concludes. By default, the OLCF will retain data in user-centric storage areas only for a designated amount of time after the user’s account is deactivated. During this time, a user can request a temporary user account extension for data access. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for details on retention timeframes for each user-centric storage area.",4.1738952686034345
"How can I view the logs of the Deployment using the oc command?
","You should now be on the 'Pod' screen with the 'Overview' tab selected From here you can get a quick idea of the amount of resources (memory, CPU etc) that your  Pod is using.

Click on the 'Logs' tab to get the logs from your pod. This will display ""Hello World!"" in our example because of our echo command. There will be a dropdown here that for our example will contain only one item named 'hello-openshift'. This is the name of the container that you are viewing the logs for inside your pod.",4.343706713340018
"How can I view the logs of the Deployment using the oc command?
","oc delete pod hello-world-pod

Deleting the pod will remove our ability to inspect the log output from oc logs so if you are debugging an issue you will want to keep the pod until the issue is resolved.

To create a single pod using the web console we will create from YAML

First, in the upper right-hand corner, click the + symbol. This can be used to add any YAML object from the web UI.

Add to Project

Make sure the project in the upper left-hand dropdown is set to the project in which you wish to deploy. Then paste this YAML into the box.",4.313943417257119
"How can I view the logs of the Deployment using the oc command?
","To roll back a deployment, run

oc rollout undo deploy/{NAME}

When using the web interface, you can view and edit a Deployment, from the sidebar, go to Applications, then Deployments.

Deployment Menu

You can get info on any deployment by clicking on it.

To edit this configuration, click Actions in the upper right hand corner, then Edit on whatever you wish to edit, or Edit Deployment if you'd rather edit the YAML directly.

Edit Deployment Config",4.258890003300421
"How do I update a Kubernetes resource using Slate?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",4.236927968061087
"How do I update a Kubernetes resource using Slate?
","This tutorial is a continuation of the https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#slate_guided_tutorial and you should start there.

You will be creating a single Pod in this tutorial which is not sufficient for a production service

Before using the CLI it would be wise to read our https://docs.olcf.ornl.gov/systems/guided_tutorial_cli.html#Getting Started on the CLI<slate_getting_started_oc> doc.",4.175859358920912
"How do I update a Kubernetes resource using Slate?
","This section will describe the project allocation process for Slate. This is applicable to both the Onyx and Marble OLCF OpenShift clusters.

In addition, we will go over the process to log into Onyx and Marble, and how namespaces work within the OpenShift context.

Please fill out the Slate Request Form in order to use Slate resources. This must be done in order to proceed.

If you have any question please contact User Assistance, via a Help Ticket Submission or by emailing help@olcf.ornl.gov.

Required information:

- Existing OLCF Project ID

- Project PI",4.154383838514024
"Can I store data on the local compute nodes on Frontier?
","Frontier is connected to Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Data will not be automatically transferred from Alpine to Orion. Frontier also has access to the center-wide NFS-based filesystem (which provides user and project home areas). Each compute node has two 1.92TB Non-Volatile Memory storage devices. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-data-storage for more",4.298206253260245
"Can I store data on the local compute nodes on Frontier?
","Each compute node on Frontier has [2x] 1.92TB Non-Volatile Memory (NVMe) storage devices (SSDs), colloquially known as a ""Burst Buffer"" with a peak sequential performance of 5500 MB/s (read) and 2000 MB/s (write). The purpose of the Burst Buffer system is to bring improved I/O performance to appropriate workloads. Users are not required to use the NVMes. Data can also be written directly to the parallel filesystem.



The NVMes on Frontier are local to each node.",4.289063534932689
"Can I store data on the local compute nodes on Frontier?
","On Frontier, there are two major types of nodes you will encounter: Login and Compute. While these are similar in terms of hardware (see: https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes), they differ considerably in their intended use.",4.2650561969099705
"How can I optimize the alignment of CPU and GPU on the node when running jobs on Crusher?
","Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.",4.344952448528427
"How can I optimize the alignment of CPU and GPU on the node when running jobs on Crusher?
","This section describes how to run programs on the Crusher compute nodes, including a brief overview of Slurm and also how to map processes and threads to CPU cores and GPUs.

Slurm is the workload manager used to interact with the compute nodes on Crusher. In the following subsections, the most commonly used Slurm commands for submitting, running, and monitoring jobs will be covered, but users are encouraged to visit the official documentation and man pages for more information.",4.245332700491418
"How can I optimize the alignment of CPU and GPU on the node when running jobs on Crusher?
","This may result in a sub-optimal alignment of CPU and GPU on the node, as shown in the example output. Unfortunately, at the moment there is not a workaround for this, however improvements are possible in future SLURM updates.

As mentioned previously, all GPUs are accessible by all MPI ranks by default, so it is possible to programatically map any combination of GPUs to MPI ranks. It should be noted however that Cray MPICH does not support GPU-aware MPI for multiple GPUs per rank, so this binding is not suggested.",4.232389765673597
"How can I specify the job name and output file in the job script?
","output to a file named RunSim123.#, where # is the job ID assigned by LSF | | 9 | Optional | Write standard error to a file named RunSim123.#, where # is the job ID assigned by LSF | | 10 |  | Blank line | | 11 |  | Change into one of the scratch filesystems | | 12 |  | Copy input files into place | | 13 |  | Run the date command to write a timestamp to the standard output file | | 14 |  | Run the executable on the allocated compute nodes | | 15 |  | Copy output files from the scratch area into a more permanent location |",4.172160018355454
"How can I specify the job name and output file in the job script?
","by a shell name. For example, to request an interactive batch job (with bash as the shell) equivalent to the sample batch script above, you would use the command: bsub -W 3:00 -nnodes 2048 -P ABC123 -Is /bin/bash",4.171053054066506
"How can I specify the job name and output file in the job script?
","cd $SLURM_SUBMIT_DIR
date
srun -n 8 ./a.out

This batch script shows examples of the three sections outlined above:

<string>:509: (INFO/1) Duplicate implicit target name: ""interpreter line"".

1: This line is optional and can be used to specify a shell to interpret the script. In this example, the bash shell will be used.

2: The job will be charged to the “XXXYYY” project.

3: The job will be named test.

4: The job will request (2) nodes.

5: The job will request (1) hour walltime.

<string>:526: (INFO/1) Duplicate implicit target name: ""shell commands"".",4.168192465869551
"What is the purpose of the ""jsrun"" command in the given scenario?
","This section describes tools that users might find helpful to better understand the jsrun job launcher.

hello_jsrun is a ""Hello World""-type program that users can run on Summit nodes to better understand how MPI ranks and OpenMP threads are mapped to the hardware. https://code.ornl.gov/t4p/Hello_jsrun A screencast showing how to use Hello_jsrun is also available: https://vimeo.com/261038849

Job Step Viewer provides a graphical view of an application's runtime layout on Summit. It allows users to preview and quickly iterate with multiple jsrun options to understand and optimize job launch.",4.372777740986479
"What is the purpose of the ""jsrun"" command in the given scenario?
",Below is a comparison table between srun and jsrun.,4.293747807852449
"What is the purpose of the ""jsrun"" command in the given scenario?
","While jsrun performs similar job launching functions as aprun and mpirun, its syntax is very different. A large reason for syntax differences is the introduction of the resource set concept. Through resource sets, jsrun can control how a node appears to each job. Users can, through jsrun command line flags, control which resources on a node are visible to a job. Resource sets also allow the ability to run multiple jsruns simultaneously within a node. Under the covers, a resource set is a cgroup.

At a high level, a resource set allows users to configure what a node look like to their job.",4.292192540553596
"How can I ensure that my job runs efficiently on Titan?
","strongly encourage users to run jobs on Titan that are as large as their

code will warrant. To that end, the OLCF implements queue policies that

enable large jobs to run in a timely fashion.



.. note::

The OLCF implements queue policies that encourage the

submission and timely execution of large, leadership-class jobs on

Titan.



The basic priority-setting mechanism for jobs waiting in the queue is

the time a job has been waiting relative to other jobs in the queue.

However, several factors are applied by the batch system to modify the",4.159724637674371
"How can I ensure that my job runs efficiently on Titan?
","would allow smaller, shorter jobs to use those otherwise idle resources,

and with the proper algorithm, the start time of the large job would not

be delayed. While this does make more effective use of the system, it

indirectly encourages the submission of smaller jobs.



The DOE Leadership-Class Job Mandate

------------------------------------



As a DOE Leadership Computing Facility, the OLCF has a mandate that a

large portion of Titan's usage come from large, *leadership-class* (aka

*capability*) jobs. To ensure the OLCF complies with DOE directives, we",4.122322357301948
"How can I ensure that my job runs efficiently on Titan?
","agreed to by the following persons as a condition of access to or use of

OLCF computational resources:



-  Principal Investigators (Non-Profit)

-  Principal Investigators (Industry)

-  All Users



**Title:** Titan Scheduling Policy **Version:** 13.02



In a simple batch queue system, jobs run in a first-in, first-out (FIFO)

order. This often does not make effective use of the system. A large job

may be next in line to run. If the system is using a strict FIFO queue,

many processors sit idle while the large job waits to run. *Backfilling*",4.113026861543686
"How can I optimize my Globus transfers for large files?
","transfers across all the users. Each user has a limit of 3 active transfers, so it is required to transfer a lot of data on each transfer than less data across many transfers.  If a folder is constituted with mixed files including thousands of small files (less than 1MB each one), it would be better to tar the small files.  Otherwise, if the files are larger, Globus will handle them.",4.405865295585159
"How can I optimize my Globus transfers for large files?
","When the installation has finished, click on the Globus icon and select Web: Transfer Files as below



Globus will ask you to login. If your institution does not have an organizational login, you may choose to either Sign in with Google or Sign in with ORCiD iD.



In the main Globus web page, select the two-panel view, then set the source and destination endpoints. (Left/Right order does not matter)



Next, navigate to the appropriate source and destination paths to select the files you want to transfer. Click the ""Start"" button to begin the transfer.",4.402918513400776
"How can I optimize my Globus transfers for large files?
","The OLCF users have access to a new functionality, using Globus to transfer files to HPSS through the endpoint ""OLCF HPSS"". Globus has restriction of 8 active transfers across all the users. Each user has a limit of 3 active transfers, so it is required to transfer a lot of data on each transfer than less data across many transfers. If a folder is constituted with mixed files including thousands of small files (less than 1MB each one), it would be better to tar the small files.  Otherwise, if the files are larger, Globus will handle them. To transfer the files, follow these steps:",4.3907584317711965
"How can I debug my application on Frontier using GDB?
",One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Frontier from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Frontier. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.,4.327305700175969
"How can I debug my application on Frontier using GDB?
",One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Frontier from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Frontier. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.,4.327305700175969
"How can I debug my application on Frontier using GDB?
","GDB, the GNU Project Debugger, is a command-line debugger useful for traditional debugging and investigating code crashes. GDB lets you debug programs written in Ada, C, C++, Objective-C, Pascal (and many other languages).

GDB is available on Summit under all compiler families:

module load gdb

To use GDB to debug your application run:

gdb ./path_to_executable

Additional information about GDB usage can befound on the GDB Documentation Page.",4.266507282201808
"How can I ensure that my job step uses the correct GPU driver on Frontier?
","Due to the unique architecture of Frontier compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.

Before jumping into the examples, it is helpful to understand the output from the hello_jobstep program:",4.26991511327104
"How can I ensure that my job step uses the correct GPU driver on Frontier?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.139006707023896
"How can I ensure that my job step uses the correct GPU driver on Frontier?
","When new users first attempt to run their application on Frontier, they often want to test with 1 MPI rank that has access to 7 CPU cores and 1 GPU. Although the job step used here is very similar to Example 1, the behavior is different:

$ OMP_NUM_THREADS=7 srun -N1 -n1 -c7 --gpus-per-task=1 --gpu-bind=closest ./hello_jobstep | sort",4.112889935926017
"Which vendor's compiler does not support OpenACC in Summit?
","The following compilers are available on Summit:

XL: IBM XL Compilers (loaded by default)

LLVM: LLVM compiler infrastructure

PGI: Portland Group compiler suite

NVHPC: Nvidia HPC SDK compiler suite

GNU: GNU Compiler Collection

NVCC: CUDA C compiler

PGI was bought out by Nvidia and have rebranded their compilers, incorporating them into the NVHPC compiler suite. There will be no more new releases of the PGI compilers.",4.329574765384512
"Which vendor's compiler does not support OpenACC in Summit?
","This section shows how to compile code with OpenACC. Currently only the Cray compiler supports OpenACC for Fortran. The AMD and GNU programming environments do not support OpenACC at all. C and C++ support for OpenACC is provided by clacc which maintains a fork of the LLVM compiler with added support for OpenACC. It can be obtained by loading the UMS modules ums, ums025, and clacc.",4.23205990719799
"Which vendor's compiler does not support OpenACC in Summit?
","| Vendor | Module | OpenACC Support | Enable OpenACC | | --- | --- | --- | --- | | IBM | xl | NONE | NONE | | GNU | system default | NONE | NONE | | GNU | gcc | 2.5 | -fopenacc | | LLVM | clang or xlflang | NONE | NONE | | PGI | pgi | 2.5 | -acc, -ta=nvidia:cc70 | | NVHPC | nvhpc | 2.5 | -acc=gpu -gpu=cc70 |

CUDA C/C++ support is provided through the cuda module or throught the nvhpc module.

nvcc : Primary CUDA C/C++ compiler

Language support

-std=c++11 : provide C++11 support

--expt-extended-lambda : provide experimental host/device lambda support",4.207758377850049
"How do I know if DDT is the right debugging tool for my application on Frontier?
",One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Frontier from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Frontier. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.,4.542501193321561
"How do I know if DDT is the right debugging tool for my application on Frontier?
",One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Frontier from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Frontier. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.,4.542501193321561
"How do I know if DDT is the right debugging tool for my application on Frontier?
",One of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Andes from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Andes. For guidance in setting up the remote client see the https://docs.olcf.ornl.gov/software/debugging/index.html page.,4.305069802576631
"Is the GCC compiler available on Frontier the same as the system/OS version of GCC?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers. Please see the following https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more detailed inforation on how to compile using these modules.",4.463705677205293
"Is the GCC compiler available on Frontier the same as the system/OS version of GCC?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers.

It is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) whenever possible. See the next section for more details.",4.4303441814695255
"Is the GCC compiler available on Frontier the same as the system/OS version of GCC?
","See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for information on compiling for AMD GPUs, and see the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks section for some detailed information to keep in mind to run more efficiently on AMD GPUs.

Frontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.",4.295419316437473
"Can I use any Swift code with Swift/T?
","Swift/T is a completely new implementation of the Swift language for high-performance computing which translates Swift scripts into MPI programs that use the Turbine (hence, /T) and ADLB runtime libraries. This tutorial shows how to get up and running with Swift/T on Summit specifically. For more information about Swift/T, please refer to its documentation.

Swift/T is available as a module on Summit, and it can be loaded as follows:

$ module load workflows
$ module load swift/1.5.0

You will also need to set the PROJECT environment variable:

$ export PROJECT=""ABC123""",4.255787042913241
"Can I use any Swift code with Swift/T?
","$ export PROJECT=""ABC123""

To run an example ""Hello world"" program with Swift/T on Summit, create a file called hello.swift with the following contents:

trace(""Hello world!"");

Now, run the program from a shell or script:

$ swift-t -m lsf hello.swift

The output should look something like the following:",4.169751845272409
"Can I use any Swift code with Swift/T?
","The suggested directory structure is to have a outer directory say swift-work that has the swift source and shell scripts. Inside of swift-work create a new directory called data.

Additionally, we will need two terminals open. In the first terminal window, navigate to the swift-work directory and invoke the data generation script like so:

$ ./gendata.sh

In the second terminal, we will run the swift workflow as follows (make sure to change the project name per your allocation):",3.967576483315837
"How can I check if my quantum software is compatible with the versions of LAPACK, libffi, ZMQ, and Forest SDK that I have installed?
","Newer: https://github.com/libffi/libffi/releases/

ZMQ download: https://github.com/zeromq/libzmq/releases

Forest SDK download: https://qcs.rigetti.com/sdk-downloads

Below are example instructions for installing the above packages into your $HOME directory. Versions may vary.

Newer versions than those used in the install instructions below are known to work on Andes; however, on Frontier, newer versions of libffi than 3.2.1 are known to cause problems.

Andes

.. code-block:: bash

    $ module load gcc cmake

Frontier

.. code-block:: bash",4.311428784278498
"How can I check if my quantum software is compatible with the versions of LAPACK, libffi, ZMQ, and Forest SDK that I have installed?
","Before trying to run the code example below, remember to set the relevant PATHs to your ffi, zmq, lapack, and forest-sdk installations if you have not already exported them (outlined above).

Below is a simple code to test if packages installed properly. Context for this example: https://pyquil-docs.rigetti.com/en/latest/start.html#run-your-first-program

from pyquil import get_qc, Program
from pyquil.gates import H, CNOT, MEASURE
from pyquil.quilbase import Declare",4.292527098020809
"How can I check if my quantum software is compatible with the versions of LAPACK, libffi, ZMQ, and Forest SDK that I have installed?
","https://docs.rigetti.com/qcs/getting-started/installing-locally

The bare-bones installation only contains the executable binaries and manual pages, and doesn’t contain any of the requisite dynamic libraries. As such, installation doesn’t require administrative or sudo privileges. This method of installation requires one, through whatever means, to install shared libraries for BLAS, LAPACK, libffi, and libzmq3. Some download methods are listed here:

Lapack (with BLAS) download: http://www.netlib.org/lapack/

libffi download:

Older versions: https://sourceware.org/ftp/libffi/",4.215821783058773
"Can I suspend a job that is currently running on Summit?
","If you submit a job to a ""normal"" Summit queue while on Citadel, such as -q batch, your job will be unable to launch.",4.256150748440815
"Can I suspend a job that is currently running on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.245469330729509
"Can I suspend a job that is currently running on Summit?
","LSF supports user-level suspension and resumption of jobs. Jobs are suspended with the bstop command and resumed with the bresume command. The simplest way to invoke these commands is to list the job id to be suspended/resumed:

bstop 12345
bresume 12345

Instead of specifying a job id, you can specify other criteria that will allow you to suspend some/all jobs that meet other criteria such as a job name, a queue name, etc. These are described in the manpages for bstop and bresume.",4.182781155594666
"How do I download the OpenShift CLI tool?
","It is a single binary that can be downloaded from a number of places (the choice is yours):

Direct from the cluster (preferred):

Marble Command Line Tools

Onyx Command Line Tools

Homebrew on MacOS (need Homebrew setup first):

The Homebrew package is not always kept up to date with the latest version of OpenShift so some client features may not be available

$ brew install openshift-cli

RHEL/CentOS (requires openshift-origin repo):

$ yum install origin-clients

From Source

https://github.com/openshift/oc",4.486013796419051
"How do I download the OpenShift CLI tool?
","Now that you have a project you can create objects inside that project. We will be doing this with the Openshift Web GUI and the oc CLI client so you can use whichever interface you are more comfortable with in this tutorial. If you are more comfortable using the command line than you are using a GUI you can now https://docs.olcf.ornl.gov/systems/guided_tutorial.html#jump to the oc portion of this document<slate_guided_tutorial_cli>. Otherwise, continue with the GUI based tutorial below.

Go to https://console-openshift-console.apps.<cluster>.ccs.ornl.gov/k8s/cluster/projects",4.204736896913068
"How do I download the OpenShift CLI tool?
","Openshift will automatically pull and build your source. If you make a change and need an updated build simply navigate to the build option on the menu to the left and select your project and click Start Build in the upper right.

The first step to creating the the build is to create a build config. This is done in one of three ways depending on how your code is structured. If you have a git repository already configured and it is public then the command

oc new-build .

will create your build config from that repository.",4.192626320789486
"How do I start the vncserver on Andes?
","Desktop 'TurboVNC: andes-gpu5.olcf.ornl.gov:1 (userA)' started on display andes-g                                                                             pu5.olcf.ornl.gov:1

Starting applications specified in /ccs/home/userA/.vnc/xstartup.turbovnc
Log file is /ccs/home/userA/.vnc/andes-gpu5.olcf.ornl.gov:1.log

**************************************************************************
Instructions",4.430142129119814
"How do I start the vncserver on Andes?
","Desktop 'TurboVNC: andes-gpu5.olcf.ornl.gov:1 (userA)' started on display andes-g                                                                             pu5.olcf.ornl.gov:1

Starting applications specified in /ccs/home/userA/.vnc/xstartup.turbovnc
Log file is /ccs/home/userA/.vnc/andes-gpu5.olcf.ornl.gov:1.log

**************************************************************************
Instructions",4.430142129119814
"How do I start the vncserver on Andes?
","localsytem: ssh -X username@andes.olcf.ornl.gov

andes: salloc -A abc123 -N 1 -t 1:00:00 --x11=batch

andes: ./matlab-vnc.sh

$ ./matlab-vnc.sh

Starting vncserver

Desktop 'TurboVNC: andes79.olcf.ornl.gov:1 (userA)' started on display andes79.olcf.ornl.gov:1

Starting applications specified in /ccs/home/userA/.vnc/xstartup.turbovnc
Log file is /ccs/home/userA/.vnc/andes79.olcf.ornl.gov:1.log

**************************************************************************
Instructions",4.413099835667408
"How do I compile with OpenMP Offload using the AMD compiler?
","This section shows how to compile with OpenMP Offload using the different compilers covered above.

Make sure the craype-accel-amd-gfx90a module is loaded when using OpenMP offload.

| Vendor | Module | Language | Compiler | OpenMP flag (GPU) | | --- | --- | --- | --- | --- | | Cray | cce | C C++ |  | -fopenmp | | Fortran | ftn (wraps crayftn) |  | | AMD | amd |  |  | -fopenmp |",4.508827611495846
"How do I compile with OpenMP Offload using the AMD compiler?
","| Vendor | Module | Language | Compiler | OpenMP flag (CPU thread) | | --- | --- | --- | --- | --- | | Cray | cce | C, C++ |  | -fopenmp | | Fortran | ftn (wraps crayftn) |  | | AMD | rocm |  |  | -fopenmp | | GCC | gcc |  |  | -fopenmp |

This section shows how to compile with OpenMP Offload using the different compilers covered above.

Make sure the craype-accel-amd-gfx90a module is loaded when using OpenMP offload.",4.474247966862524
"How do I compile with OpenMP Offload using the AMD compiler?
","If invoking amdclang, amdclang++, or amdflang directly for openmp offload, or using hipcc you will need to add:  -fopenmp -target x86_64-pc-linux-gnu -fopenmp-targets=amdgcn-amd-amdhsa -Xopenmp-target=amdgcn-amd-amdhsa -march=gfx90a.",4.419490432293781
"How do I monitor the status of my workflows in FireWorks?
","FireWorks is a free, open-source tool for defining, managing, and executing workflows. Complex workflows can be defined using Python, JSON, or YAML, are stored using MongoDB, and can be monitored through a built-in web interface. Workflow execution can be automated over arbitrary computing resources, including those that have a queueing system. FireWorks has been used to run millions of workflows encompassing tens of millions of CPU-hours across diverse application areas and in long-term production projects over the span of multiple years.",4.306783718129717
"How do I monitor the status of my workflows in FireWorks?
","To learn more about FireWorks, please refer to its extensive online documentation.

Before using FireWorks itself, you will need a MongoDB service running on https://docs.olcf.ornl.gov/systems/fireworks.html#Slate<slate>. A tutorial to deploy MongoDB is available in the Slate documentation.

You will need to know the connection information for both MongoDB so that FireWorks can be configured to connect to it.

Then, to use FireWorks on Summit, load the module as shown below:

$ module load workflows
$ module load fireworks/2.0.2",4.159395868023358
"How do I monitor the status of my workflows in FireWorks?
","module load workflows
module load fireworks/2.0.2

# Edit the following line to match your own MongoDB connection string.
export MONGODB_URI=""mongodb://admin:password@apps.marble.ccs.ornl.gov:32767/test""

jsrun -n 1 python3 demo.py

Finally, submit the batch job to LSF by executing the following command from a Summit login node:

$ bsub fireworks_demo.lsf

Congratulations! Once the batch job completes, you will find new directories beginning with launcher_ and containing FW.json files that detail exactly what happened.",4.137305268713294
"How can I customize the installation of the GitLab Runner helm chart on Slate?
","To deploy a GitLab Runner from the Developer Perspective, navigate to ""Topology"" -> ""Helm Chart"" and select ""GitLab Runner vX.X.X Provided by Slate Helm Charts"". From the new window, select ""Install Helm Chart"".

On the resulting screen, one can customize the installation of the GitLab Runner helm chart. The chart has been forked from upstream to allow for customized deployment to Slate. From the Form View, expand the ""GitLab Runner"" fields. Using the registration token retrieved in the prior section, enter token for adding the runner to the GitLab server in the empty field.",4.588642171708771
"How can I customize the installation of the GitLab Runner helm chart on Slate?
","Jenkins

OpenShift Pipelines

GitLab Runners

Deploying a GitLab Runner into a Slate project may be found in the https://docs.olcf.ornl.gov/systems/overview.html#slate_gitlab_runners document which leverages a localized Slate Helm Chart. The GitLab Pipelines documentation as well as GitLab CI/CD Examples provide more details on GitLab Runner capabilities and usage.

Jenkins

For Jenkins deployment, a Slate Helm Chart is planned to be released. Documentation concerning Jenkins Pipelines as well as Jenkins Pipeline Tutorials are available.

OpenShift Pipelines",4.37700853970888
"How can I customize the installation of the GitLab Runner helm chart on Slate?
","GitLab CI/CD jobs may be accomplished using the GitLab Runner application. An open-source application written in Go, a GitLab Runner may be installed with a variety of executors. This documentation focuses on the installation and configuration for use of the GitLab Runner Kubernetes Executor on Slate.

https://docs.gitlab.com/runner/

https://docs.gitlab.com/runner/executors/kubernetes.html

https://docs.gitlab.com/runner/install/kubernetes.html",4.2549494360852655
"What are some common values for alloc_flags?
","The -alloc_flags option to bsub is used to set allocation-wide options. These settings are applied to every compute node in a job. Only one instance of the flag is accepted, and multiple alloc_flags values should be enclosed in quotes and space-separated. For example, -alloc_flags ""gpumps smt1.

The most common values (smt{1,2,4}, gpumps, gpudefault) are detailed in the following sections.

This option can also be used to provide additional resources to GPFS service processes, described in the GPFS System Service Isolation section.",4.273756498293425
"What are some common values for alloc_flags?
","In ROCm-5.1 and earlier versions, the flag -munsafe-fp-atomics is interpreted as a suggestion by the compiler, whereas from ROCm-5.2 the flag will always enforce the use of fast hardware-based FP atomics.

The following tables summarize the result granularity of various combinations of allocators, flags and arguments.

For hipHostMalloc(), the following table shows the nature of the memory returned based on the flag passed as argument.",4.17784387982939
"What are some common values for alloc_flags?
","In ROCm-5.1 and earlier versions, the flag -munsafe-fp-atomics is interpreted as a suggestion by the compiler, whereas from ROCm-5.2 the flag will always enforce the use of fast hardware-based FP atomics.

The following tables summarize the result granularity of various combinations of allocators, flags and arguments.

For hipHostMalloc(), the following table shows the nature of the memory returned based on the flag passed as argument.",4.17784387982939
"How can I share my environment with others?
","Exporting (sharing) an environment:

You may want to share your environment with someone else. One way to do this is by creating your environment in a shared location where other users can access it. A different way (the method described below) is to export a list of all the packages and versions of your environment (an environment.yml file). If a different user provides conda the list you made, conda will install all the same package versions and recreate your environment for them -- essentially ""sharing"" your environment. To export your environment list:",4.344466423706825
"How can I share my environment with others?
","To recreate this environment in a virtual setting, we will be using Zoom + Slack. Zoom will be used as the main online tool due to its breakout room capabilities; there will be a single Zoom session, where the main room will be used for presentations, and breakout rooms will be used by individual teams (for screen sharing and verbal communication). We will also set up a Slack workspace for communication between all participants, and individual team channels for communication within teams (chat, sharing code snippets, etc.).

<p style=""font-size:20px""><b>Ok, so how do I attend?</b></p>",4.097927838318385
"How can I share my environment with others?
","It is highly recommended to create new environments in the ""Project Home"" directory (on Summit, this is /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>). This space avoids purges, allows for potential collaboration within your project, and works better with the compute nodes. It is also recommended, for convenience, that you use environment names that indicate the hostname, as virtual environments created on one system will not necessarily work on others.",4.076453550215792
"How do I create a filter file in Score-P?
","$ export SCOREP_FILTERING_FILE=scorep.filter

Now you are ready to submit your instrumented code to run with tracing enabled. This measurement will generate files of the form traces.otf. The .otf2 file format can be analyzed by a tool called Vampir .

<string>:16: (INFO/1) Duplicate explicit target name: ""vampir"".

Vampir provides a visual GUI to analyze the .otf2 trace file generated with Score-P.",4.235668791895434
"How do I create a filter file in Score-P?
","$ cat scorep.filter
SCOREP_REGION_NAMES_BEGIN
 Exclude
   matmul_sub
   matvec_sub
SCOREP_REGION_NAMES_END

One can check the effects of the filter by re-running the scorep-score command:

$ scorep-score <profile cube dir>/profile.cubex -f scorep.filter

To apply the filter to your measurement run, you must specify this in an environment variable called SCOREP_FILTERING_FILE:

$ export SCOREP_FILTERING_FILE=scorep.filter",4.229386989650824
"How do I create a filter file in Score-P?
","$ export SCOREP_ENABLE_TRACING=true

Since tracing measurements acquire significantly more output data than profiling, we need to design a filter to remove some of the most visited calls within your instrumented code. There is a tool developed by Score-P that allows us to estimate the size of the trace file (OTF2) based on information attained from the profiling generated cube file.

To gather the needed information to design a filter file, first run scorep-score:

$ scorep-score -r <profile cube dir>/profile.cubex",4.19354228420529
"What is the pytket package?
","In python, the pytket package is available for python 3.8+. The pytket and pytket-quantinuum packages are included as part of the installation instructions on Quantinuum's User Portal.

For more information on TKET, see the following links:

TKET documentation is available at https://cqcl.github.io/pytket/manual/manual_intro.html

An introduction to quantum compilation with TKET is available at https://github.com/CalMacCQ/tket_blog/blob/main/blog1_intro_to_qc.ipynb

For a video introduction to TKET, see https://www.youtube.com/watch?v=yXKSpvgAtrk",4.4473901282952335
"What is the pytket package?
","The Ensemble Toolkit (EnTK) is a Python library developed by the RADICAL Research Group at Rutgers University for developing and executing large-scale ensemble-based workflows. This tutorial shows how to get up and running with EnTK 1.13.0 on Summit specifically. For in-depth information about EnTK itself, please refer to its documentation.

Before using EnTK itself, you will need MongoDB and RabbitMQ services running on https://docs.olcf.ornl.gov/systems/entk.html#Slate<slate>. There are tutorials for MongoDB in this documentation, but the tutorial for RabbitMQ is forthcoming.",4.032449509106631
"What is the pytket package?
","Due to hardware emulation complexity, jobs using 29-32 qubits are likely to experience significantly slowed execution times.

The TKET framework is a software platform for the development and execution of gate-level quantum computation, providing state-of-the-art performance in circuit compilation. It was created and is maintained by Quantinuum. The toolset is designed to extract the most out of the available NISQ devices of today and is platform-agnostic.",3.9759225537073983
"What is the purpose of the srun command?
",The following srun options will be used in the examples below. See man srun for a complete list of options and more information.,4.460366346060972
"What is the purpose of the srun command?
",The following srun options will be used in the examples below. See man srun for a complete list of options and more information.,4.460366346060972
"What is the purpose of the srun command?
","Using srun

By default, commands will be executed on the job’s primary compute node, sometimes referred to as the job’s head node. The srun command is used to execute an MPI binary on one or more compute nodes in parallel.

srun accepts the following common options:

| -N | Minimum number of nodes | | --- | --- | | -n | Total number of MPI tasks | | --cpu-bind=no | Allow code to control thread affinity | | -c | Cores per MPI task | | --cpu-bind=cores | Bind to cores |

If you do not specify the number of MPI tasks to srun via -n, the system will default to using only one task per node.",4.395284579063464
"How can I update the label ""date"" in the Deployment update command?
","Note that in this example, I am updating the Deployment and setting the image of the container named containername and --source=istag says I am using a ImageStream tag. The ImageStream in my OpenShift project stf002 is stf002/my-image:mytag.

# Update an existing deployment called my-application
oc set image deploy/my-application containername=stf002/my-image:mytag --source=istag

# Create a new deployment with the container
oc new-app -i my-image",4.004898326860443
"How can I update the label ""date"" in the Deployment update command?
","As with other objects in OpenShift, you can create this Deployment with oc create -f {FILE_NAME}.

Once created, you can check the status of the ongoing deployment process with the command

oc rollout status deploy/{NAME}

To get basic information about the available revisions, if you had done any updates to the deployment since, run

oc rollout history deploy/{NAME}

You can view a specific revision with

oc rollout history deploy/{NAME} --revision=1

For more detailed information about a Deployment, use

oc describe deploy {NAME}

To roll back a deployment, run",3.953571221063521
"How can I update the label ""date"" in the Deployment update command?
","The app label now appears in each of the generated resources- configMap, Deployment, and service. Looking at the `kustomization.yaml file for the staging environement:

$ cat overlays/staging/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
metadata:
  name: staging-arbitrary
namePrefix: staging-
commonLabels:
  variant: staging
  org: acmeCorporation
commonAnnotations:
  note: Hello, I am staging!
resources:
- ../../base
patchesStrategicMerge:
- map.yaml",3.931008481830285
"What is the size of each register in Frontier's CU?
",registers where each register is 64 4-byte wide entries.,4.152501028617037
"What is the size of each register in Frontier's CU?
","Each CU has 4 Matrix Core Units (the equivalent of NVIDIA's Tensor core units) and 4 16-wide SIMD units. For a vector instruction that uses the SIMD units, each wavefront (which has 64 threads) is assigned to a single 16-wide SIMD unit such that the wavefront as a whole executes the instruction over 4 cycles, 16 threads per cycle. Since other wavefronts occupy the other three SIMD units at the same time, the total throughput still remains 1 instruction per cycle. Each CU maintains an instructions buffer for 10 wavefronts and also maintains 256 registers where each register is 64 4-byte wide",4.111416968495659
"What is the size of each register in Frontier's CU?
","The 2 GCDs on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The OLCF Director's Discretionary (DD) program allocates approximately 10% of the available Frontier hours in a calendar year. Frontier is allocated in *node* hours, and a typical DD project is awarded between 15,000 - 20,000 *node* hours. For more information about Frontier, please visit the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#frontier-user-guide.",4.087326971142493
"What are the requirements for using Tensor Cores in Summit?
",| (recording) https://vimeo.com/306440151 | | 2018-12-04 | Using V100 Tensor Cores | Jeff Larkin (NVIDIA) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Tensor-Cores.pdf https://vimeo.com/306437682 | | 2018-12-04 | NVIDIA Profilers | Jeff Larkin (NVIDIA) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Profilers.pdf,4.346368036231705
"What are the requirements for using Tensor Cores in Summit?
",section provides information for using the V100 Tensor Cores.,4.31240387131366
"What are the requirements for using Tensor Cores in Summit?
",| (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Tensor-Cores.pdf https://vimeo.com/346452359 | | 2019-02-12 | NVIDIA Profilers | Jeff Larkin (NVIDIA) | Summit Training Workshop (February 2019) https://www.olcf.ornl.gov/calendar/summit-training-workshop-february-2019/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Libraries.pdf https://vimeo.com/346452291 | | 2019-02-12 | GPU-Accelerated Libraries | Jeff Larkin (NVIDIA) | Summit Training Workshop (February 2019),4.3097372578477104
"How can I set the SSH path in Paraview?
","<Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Windows"">
            <Set name=""TERM_PATH"" value=""cmd"" />
            <Set name=""TERM_ARG1"" value=""/C"" />
            <Set name=""TERM_ARG2"" value=""start"" />
            <Set name=""TERM_ARG3"" value="""" />
            <Set name=""SSH_PATH"" value=""plink.exe"" />
          </Case>
          <Case value=""Unix"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.18639056001136
"How can I set the SSH path in Paraview?
","<Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Windows"">
            <Set name=""TERM_PATH"" value=""cmd"" />
            <Set name=""TERM_ARG1"" value=""/C"" />
            <Set name=""TERM_ARG2"" value=""start"" />
            <Set name=""TERM_ARG3"" value="""" />
            <Set name=""SSH_PATH"" value=""plink.exe"" />
          </Case>
          <Case value=""Unix"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.18639056001136
"How can I set the SSH path in Paraview?
","After installing, you must give ParaView the relevant server information to be able to connect to OLCF systems (comparable to VisIt's system of host profiles). The following provides an example of doing so. Although several methods may be used, the one described should work in most cases.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.

For Windows clients, it is necessary to install PuTTY to create an ssh connection in step 2.

Step 1: Save the following servers.pvsc file to your local computer",4.177514484593649
"What happens if I set the minReadySeconds field to a value greater than 0?
","All of the below strategies use readiness checks to determine if a new pod is ready for use. If any readiness check fails, the deployment configuration will continue to try to run the pod until it times out. The default timeout is 10m. This value can be set in deployment.spec.strategy.params.TimeoutSeconds

The default strategy, if omitted, is RollingUpdate",3.892480302892943
"What happens if I set the minReadySeconds field to a value greater than 0?
","To prevent reserved resources from remaining idle for an extended period of time, reservations are monitored for inactivity. If activity falls below 50% of the reserved resources for more than (30) minutes, the reservation will be canceled and the system will be returned to normal scheduling. A new reservation must be requested if this occurs.",3.888509222738322
"What happens if I set the minReadySeconds field to a value greater than 0?
","maxSurge and maxUnavailable - maxUnavailable is the maximum number of pods that can be unavailable during the update. maxSurge is the number of pods that can be scheduled above the original number of pods. Both values can be set to either a percentage (20%) or a positive integer (2). The default value for both is 25%.

These values can be used to tune a deployment for speed or availability. If you want to maintain full capacity, set maxUnavailable to 0. The maxSurge value can be used to speed up the scale up. Note that you still must stay below your project's pod quota.",3.862993828631853
"What is the importance of loading modules in the correct order?
","Modules are changed recursively. Some commands, such as module swap, are available to maintain compatibility with scripts using Tcl Environment Modules, but are not necessary since Lmod recursively processes loaded modules and automatically resolves conflicts.",4.087919125825776
"What is the importance of loading modules in the correct order?
","Modules are changed recursively. Some commands, such as module swap, are available to maintain compatibility with scripts using Tcl Environment Modules, but are not necessary since Lmod recursively processes loaded modules and automatically resolves conflicts.",4.087919125825776
"What is the importance of loading modules in the correct order?
","Users will be advised to do a module load <UMS project>, which will expose modules for the individual products associated with that project, accessed by a second module load <product>.

Project PI must ensure that support is provided for users of the product, as documented in the statement of support.

Project PI must ensure that the product is updated in response to changes in the system software environment (e.g. updates to OS, compiler, library, or other key tools) and in a timely fashion.  If this commitment is not met, the OLCF may remove the software from UMS.",4.078008621785335
"Can users apply for a user account using the myOLCF self-service portal?
",Join an Additional Project Existing users with RSA SecurID tokens should log in to the myOLCF self-service portal to apply for additional projects. Existing users without RSA SecurID tokens should fill out the Account Application Form that can be found at the top left of the myOLCF login page without needing to sign in. See the section https://docs.olcf.ornl.gov/systems/documents_and_forms.html#applying-for-a-user-account for complete details.,4.500972148384945
"Can users apply for a user account using the myOLCF self-service portal?
","You can check the general status of your application at any time using the myOLCF self-service portal's account status page. For more information, see the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#myOLCF self-service portal documentation<myolcf-overview>. If you need to make further inquiries about your application, you may email our Accounts Team at accounts@ccs.ornl.gov.",4.456242345365951
"Can users apply for a user account using the myOLCF self-service portal?
","Please see the https://docs.olcf.ornl.gov/systems/index.html#OLCF Applying for a User Account<applying-for-a-user-account> section of this site to request a new account and join an existing project.  Once submitted, the OLCF Accounts team will help walk you through the process.",4.4529963104576655
"Where can I view the status of my file transfer in Globus?
","When the installation has finished, click on the Globus icon and select Web: Transfer Files as below



Globus will ask you to login. If your institution does not have an organizational login, you may choose to either Sign in with Google or Sign in with ORCiD iD.



In the main Globus web page, select the two-panel view, then set the source and destination endpoints. (Left/Right order does not matter)



Next, navigate to the appropriate source and destination paths to select the files you want to transfer. Click the ""Start"" button to begin the transfer.",4.410785670080014
"Where can I view the status of my file transfer in Globus?
","Click in the right side “Path” box and enter the path where you want to put your data on Orion, for example, /lustre/orion/stf007/proj-shared/my_orion_data

Click the left ""Start"" button.

Click on “Activity“ in the left blue menu bar to monitor your transfer. Globus will send you an email when the transfer is complete.

<string>:5: (INFO/1) Enumerated list start value not ordinal-1: ""2"" (ordinal 2)",4.344123278163328
"Where can I view the status of my file transfer in Globus?
","Visit www.globus.org and login



Then select the organization that you belong, if you don't work for ORNL, do not select ORNL. If your organization is not in the list, create a Globus account



Search for the endpoint OLCF DTN





Declare path



Open a second panel to declare the new endpoint called OLCF HPSS and use the appropriate path for HPSS





Select your file/folder and click start. hen an activity report will appear and you can click on it to see the status. When the transfer is finished or failed, you will receive an email",4.259604941013722
"What is the advantage of using the ROCm Toolchain in compiling HIP kernels?
","hipcc requires the ROCm Toolclain, See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#exposing-the-rocm-toolchain-to-your-programming-environment





Computational work on Frontier is performed by jobs. Jobs typically consist of several componenets:

A batch submission script

A binary executable

A set of input files for the executable

A set of output files created by the executable

In general, the process for running a job is to:

Prepare executables and input files.

Write a batch script.

Submit the batch script to the batch scheduler.",4.234358046269929
"What is the advantage of using the ROCm Toolchain in compiling HIP kernels?
","The following modules help you expose the ROCm Toolchain to your programming Environment:

| Programming Environment Module | Module that gets you ROCm Toolchain | How you load it: | | --- | --- | --- | | PrgEnv-amd | amd | amd  is loaded automatically with module load PrgEnv-amd | | PrgEnv-cray or PrgEnv-gnu | amd-mixed | module load amd-mixed |",4.199491169845465
"What is the advantage of using the ROCm Toolchain in compiling HIP kernels?
","As mentioned above, HIP can be used on systems running on either the ROCm or CUDA platform, so OLCF users can start preparing their applications for Frontier today on Summit. To use HIP on Summit, you must load the HIP module:

$ module load hip-cuda

CUDA 11.4.0 or later must be loaded in order to load the hip-cuda module. hipBLAS, hipFFT, hipSOLVER, hipSPARSE, and hipRAND have also been added to hip-cuda.",4.197367260165514
"How can I add a label to a specific resource in Slate?
","The meta information in the kustomization.yaml illustrates what is referred to as a cross-cutting field. In this case, the commonLabels block adds a label app: hello which will be included in all of the resources specified in the resource files. Cross-cutting fields could also be used to set the namespace (namespace) for the resources to be created in, add a prefix (namePrefix) or suffix (nameSuffix) to all resource names, or add a set of annotations (commonAnnotations).

To see the results of the commonLabels field, the kustomize build command will display the output for inspection:",4.067139342453307
"How can I add a label to a specific resource in Slate?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",4.032903344406293
"How can I add a label to a specific resource in Slate?
","The app label now appears in each of the generated resources- configMap, Deployment, and service. Looking at the `kustomization.yaml file for the staging environement:

$ cat overlays/staging/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
metadata:
  name: staging-arbitrary
namePrefix: staging-
commonLabels:
  variant: staging
  org: acmeCorporation
commonAnnotations:
  note: Hello, I am staging!
resources:
- ../../base
patchesStrategicMerge:
- map.yaml",4.011002328395707
"How do I access the output of my MLflow job on Summit?
","MLflow is an open source platform to manage the Machine Learning (ML) lifecycle, including experimentation, reproducibility, deployment, and a central model registry. To learn more about MLflow, please refer to its documentation.

In order to use MLflow on Summit, load the module as shown below:

$ module load workflows
$ module load mlflow/1.22.0

Run the following command to verify that MLflow is available:

$ mlflow --version
mlflow, version 1.22.0

To run this MLflow demo on Summit, you will create a directory with two files and then submit a batch job to LSF from a Summit login node.",4.372330504086401
"How do I access the output of my MLflow job on Summit?
","To access Summit and Frontier's compute resources for SPI workflows, you must first log into a https://docs.olcf.ornl.gov/systems/index.html#Citadel login node<citadel-login-nodes> and then submit a batch job to one of the SPI specific batch queues.",4.163513613081216
"How do I access the output of my MLflow job on Summit?
","Finally, run the demo program by executing the following commands from a Summit login node:

$ source setup.bash
$ python3 demo.py3

Congratulations! You should now see interactive output from EnTK while it launches and monitors your job on Summit.",4.13307613345555
"Can I run MinIO on an isolated volume dedicated to the MinIO server?
","MinIO running on a dedicated volume, allocated automatically from the NetApp storage server, isolated to the MinIO server.

It is important to note that we are also launching MinIO in standalone mode, which is a single MinIO server instance. MinIO also supports distributed mode for more robust implementations, but we are not setting that up in this example.

<string>:5: (INFO/1) Duplicate explicit target name: ""user assistance"".",4.391786393357451
"Can I run MinIO on an isolated volume dedicated to the MinIO server?
","Depending on you how configured your deployment, this could be your NFS or GPFS project space or an isolated volume dedicated/isolated to this MinIO server.

Within the GUI you can create buckets and upload/download data. If you are running this on NFS or GPFS the bucket will map to a directory.",4.307266714025783
"Can I run MinIO on an isolated volume dedicated to the MinIO server?
","MinIO is a high-performance software-defined object storage suite that enables the ability to easily deploy cloud-native data infrastructure for various data workloads. In this example we are deploying a simple, standalone implementation of MinIO on our cloud-native platform, Slate (https://docs.olcf.ornl.gov/systems/minio.html#slate_overview).

This service will only be accessible from inside of ORNL's network.

If your project requires an externally facing service available to the Internet, please contact User Assistance by submitting a help ticket. There is a process to get such approval.",4.213331697474722
"How can I build a container image with Podman on Summit?
","Podman is a container framework from Red Hat that functions as a drop in replacement for Docker. On Summit, we will use Podman for building container images and converting them into tar files for Singularity to use. Podman makes use of Dockerfiles to describe the images to be built. We cannot use Podman to run containers as it doesn't properly support MPI on Summit, and Podman does not support storing its container images on GPFS or NFS.",4.534149350253824
"How can I build a container image with Podman on Summit?
","Users can build container images with Podman, convert it to a SIF file, and run the container using the Singularity runtime. This page is intended for users with some familiarity with building and running containers.

Users will make use of two applications on Summit - Podman and Singularity - in their container build and run workflow. Both are available without needing to load any modules.",4.506984296613206
"How can I build a container image with Podman on Summit?
","Users will be building and running containers on Summit without root permissions i.e. containers on Summit are rootless.  This means users can get the benefits of containers without needing additional privileges. This is necessary for a shared system like Summit. And this is part of the reason why Docker doesn't work on Summit. Podman and Singularity provides rootless support but to different extents hence why users need to use a combination of both.

Users will need to set up a file in their home directory /ccs/home/<username>/.config/containers/storage.conf with the following content:",4.422311772333431
"How can I search for module files containing a specific string in Frontier?
","Searching for modules

Modules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the spider sub-command can be used as summarized in the following table.",4.053694110989753
"How can I search for module files containing a specific string in Frontier?
","<p style=""font-size:20px""><b>Frontier: Darshan Runtime 3.4.0 (May 10, 2023)</b></p>

The Darshan Runtime modulefile darshan-runtime/3.4.0 on Frontier is now loaded by default. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>.

Unloading darshan-runtime modulefile is recommended for users profiling their applications with other profilers to prevent conflicts.",3.993038545657941
"How can I search for module files containing a specific string in Frontier?
","Modules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the spider sub-command can be used as summarized in the following table.",3.992570997304438
"What is the role of the OLCF in maintaining the software environment for products?
","The OLCF provides a comprehensive suite of hardware and software resources for the creation, manipulation, and retention of scientific data. This document comprises guidelines for acceptable use of those resources. It is an official policy of the OLCF, and as such, must be agreed to by relevant parties as a condition of access to and use of OLCF computational resources.",4.422241950498928
"What is the role of the OLCF in maintaining the software environment for products?
","Users should acknowledge the OLCF in all publications and presentations that speak to work performed on OLCF resources:

This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725.



To request software installation, please contact help@olcf.ornl.gov with a description of the software, including the following details:

Software name.

Description of the software and its purpose. Is it export controlled?",4.413028110367817
"What is the role of the OLCF in maintaining the software environment for products?
","Shell and programming environments

OLCF systems provide hundreds of software packages and scientific libraries pre-installed at the system-level for users to take advantage of. To facilitate this, environment management tools are employed to handle necessary changes to the shell dynamically. The sections below provide information about using the management tools at the OLCF.



Default shell

A user's default shell is selected when completing the user account request form. The chosen shell is set across all OLCF resources.  Currently, supported shells include:

bash

tsch

csh

ksh",4.37508433303586
"How can I hold a job in the queue?
","Sometimes you may need to place a hold on a job to keep it from starting. For example, you may have submitted it assuming some needed data was in place but later realized that data is not yet available. This can be done with the scontrol hold command. Later, when the data is ready, you can release the job (i.e. tell the system that it's now OK to run the job) with the scontrol release command. For example:

| scontrol hold 12345 | Place job 12345 on hold | | --- | --- | | scontrol release 12345 | Release job 12345 (i.e. tell the system it's OK to run it) |",4.200672033198457
"How can I hold a job in the queue?
","Most users will find batch jobs an easy way to use the system, as they allow you to ""hand off"" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.",4.195707811076503
"How can I hold a job in the queue?
","In a simple batch queue system, jobs run in a first-in, first-out (FIFO) order. This often does not make effective use of the system. A large job may be next in line to run. If the system is using a strict FIFO queue, many processors sit idle while the large job waits to run. Backfilling would allow smaller, shorter jobs to use those otherwise idle resources, and with the proper algorithm, the start time of the large job would not be delayed. While this does make more effective use of the system, it indirectly encourages the submission of smaller jobs.",4.168376530317514
"How can I view the batch queue using the squeue command?
","The squeue command is used to show the batch queue. You can filter the level of detail through several command-line options. For example:

| squeue -l | Show all jobs currently in the queue | | --- | --- | |  | Show all of your jobs currently in the queue |

The sacct command gives detailed information about jobs currently in the queue and recently-completed jobs. You can also use it to see the various steps within a batch jobs.",4.608975286821159
"How can I view the batch queue using the squeue command?
","$ squeue -l -u $USER

sacct

The Slurm utility sacct can be used to view jobs currently in the queue and those completed within the last few days. The utility can also be used to see job steps in each batch job.

To see all jobs currently in the queue:

$ sacct -a -X

To see all jobs including steps owned by userA currently in the queue:

$ sacct -u userA

To see all steps submitted to job 123:

$ sacct -j 123

To see all of your jobs that completed on 2019-06-10:",4.321223830573377
"How can I view the batch queue using the squeue command?
","| Command | Action/Task | LSF Equivalent | | --- | --- | --- | | squeue | Show the current queue | bjobs | | sbatch | Submit a batch script | bsub | | salloc | Submit an interactive job | bsub -Is $SHELL | | srun | Launch a parallel job | jsrun | | sinfo | Show node/partition info | bqueues or bhosts | | sacct | View accounting information for jobs/job steps | bacct | | scancel | Cancel a job or job step | bkill | | scontrol | View or modify job configuration. | bstop, bresume, bmod |",4.309255593491815
"Where can I find the Forest SDK for quantum software development?
","Quil is the Rigetti-developed quantum instruction/assembly language. PyQuil is a Python library for writing and running quantum programs using Quil.

Installing pyQuil requires installing the Forest SDK. To quote Rigetti: ""pyQuil, along with quilc, the QVM, and other libraries, make up what is called the Forest SDK"". Because we don't have Docker functionality and due to normal users not having sudo privileges, this means that you will have to install the SDK via the ""bare-bones"" method. The general info below came from:

https://pyquil-docs.rigetti.com/en/stable/start.html",4.258251082925592
"Where can I find the Forest SDK for quantum software development?
","Users are able to install Rigetti software locally for the purpose of development using a provided Quantum Virtual Machine, or QVM, an implementation of a quantum computer simulator that can run Rigetti's Quil programs.  This can be done via two methods:

Installing manually: https://docs.rigetti.com/qcs/getting-started/installing-locally

Docker: https://hub.docker.com/r/rigetti/forest",4.219399009528082
"Where can I find the Forest SDK for quantum software development?
","Newer: https://github.com/libffi/libffi/releases/

ZMQ download: https://github.com/zeromq/libzmq/releases

Forest SDK download: https://qcs.rigetti.com/sdk-downloads

Below are example instructions for installing the above packages into your $HOME directory. Versions may vary.

Newer versions than those used in the install instructions below are known to work on Andes; however, on Frontier, newer versions of libffi than 3.2.1 are known to cause problems.

Andes

.. code-block:: bash

    $ module load gcc cmake

Frontier

.. code-block:: bash",4.149145549342866
"How can I ensure that my MPI rank is mapped to the correct GPU and CPU cores in Frontier?
","As has been pointed out previously in the Frontier documentation, notice that GPUs are NOT mapped to MPI ranks in sequential order (e.g., MPI rank 0 is mapped to physical CPU cores 1-7 and GPU 4, MPI rank 1 is mapped to physical CPU cores 9-15 and GPU 5), but this IS expected behavior. It is simply a consequence of the Frontier node architectures as shown in the Frontier Node Diagram and subsequent https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Note on NUMA domains <numa-note>.

Example 2: 1 MPI rank with 7 CPU cores and 1 GPU (single-node)",4.485688680325314
"How can I ensure that my MPI rank is mapped to the correct GPU and CPU cores in Frontier?
","This section describes how to map processes (e.g., MPI ranks) and process threads (e.g., OpenMP threads) to the CPUs, GPUs, and NICs on Frontier.

Users are highly encouraged to use the CPU- and GPU-mapping programs used in the following sections to check their understanding of the job steps (i.e., srun commands) they intend to use in their actual jobs.

For the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-cpu-map and https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-multi-map sections:",4.409661778665093
"How can I ensure that my MPI rank is mapped to the correct GPU and CPU cores in Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.408521031147381
"Can I use a graphical user interface (GUI) to interact with Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.254489349114456
"Can I use a graphical user interface (GUI) to interact with Summit?
","below), you can choose between the rendering options by loading its corresponding module on the system you're connected to. For example, to see these modules on Summit:",4.193858481740836
"Can I use a graphical user interface (GUI) to interact with Summit?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.188978712326369
"How can I get more information about connecting to OLCF resources?
","Project members

The myOLCF site is provided to aid in the utilization and management of OLCF allocations. See the https://docs.olcf.ornl.gov/services_and_applications/myolcf/index.html for more information.

If you have any questions or have a request for additional data, please contact the OLCF User Assistance Center.",4.40209357739361
"How can I get more information about connecting to OLCF resources?
","New to the Oak Ridge Leadership Computing Facility?

Welcome! The information below introduces how we structure user accounts, projects, and system allocations. It's all you need to know about getting to work. In general, OLCF resources are granted to projects in allocations, and are made available to the users associated with each project.",4.3737495804962245
"How can I get more information about connecting to OLCF resources?
","When all of the above steps are completed, your user account will be created and you will be notified by email. Now that you have a user account and it has been associated with a project, you're ready to get to work. This website provides extensive documentation for OLCF systems, and can help you efficiently use your project's allocation. We recommend reading the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#System User Guides<system-user-guides> for the machines you will be using often.",4.354078950091913
"Can a user request a temporary user account extension for data access?
","The user data retention policy exists to reclaim storage space after a user account is deactivated, e.g., after the user’s involvement on all OLCF projects concludes. By default, the OLCF will retain data in user-centric storage areas only for a designated amount of time after the user’s account is deactivated. During this time, a user can request a temporary user account extension for data access. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for details on retention timeframes for each user-centric storage area.",4.185079078272719
"Can a user request a temporary user account extension for data access?
","The project data retention policy exists to reclaim storage space after a project ends. By default, the OLCF will retain data in project-centric storage areas only for a designated amount of time after the project end date. During this time, a project member can request a temporary user account extension for data access. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for details on purge and retention timeframes for each project-centric storage area.",4.11366792988273
"Can a user request a temporary user account extension for data access?
","First-time users should apply for an account using the Account Request Form. You will need the correct 6 character project ID from your PI.

When our accounts team begins processing your application, you will receive an automated email containing a unique 36-character confirmation code. Make note of it; you can use it to check the status of your application at any time.

The principal investigator (PI) of the project must approve your account and system access. We will make the project PI aware of your request.",4.101144918677054
"What is the difference between the time limit syntax for Andes, Summit, and Frontier?
","The above procedure can also be followed to connect to Summit or Frontier, with the main difference being the number of available processors. The time limit syntax for Andes, Summit, and Frontier also differ. Summit uses the format HH:MM while Andes and Frontier follow HH:MM:SS.

Please do not run VisIt's GUI client from an OLCF machine. You will get much better performance if you install a client on your workstation and launch locally. You can directly connect to OLCF machines from inside VisIt and access your data remotely.",4.455326029980677
"What is the difference between the time limit syntax for Andes, Summit, and Frontier?
","As the OLCF brings Frontier into full production and begins preparations for future resources, you should be aware of plans that will impact Summit, Andes, and the Alpine filesystem in 2023.

Please pay attention to the following key dates as you plan your science campaigns and data migration for the remainder of the year:",4.185074647478931
"What is the difference between the time limit syntax for Andes, Summit, and Frontier?
","Summit

Andes

Frontier

Scientific simulations generate large amounts of data on Summit (about 100 Terabytes per day for some applications). Because of how large some datafiles may be, it is important that writing and reading these files is done as fast as possible. Less time spent doing input/output (I/O) leaves more time for advancing a simulation or analyzing data.",4.139625038454595
"How can I see the detailed status of my job?
","The most straightforward monitoring is with the bjobs command. This command will show the current queue, including both pending and running jobs. Running bjobs -l will provide much more detail about a job (or group of jobs). For detailed output of a single job, specify the job id after the -l. For example, for detailed output of job 12345, you can run bjobs -l 12345 . Other options to bjobs are shown below. In general, if the command is specified with -u all it will show information for all users/all jobs. Without that option, it only shows your jobs. Note that this is not an exhaustive list.",4.2407871780806925
"How can I see the detailed status of my job?
","bjobs and jobstat help to identify what’s currently running and scheduled to run, but sometimes it’s beneficial to know how much of the system is not currently in use or scheduled for use.",4.171920879794234
"How can I see the detailed status of my job?
","Provides additional details of given job.

The sview tool provide a graphical queue monitoring tool. To use, you will need an X server running on your local system. You will also need to tunnel X traffic through your ssh connection:

local-system> ssh -Y username@andes.ccs.ornl.gov
andes-login> sview",4.165355948726433
"What are some environment variables available to my batch job on Summit?
","On Summit and Andes, batch-interactive jobs using bash (i.e. those submitted with bsub -Is or salloc) run as interactive, non-login shells (and therefore source ~/.bashrc, if it exists). Regular batch jobs using bash on those systems are non-interactive, non-login shells and source the file defined by the variable $BASH_ENV in the shell from which you submitted the job. This variable is not set by default, so this means that none of these files will be sourced for a regular batch job unless you explicitly set that variable.",4.338215444734286
"What are some environment variables available to my batch job on Summit?
","LSF provides a number of environment variables in your job’s shell environment. Many job parameters are stored in environment variables and can be queried within the batch job. Several of these variables are summarized in the table below. This is not an all-inclusive list of variables available to your batch job; in particular only LSF variables are discussed, not the many “standard” environment variables that will be available (such as $PATH).",4.324170311998286
"What are some environment variables available to my batch job on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.29084439398146
"How can I determine if my project generates prohibited data types or information that falls under export control?
","these prohibited data types or information that falls under Export Control. For questions, contact help@nccs.gov.",4.414523371852638
"How can I determine if my project generates prohibited data types or information that falls under export control?
","these prohibited data types or information that falls under Export Control. For questions, contact help@olcf.ornl.gov.",4.396863253028567
"How can I determine if my project generates prohibited data types or information that falls under export control?
","Export Control: The project request will be reviewed by ORNL Export Control to determine whether sensitive or proprietary data will be generated or used. The results of this review will be forwarded to the PI. If the project request is deemed sensitive and/or proprietary, the OLCF Security Team will schedule a conference call with the PI to discuss the data protection needs.",4.266423578350616
"Who owns the Member Work directories?
","3

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

4

Retention is not applicable as files will follow purge cycle.",4.420492734452807
"Who owns the Member Work directories?
","3

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

4

Retention is not applicable as files will follow purge cycle.",4.420492734452807
"Who owns the Member Work directories?
","Footnotes

1

Permissions on Member Work directories can be controlled to an extent by project members. By default, only the project member has any accesses, but accesses can be granted to other project members by setting group permissions accordingly on the Member Work directory. The parent directory of the Member Work directory prevents accesses by ""UNIX-others"" and cannot be changed (security measures).

2

Retention is not applicable as files will follow purge cycle.",4.403504376236521
"Can you provide an example of a variable in the values.yaml file that can be modified to configure the MinIO application instance?
","Where you cloned the slate_helm_examples repository, in the 'slate_helm_examples/charts/minio-standalone` directory, you will see a values.yaml file. This file containes variables for the Helm chart deployment.

This is how we configure your instance of the MinIO application. All of these changes will be to your local copy of values.yaml.

Here is what it looks like:",4.353330445892336
"Can you provide an example of a variable in the values.yaml file that can be modified to configure the MinIO application instance?
","In this example, we'll be using the latter method. Let's create a file called values.yaml with the following values:

You may need to adjust the persistence > size value down depending on the storage quotas set for your project. Your project quotas can be found at https://quotas.CLUSTER.ccs.ornl.gov, where CLUSTER is replaced with the cluster you are running on (Marble/Onyx).",4.202506815652518
"Can you provide an example of a variable in the values.yaml file that can be modified to configure the MinIO application instance?
","What do you need to consider?

What should I name my host value? (This will be the URL in which you access your MinIO instance)

What should I name my application? (This is the name value and should be unique to you or your project)

Do I want MinIO to run on an OLCF filesytem? (It can run on NFS or GPFS project spaces. If you do not run it on an OLCF filesystem it uses an isolated volume dedicated to the MinIO server)

What do you need to configure?

host (Set the URL of your application)

name (Set the name of your application)",4.194910838844688
"Can I use my personal SecurID fob and PIN to access SPI resources?
","For users with accounts on non-SPI resources, you will use the same SecurID fob and PIN, but you must specify your unique SPI userID when you connect.  The ID will be used to place you in the proper UNIX groups allowing access to the project specific data, directories, and allocation.",4.494577681151094
"Can I use my personal SecurID fob and PIN to access SPI resources?
","Similar to the non-SPI resources, SPI resources require two-factor authentication.  If you are new to the center, you will receive a SecurID fob during the account approval/creation process.  If you are an existing user of non-SPI resources, you can use the same SecurID fob and PIN used on your non-SPI account.

Also similar to non-SPI resources, you will connect directly to the SPI resources through ssh.

ORNL's KDI users are an exception and cannot, by policy, log directly into SPI resources.  KDI users, please follow the KDI documented procedures:",4.438069340659173
"Can I use my personal SecurID fob and PIN to access SPI resources?
","You will also be sent a request to complete identity verification. When your account is approved, your RSA SecurID token will also be enabled. Please refer to our https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#system-user-guides for more information on host access. DO NOT share your PIN or RSA SecurID token with anyone. Sharing of accounts will result in termination. If your SecurID token is stolen or misplaced, contact the OLCF immediately and report the missing token. Upon termination of your account access, return the token to the OLCF in person or via mail.",4.211435080669141
"How can I instrument the callpath using TAU?
","TAU is a portable profiling and tracing toolkit that supports many programming languages. The instrumentation can be done by inserting in the source code using an automatic tool based on the Program Database Toolkit (PDT), on the compiler instrumentation, or manually using the instrumentation API.

Webpage: https://www.cs.uoregon.edu/research/tau/home.php",4.391962859653267
"How can I instrument the callpath using TAU?
","export TAU_OPTIONS=“-optTauSelectFile=phase.tau”

Now when you instrument your application, the phase called phase 1 are the lines 300-327 of the file miniWeather_mpi.cpp. Every call will be instrumented. This could create signiificant overhead, thus you should be careful when you use it.

Create a file called phases.tau.

BEGIN_INSTRUMENT_SECTION
static phase name=""phase1"" file=""miniWeather_mpi.cpp"" line=300 to line=327
static phase name=""phase2"" file=""miniWeather_mpi.cpp"" line=333 to line=346
END_INSTRUMENT_SECTION

Declare the TAU_OPTIONS variable.",4.302714652260256
"How can I instrument the callpath using TAU?
","A similar approach for other metrics, not all of them can be used. TAU provides a tool called tau_cupti_avail, where we can see the list of available metrics, then we have to figure out which CUPTI metrics use these ones.

Activate tracing and declare the data format to OTF2. OTF2 format is supported only by MPI and OpenSHMEM applications.

$ export TAU_TRACE=1
$ export TAU_TRACE_FORMAT=otf2

Use Vampir for visualization.

For example, do not instrument routine sort*(int *)

Create a file select.tau

BEGIN_EXCLUDE_LIST
void sort_#(int *)
END_EXCLUDE_LIST

Declare the TAU_OPTIONS variable",4.27333685354819
"Can I use other compilers on Frontier in addition to the ones provided by the PrgEnv-<compiler> modules?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers. Please see the following https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more detailed inforation on how to compile using these modules.",4.420551446775648
"Can I use other compilers on Frontier in addition to the ones provided by the PrgEnv-<compiler> modules?
","Cray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers.

It is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) whenever possible. See the next section for more details.",4.377609167158888
"Can I use other compilers on Frontier in addition to the ones provided by the PrgEnv-<compiler> modules?
","Cray provides PrgEnv-<compiler> modules (e.g., PrgEnv-cray) that load compatible components of a specific compiler toolchain. The components include the specified compiler as well as MPI, LibSci, and other libraries. Loading the PrgEnv-<compiler> modules also defines a set of compiler wrappers for that compiler toolchain that automatically add include paths and link in libraries for Cray software. Compiler wrappers are provided for C (cc), C++ (CC), and Fortran (ftn).",4.306304115589027
"How do I specify the full metric name when using nv-nsight-cu-cli?
","If you want to collect information on just a specific performance measurement, for example the number of bytes written to DRAM, you can do so with the --metrics option:

summit> jsrun -n1 -a1 -g1 nv-nsight-cu-cli -k vectorAdd --metrics dram__bytes_write.sum ./vectorAdd

The list of available metrics can be obtained with nv-nsight-cu-cli --query-metrics. Most metrics have both a base name and suffix. Together these  make up the full metric name to pass to nv-nsight-cu-cli. To list the full names for a collection of metrics, use --query-metrics-mode suffix --metrics <metrics list>.",4.417362968648856
"How do I specify the full metric name when using nv-nsight-cu-cli?
","The Nsight Compute command-line interface, nv-nsight-cu-cli, can be prefixed to your application to collect a report.

summit> module load nsight-compute

summit> jsrun -n1 -a1 -g1 nv-nsight-cu-cli ./vectorAdd

Similar to Nsight Systems, Nsight Compute will create a temporary report file, even when -o is not specified.",4.281208362564909
"How do I specify the full metric name when using nv-nsight-cu-cli?
","As with Nsight Systems, there is a graphical user interface you can load a report file into (The GUI is only available for Windows, x86_64 Linux and Mac). Use the -o flag to create a file (the added report extension will be .nsight-cuprof-report), copy it to your local system, and use the File > Open File menu item. If you are using multiple MPI ranks, make sure you name each one independently. Nsight Compute does not yet support the %q syntax (this will come in a future release), so your job script will have to do the naming manually; for example, you can create a simple shell script:",4.12421051183376
"What is the purpose of the ""args"" field in a container's spec?
","apiVersion: v1
kind: Pod
metadata:
  # Pod name
  name: test-pod
spec:
  containers:
    # Container name
    - name: test-container
      # Using the base image
      image: ""image-registry.openshift-image-registry.svc:5000/openshift/ccs-rhel7-base-amd64""
      # Starting a shell
      command: [""/bin/sh"",""-c""]
      # Echoing a Hello World followed by an infinitely waiting cat
      args: [""echo 'Hello World!'; cat""]
      # Need a tty if we are to SSH. Need stdin for tty
      tty: true
      stdin: true",3.978478747652235
"What is the purpose of the ""args"" field in a container's spec?
","apiVersion: v1
kind: Pod
metadata:
  # Pod name
  name: test-pod
spec:
  containers:
    # Container name
    - name: test-container
      # Using the base image
      image: ""image-registry.openshift-image-registry.svc:5000/openshift/ccs-rhel7-base-amd64""
      # Generic command that will not return
      command: [""cat""]
      # Need a tty if we are to SSH. Need stdin for tty
      tty: true
      stdin: true",3.954667049099621
"What is the purpose of the ""args"" field in a container's spec?
","apiVersion: v1
kind: Pod
metadata:
  # Pod name
  name: test-pod
spec:
  containers:
    # Container name
    - name: test-pod
      # Using the base image
      image: ""image-registry.openshift-image-registry.svc:5000/openshift/ccs-rhel7-base-amd64""
      # Generic command that will not return
      command: [""cat""]
      # Need a tty if we are to SSH. Need stdin for tty
      tty: true
      stdin: true
      volumeMounts:
        # Where in the pod the volume will be mounted
        - mountPath: /etc/test-volume
          # What the volume was named
          name: test-pod-volume",3.939368919689196
"Can I use the same mapping program for both CPU and GPU mappings on Frontier?
","This section describes how to map processes (e.g., MPI ranks) and process threads (e.g., OpenMP threads) to the CPUs, GPUs, and NICs on Frontier.

Users are highly encouraged to use the CPU- and GPU-mapping programs used in the following sections to check their understanding of the job steps (i.e., srun commands) they intend to use in their actual jobs.

For the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-cpu-map and https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-multi-map sections:",4.318100357899495
"Can I use the same mapping program for both CPU and GPU mappings on Frontier?
","In this sub-section, an MPI+OpenMP+HIP ""Hello, World"" program (hello_jobstep) will be used to show how to make only specific GPUs available to processes - which we will refer to as ""GPU mapping"". Again, Slurm's https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive method was used to request an allocation of 2 compute nodes for these examples: salloc -A <project_id> -t 30 -p <parition> -N 2. The CPU mapping part of this example is very similar to the example used above in the Multithreading sub-section, so the focus here will be on the GPU mapping part.",4.243009816067511
"Can I use the same mapping program for both CPU and GPU mappings on Frontier?
","A simple MPI+OpenMP ""Hello, World"" program (hello_mpi_omp) will be used to clarify the mappings.

For the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-gpu-map section:

An MPI+OpenMP+HIP ""Hello, World"" program (hello_jobstep) will be used to clarify the GPU mappings.

Additionally, it may be helpful to cross reference the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#simplified Frontier node diagram <frontier-simple> -- specifically the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>.",4.237170947988052
"What is the issue with the first attempt at running the program on Frontier?
",Programming Environment  Frontier utilizes the Cray Programming Environment.  Many aspects including LMOD are similar to Summit's environment.  But Cray's compiler wrappers and the Cray and AMD compilers are worth noting.  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more information.  AMD GPUs  Each frontier node has 4 AMD MI250X accelerators with two Graphic Compute Dies (GCDs) in each accelerator. The system identifies each GCD as an independent device (so for simplicity we use the term GPU when we talk about a GCD) for a total of 8,4.1390463492480105
"What is the issue with the first attempt at running the program on Frontier?
",This section of the Summit User Guide is intended to show current OLCF users how to start preparing their applications to run on the upcoming Frontier system. We will continue to add more topics to this section in the coming months. Please see the topics below to get started.,4.132420486655955
"What is the issue with the first attempt at running the program on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.107393331095181
"Can I use scratch for data that requires strict data security?
","Never allow access to your sensitive data to anyone outside of your group.

Transfer of sensitive data must be through the use encrypted methods (scp, sftp, etc).

All sensitive data must be removed from all OLCF resources when your project has concluded.",4.099768258327314
"Can I use scratch for data that requires strict data security?
","In addition, to protect sensitive data and code identified by these export control restrictions, data and code must be handled in accordance with the following rules:

HIPAA/ITAR regulated data is only allowed within the following directory:

| Area | Path | Type | | --- | --- | --- | | Project scratch | /gpfs/arx/[projid] | GPFS |

This is your project's scratch space on the ""arx"" filesystem. Only members of your project will have access.",4.094265908834584
"Can I use scratch for data that requires strict data security?
","Portions of data and/or software used in your project require extra protections due to requirements for protecting HIPAA/ITAR or other sensitive or controlled information. There are countries from which citizens are restricted from accessing sensitive/controlled information and therefore cannot be a part of your project. When you request users to be added to your project, our user assistance center will check the nationality of those users for conflict.",4.046629098185643
"How does the Infinity Fabric impact CPU accesses to GPU HBM on Crusher?
","The 2 GCDs on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The GCDs on different MI250X are connected with Infinity Fabric GPU-GPU in the arrangement shown in the Crusher Node Diagram below, where the peak bandwidth ranges from 50-100 GB/s based on the number of Infinity Fabric connections between individual GCDs.",4.282808548733302
"How does the Infinity Fabric impact CPU accesses to GPU HBM on Crusher?
","The page migration behavior summarized by the following tables represents the current, observable behavior. Said behavior will likely change in the near future.  CPU accesses to migratable memory may behave differently than other platforms you're used to. On Crusher, pages will not migrate from GPU HBM to CPU DDR4 based on access patterns alone. Once a page has migrated to GPU HBM it will remain there even if the CPU accesses it, and all accesses which do not resolve in the CPU cache will occur over the Infinity Fabric between the AMD ""Optimized 3rd Gen EPYC"" CPU and AMD MI250X GPU. Pages",4.247931350958546
"How does the Infinity Fabric impact CPU accesses to GPU HBM on Crusher?
","Each Crusher compute node consists of [1x] 64-core AMD EPYC 7A53 ""Optimized 3rd Gen EPYC"" CPU (with 2 hardware threads per physical core) with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X, each with 2 Graphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can think of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth memory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU, allowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of 36+36 GB/s. The 2 GCDs on the same MI250X are connected with",4.243906150689304
"What is the benefit of using a filter file in Score-P?
","$ cat scorep.filter
SCOREP_REGION_NAMES_BEGIN
 Exclude
   matmul_sub
   matvec_sub
SCOREP_REGION_NAMES_END

One can check the effects of the filter by re-running the scorep-score command:

$ scorep-score <profile cube dir>/profile.cubex -f scorep.filter

To apply the filter to your measurement run, you must specify this in an environment variable called SCOREP_FILTERING_FILE:

$ export SCOREP_FILTERING_FILE=scorep.filter",4.175179746761664
"What is the benefit of using a filter file in Score-P?
","$ export SCOREP_FILTERING_FILE=scorep.filter

Now you are ready to submit your instrumented code to run with tracing enabled. This measurement will generate files of the form traces.otf. The .otf2 file format can be analyzed by a tool called Vampir .

<string>:16: (INFO/1) Duplicate explicit target name: ""vampir"".

Vampir provides a visual GUI to analyze the .otf2 trace file generated with Score-P.",4.155406636466661
"What is the benefit of using a filter file in Score-P?
","$ export SCOREP_ENABLE_TRACING=true

Since tracing measurements acquire significantly more output data than profiling, we need to design a filter to remove some of the most visited calls within your instrumented code. There is a tool developed by Score-P that allows us to estimate the size of the trace file (OTF2) based on information attained from the profiling generated cube file.

To gather the needed information to design a filter file, first run scorep-score:

$ scorep-score -r <profile cube dir>/profile.cubex",4.153341091422646
"What is the total number of compute nodes in Crusher?
","Due to the unique architecture of Crusher compute nodes and the way that Slurm currently allocates GPUs and CPU cores to job steps, it is suggested that all 8 GPUs on a node are allocated to the job step to ensure that optimal bindings are possible.",4.334746411273493
"What is the total number of compute nodes in Crusher?
","Crusher is an National Center for Computational Sciences (NCCS) moderate-security system that contains identical hardware and similar software as the upcoming Frontier system. It is used as an early-access testbed for Center for Accelerated Application Readiness (CAAR) and Exascale Computing Project (ECP) teams as well as NCCS staff and our vendor partners. The system has 2 cabinets, the first with 128 compute nodes and the second with 64 compute nodes, for a total of 192 compute nodes.",4.324327812151662
"What is the total number of compute nodes in Crusher?
","Each Crusher compute node consists of [1x] 64-core AMD EPYC 7A53 ""Optimized 3rd Gen EPYC"" CPU (with 2 hardware threads per physical core) with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X, each with 2 Graphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can think of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth memory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU, allowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of 36+36 GB/s. The 2 GCDs on the same MI250X are connected with",4.275591434606273
"What is the advantage of using CuPy over NumPy?
","CuPy is a library that implements NumPy arrays on NVIDIA GPUs by utilizing CUDA Toolkit libraries like cuBLAS, cuRAND, cuSOLVER, cuSPARSE, cuFFT, cuDNN and NCCL. Although optimized NumPy is a significant step up from Python in terms of speed, performance is still limited by the CPU (especially at larger data sizes) -- this is where CuPy comes in. Because CuPy's interface is nearly a mirror of NumPy, it acts as a replacement to run existing NumPy/SciPy code on NVIDIA CUDA platforms, which helps speed up calculations further. CuPy supports most of the array operations that NumPy provides,",4.518872638120149
"What is the advantage of using CuPy over NumPy?
","As noted in AMD+CuPy limitations, data sizes explored here hang. So, this section currently does not apply to Frontier.

Now that you know how to use CuPy, time to see the actual benefits that CuPy provides for large datasets. More specifically, let's see how much faster CuPy can be than NumPy on Summit. You won't need to fix any errors; this is mainly a demonstration on what CuPy is capable of.

There are a few things to consider when running on GPUs, which also apply to using CuPy:

Higher precision means higher cost (time and space)

The structuring of your data is important",4.429139250502926
"What is the advantage of using CuPy over NumPy?
","You have now discovered what CuPy can provide! Now you can try speeding up your own codes by swapping CuPy and NumPy where you can.

CuPy User Guide

CuPy Website

CuPy API Reference

CuPy Release Notes",4.412839623724315
"How do I allocate batch nodes for my job in Summit when using backgrounded processes?
","summit>



Jsrun provides the ability to launch multiple jsrun job launches within a single batch job allocation. This can be done within a single node, or across multiple nodes.

By default, multiple invocations of jsrun in a job script will execute serially in order. In this configuration, jobs will launch one at a time and the next one will not start until the previous is complete. The batch node allocation is equal to the largest jsrun submitted, and the total walltime must be equal to or greater then the sum of all jsruns issued.",4.302838955172278
"How do I allocate batch nodes for my job in Summit when using backgrounded processes?
","Jobs on Summit are scheduled in full node increments; a node's cores cannot be allocated to multiple jobs. Because the OLCF charges based on what a job makes unavailable to other users, a job is charged for an entire node even if it uses only one core on a node. To simplify the process, users request and are allocated multiples of entire nodes through LSF.

Allocations on Summit are separate from those on Andes and other OLCF resources.

The node-hour charge for each batch job will be calculated as follows:

node-hours = nodes requested * ( batch job endtime - batch job starttime )",4.300568737625755
"How do I allocate batch nodes for my job in Summit when using backgrounded processes?
","To execute multiple job steps concurrently, standard UNIX process backgrounding can be used by adding a & at the end of the command. This will return control to the job script and execute the next command immediately, allowing multiple job launches to start at the same time. The jsruns will not share core/gpu resources in this configuration. The batch node allocation is equal to the sum of those of each jsrun, and the total walltime must be equal to or greater than that of the longest running jsrun task.",4.292431433437997
"What is the purpose of the --exclude=NONE option in sbcast?
","Notice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable. Once libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.

As mentioned above, you can use --exclude=NONE on sbcast to send all libraries along with the binary. Using --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time. A job script for the previous example, modified for sending all libraries is shown below.",4.276780070369163
"What is the purpose of the --exclude=NONE option in sbcast?
","Notice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable. Once libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.

As mentioned above, you can alternatively use --exclude=NONE on sbcast to send all libraries along with the binary. Using --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time. A job script for the previous example, modified for sending all libraries is shown below.",4.265951328925685
"What is the purpose of the --exclude=NONE option in sbcast?
","# SBCAST executable from Orion to NVMe -- NOTE: ``-C nvme`` is needed in SBATCH headers to use the NVMe drive
# NOTE: dlopen'd files will NOT be picked up by sbcast
# SBCAST automatically excludes several directories: /lib,/usr/lib,/lib64,/usr/lib64,/opt
#   - These directories are node-local and are very fast to read from, so SBCASTing them isn't critical
#   - see ``$ scontrol show config | grep BcastExclude`` for current list
#   - OLCF-provided libraries in ``/sw`` are not on the exclusion list. ``/sw`` is an NFS shared file system
#   - To override, add ``--exclude=NONE`` to arguments",4.117161317289113
"How can I ensure that MPI ranks are properly distributed across L3 cache regions in Frontier?
","There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.

Assigning MPI ranks in a ""round-robin"" (cyclic) manner across L3 cache regions (sockets) is the default behavior on Frontier. This mode will assign consecutive MPI tasks to different sockets before it tries to ""fill up"" a socket.",4.454630927859774
"How can I ensure that MPI ranks are properly distributed across L3 cache regions in Frontier?
","Because the distribution across L3 cache regions has been changed to a ""packed"" (block) configuration, caution must be taken to ensure MPI ranks end up in the L3 cache regions where the GPUs they intend to be mapped to are located. To accomplish this, the number of physical CPU cores assigned to an MPI rank was increased - in this case to 4. Doing so ensures that only 2 MPI ranks can fit into a single L3 cache region. If the value of -c was left at 1, all 8 MPI ranks would be ""packed"" into the first L3 region, where the ""closest"" GPU would be GPU 4 - the only GPU in that L3 region.",4.451239842693752
"How can I ensure that MPI ranks are properly distributed across L3 cache regions in Frontier?
","Because the distribution across L3 cache regions has been changed to a ""packed"" (block) configuration, caution must be taken to ensure MPI ranks end up in the L3 cache regions where the GPUs they intend to be mapped to are located. To accomplish this, the number of physical CPU cores assigned to an MPI rank was increased - in this case to 4. Doing so ensures that only 2 MPI ranks can fit into a single L3 cache region. If the value of -c was left at 1, all 8 MPI ranks would be ""packed"" into the first L3 region, where the ""closest"" GPU would be GPU 4 - the only GPU in that L3 region.  Notice",4.436478809491816
"How can I install h5py from source on Frontier?
","This guide has been adapted for Frontier only for a conda workflow. Using the default cray-python module on Frontier does not work with parallel h5py (because Python 3.9 is incompatible). Thus, this guide assumes that you are using a personal https://docs.olcf.ornl.gov/software/python/miniconda.html.

For venv users only interested in installing mpi4py, the pip command in this guide is still accurate.

This guide has been adapted from a challenge in OLCF's Hands-On with Summit GitHub repository (Python: Parallel HDF5).",4.36615932071138
"How can I install h5py from source on Frontier?
","Andes

.. code-block:: bash

   $ MPICC=""mpicc -shared"" pip install --no-cache-dir --no-binary=mpi4py mpi4py

Frontier

.. code-block:: bash

   $ MPICC=""cc -shared"" pip install --no-cache-dir --no-binary=mpi4py mpi4py

The MPICC flag ensures that you are using the correct C wrapper for MPI on the system. Building from source typically takes longer than a simple conda install, so the download and installation may take a couple minutes. If everything goes well, you should see a ""Successfully installed mpi4py"" message.

Next, install h5py from source.

Summit

.. code-block:: bash",4.284052797107826
"How can I install h5py from source on Frontier?
","Frontier

.. code-block:: bash

   #!/bin/bash
   #SBATCH -A <PROJECT_ID>
   #SBATCH -J h5py
   #SBATCH -N 1
   #SBATCH -p batch
   #SBATCH -t 0:05:00

   unset SLURM_EXPORT_ENV

   cd $SLURM_SUBMIT_DIR
   date

   module load PrgEnv-gnu
   module load hdf5
   export PATH=""/path/to/your/miniconda/bin:$PATH""

   source activate /ccs/proj/<project_id>/<user_id>/envs/frontier/h5pympi-frontier

   srun -n42 python3 hdf5_parallel.py",4.281501517137191
"What is the purpose of the `--exclude` option in `sbcast`?
","Notice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable. Once libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.

As mentioned above, you can use --exclude=NONE on sbcast to send all libraries along with the binary. Using --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time. A job script for the previous example, modified for sending all libraries is shown below.",4.240091952802082
"What is the purpose of the `--exclude` option in `sbcast`?
","Notice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable. Once libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.

As mentioned above, you can alternatively use --exclude=NONE on sbcast to send all libraries along with the binary. Using --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time. A job script for the previous example, modified for sending all libraries is shown below.",4.23944355884354
"What is the purpose of the `--exclude` option in `sbcast`?
","# SBCAST executable from Orion to NVMe -- NOTE: ``-C nvme`` is needed in SBATCH headers to use the NVMe drive
# NOTE: dlopen'd files will NOT be picked up by sbcast
# SBCAST automatically excludes several directories: /lib,/usr/lib,/lib64,/usr/lib64,/opt
#   - These directories are node-local and are very fast to read from, so SBCASTing them isn't critical
#   - see ``$ scontrol show config | grep BcastExclude`` for current list
#   - OLCF-provided libraries in ``/sw`` are not on the exclusion list. ``/sw`` is an NFS shared file system
#   - To override, add ``--exclude=NONE`` to arguments",4.068481870289475
"What is the outcome of the error?
","During handling of the above exception, another exception occurred:",3.990674453059771
"What is the outcome of the error?
","(it's in use, reserved, down, draining, etc.) | | JobLaunchFailure | Job failed to launch (could due to system problems, invalid program name, etc.) | | NonZeroExitCode | The job exited with some code other than 0 |",3.9277508337905647
"What is the outcome of the error?
","system failure or data loss or corruption for any reason including, but not limited to: negligence, malicious action, accidental loss, software errors, hardware failures, network losses, or inadequate configuration of any computing resource or ancillary system.",3.9265471360838458
"How can I compile a C++ program using the Intel C++ Compiler on the Andes cluster?
","The OLCF provides hundreds of pre-installed software packages and scientific libraries for your use, in addition to taking software installation requests. See the software page for complete details on existing installs.

Compiling code on andes is typical of commodity or Beowulf-style HPC Linux clusters.

The following compilers are available on Andes:

intel, intel composer xe (default)

pgi, the portland group compilar suite

gcc, the gnu compiler collection",4.3329937450607545
"How can I compile a C++ program using the Intel C++ Compiler on the Andes cluster?
","Installed Software

The OLCF provides hundreds of pre-installed software packages and scientific libraries for your use, in addition to taking software installation requests. See the software page for complete details on existing installs.

Compiling

Compiling code on andes is typical of commodity or Beowulf-style HPC Linux clusters.

Available compilers

The following compilers are available on Andes:

intel, intel composer xe (default)

pgi, the portland group compilar suite

gcc, the gnu compiler collection",4.295361871030766
"How can I compile a C++ program using the Intel C++ Compiler on the Andes cluster?
","If a different compiler is required, it is important to use the correct environment for each compiler. To aid users in pairing the correct compiler and environment, the module system on andes automatically pulls in libraries compiled with a given compiler when changing compilers. The compiler modules will load the correct pairing of compiler version, message passing libraries, and other items required to build and run code. To change the default loaded intel environment to the gcc environment for example, use:

$ module load gcc",4.271706561933225
"What is the purpose of the ""-r"" flag in the jsrun command?
","In addition, to debug slow startup JSM provides the option to create a progress file. The file will show information that can be helpful to pinpoint if a specific node is hanging or slowing down the job step launch. To enable it, you can use: jsrun --progress ./my_progress_file_output.txt.

When using file-based specification of resource sets with jsrun -U, the -a flag (number of tasks per resource set) is ignored. This has been reported to IBM and they are investigating. It is generally recommended to use jsrun explicit resource files (ERF) with --erf_input and --erf_output instead of -U.",4.263990295504939
"What is the purpose of the ""-r"" flag in the jsrun command?
","While jsrun performs similar job launching functions as aprun and mpirun, its syntax is very different. A large reason for syntax differences is the introduction of the resource set concept. Through resource sets, jsrun can control how a node appears to each job. Users can, through jsrun command line flags, control which resources on a node are visible to a job. Resource sets also allow the ability to run multiple jsruns simultaneously within a node. Under the covers, a resource set is a cgroup.

At a high level, a resource set allows users to configure what a node look like to their job.",4.260665768264476
"What is the purpose of the ""-r"" flag in the jsrun command?
","As on any system, it is useful to keep in mind the hardware underneath every execution. This is particularly true when laying out resource sets.

jsrun    [ -n #resource sets ]   [tasks, threads, and GPUs within each resource set]   program [ program args ]

Below are common jsrun options. More flags and details can be found in the jsrun man page. The defaults listed in the table below are the OLCF defaults and take precedence over those mentioned in the man page.",4.250806928455209
"How do I manage resources in a project other than where ArgoCD is deployed on Slate?
","The Red Hat OpenShift GitOps operator is already installed onto each of the Slate clusters. However, to use ArgoCD an instance will need to be added into a project namespace. If one is to use ArgoCD to manage resources in a single project, then the ArgoCD instance could be located in the same project as the resources being managed. However, if the application being managed is resource (CPU and memory) sensitive or if resources in multiple projects are to be managed, it may be better to have the ArgoCD instance deployed into a separate project to allow for better control of resources allocated",4.376989910118203
"How do I manage resources in a project other than where ArgoCD is deployed on Slate?
",allow for better control of resources allocated to ArgoCD.,4.324305015662628
"How do I manage resources in a project other than where ArgoCD is deployed on Slate?
","From the ArgoCD Applications screen, click the Create Application button. In the General application settings, give the application deployment a name for ArgoCD to refer to in the display. For Project usually the ArgoCD default project created during the ArgoCD instance installation is sufficient. However, your workload may benefit from a different logical grouping by using multiple ArgoCD projects. If it is desired to have ArgoCD automatically deploy resources to an OpenShift namespace, change the Sync Policy to Automatic. Check the Prune Resources to automatically remove objects when they",4.261791208327753
"How can I verify that my MPI ranks are running correctly?
","Let's test that mpi4py is working properly first by executing the example Python script ""hello_mpi.py"":

# hello_mpi.py
from mpi4py import MPI

comm = MPI.COMM_WORLD      # Use the world communicator
mpi_rank = comm.Get_rank() # The process ID (integer 0-41 for a 42-process job)

print('Hello from MPI rank %s !' %(mpi_rank))

To do so, submit a job to the batch queue:

Summit

.. code-block:: bash

   $ bsub -L $SHELL submit_hello.lsf

Andes

.. code-block:: bash

   $ sbatch --export=NONE submit_hello.sl

Frontier

.. code-block:: bash

   $ sbatch --export=NONE submit_hello.sl",4.223646174894004
"How can I verify that my MPI ranks are running correctly?
","There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.

Assigning MPI ranks in a ""round-robin"" (cyclic) manner across L3 cache regions (sockets) is the default behavior on Frontier. This mode will assign consecutive MPI tasks to different sockets before it tries to ""fill up"" a socket.",4.222697604920775
"How can I verify that my MPI ranks are running correctly?
","There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.",4.215975556805855
"What should I do if I encounter issues with my account application?
","First-time users should apply for an account using the Account Request Form. You will need the correct 6 character project ID from your PI.

When our accounts team begins processing your application, you will receive an automated email containing a unique 36-character confirmation code. Make note of it; you can use it to check the status of your application at any time.

The principal investigator (PI) of the project must approve your account and system access. We will make the project PI aware of your request.",4.189376554435505
"What should I do if I encounter issues with my account application?
","When our accounts team begins processing your application, you will receive an automated email containing an unique 36-character confirmation code. Make note of it; you can use it to check the status of your application at any time.

The principal investigator (PI) of the project must approve your account and system access. We will make the project PI aware of your request.",4.173150438727414
"What should I do if I encounter issues with my account application?
","Detailed instructions for account application can be found in the https://docs.olcf.ornl.gov/systems/frequently_asked_questions.html#applying-for-a-user-account section of the https://docs.olcf.ornl.gov/systems/frequently_asked_questions.html#accounts-and-projects page.

Detailed instructions for checking the status of your account application can be found in the https://docs.olcf.ornl.gov/systems/frequently_asked_questions.html#checking-application-status section of the https://docs.olcf.ornl.gov/systems/frequently_asked_questions.html#accounts-and-projects page.",4.161125583975504
"Can a project's allocation be increased to allow for more jobs to run?
","Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.335536705314055
"Can a project's allocation be increased to allow for more jobs to run?
","Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used. For example, consider that job1 is submitted at the same time as job2. The project associated with",4.323607859691653
"Can a project's allocation be increased to allow for more jobs to run?
","Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used. For example, consider that job1 is submitted at the same time as job2. The project associated with",4.323607859691653
"How can I run the Singularity image?
","Users can build container images with Podman, convert it to a SIF file, and run the container using the Singularity runtime. This page is intended for users with some familiarity with building and running containers.

Users will make use of two applications on Summit - Podman and Singularity - in their container build and run workflow. Both are available without needing to load any modules.",4.336853819638714
"How can I run the Singularity image?
","Singularity is a container framework from Sylabs. On Summit, we will use Singularity solely as the runtime. We will convert the tar files of the container images Podman creates into sif files, store those sif files on GPFS, and run them with Jsrun. Singularity also allows building images but ordinary users cannot utilize that on Summit due to additional permissions not allowed for regular users.",4.335350736794872
"How can I run the Singularity image?
","h41n08
h41n08

Here, Jsrun starts 2 separate Singularity container runtimes since we pass the -n2 flag to start two processes. Each Singularity container runtime then loads the container image simple.sif and executes the hostname command from that container. If we had requested 2 nodes in the batch script and had run jsrun -n2 -r1 singularity exec ./simple.sif hostname, Jsrun would've started a Singularity runtime on each node and the output would look something like

h41n08
h41n09



Creating Singularity containers that run MPI programs require a few additional steps.",4.264616783084378
"Can I use Slate to create resources in a specific namespace?
","Once your Slate Project Allocation Request is approved, you can create your own namespaces and move your allocation around those namespaces via the quota dashboard located at https://quota.marble.ccs.ornl.gov and https://quota.onyx.ccs.ornl.gov. The terms ""namespace"" and ""project"" may get used interchangeably when referring to your project's usable space within the requested resource boundaries (CPU/Memory/Storage).



The OC tool provides CLI access to the OpenShift cluster. It needs to be installed on your machine.",4.253645842758224
"Can I use Slate to create resources in a specific namespace?
","This section will describe the project allocation process for Slate. This is applicable to both the Onyx and Marble OLCF OpenShift clusters.

In addition, we will go over the process to log into Onyx and Marble, and how namespaces work within the OpenShift context.

Please fill out the Slate Request Form in order to use Slate resources. This must be done in order to proceed.

If you have any question please contact User Assistance, via a Help Ticket Submission or by emailing help@olcf.ornl.gov.

Required information:

- Existing OLCF Project ID

- Project PI",4.22152979432542
"Can I use Slate to create resources in a specific namespace?
","Beyond the standard CPU/Memory/Disk allocation, Slate also has other resources that can be allocated such as GPUs. Check with OLCF support first to make sure that your project can schedule these resources.

GPUs can be scheduled and made available directly in a pod. Kubernetes handles configuring the drivers in the container image.

GPUs in Slate are still an experimental functionality, please reach out to OLCF support so we can better understand your use case",4.197971848437299
"What is the purpose of the script?
",to use this script (especially after system upgrades) for testing purposes.,4.142146199567194
"What is the purpose of the script?
","Batch scripts, or job submission scripts, are the mechanism by which a user configures and submits a job for execution. A batch script is simply a shell script that also includes commands to be interpreted by the batch scheduling software (e.g. Slurm).",4.01110110034389
"What is the purpose of the script?
","The execution section of a script will be interpreted by a shell and can contain multiple lines of executables, shell commands, and comments.  when the job's queue wait time is finished, commands within this section will be executed on the primary compute node of the job's allocated resources. Under normal circumstances, the batch job will exit the queue after the last line of the script is executed.

Example Batch Script

#!/bin/bash
#SBATCH -A XXXYYY
#SBATCH -J test
#SBATCH -N 2
#SBATCH -t 1:00:00

cd $SLURM_SUBMIT_DIR
date
srun -n 8 ./a.out",3.983829652716455
"How do I troubleshoot issues with my SPI workflows on the OLCF's HPC resources?
","The Citadel framework allows use of the OLCF's existing HPC resources Summit and Frontier for SPI workflows.  Citadel adds measures to ensure separation of SPI and non-SPI workflows and data. This section provides differences when using OLCF resources for SPI and non-SPI workflows.  Because the Citadel framework just adds another security layer to existing HPC resources, many system use methods are the same between SPI and non-SPI workflows.  For example, compiling, batch scheduling, and job layout are the same between the two security enclaves.  Because of this, the existing resource user",4.336063506538536
"How do I troubleshoot issues with my SPI workflows on the OLCF's HPC resources?
","Similar to standard OLCF workflows, to run SPI workflows on OLCF resources, you will first need an approved project.  The project will be awarded an allocation(s) that will allow resource access and use.  The first step to requesting an allocation is to complete the project request form.  The form will initiate the process that includes peer review, export control review, agreements, and others.  Once submitted, members of the OLCF accounts team will help walk you through the process.

Please see the OLCF Accounts and Projects section of this site to request a new project.",4.325316946541944
"How do I troubleshoot issues with my SPI workflows on the OLCF's HPC resources?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.  This page lists the differences you may see when using OLCF resources to execute SPI workflows.  The page will also point to sections within the site where more information on standard non-SPI use can be found.",4.3181792427658445
"What is the purpose of the `--W` flag in the `bsrun` command?
","| Option | Example Usage | Description | | --- | --- | --- | | -W | #BSUB -W 50 | Requested maximum walltime. NOTE: The format is [hours:]minutes, not [[hours:]minutes:]seconds like PBS/Torque/Moab | | -nnodes | #BSUB -nnodes 1024 | Number of nodes NOTE: There is specified with only one hyphen (i.e. -nnodes, not --nnodes) | | -P | #BSUB -P ABC123 | Specifies the project to which the job should be charged | | -o | #BSUB -o jobout.%J | File into which job STDOUT should be directed (%J will be replaced with the job ID number) If you do not also specify a STDERR file with -e or -eo, STDERR will",4.067972916998947
"What is the purpose of the `--W` flag in the `bsrun` command?
","by a shell name. For example, to request an interactive batch job (with bash as the shell) equivalent to the sample batch script above, you would use the command: bsub -W 3:00 -nnodes 2048 -P ABC123 -Is /bin/bash",4.05417812658744
"What is the purpose of the `--W` flag in the `bsrun` command?
","Dependency expressions can be combined with logical operators. For example, if you want a job held until job 12345 is DONE and job 12346 has started, you can use #BSUB -w ""done(12345) && started(12346)""



The default job launcher for Summit is jsrun. jsrun was developed by IBM for the Oak Ridge and Livermore Power systems. The tool will execute a given program on resources allocated through the LSF batch scheduler; similar to mpirun and aprun functionality.

The following compute node image will be used to discuss jsrun resource sets and layout.



1 node

2 sockets (grey)",3.997081240765573
"How can subcontractors access the Nondisclosure Agreement Form?
","Nondisclosure Agreement Form Required from subcontractors only.

Sensitive Data Rules All users must agree to abide by all security measures described in this document when performing any work on OLCF resources that is not fundamental research and/or publicly available information.



You no longer need to use a special form for the following requests. If you need to request any of the following, please do so via email to help@olcf.ornl.gov.

Relaxed queue limits for one or more jobs (longer walltime, higher priority)

System reservation (a dedicated set of nodes at a specific date/time)",4.29362801537043
"How can subcontractors access the Nondisclosure Agreement Form?
",if you require a copy of your User Agreement.,4.0580214837868205
"How can subcontractors access the Nondisclosure Agreement Form?
","ORNL Personnel Access System (PAS): All PI’s are required to be entered into the ORNL PAS system. An OLCF Accounts Manager will send the PI a PAS invitation to submit all the pertinent information. Please note that processing a PAS request may take 15 or more days.

User Agreement/Appendix A or Subcontract: A User Agreement/Appendix A or Subcontract must be executed between UT-Battelle and the PI’s institution. If our records indicate this requirement has not been met, all necessary documents will be provided to the applicant by an OLCF Accounts Manager.",3.9886247462237394
"How do I unload the darshan-runtime module using Score-P?
","You will need to unload the darshan-runtime module. In some instances you may need to unload the xalt and xl modules.  $ module unload darshan-runtime

Serial

..  C

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load scorep
        $ module load gcc
        $ scorep gcc -c test.c
        $ scorep gcc -o test test.o

..  C++

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load scorep
        $ module load gcc
        $ scorep g++ -c test.cpp main.cpp
        $ scorep g++ -o test test.o main.o

..  Fortran",4.518734683682868
"How do I unload the darshan-runtime module using Score-P?
","..  C++

    .. code-block:: bash

          $ module unload darshan-runtime
          $ module load scorep
          $ module load spectrum-mpi
          $ module load gcc
          $ scorep mpiCC -c test.cpp main.cpp
          $ scorep mpiCC -o test test.o main.o

..  Fortran

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load gcc
        $ module load Scorep
        $ scorep mpifort -c test.f90
        $ scorep mpifort -o test test.o

MPI + OpenMP

..  C

    .. code-block:: bash",4.273691290833454
"How do I unload the darshan-runtime module using Score-P?
","..  Fortran

    .. code-block:: bash

        $ module unload darshan-runtime
        $ module load scorep
        $ module load gcc
        $ scorep gfortran -c test_def.f90 test.f90 main.f90
        $ scorep gfortran -o test test_def.o test.o main.o

MPI

..  C

    .. code-block:: bash

          $ module unload darshan-runtime
          $ module load scorep
          $ module load spectrum-mpi
          $ module load gcc
          $ scorep mpicc -c test.c main.c
          $ scorep mpicc -o test test.o main.o

..  C++

    .. code-block:: bash",4.222612135074496
"How do I access the Quantinuum User Interface?
","After submitting the OLCF quantum account application and receiving approval, you will receive an email from Quantinuum inviting you to create your quantum account. Once logged in, users will have access to Quantinuum's User Interface, https://um.qapi.quantinuum.com, their online platform for managing jobs and accessing the available quantum systems, including the System Model H1, via the cloud. From the UI, users can view system status and upcoming system availability, as well as monitor batch submissions and job history. Information on using the quantum resources via Jupyter notebooks is",4.386546338590096
"How do I access the Quantinuum User Interface?
","Information on submitting jobs to Quantinuum systems, system availability, checking job status, and tracking usage can be found in the Quantinuum Systems User Guide under the Examples tab on the Quantinuum User Portal.

Users have access to the API validator to check program syntax, and to the Quantinuum System Model H1 emulator, which returns actual results back as if users submitted code to the real quantum hardware.",4.384023727399296
"How do I access the Quantinuum User Interface?
","Users can access information about IBM Quantum's systems, view queue information, and submit jobs on their cloud dashboard at https://quantum-computing.ibm.com. The cloud dashboard allows access to IBM Quantum's graphical circuit builder, Quantum Composer, IBM's Quantum Lab, and associated program examples.  A Jupyterlab server is provisioned with IBM Quantum's Qiskit programming framework for job submission.",4.277899349193287
"How are batch jobs that do not write end records handled on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.230245236390443
"How are batch jobs that do not write end records handled on Summit?
","The batch-hm queue (and the batch-hm-spi queue for Moderate Enhanced security enclave projects) is used to access Summit's high-memory nodes.  Jobs may use all 54 nodes. It enforces the following policies:

Limit of (4) eligible-to-run jobs per user.

Jobs in excess of the per user limit above will be placed into a held state, but will change to eligible-to-run at the appropriate time.

Users may have only (25) jobs queued in the batch-hm queue at any state at any time. Additional jobs will be rejected at submit time.

batch-hm job limits:",4.197890047782235
"How are batch jobs that do not write end records handled on Summit?
","| Bin | Min Nodes | Max Nodes | Max Walltime (Hours) | Aging Boost (Days) | | --- | --- | --- | --- | --- | | 1 | 2,765 | 4,608 | 24.0 | 15 | | 2 | 922 | 2,764 | 24.0 | 10 | | 3 | 92 | 921 | 12.0 | 0 | | 4 | 46 | 91 | 6.0 | 0 | | 5 | 1 | 45 | 2.0 | 0 |

The batch queue (and the batch-spi queue for Moderate Enhanced security enclave projects) is the default queue for production work on Summit.  Most work on Summit is handled through this queue. It enforces the following policies:

Limit of (4) eligible-to-run jobs per user.",4.172328714366874
"How do I set the SSH path in Paraview?
","<Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Windows"">
            <Set name=""TERM_PATH"" value=""cmd"" />
            <Set name=""TERM_ARG1"" value=""/C"" />
            <Set name=""TERM_ARG2"" value=""start"" />
            <Set name=""TERM_ARG3"" value="""" />
            <Set name=""SSH_PATH"" value=""plink.exe"" />
          </Case>
          <Case value=""Unix"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.192242101905348
"How do I set the SSH path in Paraview?
","<Set name=""TERM_ARG3"" value=""-e"" />
            <Set name=""SSH_PATH"" value=""ssh"" />
          </Case>
          <Case value=""Windows"">
            <Set name=""TERM_PATH"" value=""cmd"" />
            <Set name=""TERM_ARG1"" value=""/C"" />
            <Set name=""TERM_ARG2"" value=""start"" />
            <Set name=""TERM_ARG3"" value="""" />
            <Set name=""SSH_PATH"" value=""plink.exe"" />
          </Case>
          <Case value=""Unix"">
            <Set name=""TERM_PATH"" value=""xterm"" />
            <Set name=""TERM_ARG1"" value=""-T"" />
            <Set name=""TERM_ARG2"" value=""ParaView"" />",4.192242101905348
"How do I set the SSH path in Paraview?
","After installing, you must give ParaView the relevant server information to be able to connect to OLCF systems (comparable to VisIt's system of host profiles). The following provides an example of doing so. Although several methods may be used, the one described should work in most cases.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.

For Windows clients, it is necessary to install PuTTY to create an ssh connection in step 2.

Step 1: Save the following servers.pvsc file to your local computer",4.191505393179877
"How can I recall a saved collection of modules in Andes?
","Lmod supports caching commonly used collections of environment modules on a per-user basis in $HOME/.lmod.d. To create a collection called ""NAME"" from the currently loaded modules, simply call module save NAME. Omitting ""NAME"" will set the user’s default collection. Saved collections can be recalled and examined with the commands summarized in the following table.",4.21644106203786
"How can I recall a saved collection of modules in Andes?
","Defining custom module collections

Lmod supports caching commonly used collections of environment modules on a per-user basis in $home/.lmod.d. To create a collection called ""NAME"" from the currently loaded modules, simply call module save NAME. omitting ""NAME"" will set the user’s default collection. Saved collections can be recalled and examined with the commands summarized in the following table.",4.21353119434064
"How can I recall a saved collection of modules in Andes?
","Lmod supports caching commonly used collections of environment modules on a per-user basis in $home/.lmod.d. To create a collection called ""NAME"" from the currently loaded modules, simply call module save NAME. omitting ""NAME"" will set the user’s default collection. Saved collections can be recalled and examined with the commands summarized in the following table.",4.211996022658712
"How do I set the storage quota for my MinIO instance?
","Depending on you how configured your deployment, this could be your NFS or GPFS project space or an isolated volume dedicated/isolated to this MinIO server.

Within the GUI you can create buckets and upload/download data. If you are running this on NFS or GPFS the bucket will map to a directory.",4.184740040882911
"How do I set the storage quota for my MinIO instance?
","Each user-centric and project-centric storage area has an associated quota, which could be a hard (systematically-enforceable) quota or a soft (policy-enforceable) quota. Storage usage will be monitored continually. When a user or project exceeds a soft quota for a storage area, the user or project PI will be contacted and will be asked if at all possible to purge data from the offending area. See the section https://docs.olcf.ornl.gov/systems/olcf_policy_guide.html#retention-policy for details on quotas for each storage area.",4.17471435310015
"How do I set the storage quota for my MinIO instance?
","MinIO is a high-performance software-defined object storage suite that enables the ability to easily deploy cloud-native data infrastructure for various data workloads. In this example we are deploying a simple, standalone implementation of MinIO on our cloud-native platform, Slate (https://docs.olcf.ornl.gov/systems/minio.html#slate_overview).

This service will only be accessible from inside of ORNL's network.

If your project requires an externally facing service available to the Internet, please contact User Assistance by submitting a help ticket. There is a process to get such approval.",4.15035295053006
"How does core specialization work in Frontier?
","By default, Frontier reserves the first core in each L3 cache region. Frontier uses low-noise mode, which constrains all system processes to core 0. Low-noise mode cannot be disabled by users. In addition, Frontier uses SLURM core specialization (-S 8 flag at job allocation time, e.g., sbatch) to reserve one core from each L3 cache region, leaving 56 allocatable cores. Set -S 0 at job allocation to override this setting.",4.198414560452966
"How does core specialization work in Frontier?
","This example assumes the use of a core specialization of -S 0.  Because Frontier's default core specialization (-S 8) reserves the first core in each L3 region, the ""packed"" mode can be problematic because the 7 cores available in each L3 region won't necessarily divide evenly. This can lead to tasks potentially spanning multiple L3 regions with its assigned cores, which creates problems when Slurm tries to assign GPUs to a given task.",4.197773467709196
"How does core specialization work in Frontier?
","This example assumes the use of a core specialization of -S 0.  Because Frontier's default core specialization (-S 8) reserves the first core in each L3 region, the ""packed"" mode can be problematic because the 7 cores available in each L3 region won't necessarily divide evenly. This can lead to tasks potentially spanning multiple L3 regions with its assigned cores, which creates problems when Slurm tries to assign GPUs to a given task.",4.197773467709196
"How do I specify the job name for my job?
","by a shell name. For example, to request an interactive batch job (with bash as the shell) equivalent to the sample batch script above, you would use the command: bsub -W 3:00 -nnodes 2048 -P ABC123 -Is /bin/bash",4.099248017341885
"How do I specify the job name for my job?
",you can optionally specify a +time value for each jobid.,4.084074041241957
"How do I specify the job name for my job?
","Slurm reads a number of environment variables, many of which can provide the same information as the job options noted above. We recommend using the job options rather than environment variables to specify job options, as it allows you to have everything self-contained within the job submission script (rather than having to remember what options you set for a given job).

Slurm also provides a number of environment variables within your running job. The following table summarizes those that may be particularly useful within your job (e.g. for naming output log files):",4.070872022449904
"What is the difference between the git repository for ArgoCD and the one used to build the application?
","Image of the ArgoCD applications tab.

Prior to using ArgoCD, a git repository containing Kubernetes custom resources needs to be added for use.

The git repository containing Kubernetes custom resources is typically not the same git repository used to build the application.

There are three ways for ArgoCD to connect to a git repository:

connect using ssh

connect using https

connect using GitHub App",4.394250785276166
"What is the difference between the git repository for ArgoCD and the one used to build the application?
","Unlike the CI/CD tools mentioned above, ArgoCD is not used for testing and creating container images. Rather, ArgoCD manages Kubernetes application deployments in an automated and consistent manner using custom resource files versioned in a git repository. Additionally, individual development, test and production deployments across multiple projects may be accomplished using a singular git repository. Whenever a change occurs in the git repository, ArgoCD will make the necessary changes to a project by adding, reconfiguring, or removing resources. In other words, the CD in ArgoCD is for",4.340309046767208
"What is the difference between the git repository for ArgoCD and the one used to build the application?
","With a git repository defined in the ArgoCD Repositories settings that has kustomize environments ready for use, one can start using ArgoCD to deploy and manage kubernetes resources.",4.308294905902314
"How many CPU cores do login nodes have?
","Login nodes should be used for basic tasks such as file editing, code compilation, data backup, and job submission. Login nodes should not be used for memory- or compute-intensive tasks. Users should also limit the number of simultaneous tasks performed on the login resources. For example, a user should not run (10) simultaneous tar processes on a login node.

Compute-intensive, memory-intensive, or otherwise disruptive processes running on login nodes may be killed without warning.",4.338272829478792
"How many CPU cores do login nodes have?
","Login nodes

Andes features 8 login nodes which are identical to the batch partition compute nodes.  The login nodes provide an environment for editing, compiling, and launching codes onto the compute nodes. All Andes users will access the system through these same login nodes, and as such, any CPU- or memory-intensive tasks on these nodes could interrupt service to other users. As a courtesy, we ask that you refrain from doing any analysis or visualization tasks on the login nodes.",4.336060028510888
"How many CPU cores do login nodes have?
","Andes features 8 login nodes which are identical to the batch partition compute nodes.  The login nodes provide an environment for editing, compiling, and launching codes onto the compute nodes. All Andes users will access the system through these same login nodes, and as such, any CPU- or memory-intensive tasks on these nodes could interrupt service to other users. As a courtesy, we ask that you refrain from doing any analysis or visualization tasks on the login nodes.",4.327385079026368
"How do I know which SMT level is best for my workload on Summit?
",The default SMT level is 4.,4.280015897516545
"How do I know which SMT level is best for my workload on Summit?
",| (recording) http://vimeo.com/306890517 | | 2018-12-05 | IBM Power9 SMT Deep Dive | Brian Thompto (IBM) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_thompto_smt.pdf https://vimeo.com/306890804 | | 2018-12-05 | Network Features & MPI Tuning | Christopher Zimmer (OLCF) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording),4.153979383759522
"How do I know which SMT level is best for my workload on Summit?
","This section provides some of the most commonly used LSF commands as well as some of the most useful options to those commands and information on jsrun, Summit's job launch command. Many commands have much more information than can be easily presented here. More information about these commands is available via the online manual (i.e. man jsrun). Additional LSF information can be found on IBM’s website.

Each physical core on Summit contains 4 hardware threads. The SMT level can be set using LSF flags (the default is smt4):

SMT1",4.151008038736766
"Can you use GPUs with pbdR?
","For parallelism, you should use pbdR packages, Rmpi directly, or an interface which can use Rmpi as a backend. We address GPUs specifically next.

There are some R packages which can use GPUs, such as xgboost. There is also the gpuR series of packages. Several pbdR packages support GPU computing. It is also possible to offload some linear algebra computations (specifically matrix-matrix products, and methods which are computationally dominated by them) to the GPU using NVIDIA’s NVBLAS.",4.337115213401081
"Can you use GPUs with pbdR?
","If you want to do GPU computing on Summit with R, we would love to collaborate with you (see contact details at the bottom of this document).

For more information about using R and pbdR effectively in an HPC environment such as Summit, please see the R and pbdR articles. These are long-form articles that introduce HPC concepts like MPI programming in much more detail than we do here.

Also, if you want to use R and/or pbdR on Summit, please feel free to contact us directly:

Mike Matheson - mathesonma AT ornl DOT gov

George Ostrouchov - ostrouchovg AT ornl DOT gov",4.28841062654403
"Can you use GPUs with pbdR?
","There are a variety of programming models available to program GPUs (e.g. CUDA, OpenACC, OpenMP offloading, etc.) and you are welcome to use any of them at these events.

<p style=""font-size:20px""><b>Why participate?</b></p>",4.110694214729694
"What is the purpose of the ""cudaMemcpy(da, ha, N*sizeof(int), cudaMemcpyHostToDevice)"" line in the code?
","#include <stdio.h>
#define N 1000

__global__
void add(int *a, int *b) {
    int i = blockIdx.x;
    if (i<N) {
        b[i] = 2*a[i];
    }
}

int main() {
    int ha[N], hb[N];

    int *da, *db;
    cudaMalloc((void **)&da, N*sizeof(int));
    cudaMalloc((void **)&db, N*sizeof(int));

    for (int i = 0; i<N; ++i) {
        ha[i] = i;
    }
cudaMemcpy(da, ha, N*sizeof(int), cudaMemcpyHostToDevice);

add<<<N, 1>>>(da, db);

cudaMemcpy(hb, db, N*sizeof(int), cudaMemcpyDeviceToHost);",4.248197580934586
"What is the purpose of the ""cudaMemcpy(da, ha, N*sizeof(int), cudaMemcpyHostToDevice)"" line in the code?
","for (int i = 0; i<N; ++i) {
    if(i+i != hb[i]) {
        printf(""Something went wrong in the GPU calculation\n"");
    }
}
printf(""COMPLETE!"");
     cudaFree(da);
     cudaFree(db);

     return 0;
}

Create a file named gpuexample.dockerfile with the following contents

FROM code.ornl.gov:4567/olcfcontainers/olcfbaseimages/mpiimage-centos-cuda:latest
RUN mkdir /app
COPY cudaexample.cu /app
RUN cd /app && nvcc -o cudaexample cudaexample.cu

Run the following commands to build the container image with Podman and convert it to Singularity",4.107317247528885
"What is the purpose of the ""cudaMemcpy(da, ha, N*sizeof(int), cudaMemcpyHostToDevice)"" line in the code?
","CUDA-aware MPI allows GPU buffers (e.g., GPU memory allocated with cudaMalloc) to be used directly in MPI calls rather than requiring data to be manually transferred to/from a CPU buffer (e.g., using cudaMemcpy) before/after passing data in MPI calls. By itself, CUDA-aware MPI does not specify whether data is staged through CPU memory or, for example, transferred directly between GPUs when passing GPU buffers to MPI calls. That is where GPUDirect comes in.",4.00774138015207
"What is the difference between the ""sleep"" and ""wait"" commands in the provided code?
",".. code:: bash



The --exact parameter is important to avoid the error message srun: Job <job id> step creation temporarily disabled, retrying (Requested nodes are busy). The wait command is also critical, or your job script and allocation will immediately end after launching your jobs in the background. The sleep command is currently required to work around a known issue that causes MPICH ERROR. See https://docs.olcf.ornl.gov/systems/crusher_quick_start_guide.html#crusher-known-issues for more information and alternative workarounds. sleep will no longer be needed in a future update.",4.044481213694662
"What is the difference between the ""sleep"" and ""wait"" commands in the provided code?
","The wait command is needed so the job script (and allocation) do not immediately end after launching the job steps in the background.  The sleep 1 is needed to give Slurm sufficient time to launch each job step.

As mentioned previously, all GPUs are accessible by all MPI ranks by default, so it is possible to programatically map any combination of GPUs to MPI ranks. It should be noted however that Cray MPICH does not support GPU-aware MPI for multiple GPUs per rank, so this binding is not suggested.

<string>:5: (INFO/1) Duplicate explicit target name: ""frontier node diagram"".",4.015608368201241
"What is the difference between the ""sleep"" and ""wait"" commands in the provided code?
","The output shows that each independent process ran on its own CPU core and GPU on the same single node. To show that the ranks ran simultaneously, date was called before each job step and a 20 second sleep was added to the end of the hello_jobstep program. So the output also shows that the first job step was submitted at :45 and the subsequent job steps were all submitted between :46 and :52. But because each hello_jobstep sleeps for 20 seconds, the subsequent job steps must have all been running while the first job step was still sleeping (and holding up its resources). And the same argument",4.010670703350271
"How do I create resource sets for my application on Summit?
","In addition to this Summit User Guide, there are other sources of documentation, instruction, and tutorials that could be useful for Summit users.",4.246998631471942
"How do I create resource sets for my application on Summit?
",For Summit:,4.223657459469889
"How do I create resource sets for my application on Summit?
","below), you can choose between the rendering options by loading its corresponding module on the system you're connected to. For example, to see these modules on Summit:",4.194634389592766
"What is the purpose of the -p flag in the Miniconda installer?
","The -p flag specifies the prefix path for where to install miniconda.

The -u updates any current installations at the -p location (not necessary if you didn't do a ""mkdir"" beforehand).



While running the installer, you will be prompted with something like this:

Do you wish the installer to initialize Miniconda3 by running conda init? [yes|no]

It is MUCH SAFER to answer ""no"" and to just export the PATH manually when on Frontier to avoid clashing:

export PATH=""/path/to/your/miniconda/bin:$PATH""",4.448375045338208
"What is the purpose of the -p flag in the Miniconda installer?
","The -p flag specifies the desired path and name of your new virtual environment. The directory structure is case sensitive, so be sure to insert <YOUR_PROJECT_ID> as lowercase. Directories will be created if they do not exist already (provided you have write-access in that location). Instead, one can solely use the --name <your_env_name> flag which will automatically use your $HOME directory.",4.113121547728496
"What is the purpose of the -p flag in the Miniconda installer?
","Because issues can arise when using conda and pip together (see link in https://docs.olcf.ornl.gov/systems/conda_basics.html#conda-refs), it is recommended to do this only if absolutely necessary.

To build a package from source, use pip install --no-binary=<package_name> <package_name>:

$ CC=gcc pip install --no-binary=numpy numpy

The CC=gcc flag will ensure that you are using the proper compiler and wrapper. Building from source results in a longer installation time for packages, so you may need to wait a few minutes for the install to finish.",4.089258395104626
"Are there any restrictions on the type of data that can be stored on the BurstBuffers on Summit?
","Each compute node on Summit has a 1.6TB Non-Volatile Memory (NVMe) storage device (high-memory nodes have a 6.4TB NVMe storage device), colloquially known as a ""Burst Buffer"" with theoretical performance peak of 2.1 GB/s for writing and 5.5 GB/s for reading. The NVMes could be used to reduce the time that applications wait for I/O.  More information can be found later in the Burst Buffer section.



<string>:208: (INFO/1) Duplicate implicit target name: ""software"".",4.205237500791726
"Are there any restrictions on the type of data that can be stored on the BurstBuffers on Summit?
","Each compute node on Summit has a 1.6TB Non-Volatile Memory (NVMe) storage device (high-memory nodes have a 6.4TB NVMe storage device), colloquially known as a ""Burst Buffer"" with theoretical performance peak of 2.1 GB/s for writing and 5.5 GB/s for reading. 100GB of each NVMe is reserved for NFS cache to help speed access to common libraries. When calculating maximum usable storage size, this cache and formatting overhead should be considered; We recommend a maximum storage of 1.4TB (6TB for high-memory nodes). The NVMes could be used to reduce the time that applications wait for I/O. Using",4.119022827170036
"Are there any restrictions on the type of data that can be stored on the BurstBuffers on Summit?
","Most Summit nodes contain 512 GB of DDR4 memory for use by the POWER9 processors, 96 GB of High Bandwidth Memory (HBM2) for use by the accelerators, and 1.6TB of non-volatile memory that can be used as a burst buffer. A small number of nodes (54) are configured as ""high memory"" nodes. These nodes contain 2TB of DDR4 memory, 192GB of HBM2, and 6.4TB of non-volatile memory.",4.104504576304421
"How do I convert my container image created with Podman on Summit to a tar file?
","Podman is a container framework from Red Hat that functions as a drop in replacement for Docker. On Summit, we will use Podman for building container images and converting them into tar files for Singularity to use. Podman makes use of Dockerfiles to describe the images to be built. We cannot use Podman to run containers as it doesn't properly support MPI on Summit, and Podman does not support storing its container images on GPFS or NFS.",4.3972713964675165
"How do I convert my container image created with Podman on Summit to a tar file?
","Users can build container images with Podman, convert it to a SIF file, and run the container using the Singularity runtime. This page is intended for users with some familiarity with building and running containers.

Users will make use of two applications on Summit - Podman and Singularity - in their container build and run workflow. Both are available without needing to load any modules.",4.339371465833589
"How do I convert my container image created with Podman on Summit to a tar file?
","Singularity is a container framework from Sylabs. On Summit, we will use Singularity solely as the runtime. We will convert the tar files of the container images Podman creates into sif files, store those sif files on GPFS, and run them with Jsrun. Singularity also allows building images but ordinary users cannot utilize that on Summit due to additional permissions not allowed for regular users.",4.250542301701131
"Can I use HIP on systems running on both the ROCm and CUDA platform?
","As mentioned above, HIP can be used on systems running on either the ROCm or CUDA platform, so OLCF users can start preparing their applications for Frontier today on Summit. To use HIP on Summit, you must load the HIP module:

$ module load hip-cuda

CUDA 11.4.0 or later must be loaded in order to load the hip-cuda module. hipBLAS, hipFFT, hipSOLVER, hipSPARSE, and hipRAND have also been added to hip-cuda.",4.495678640949109
"Can I use HIP on systems running on both the ROCm and CUDA platform?
","HIP (Heterogeneous-Compute Interface for Portability) is a C++ runtime API that allows developers to write portable code to run on AMD and NVIDIA GPUs. It is an interface that uses the underlying Radeon Open Compute (ROCm) or CUDA platform that is installed on a system. The API is similar to CUDA so porting existing codes from CUDA to HIP should be fairly straightforward in most cases. In addition, HIP provides porting tools which can be used to help port CUDA codes to the HIP layer, with no overhead compared to the original CUDA application. HIP is not intended to be a drop-in replacement",4.3569223926351
"Can I use HIP on systems running on both the ROCm and CUDA platform?
","Key features include:

HIP is a thin layer and has little or no performance impact over coding directly in CUDA.

HIP allows coding in a single-source C++ programming language including features such as templates, C++11 lambdas, classes, namespaces, and more.

The “hipify” tools automatically convert source from CUDA to HIP.

Developers can specialize for the platform (CUDA or HIP) to tune for performance or handle tricky cases.",4.351434802490955
"How can I change into one of the scratch filesystems on Summit?
","Please note, Summit will mount a new filesystem once returned to service.  Data stored on Alpine at the time of its decommission on January 01 will not be available.  Users will be responsible for transferring data onto Summit's new filesystem



Alpine II will be available early 2024.

The previous center-wide GPFS scratch filesystem, Alpine, will be decommissioned in January 2024. A new scratch filesystem will be made available for projects with 2024 Summit allocations in early 2024. Users will be responsible for transferring any needed data onto the new scratch filesystem once available.",4.265276832179962
"How can I change into one of the scratch filesystems on Summit?
","Summit will accept batch jobs prior to 08:00 on December 18, but only batch jobs that will complete prior to 08:00 Dec 18 will run.  All batch jobs remaining in the queue at 08:00, Dec 18 will be deleted.



Alpine will be unmounted from Andes on December 19.  Jobs must be modified to use Orion as their scratch filesystem prior to this day.



In preparation for Alpine's decommission on January 01, Alpine will become read-only from all OLCF systems on December 19.",4.1417783265453325
"How can I change into one of the scratch filesystems on Summit?
","Notable differences between Summit and Frontier:  Orion scratch filesystem  Frontier mounts Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace. Frontier will not mount Alpine and Summit will not mount Orion. Data will not be automatically transferred from Alpine to Orion, so we recommend that users move only needed data between the file systems with Globus.  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-data-storage section or this recording for more information.  Cray Programming Environment  Frontier utilizes the",4.12976089434894
"How do I know if the directory structure for my new environment already exists?
","The -p flag specifies the desired path and name of your new virtual environment. The directory structure is case sensitive, so be sure to insert <YOUR_PROJECT_ID> as lowercase. Directories will be created if they do not exist already (provided you have write-access in that location). Instead, one can solely use the --name <your_env_name> flag which will automatically use your $HOME directory.",4.129260907466676
"How do I know if the directory structure for my new environment already exists?
","It is highly recommended to create new environments in the ""Project Home"" directory (on Summit, this is /ccs/proj/<YOUR_PROJECT_ID>/<YOUR_USER_ID>). This space avoids purges, allows for potential collaboration within your project, and works better with the compute nodes. It is also recommended, for convenience, that you use environment names that indicate the hostname, as virtual environments created on one system will not necessarily work on others.",4.106945226267075
"How do I know if the directory structure for my new environment already exists?
","The suggested directory structure is to have a outer directory say swift-work that has the swift source and shell scripts. Inside of swift-work create a new directory called data.

Additionally, we will need two terminals open. In the first terminal window, navigate to the swift-work directory and invoke the data generation script like so:

$ ./gendata.sh

In the second terminal, we will run the swift workflow as follows (make sure to change the project name per your allocation):",4.0264409168324935
"How can I start a remote shell in a specific container within a pod using the oc rsh command?
","Finally, to get a remote shell in the pod we run the oc rsh <POD_NAME> command. This will default to using /bin/sh in the pod. If a different shell is required, we can provide the optional --shell=/path/to/shell flag. For example, if we wanted to open a bash shell in the pod we would run the following command:

oc rsh --shell='/bin/bash' <POD_NAME>

If you have multiple containers in your pod, the oc rsh <POD_NAME> command will default to the first container. If you would like to start a remote shell in one of the other containers, you can use the optional -c <CONTAINER_NAME> flag.",4.69648820558522
"How can I start a remote shell in a specific container within a pod using the oc rsh command?
","If we have a pod that is crash looping then it is exiting too quickly to spawn a shell inside the container. We can use oc debug to start a pod with that same image but instead of the image entrypoint we use /bin/sh instead.

$ oc debug misbehaving-pod-1
Defaulting container name to bad.
Use 'oc describe pod/misbehaving-pod-1' to see all of the containers in this pod.

Debugging with pod/misbehaving-pod-1, original command: <image entrypoint>
Waiting for pod to start ...
If you don't see a command prompt, try pressing enter.
/ $

What if we want to get a shell inside of the container to debug?",4.348126224344204
"How can I start a remote shell in a specific container within a pod using the oc rsh command?
","oc port-forward hello-world-pod 8080:8080

Then, since oc port-forward stays in the foreground, we run curl http://localhost:8080 in a second terminal.

The initial port in the port pair references a non-allocated port on our local system similar to how SSH port forwarding works.

Pods also have logs. And we can see the logs for the pod: (whatever was printed to stdout from within the container, not kubernetes).

oc logs hello-world-pod

Now lets delete our pod:

oc delete pod hello-world-pod",4.228845668056554
"What is the operating system of the current R version that I am using?
","Several versions of R are available on Summit. You can see which by entering the command module spider r. Throughout this example, we will be using R version 3.6.1.

If you have logged in with the default modules, then you need to swap xl for gcc and the load R:

module swap xl gcc/6.4.0
module load r/3.6.1

If we do that and launch R, then we see:",4.200123213797207
"What is the operating system of the current R version that I am using?
","If we do that and launch R, then we see:

version
## platform       powerpc64le-unknown-linux-gnu
## arch           powerpc64le
## os             linux-gnu
## system         powerpc64le, linux-gnu
## status
## major          3
## minor          6.1
## year           2019
## month          07
## day            05
## svn rev        76782
## language       R
## version.string R version 3.6.1 (2019-07-05)
## nickname       Action of the Toes",4.1911401604306295
"What is the operating system of the current R version that I am using?
","sessionInfo()
## R version 3.6.1 (2019-07-05)
## Platform: powerpc64le-unknown-linux-gnu (64-bit)
## Running under: Red Hat Enterprise Linux Server 7.6 (Maipo)
##
## Matrix products: default
## BLAS/LAPACK: /autofs/nccs-svm1_sw/summit/r/3.6.1/rhel7.6_gnu6.4.0/lib64/R/lib/libRblas.so
##
## locale:
## [1] C
##
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base
##
## loaded via a namespace (and not attached):
## [1] compiler_3.6.1",4.155983868031182
"Can I use cuda-gdb to set breakpoints inside CUDA kernels on Summit in ""batch"" or ""offline"" mode?
",cuda-gdb allows for breakpoints to be set inside CUDA kernels to inspect the program state on the GPU. This can be a valuable debugging tool but breaking inside kernels does incur significant overhead that should be included in your expected runtime.,4.477848204883061
"Can I use cuda-gdb to set breakpoints inside CUDA kernels on Summit in ""batch"" or ""offline"" mode?
","The time required to hit a breakpoint inside a CUDA kernel depends on how many CUDA threads are used to execute the kernel. It may take several seconds to stop at kernel breakpoints for very large numbers of threads. For this reason, it is recommended to choose breakpoints judiciously, especially when running the debugger in ""batch"" or ""offline"" mode where this overhead may be misperceived as the code hanging. If possible, debugging a smaller problem size with fewer active threads can be more pleasant.",4.411535634851488
"Can I use cuda-gdb to set breakpoints inside CUDA kernels on Summit in ""batch"" or ""offline"" mode?
","GDB, the GNU Project Debugger, is a command-line debugger useful for traditional debugging and investigating code crashes. GDB lets you debug programs written in Ada, C, C++, Objective-C, Pascal (and many other languages).

GDB is available on Summit under all compiler families:

module load gdb

To use GDB to debug your application run:

gdb ./path_to_executable

Additional information about GDB usage can befound on the GDB Documentation Page.",4.132717359582801
"What is the network policy namespace for the MinIO deployment?
","Network policies are specifications of how groups of pods are allowed to communicate with each other and other network endpoints. A pod is selected in a Network Policy based on a label so the Network Policy can define rules to specify traffic to that pod.

When you create a namespace in Slate with the Quota Dashboard some Network Policies are added to your newly created namespace. This means that pods inside your project are isolated from connections not defined in these Network Policies.",4.240120409580753
"What is the network policy namespace for the MinIO deployment?
","name (Set the name of your application)

use_olcf_fs (Controls if NCCS filesystems are used or not - enabled or disabled)

olcf_mount (Set the mount path to your project directory (i.e /ccs/proj/<projectID>/minio/))

pvc_storage (Set the quota for your dedicated storage if use_olcf_fs is disabled)

network_policy_namespace (Set the network policy's namespace to your project name, this will be the output of the oc project command)

The below is not provided in the above configuration, but it must be done for the MinIO application to start properly.",4.146601608958283
"What is the network policy namespace for the MinIO deployment?
","We hope this provides a starting point for more robust implementations of MinIO, if your workload/project benefits from that. In addition, it gives insight into some of the core building blocks for establishing your own, different, applications on Slate.

This example deployment of MinIO enables two possible configurations (which we configure in the following sections):

MinIO running on a NCCS filesystem (GPFS or NFS - exposing filesystem project space through the MinIO GUI).",4.134632533668405
"How are projects that overrun their allocation handled on OLCF systems?
","Allocation Overuse Policy

Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.5527790398044266
"How are projects that overrun their allocation handled on OLCF systems?
","Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.508865219786724
"How are projects that overrun their allocation handled on OLCF systems?
","Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used. For example, consider that job1 is submitted at the same time as job2. The project associated with",4.474438579662779
"Can I set the number of threads for each task dynamically in Summit?
","In addition to specifying the SMT level, you can also control the number of threads per MPI task by exporting the OMP_NUM_THREADS environment variable. If you don't export it yourself, Jsrun will automatically set the number of threads based on the number of cores requested (-c) and the binding (-b) option. It is better to be explicit and set the OMP_NUM_THREADS value yourself rather than relying on Jsrun constructing it for you. Especially when you are using Job Step Viewer which relies on the presence of that environment variable to give you visual thread assignment information.",4.1925609861850575
"Can I set the number of threads for each task dynamically in Summit?
","summit> setenv OMP_NUM_THREADS 4
summit> jsrun -n12 -a1 -c4 -g1 -b packed:4 -d packed ./a.out
Rank: 0; RankCore: 0; Thread: 0; ThreadCore: 0; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}
Rank: 0; RankCore: 0; Thread: 1; ThreadCore: 4; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}
Rank: 0; RankCore: 0; Thread: 2; ThreadCore: 8; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}
Rank: 0; RankCore: 0; Thread: 3; ThreadCore: 12; Hostname: a33n06; OMP_NUM_PLACES: {0},{4},{8},{12}",4.139449507851609
"Can I set the number of threads for each task dynamically in Summit?
","Hardware threads are a feature of the POWER9 processor through which individual physical cores can support multiple execution streams, essentially looking like one or more virtual cores (similar to hyperthreading on some Intel      microprocessors). This feature is often called Simultaneous Multithreading or SMT. The POWER9 processor on Summit supports SMT levels of 1, 2, or 4, meaning (respectively) each physical core looks like 1, 2, or 4 virtual cores. The SMT level is controlled by the -alloc_flags option to bsub. For example, to set the SMT level to 2, add the line #BSUB –alloc_flags",4.133564195879909
"What is the name of the library that provides the DNS resolution functionality?
","Some libraries still resolved to paths outside of /mnt/bb, and the reason for that is that the executable may have several paths in RPATH.



Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-user-guide.",3.867059215913896
"What is the name of the library that provides the DNS resolution functionality?
","In general, using FQDNs to access a service is more convenient. If your service communicates over HTTP or HTTPS, you can set up a https://docs.olcf.ornl.gov/systems/services.html#slate_routes to achieve this. If your service uses another protocol, you can use https://docs.olcf.ornl.gov/systems/services.html#slate_nodeports.

NodePort is a type of Service that reserves a port across the cluster that can route traffic to your pods. This is more flexible than a Route and can handle any TCP or UDP traffic.",3.843258290086038
"What is the name of the library that provides the DNS resolution functionality?
","OLCF systems provide many software packages and scientific libraries pre-installed at the system-level for users to take advantage of. In order to link these libraries into an application, users must direct the compiler to their location. The module show command can be used to determine the location of a particular library. For example",3.81628732888009
"How do I ensure that my job runs on a specific architecture on Summit?
","Currently, the install steps listed below only work for our x86_64 based systems (Andes, Frontier, etc.). The steps can be explored on Summit, but -- due to Summit's Power architecture -- is not recommended or guaranteed to work.

Both Qiskit and pyQuil can live in the same Python environment if desired. However, as this may not always be the case, it is highly recommened to use separate environments if possible or test if packages still function after modifying your environment.",4.206147747351344
"How do I ensure that my job runs on a specific architecture on Summit?
","Summit node architecture diagram

The basic building block of Summit is the IBM Power System AC922 node. Each of the approximately 4,600 compute nodes on Summit contains two IBM POWER9 processors and six NVIDIA Tesla V100 accelerators and provides a theoretical double-precision capability of approximately 40 TF. Each POWER9 processor is connected via dual NVLINK bricks, each capable of a 25GB/s transfer rate in each direction.",4.199452391849468
"How do I ensure that my job runs on a specific architecture on Summit?
",The following sections will provide more information regarding running jobs on Summit. Summit uses IBM Spectrum Load Sharing Facility (LSF) as the batch scheduling system.,4.195744611371234
"What is the purpose of the --login option when starting a Summit shell?
","| Node Type | Description | | --- | --- | | Login | When you connect to Summit, you're placed on a login node. This is the place to write/edit/compile your code, manage data, submit jobs, etc. You should never launch parallel jobs from a login node nor should you run threaded jobs on a login node. Login nodes are shared resources that are in use by many users simultaneously. | | Launch | When your batch script (or interactive batch job) starts running, it will execute on a Launch Node. (If you were a user of Titan, these are similar in function to service nodes on that system). All commands",4.219614622990871
"What is the purpose of the --login option when starting a Summit shell?
","On Summit and Andes, batch-interactive jobs using bash (i.e. those submitted with bsub -Is or salloc) run as interactive, non-login shells (and therefore source ~/.bashrc, if it exists). Regular batch jobs using bash on those systems are non-interactive, non-login shells and source the file defined by the variable $BASH_ENV in the shell from which you submitted the job. This variable is not set by default, so this means that none of these files will be sourced for a regular batch job unless you explicitly set that variable.",4.194660386335662
"What is the purpose of the --login option when starting a Summit shell?
","Recall from the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#system-overview section that Summit has three types of nodes: login, launch, and compute. When you log into the system, you are placed on a login node. When your https://docs.olcf.ornl.gov/systems/summit_user_guide.html#batch-scripts or https://docs.olcf.ornl.gov/systems/summit_user_guide.html#interactive-jobs run, the resulting shell will run on a launch node. Compute nodes are accessed via the jsrun command. The jsrun command should only be issued from within an LSF job (either batch or interactive) on a launch node.",4.180991534990556
"How can I enable expert mode in the deep learning module on Summit?
","below), you can choose between the rendering options by loading its corresponding module on the system you're connected to. For example, to see these modules on Summit:",4.132253583794168
"How can I enable expert mode in the deep learning module on Summit?
","MLflow is an open source platform to manage the Machine Learning (ML) lifecycle, including experimentation, reproducibility, deployment, and a central model registry. To learn more about MLflow, please refer to its documentation.

In order to use MLflow on Summit, load the module as shown below:

$ module load workflows
$ module load mlflow/1.22.0

Run the following command to verify that MLflow is available:

$ mlflow --version
mlflow, version 1.22.0

To run this MLflow demo on Summit, you will create a directory with two files and then submit a batch job to LSF from a Summit login node.",4.11629240091275
"How can I enable expert mode in the deep learning module on Summit?
","IBM Watson Machine Learning Community Edition (ibm-wml-ce) has been replaced by Open-CE. The Open-CE environment is provided on Summit through the module open-ce, which is built based on the Open Cognitive Environment. Open-CE is a Python Anaconda environment that is pre-loaded with many popular machine learning frameworks and tuned to Summit's Power9+NVIDIA Volta hardware.

To access the latest analytics packages use the module load command:

module load open-ce",4.080358405219828
"Why are FP32 LDS atomicAdd() operations slower than equivalent FP64 LDS atomicAdd() operations in some cases?
","Hardware FP atomic operations performed in LDS memory are usually always faster than an equivalent CAS loop, in particular when contention on LDS memory locations is high. Because of a hardware design choice, FP32 LDS atomicAdd() operations can be slower than equivalent FP64 LDS atomicAdd(), in particular when contention on memory locations is low (e.g. random access pattern). The aforementioned behavior is only true for FP atomicAdd() operations. Hardware atomic operations for CAS/Min/Max on FP32 are usually faster than the FP64 counterparts. In cases when contention is very low, a FP32 CAS",4.61428586847086
"Why are FP32 LDS atomicAdd() operations slower than equivalent FP64 LDS atomicAdd() operations in some cases?
","Hardware FP atomic operations performed in LDS memory are usually always faster than an equivalent CAS loop, in particular when contention on LDS memory locations is high. Because of a hardware design choice, FP32 LDS atomicAdd() operations can be slower than equivalent FP64 LDS atomicAdd(), in particular when contention on memory locations is low (e.g. random access pattern). The aforementioned behavior is only true for FP atomicAdd() operations. Hardware atomic operations for CAS/Min/Max on FP32 are usually faster than the FP64 counterparts. In cases when contention is very low, a FP32 CAS",4.61428586847086
"Why are FP32 LDS atomicAdd() operations slower than equivalent FP64 LDS atomicAdd() operations in some cases?
","In cases when contention is very low, a FP32 CAS loop implementing an atomicAdd() operation could be faster than an hardware FP32 LDS atomicAdd(). Applications using single precision FP atomicAdd() are encouraged to experiment with the use of double precision to evaluate the trade-off between high atomicAdd() performance vs. potential lower occupancy due to higher LDS usage.",4.596133319874514
"How do I get started with HPCToolkit on Frontier?
",Please note that the HPSS is not mounted directly onto Frontier nodes. There are two main methods for accessing and moving data to/from the HPSS. The first is to use the command line utilities hsi and htar. The second is to use the Globus data transfer service. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-hpss for more information on both of these methods.,4.266295154049512
"How do I get started with HPCToolkit on Frontier?
","hipcc requires the ROCm Toolclain, See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#exposing-the-rocm-toolchain-to-your-programming-environment





Computational work on Frontier is performed by jobs. Jobs typically consist of several componenets:

A batch submission script

A binary executable

A set of input files for the executable

A set of output files created by the executable

In general, the process for running a job is to:

Prepare executables and input files.

Write a batch script.

Submit the batch script to the batch scheduler.",4.202304291287209
"How do I get started with HPCToolkit on Frontier?
","Programming models supported by HPCToolkit include MPI, OpenMP, OpenACC, CUDA, OpenCL, DPC++, HIP, RAJA, Kokkos, and others.

Below is an example that generates a profile and loads the results in their GUI-based viewer.

module use /gpfs/alpine/csc322/world-shared/modulefiles/x86_64
module load hpctoolkit

# 1. Profile and trace an application using CPU time and GPU performance counters
srun <srun_options> hpcrun -o <measurement_dir> -t -e CPUTIME -e gpu=amd <application>

# 2. Analyze the binary of executables and its dependent libraries
hpcstruct <measurement_dir>",4.185603178717551
"Can I use Edge Termination and Passthrough Termination together in Slate?
","With Passthrough Termination, the encrypted traffic goes straight to the pod with no TLS termination. This is useful if you are running a service such as HTTPD that is handling TLS termination itself. Another use case example could be doing mutual TLS authentication from a pod.

The following command will create a secured route with passthrough termination.

oc create route passthrough --service=my-project \
  --hostname=my-project.apps.<cluster>.ccs.ornl.gov

The produced yaml will look like this:",4.058469379500255
"Can I use Edge Termination and Passthrough Termination together in Slate?
","Edge Termination terminates TLS at the router, before sending traffic to the service. We have a wildcard certificate on the routers for each cluster. These will be used by default if no certificate is provided, and this is the preferred method for securing a route.

$ oc create route edge --service=my-project \
  --hostname=my-project.apps.<cluster>.ccs.ornl.gov

If you would like to use your own keys with edge termination, this can be done with a command similar to this example.",4.01287087823037
"Can I use Edge Termination and Passthrough Termination together in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",3.9468972344070337
"How can I see the environment changes made by a module in Frontier?
","<p style=""font-size:20px""><b>Frontier: Darshan Runtime 3.4.0 (May 10, 2023)</b></p>

The Darshan Runtime modulefile darshan-runtime/3.4.0 on Frontier is now loaded by default. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>.

Unloading darshan-runtime modulefile is recommended for users profiling their applications with other profilers to prevent conflicts.",4.115762533373272
"How can I see the environment changes made by a module in Frontier?
",Programming Environment  Frontier utilizes the Cray Programming Environment.  Many aspects including LMOD are similar to Summit's environment.  But Cray's compiler wrappers and the Cray and AMD compilers are worth noting.  See the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers section for more information.  AMD GPUs  Each frontier node has 4 AMD MI250X accelerators with two Graphic Compute Dies (GCDs) in each accelerator. The system identifies each GCD as an independent device (so for simplicity we use the term GPU when we talk about a GCD) for a total of 8,4.096361182180929
"How can I see the environment changes made by a module in Frontier?
","below), you can choose between the rendering options by loading its corresponding module on the system you're connected to. For example, to see these modules on Summit:",4.083128638941627
"How can I distinguish SPI project IDs from non-SPI project IDs?
","In order to help ensure data separation, each SPI user is given a unique userID for each project. SPI userIDs use the format: <userid>_<proj>_mde . For example: userx_abc123_mde. SPI usernames will not overlap with usernames used in the non-SPI enclaves. Unlike non-SPI usernames, SPI usernames only exist in a single project. Users on multiple SPI projects will have a unique username for each SPI project.",4.330127983078039
"How can I distinguish SPI project IDs from non-SPI project IDs?
","SPI project IDs may look similar to those used in the non-SPI moderate enclave but will always append _mde to the name. For example: abc123_mde.

Projects cannot overlap non-SPI and SPI enclaves. SPI projects will only exist on SPI resources.

<string>:3: (INFO/1) Duplicate explicit target name: ""olcf accounts and projects"".

More information on the OLCF account process can be found in the OLCF Accounts and Projects section of this site.",4.310183503853916
"How can I distinguish SPI project IDs from non-SPI project IDs?
","In order to help ensure data separation, each SPI user is given a unique userid for each project. SPI user names use the format: <userid>_<proj>_mde . For example: usera_prj123_mde.

SPI usernames will not overlap with usernames used in the non-SPI enclaves. Unlike non-SPI usernames, SPI usernames only exist in a single project. Users on multiple SPI projects will have a unique username for each SPI project.  You must specify your unique SPI username matching the target project when connecting.",4.296793420596038
"How do I get more detailed information on rocprof profiling modes?
","the total duration and the average duration (in nanoseconds) of the kernel, and the GPU usage percentage. More detailed infromation on rocprof profiling modes can be found at ROCm Profiler documentation.",4.370746445057673
"How do I get more detailed information on rocprof profiling modes?
","the total duration and the average duration (in nanoseconds) of the kernel, and the GPU usage percentage. More detailed infromation on rocprof profiling modes can be found at ROCm Profiler documentation.",4.370746445057673
"How do I get more detailed information on rocprof profiling modes?
","Integer instructions and cache levels are currently not documented here.

To get started, you will need to make an input file for rocprof, to be passed in through rocprof -i <input_file> --timestamp on -o my_output.csv <my_exe>. Below is an example, and contains the information needed to roofline profile GPU 0, as seen by each rank:",4.252508261578733
"Where can I download the OC tool?
","It is a single binary that can be downloaded from a number of places (the choice is yours):

Direct from the cluster (preferred):

Marble Command Line Tools

Onyx Command Line Tools

Homebrew on MacOS (need Homebrew setup first):

The Homebrew package is not always kept up to date with the latest version of OpenShift so some client features may not be available

$ brew install openshift-cli

RHEL/CentOS (requires openshift-origin repo):

$ yum install origin-clients

From Source

https://github.com/openshift/oc",4.137391200402162
"Where can I download the OC tool?
","For a full list of software availability and latest news at the OLCF, please reference the https://docs.olcf.ornl.gov/software/software-news.html section in OLCF's User Documentation.",4.0311573818771285
"Where can I download the OC tool?
",Chatterjee (OLCF) | Profiling Tools Workshop https://www.olcf.ornl.gov/calendar/profiling-tools-workshop/ | (slides) https://www.olcf.ornl.gov/wp-content/uploads/2019/08/optimizingDCA_profilingWorkshop.pdf | | 2019-08-08 | Advanced Score-P | Mike Brim (OLCF) | Profiling Tools Workshop https://www.olcf.ornl.gov/calendar/profiling-tools-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2019/08/ScorepAdvanced.pdf https://vimeo.com/428153152 | | 2019-08-08 | Performance Analysis with Scalasca | George Makomanolis (OLCF) | Profiling Tools Workshop,4.0185069175898525
"Are there any consequences for non-compliance with OLCF Policy and export control regulations?
","Order is for convenience and no implication of priority is implied.

Products installed should be limited to those explicitly listed in the project application and approved by the OLCF.

The project application is reviewed by the Export Control Office. If you would like to install additional packages not listed in your original application, the Project PI must contact the OLCF at help@olcf.ornl.gov before making changes.

Products must provide appropriate modules for their software.",4.2524142421575135
"Are there any consequences for non-compliance with OLCF Policy and export control regulations?
","Applicants who appear on a restricted foreign country listing in section 15 CFR 740.7 License Exceptions for Computers are denied access based on US Foreign Policy. The countries cited are Cuba, Iran, North Korea, Sudan, and Syria. Additionally, no work may be performed on OLCF computers on behalf of foreign nationals from these countries.

Users may not deliberately interfere with other users accessing system resources.",4.199145450559295
"Are there any consequences for non-compliance with OLCF Policy and export control regulations?
","All software used on OLCF computers must be appropriately acquired and used according to the appropriate software license agreement. Possession, use, or transmission of illegally obtained software is prohibited. Likewise, users shall not copy, store, or transfer copyrighted software, except as permitted by the owner of the copyright. Only export-controlled codes approved by the Export Control Office may be run by parties with sensitive data agreements.

Users must not intentionally introduce or use malicious software such as computer viruses, Trojan horses, or worms.",4.19736678701591
"How can I randomize the server port in Paraview?
","Although in a single machine setup both the ParaView client and server run on the same host, this need not be the case. It is possible to run a local ParaView client to display and interact with your data while the ParaView server runs in an Andes or Summit batch job, allowing interactive analysis of very large data sets.",4.051456875786296
"How can I randomize the server port in Paraview?
","After installing, you must give ParaView the relevant server information to be able to connect to OLCF systems (comparable to VisIt's system of host profiles). The following provides an example of doing so. Although several methods may be used, the one described should work in most cases.

For macOS clients, it is necessary to install XQuartz (X11) to get a command prompt in which you will securely enter your OLCF credentials.

For Windows clients, it is necessary to install PuTTY to create an ssh connection in step 2.

Step 1: Save the following servers.pvsc file to your local computer",4.015017914476686
"How can I randomize the server port in Paraview?
","<Argument value=""$NUM_NODES$""/>
          <Argument value=""$MINUTES$""/>
          <Argument value=""$PV_SERVER_PORT$""/>
          <Argument value=""$PV_VERSION_FULL$""/>
          <Argument value=""$HEADLESS_API$""/>
          <Argument value=""/sw/andes/paraview/pvsc/ORNL/andes.cfg""/>
          <Argument value=""PROJECT=$PROJECT$""/>
          <Argument value=""NUM_MPI_TASKS=$NUM_MPI_TASKS$""/>
          <Argument value=""NUM_CORES_PER_MPI_TASK=$NUM_CORES_PER_MPI_TASK$""/>
        </Arguments>
      </Command>
    </CommandStartup>
  </Server>
</Servers>",4.010553889278379
"Can I use a script to submit multiple jsrun jobs to Summit?
","summit>



Jsrun provides the ability to launch multiple jsrun job launches within a single batch job allocation. This can be done within a single node, or across multiple nodes.

By default, multiple invocations of jsrun in a job script will execute serially in order. In this configuration, jobs will launch one at a time and the next one will not start until the previous is complete. The batch node allocation is equal to the largest jsrun submitted, and the total walltime must be equal to or greater then the sum of all jsruns issued.",4.423133538681168
"Can I use a script to submit multiple jsrun jobs to Summit?
","Summit has a node hierarchy that can be very confusing for the uninitiated. The Summit User Guide explains this in depth. This has some consequences that may be unusual for R programmers. A few important ones to note are:

You must have a script that you can run in batch, e.g. with Rscript or R CMD BATCH

All data that needs to be visible to the R process (including the script and your packages) must be on gpfs (not your home directory!).

You must launch your script from the launch nodes with jsrun.

We'll start with something very simple. The script is:

""hello world""",4.292290238578396
"Can I use a script to submit multiple jsrun jobs to Summit?
","As is the case on other OLCF systems, computational work on Summit is performed within jobs. A typical job consists of several components:

A submission script

An executable

Input files needed by the executable

Output files created by the executable

In general, the process for running a job is to:

Prepare executables and input files

Write the batch script

Submit the batch script

Monitor the job's progress before and during execution",4.282042922919992
"What is the purpose of TAU_CALLPATH?
","$ module show tau
---------------------------------------------------------------
   /sw/summit/modulefiles/core/tau/2.28.1:
---------------------------------------------------------------
whatis(""TAU 2.28.1 github "")
setenv(""TAU_DIR"",""/sw/summit/tau/tau2/ibm64linux"")
prepend_path(""PATH"",""/sw/summit/tau/tau2/ibm64linux/bin"")
help([[https://www.olcf.ornl.gov/software_package/tau
]])

The available Makefiles are named per-compiler and are located in:",4.204110654351839
"What is the purpose of TAU_CALLPATH?
","TAU is installed with Program Database Toolkit (PDT) on Summit. PDT is a framework for analyzing source code written in several programming languages. Moreover, Performance Application Programming Interface (PAPI) is supported. PAPI counters are used to assess the CPU performance. In this section, some approaches for profiling and tracing will be presented.

In most cases, we need to use wrappers to recompile the application:

For C: replace the compiler with the TAU wrapper tau_cc.sh

For C++: replace the compiler with the TAU wrapper tau_cxx.sh",4.167328649168318
"What is the purpose of TAU_CALLPATH?
","TAU is a portable profiling and tracing toolkit that supports many programming languages. The instrumentation can be done by inserting in the source code using an automatic tool based on the Program Database Toolkit (PDT), on the compiler instrumentation, or manually using the instrumentation API.

Webpage: https://www.cs.uoregon.edu/research/tau/home.php",4.16134158932414
"Which environment has the oldest version of PyTorch?
","This guide introduces a user to the basic workflow of using conda environments, as well as providing an example of how to create a conda environment that uses a different version of Python than the base environment uses on Summit. Although Summit is being used in this guide, all of the concepts still apply to other OLCF systems. If using Frontier, this assumes you already have installed your own version of conda (e.g., by https://docs.olcf.ornl.gov/software/python/miniconda.html).

OLCF Systems this guide applies to:

Summit

Andes

Frontier (if using conda)",3.939771066239148
"Which environment has the oldest version of PyTorch?
","As noted in the :doc:`/software/python/index` page, it is highly recommended to create new environments in the ""Project Home"" directory.  <string>:119: (INFO/1) No role entry for ""doc"" in module ""docutils.parsers.rst.languages.en"". Trying ""doc"" as canonical role name.  <string>:119: (ERROR/3) Unknown interpreted text role ""doc"".

NumPy is installed ahead of time because h5py depends on it.

After following the prompts for creating your new environment, you can now activate it:

Summit

.. code-block:: bash

   $ source activate /ccs/proj/<project_id>/<user_id>/envs/summit/h5pympi-summit",3.933824903448455
"Which environment has the oldest version of PyTorch?
",| Environment | open-ce/1.2.0 | open-ce/1.4.0 | open-ce/1.5.0 | open-ce/1.5.2 | | --- | --- | --- | --- | --- | | Package | Tensorflow 2.4.1 https://github.com/open-ce/tensorflow-feedstock | Tensorflow 2.6.0 https://github.com/open-ce/tensorflow-feedstock | Tensorflow 2.7.0 https://github.com/open-ce/tensorflow-feedstock | Tensorflow 2.7.1 https://github.com/open-ce/tensorflow-feedstock | | PyTorch 1.7.1 https://github.com/open-ce/pytorch-feedstock | PyTorch 1.9.0 https://github.com/open-ce/pytorch-feedstock | PyTorch 1.10.0 https://github.com/open-ce/pytorch-feedstock | PyTorch 1.10.2,3.9293347889550057
"What is the role of the HIP runtime in memory management in Crusher?
","The AMD MI250X and operating system on Crusher supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Crusher. In the following section we'll discuss how the heterogenous memory space on a Crusher node is surfaced within your application.",4.1701357753570685
"What is the role of the HIP runtime in memory management in Crusher?
","If HSA_XNACK=0, page faults in GPU kernels are not handled and will terminate the kernel. Therefore all memory locations accessed by the GPU must either be resident in the GPU HBM or mapped by the HIP runtime. Memory regions may be migrated between the host DDR4 and GPU HBM using explicit HIP library functions such as hipMemAdvise and hipPrefetchAsync, but memory will not be automatically migrated based on access patterns alone.",4.155257344699122
"What is the role of the HIP runtime in memory management in Crusher?
","If HSA_XNACK=0, page faults in GPU kernels are not handled and will terminate the kernel. Therefore all memory locations accessed by the GPU must either be resident in the GPU HBM or mapped by the HIP runtime. Memory regions may be migrated between the host DDR4 and GPU HBM using explicit HIP library functions such as hipMemAdvise and hipPrefetchAsync, but memory will not be automatically migrated based on access patterns alone.",4.155257344699122
"Can I change the priority of my job after it's been submitted?
","number of processors requested above, this is an adjustment to the

apparent submit time of the job. However, this adjustment has the effect

of making jobs appear much younger than jobs submitted under projects

that have not exceeded their allocation. In addition to the priority

change, these jobs are also limited in the amount of wall time that can

be used. For example, consider that ``job1`` is submitted at the same

time as ``job2``. The project associated with ``job1`` is over its

allocation, while the project for ``job2`` is not. The batch system will",4.18884130502527
"Can I change the priority of my job after it's been submitted?
","Projects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used.",4.187075953749243
"Can I change the priority of my job after it's been submitted?
","The basic priority-setting mechanism for jobs waiting in the queue is the time a job has been waiting relative to other jobs in the queue.

If your jobs require resources outside these queue policies such as higher priority or longer walltimes, please contact help@olcf.ornl.gov.

Jobs are aged according to the job's requested processor count (older age equals higher queue priority). Each job's requested processor count places it into a specific bin. Each bin has a different aging parameter, which all jobs in the bin receive.",4.171113669111585
"Can I place dependencies on jobs to control their execution order?
","Oftentimes, a job will need data from some other job in the queue, but it's nonetheless convenient to submit the second job before the first finishes. Slurm allows you to submit a job with constraints that will keep it from running until these dependencies are met. These are specified with the -d option to Slurm. Common dependency flags are summarized below. In each of these examples, only a single jobid is shown but you can specify multiple job IDs as a colon-delimited list (i.e. #SBATCH -d afterok:12345:12346:12346). For the after dependency, you can optionally specify a +time value for",4.167491793842851
"Can I place dependencies on jobs to control their execution order?
","Most users will find batch jobs an easy way to use the system, as they allow you to ""hand off"" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.",4.096327434012086
"Can I place dependencies on jobs to control their execution order?
","Dependency expressions can be combined with logical operators. For example, if you want a job held until job 12345 is DONE and job 12346 has started, you can use #BSUB -w ""done(12345) && started(12346)""



The default job launcher for Summit is jsrun. jsrun was developed by IBM for the Oak Ridge and Livermore Power systems. The tool will execute a given program on resources allocated through the LSF batch scheduler; similar to mpirun and aprun functionality.

The following compute node image will be used to discuss jsrun resource sets and layout.



1 node

2 sockets (grey)",4.072504802971158
"How can I be verbose with Score-P?
","To instrument your code, you need to compile the code using the Score-P instrumentation command (scorep), which is added as a prefix to your compile statement. In most cases the Score-P instrumentor is able to automatically detect the programming paradigm from the set of compile and link options given to the compiler. Some cases will, however, require some additional link options within the compile statement e.g. CUDA instrumentation.

Below are some basic examples of the different instrumentation scenarios:",4.235778078711894
"How can I be verbose with Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi-processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations.

For detailed information about using Score-P on Summit and the builds available, please see the Score-P Software Page.",4.192920677834521
"How can I be verbose with Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations. It works in combination with Periscope, Scalasca, Vampir, and Tau.

Steps in a typical Score-P workflow to run on https://docs.olcf.ornl.gov/systems/Scorep.html#summit-user-guide:",4.180126187340117
"What is the purpose of the hipMemAdvise function in Frontier?
","Finally, the following table summarizes the nature of the memory returned based on the flag passed as argument to hipMallocManaged() and the use of CPU regular malloc() routine with the possible use of hipMemAdvise().

| API | MemAdvice | Result | | --- | --- | --- | | hipMallocManaged() |  | Fine grained | | hipMallocManaged() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained | | malloc() |  | Fine grained | | malloc() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained |",4.118767702906378
"What is the purpose of the hipMemAdvise function in Frontier?
","Finally, the following table summarizes the nature of the memory returned based on the flag passed as argument to hipMallocManaged() and the use of CPU regular malloc() routine with the possible use of hipMemAdvise().

| API | MemAdvice | Result | | --- | --- | --- | | hipMallocManaged() |  | Fine grained | | hipMallocManaged() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained | | malloc() |  | Fine grained | | malloc() | hipMemAdvise (hipMemAdviseSetCoarseGrain) | Coarse grained |",4.118767702906378
"What is the purpose of the hipMemAdvise function in Frontier?
","If HSA_XNACK=0, page faults in GPU kernels are not handled and will terminate the kernel. Therefore all memory locations accessed by the GPU must either be resident in the GPU HBM or mapped by the HIP runtime. Memory regions may be migrated between the host DDR4 and GPU HBM using explicit HIP library functions such as hipMemAdvise and hipPrefetchAsync, but memory will not be automatically migrated based on access patterns alone.",4.00885203087454
"Where do I enter the path for my data on Orion in Frontier?
","Click in the right side “Path” box and enter the path where you want to put your data on Orion, for example, /lustre/orion/stf007/proj-shared/my_orion_data

Click the left ""Start"" button.

Click on “Activity“ in the left blue menu bar to monitor your transfer. Globus will send you an email when the transfer is complete.

<string>:5: (INFO/1) Enumerated list start value not ordinal-1: ""2"" (ordinal 2)",4.284822505738707
"Where do I enter the path for my data on Orion in Frontier?
","Frontier is connected to Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Data will not be automatically transferred from Alpine to Orion. Frontier also has access to the center-wide NFS-based filesystem (which provides user and project home areas). Each compute node has two 1.92TB Non-Volatile Memory storage devices. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-data-storage for more",4.223945593003499
"Where do I enter the path for my data on Orion in Frontier?
","Though the NFS filesystem's User Home and Project Home areas are read/write from Frontier's compute nodes, we strongly recommend that users launch and run jobs from the Lustre Orion parallel filesystem instead due to its larger storage capacity and superior performance. Please see below for Lustre Orion filesystem storage areas and paths.",4.188193844463333
"What is the vendor of the nvc compiler on Summit?
","The following compilers are available on Summit:

XL: IBM XL Compilers (loaded by default)

LLVM: LLVM compiler infrastructure

PGI: Portland Group compiler suite

NVHPC: Nvidia HPC SDK compiler suite

GNU: GNU Compiler Collection

NVCC: CUDA C compiler

PGI was bought out by Nvidia and have rebranded their compilers, incorporating them into the NVHPC compiler suite. There will be no more new releases of the PGI compilers.",4.38551459566718
"What is the vendor of the nvc compiler on Summit?
",The NVMes on Summit are local to each node.,4.188052623021457
"What is the vendor of the nvc compiler on Summit?
",| (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_thompto.pdf https://vimeo.com/306003413 | | 2018-12-03 | NVIDIA V100 | Jeff Larkin (NVIDIA) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording) https://www.olcf.ornl.gov/wp-content/uploads/2018/12/summit_workshop_Volta-Architecture.pdf https://vimeo.com/306004462 | | 2018-12-03 | Summit System Overview | Scott Atchley (OLCF) | Summit Training Workshop https://www.olcf.ornl.gov/calendar/summit-training-workshop/ | (slides | recording),4.171676268219368
"What is the purpose of the ""strategy"" field in a Deployment object in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.220377881673496
"What is the purpose of the ""strategy"" field in a Deployment object in Slate?
","Edit Deployment Config



A deployment strategy is a method for upgrading an application. The goal of deployment strategies is to make an update with no downtime to the end users.

The two most common values here will be RollingUpdate and Recreate. The default is RollingUpdate.

Since the end user usually will be accessing an application with a route, the deployment strategy can focus on deployment configuration features. Here are a few examples of the deployment configuration based strategies.",4.215993303839699
"What is the purpose of the ""strategy"" field in a Deployment object in Slate?
","Blue-green deployments are defined as running two versions of an application at the same time, then moving traffic from the old production version (the green version) to the new production version (the blue version). You could use a Rolling Deployment Strategy for this, but for the sake of showing how route-based deployments work, we'll use a route.",4.050974518338638
"Is there a risk of data becoming unavailable if it is not migrated from Alpine to Orion?
","To assist you with moving your data off of Alpine, the DTN's mount the new Orion filesystem and all projects with access to Alpine have now been granted access to the Orion filesystem.

Please do not wait to migrate needed data, begin migrating all needed data now.",4.401994633171171
"Is there a risk of data becoming unavailable if it is not migrated from Alpine to Orion?
",Data will not be automatically transferred from Alpine to Orion. Users should consider the data needed from Alpine and transfer it. Globus is the preferred method and there is access to both Orion and Alpine through the Globus OLCF DTN endpoint. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-transferring-data-globus. Use of HPSS to specifically stage data for the Alpine to Orion transfer is discouraged.,4.380928018662457
"Is there a risk of data becoming unavailable if it is not migrated from Alpine to Orion?
","We highly encourage all teams to start migrating and/or deleting data from the Alpine filesystem now.  If you wait too late in the year to begin the transition, you will run the risk of running out of time to move your data before the system is decommissioned.  It is important to note that any data remaining on the Alpine filesystem after December 31, 2023, will truly be unavailable and not recoverable in any way as the system will be dismantled and the drives will be shredded.

Moving data off-site

Globus is the suggested tool to move data off-site",4.346265891536038
"How do I write a batch script for a job on Frontier?
","Submit the batch script to the batch scheduler.

Optionally monitor the job before and during execution.

The following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.",4.539585678864279
"How do I write a batch script for a job on Frontier?
","The most common way to interact with the batch system is via batch scripts. A batch script is simply a shell script with added directives to request various resoruces from or provide certain information to the scheduling system.  Aside from these directives, the batch script is simply the series of commands needed to set up and run your job.

To submit a batch script, use the command sbatch myjob.sl

Consider the following batch script:

#!/bin/bash
#SBATCH -A ABC123
#SBATCH -J RunSim123
#SBATCH -o %x-%j.out
#SBATCH -t 1:00:00
#SBATCH -p batch
#SBATCH -N 1024",4.286213791826775
"How do I write a batch script for a job on Frontier?
","Frontier

.. code-block:: bash

    #!/bin/bash
    #SBATCH -A ABC123
    #SBATCH -J job_name
    #SBATCH -N 1
    #SBATCH -t 0:05:00
    #SBATCH -p batch

    unset SLURM_EXPORT_ENV

    cd $SLURM_SUBMIT_DIR
    date",4.27387960913658
"What is the main functionality of cuGraph in RAPIDS?
","cuML is a suite of libraries that implement machine learning algorithms and mathematical primitives functions that share compatible APIs with other RAPIDS projects.

cuGraph is a GPU accelerated graph analytics library, with functionality like NetworkX, which is seamlessly integrated into the RAPIDS data science platform.

dask-cuda extends Dask where it is necessary to scale up and scale out RAPIDS workflows.

cuCIM is a GPU accelerated n-dimensional image processing and image I/O library. It has a similar API to scikit-image.",4.301594876516915
"What is the main functionality of cuGraph in RAPIDS?
","RAPIDS is a suite of libraries to execute end-to-end data science and analytics pipelines on GPUs. RAPIDS utilizes NVIDIA CUDA primitives for low-level compute optimization through user-friendly Python interfaces. An overview of the RAPIDS libraries available at OLCF is given next.

cuDF is a Python GPU DataFrame library (built on the Apache Arrow columnar memory format) for loading, joining, aggregating, filtering, and otherwise manipulating data.",4.248478769747371
"What is the main functionality of cuGraph in RAPIDS?
","Running RAPIDS multi-gpu/multi-node workloads requires a dask-cuda cluster. Setting up a dask-cuda cluster on Summit requires two components:

dask-scheduler.

dask-cuda-workers.

Once the dask-cluster is running, the RAPIDS script should perform four main tasks. First, connect to the dask-scheduler; second, wait for all workers to start; third, do some computation, and fourth, shutdown the dask-cuda-cluster.

Reference of multi-gpu/multi-node operation with cuDF, cuML, cuGraph is available in the next links:

10 Minutes to cuDF and Dask-cuDF.

cuML's Multi-Node, Multi-GPU Algorithms.",4.129804213928619
"How do I profile my code using Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations. It works in combination with Periscope, Scalasca, Vampir, and Tau.

Steps in a typical Score-P workflow to run on https://docs.olcf.ornl.gov/systems/Scorep.html#summit-user-guide:",4.418313438224443
"How do I profile my code using Score-P?
","To instrument your code, you need to compile the code using the Score-P instrumentation command (scorep), which is added as a prefix to your compile statement. In most cases the Score-P instrumentor is able to automatically detect the programming paradigm from the set of compile and link options given to the compiler. Some cases will, however, require some additional link options within the compile statement e.g. CUDA instrumentation.

Below are some basic examples of the different instrumentation scenarios:",4.414361002501512
"How do I profile my code using Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi-processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations.

For detailed information about using Score-P on Summit and the builds available, please see the Score-P Software Page.",4.407477785249872
"What is the advantage of using cuda-gdb to set breakpoints inside CUDA kernels on Summit?
",cuda-gdb allows for breakpoints to be set inside CUDA kernels to inspect the program state on the GPU. This can be a valuable debugging tool but breaking inside kernels does incur significant overhead that should be included in your expected runtime.,4.573933860762004
"What is the advantage of using cuda-gdb to set breakpoints inside CUDA kernels on Summit?
","The time required to hit a breakpoint inside a CUDA kernel depends on how many CUDA threads are used to execute the kernel. It may take several seconds to stop at kernel breakpoints for very large numbers of threads. For this reason, it is recommended to choose breakpoints judiciously, especially when running the debugger in ""batch"" or ""offline"" mode where this overhead may be misperceived as the code hanging. If possible, debugging a smaller problem size with fewer active threads can be more pleasant.",4.359903876611746
"What is the advantage of using cuda-gdb to set breakpoints inside CUDA kernels on Summit?
","GDB, the GNU Project Debugger, is a command-line debugger useful for traditional debugging and investigating code crashes. GDB lets you debug programs written in Ada, C, C++, Objective-C, Pascal (and many other languages).

GDB is available on Summit under all compiler families:

module load gdb

To use GDB to debug your application run:

gdb ./path_to_executable

Additional information about GDB usage can befound on the GDB Documentation Page.",4.194265939490917
"How can existing users with RSA SecurID tokens log in to the myOLCF self-service portal?
",Join an Additional Project Existing users with RSA SecurID tokens should log in to the myOLCF self-service portal to apply for additional projects. Existing users without RSA SecurID tokens should fill out the Account Application Form that can be found at the top left of the myOLCF login page without needing to sign in. See the section https://docs.olcf.ornl.gov/systems/documents_and_forms.html#applying-for-a-user-account for complete details.,4.603599766461009
"How can existing users with RSA SecurID tokens log in to the myOLCF self-service portal?
","If you already have a user account at the OLCF, your existing credentials can be leveraged across multiple projects.

If your user account has an associated RSA SecurID (i.e. you have an ""OLCF Moderate"" account), you gain access to another project by logging in to the myOLCF self-service portal and filling out the application under My Account > Join Another Project. For more information, see the https://docs.olcf.ornl.gov/systems/accounts_and_projects.html#myOLCF self-service portal documentation<myolcf-overview>.",4.487478234811665
"How can existing users with RSA SecurID tokens log in to the myOLCF self-service portal?
","myOLCF is currently available to OLCF Moderate user accounts; i.e., users that authenticate to OLCF systems with an RSA SecurID token. Visit https://my.olcf.ornl.gov and authenticate with your OLCF Moderate username and RSA SecurID PASSCODE (PIN followed by the 6-digit tokencode).

The myOLCF login page

OLCF Open user accounts, i.e., users that authenticate to OLCF systems with a password, cannot access myOLCF at this time, as we are still investigating the feasibility of supporting password-only authentication.",4.472032544024549
"How long does it take for my account application to be processed?
","When our accounts team begins processing your application, you will receive an automated email containing an unique 36-character confirmation code. Make note of it; you can use it to check the status of your application at any time.

The principal investigator (PI) of the project must approve your account and system access. We will make the project PI aware of your request.",4.285180331809907
"How long does it take for my account application to be processed?
","First-time users should apply for an account using the Account Request Form. You will need the correct 6 character project ID from your PI.

When our accounts team begins processing your application, you will receive an automated email containing a unique 36-character confirmation code. Make note of it; you can use it to check the status of your application at any time.

The principal investigator (PI) of the project must approve your account and system access. We will make the project PI aware of your request.",4.206749594728592
"How long does it take for my account application to be processed?
","On the ""Policies & Agreements"" page click the links to read the polices and then answer ""Yes"" to affirm you have read each. Certify your application by selecting ""Yes"" as well. Then Click ""Submit.""

<string>:5: (INFO/1) Enumerated list start value not ordinal-1: ""8"" (ordinal 8)

After submitting your application, it will need to pass through the approval process. Depending on when you submit, approval might not occur until the next business day.",4.199635955740408
"How do I map traffic to a specific deployment in Slate?
","While some strategies leverage features of Deployments, others leverage features of routes. If you haven't read the docs on https://docs.olcf.ornl.gov/systems/deployment.html#slate_services or https://docs.olcf.ornl.gov/systems/deployment.html#slate_routes, read those first before trying these more advanced strategies.

Since routes are intended for HTTP and HTTPS traffic, these strategies are best used for web applications.",4.358465445605459
"How do I map traffic to a specific deployment in Slate?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.124567870407063
"How do I map traffic to a specific deployment in Slate?
","By default, a route will only expose your https://docs.olcf.ornl.gov/systems/route.html#slate_services to NCCS networks. If you need your service exposed to the world outside ORNL, you will first need to get your project approved for external routes. To do this, submit a systems ticket. In the description, give us your project name and a brief reasoning for why exposing externally is needed.

We will let you know once your project is able to set up external routes.",4.114916527596511
"What is the figure shown in the diagram representing the NDS servers on Summit?
",Figure 1. An example of the NDS servers on Summit,4.755177883591152
"What is the figure shown in the diagram representing the NDS servers on Summit?
","Summit node architecture diagram

The basic building block of Summit is the IBM Power System AC922 node. Each of the approximately 4,600 compute nodes on Summit contains two IBM POWER9 processors and six NVIDIA Tesla V100 accelerators and provides a theoretical double-precision capability of approximately 40 TF. Each POWER9 processor is connected via dual NVLINK bricks, each capable of a 25GB/s transfer rate in each direction.",4.2684270593744
"What is the figure shown in the diagram representing the NDS servers on Summit?
","Summit is an IBM system located at the Oak Ridge Leadership Computing Facility. With a theoretical peak double-precision performance of approximately 200 PF, it is one of the most capable systems in the world for a wide range of traditional computational science applications. It is also one of the ""smartest"" computers in the world for deep learning applications with a mixed-precision capability in excess of 3 EF.



Summit node architecture diagram",4.14540208232422
"Is there a driver bug involving cudaManagedMemory on Summit?
","There is a (very rare) driver bug involving cudaManagedMemory that can cause a kernel panic. If you encounter this bug, please contact the OLCF User Support team. The easiest mitigation is for the user code to initialize a context on every GPU with which it intends to interact (for example by calling cudaFree(0) while each device is active).",4.336596983205046
"Is there a driver bug involving cudaManagedMemory on Summit?
","Using the hip-cuda/5.1.0 module on Summit requires CUDA v11.4.0 or later. However, only CUDA v11.0.3 and earlier currently support GPU-aware MPI, so GPU-aware MPI is currently not available when using HIP on Summit.

Any codes compiled with post 11.0.3 CUDA (cuda/11.0.3) will not work with MPS enabled (-alloc_flags ""gpumps"") on Summit. The code will hang indefinitely. CUDA v11.0.3 is the latest version that is officially supported by IBM's software stack installed on Summit. We are continuing to look into this issue.",4.241684488340626
"Is there a driver bug involving cudaManagedMemory on Summit?
","Although there are newer CUDA modules on Summit, cuda/11.0.3 is the latest version that is officially supported by the version of IBM's software stack installed on Summit. When loading the newer CUDA modules, a message is printed to the screen stating that the module is for “testing purposes only”. These newer unsupported CUDA versions might work with some users’ applications, but importantly, they are known not to work with GPU-aware MPI.",4.1947646469501
"What is the size of the lmp file on Frontier?
","Frontier is connected to Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Data will not be automatically transferred from Alpine to Orion. Frontier also has access to the center-wide NFS-based filesystem (which provides user and project home areas). Each compute node has two 1.92TB Non-Volatile Memory storage devices. See https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-data-storage for more",4.075915315290985
"What is the size of the lmp file on Frontier?
","Frontier mounts Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Frontier will not mount Alpine when Orion is in production.",4.047775090779785
"What is the size of the lmp file on Frontier?
","Summit

Andes

Frontier

Scientific simulations generate large amounts of data on Summit (about 100 Terabytes per day for some applications). Because of how large some datafiles may be, it is important that writing and reading these files is done as fast as possible. Less time spent doing input/output (I/O) leaves more time for advancing a simulation or analyzing data.",3.971132816129801
"What table summarizes the current under-utilization policy?
",The OLCF has a pull-back policy for under-utilization of INCITE allocations. Under-utilized INCITE project allocations will have core-hours removed from their outstanding core-hour project balance at specific times during the INCITE calendar year. The following table summarizes the current under-utilization policy:,4.102032419880781
"What table summarizes the current under-utilization policy?
","Utilization is calculated daily using batch jobs which complete between 00:00 and 23:59 of the previous day. For example, if a job moves into a run state on Tuesday and completes Wednesday, the job's utilization will be recorded Thursday. Only batch jobs which write an end record are used to calculate utilization. Batch jobs which do not write end records due to system failure or other reasons are not used when calculating utilization. Jobs which fail because of run-time errors (e.g. the user's application causes a segmentation fault) are counted against the allocation.",4.010856561208152
"What table summarizes the current under-utilization policy?
","Utilization is calculated daily using batch jobs which complete between 00:00 and 23:59 of the previous day. For example, if a job moves into a run state on Tuesday and completes Wednesday, the job's utilization will be recorded Thursday. Only batch jobs which write an end record are used to calculate utilization. Batch jobs which do not write end records due to system failure or other reasons are not used when calculating utilization. Jobs which fail because of run-time errors (e.g. the user's application causes a segmentation fault) are counted against the allocation.",4.010856561208152
"How can I override the default exclusion list for SBCAST?
","Notice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable. Once libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.

As mentioned above, you can alternatively use --exclude=NONE on sbcast to send all libraries along with the binary. Using --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time. A job script for the previous example, modified for sending all libraries is shown below.",4.020763314956891
"How can I override the default exclusion list for SBCAST?
","Notice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable. Once libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.

As mentioned above, you can use --exclude=NONE on sbcast to send all libraries along with the binary. Using --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time. A job script for the previous example, modified for sending all libraries is shown below.",4.0161515006136375
"How can I override the default exclusion list for SBCAST?
","# If you SBCAST **all** your libraries (ie, `--exclude=NONE`), you may use the following line:
#export LD_LIBRARY_PATH=""/mnt/bb/$USER/${exe}_libs:$(pkg-config --variable=libdir libfabric)""
# Use with caution -- certain libraries may use ``dlopen`` at runtime, and that is NOT covered by sbcast
# If you use this option, we recommend you contact OLCF Help Desk for the latest list of additional steps required",4.00273394549281
"What is the purpose of the -optCompInst flag in TAU?
","| | -optTwauWrapFile=""<file>"" | Specify path to link_options.tau generated by tau_gen_wrapper | | -optHeaderInst | Enable instrumentation of headers | | -optLinking="""" | Options passed to the linker | | -optCompile="""" | Options passed to the compiler | | -optPdtF95Opts="""" | Add options to the Fortran parser in PDT | | -optPdtF95Reset="""" | Reset options for Fortran parser in PDT | | -optPdtCOpts="""" | Options for C parser in PDT | | -optPdtCXXOpts="""" | Options for C++ parser in PDT |",4.170177208201726
"What is the purpose of the -optCompInst flag in TAU?
","Below, we'll look at using TAU to profile each case.

Edit the cmake_summit_pgi.sh and replace mpic++ with tau_cxx.sh. This applies only for the non-GPU versions.

TAU works with special TAU makefiles to declare what programming models are expected from the application:

The available makefiles are located inside TAU installation:",4.145280475179401
"What is the purpose of the -optCompInst flag in TAU?
","export TAU_OPTIONS=“-optTauSelectFile=phase.tau”

Now when you instrument your application, the phase called phase 1 are the lines 300-327 of the file miniWeather_mpi.cpp. Every call will be instrumented. This could create signiificant overhead, thus you should be careful when you use it.

Create a file called phases.tau.

BEGIN_INSTRUMENT_SECTION
static phase name=""phase1"" file=""miniWeather_mpi.cpp"" line=300 to line=327
static phase name=""phase2"" file=""miniWeather_mpi.cpp"" line=333 to line=346
END_INSTRUMENT_SECTION

Declare the TAU_OPTIONS variable.",4.128552699007949
"What port is exposed by the service to the route?
","In OpenShift, a Route exposes https://docs.olcf.ornl.gov/systems/route.html#slate_services with a host name, such as https://my-project.apps.<cluster>.ccs.ornl.gov.

A Service can be exposed with routes over HTTP (insecure), HTTPS (secure), and TLS with SNI (also secure).

Routes are best used when you have created a service which communicates over HTTP or HTTPS, and you want this service to be accessible from outside the cluster with a FQDN.

If your application doesn't communicate over HTTP or HTTPS, you should use https://docs.olcf.ornl.gov/systems/route.html#slate_nodeports instead.",4.267160758853114
"What port is exposed by the service to the route?
","Route Exposed

While a route usually points to one service through the to parameter in the configuration, it is possible to have as many as four services to load balance between. This is used with A/B deployments.

Here is an example route which points to 3 services:",4.251261358770507
"What port is exposed by the service to the route?
","Note that a NodePort value will automatically be given by the service controller.

Your service can then be accessed by the scheme apps.{cluster}.ccs.ornl.gov:{nodePort}.

In this example, if the service was running on marble, I could access it with apps.marble.ccs.ornl.gov:30298",4.232147116945141
"What is the deadline for submitting a request for credits to run a job on the System Model H1 family and System Model H2 hardware?
","Running a job on the System Model H1 family and System Model H2 hardware requires Quantinuum Credits. Additional information on credit usage can be found in the Quantinuum Systems User Guide under the Examples tab on the Quantinuum User Portal. Due to increased demand and to make the most efficient use of credits, the following allocating policy will go into effect starting October 1st 2022:

Any request for credits must be submitted by the project Principle Investigator (PI) to help@olcf.ornl.gov",4.576426743854848
"What is the deadline for submitting a request for credits to run a job on the System Model H1 family and System Model H2 hardware?
","Allocations will be granted on a monthly basis to maximize the availability of the H1 family and H2 machines. Please note that allocations do not carry over to the next month and must be consumed in the month granted.

Allocation requests requiring 20 qubits and under will be considered for H1 family machines, and allocation requests requiring 21-32 qubits will be considered for H2.

Allocation requests for the following month must be submitted no later than the 25th of the preceding month.  The uptime schedule is available on the Calendar tab of the Quantinuum User Portal.",4.250702964125449
"What is the deadline for submitting a request for credits to run a job on the System Model H1 family and System Model H2 hardware?
","Information on submitting jobs to Quantinuum systems, system availability, checking job status, and tracking usage can be found in the Quantinuum Systems User Guide under the Examples tab on the Quantinuum User Portal.

Users have access to the API validator to check program syntax, and to the Quantinuum System Model H1 emulator, which returns actual results back as if users submitted code to the real quantum hardware.",4.1576105904503065
"What is the maximum number of processors I can use for the ""batch"" queue?
","The batch-hm queue (and the batch-hm-spi queue for Moderate Enhanced security enclave projects) is used to access Summit's high-memory nodes.  Jobs may use all 54 nodes. It enforces the following policies:

Limit of (4) eligible-to-run jobs per user.

Jobs in excess of the per user limit above will be placed into a held state, but will change to eligible-to-run at the appropriate time.

Users may have only (25) jobs queued in the batch-hm queue at any state at any time. Additional jobs will be rejected at submit time.

batch-hm job limits:",4.255045605232115
"What is the maximum number of processors I can use for the ""batch"" queue?
","| Bin | Min Nodes | Max Nodes | Max Walltime (Hours) | Aging Boost (Days) | | --- | --- | --- | --- | --- | | 1 | 5,645 | 9,408 | 12.0 | 8 | | 2 | 1,882 | 5,644 | 12.0 | 4 | | 3 | 184 | 1,881 | 12.0 | 0 | | 4 | 92 | 183 | 6.0 | 0 | | 5 | 1 | 91 | 2.0 | 0 |

The batch queue is the default queue for production work on Frontier. Most work on Frontier is handled through this queue. The following policies are enforced for the batch queue:

Limit of four eligible-to-run jobs per user. (Jobs in excess of this number will be held, but will move to the eligible-to-run state at the appropriate time.)",4.231315796554377
"What is the maximum number of processors I can use for the ""batch"" queue?
","| Bin | Min Nodes | Max Nodes | Max Walltime (Hours) | Aging Boost (Days) | | --- | --- | --- | --- | --- | | 1 | 2,765 | 4,608 | 24.0 | 15 | | 2 | 922 | 2,764 | 24.0 | 10 | | 3 | 92 | 921 | 12.0 | 0 | | 4 | 46 | 91 | 6.0 | 0 | | 5 | 1 | 45 | 2.0 | 0 |

The batch queue (and the batch-spi queue for Moderate Enhanced security enclave projects) is the default queue for production work on Summit.  Most work on Summit is handled through this queue. It enforces the following policies:

Limit of (4) eligible-to-run jobs per user.",4.229313823814928
"How much storage is available in the scratch directory?
","Each project is granted a Project Work directory; these reside in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. Project Work directories can be accessed by all members of a project and are intended for sharing data within a project. Project Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but",4.160624703571251
"How much storage is available in the scratch directory?
","Each project has a World Work directory that resides in the center-wide, high-capacity Spectrum Scale file system on large, fast disk areas intended for global (parallel) access to temporary/scratch storage. World Work areas can be accessed by all users of the system and are intended for sharing of data between projects. World Work directories are provided commonly across most systems. Because of the scratch nature of the file system, it is not backed up and files are automatically purged on a regular bases. Files should not be retained in this file system for long, but rather should be",4.159009908529085
"How much storage is available in the scratch directory?
","footnotes

1

This entry is for legacy User Archive directories which contained user data on January 14, 2020. There is also a quota/limit of 2,000 files on this directory.

2

User Archive directories that were created (or had no user data) after January 14, 2020. Settings other than permissions are not applicable because directories are root-owned and contain no user files.

Moderate Enhanced projects do not have HPSS storage",4.110891841599114
"How do I add the upstream stable repository to Helm?
","One nice feature of helm is that it uses the underlying authentication credentials to kubernetes, so once you login with oc login, the helm client will authenticate automatically.

By default, helm doesn't have any chart repositories, so let's add the upstream stable repository.

helm repo add stable https://charts.helm.sh/stable

Now you can install helm charts with helm install stable/<package_name>. You can think of this command as a parallel to running yum install on a RHEL/CentOS-based system, or apt install on a debian-based system.",4.442255352579573
"How do I add the upstream stable repository to Helm?
","Note that this searches much more than the stable repo we added above, so you may need to add another repo with helm repo add. Be sure to run helm repo update before installing new charts, to make sure the charts are up to date.

You can also search only the repos that you have added to your local client with helm search repo",4.330758029956697
"How do I add the upstream stable repository to Helm?
","It is also possible to write your own charts for helm, if you have an application that can be deployed to many namespaces or that could benefit from templating objects. How to write charts is outside the scope of this documentation, but the upstream docs are a great place to start.",4.213869073363228
"Can rocprof profile GPUs from different vendors?
","rocprof gathers metrics on kernels run on AMD GPU architectures. The profiler works for HIP kernels, as well as offloaded kernels from OpenMP target offloading, OpenCL, and abstraction layers such as Kokkos. For a simple view of kernels being run, rocprof --stats --timestamp on is a great place to start. With the --stats option enabled, rocprof will generate a file that is named results.stats.csv by default, but named <output>.stats.csv if the -o flag is supplied. This file will list all kernels being run, the number of times they are run, the total duration and the average duration (in",4.251100242190563
"Can rocprof profile GPUs from different vendors?
","rocprof gathers metrics on kernels run on AMD GPU architectures. The profiler works for HIP kernels, as well as offloaded kernels from OpenMP target offloading, OpenCL, and abstraction layers such as Kokkos. For a simple view of kernels being run, rocprof --stats --timestamp on is a great place to start. With the --stats option enabled, rocprof will generate a file that is named results.stats.csv by default, but named <output>.stats.csv if the -o flag is supplied. This file will list all kernels being run, the number of times they are run, the total duration and the average duration (in",4.251100242190563
"Can rocprof profile GPUs from different vendors?
","Integer instructions and cache levels are currently not documented here.

To get started, you will need to make an input file for rocprof, to be passed in through rocprof -i <input_file> --timestamp on -o my_output.csv <my_exe>. Below is an example, and contains the information needed to roofline profile GPU 0, as seen by each rank:",4.221870742659309
"Can I use the partner project on Summit/Frontier for all my build workflows?
",project on Summit/Frontier.  The partner project would provide login access to the non-SPI Summit/Frontier login nodes and a build location that is writable from the non-SPI Summit/Frontier and read-only from within the Citadel framework.  For example the partner project would provide the ability to build on Summit/Frontier in /sw/summit/mde/abc123_mde where abc123_mde is replaced by your Citadel project. This location is writable from Summit/Frontier but only readable from within the Citadel framework.,4.422999461210886
"Can I use the partner project on Summit/Frontier for all my build workflows?
",User Guide<summit-user-guide> and https://docs.olcf.ornl.gov/systems/index.html#Frontier User Guide<frontier-user-guide> can be used when building workflows for the non-SPI as well as the Citadel framework.,4.198240187589154
"Can I use the partner project on Summit/Frontier for all my build workflows?
","Connect to Summit and navigate to your project space

For the following examples, we'll use the MiniWeather application: https://github.com/mrnorman/miniWeather

$ git clone https://github.com/mrnorman/miniWeather.git

We'll use the PGI compiler; this application supports serial, MPI, MPI+OpenMP, and MPI+OpenACC

$ module load pgi
$ module load parallel-netcdf

Different compilations for Serial, MPI, MPI+OpenMP, and MPI+OpenACC:

$ module load cmake
$ cd miniWeather/c/build
$ ./cmake_summit_pgi.sh
$ make serial
$ make mpi
$ make openmp
$ make openacc",4.136030108634758
"What is the definition of sensitive or restricted information?
","Sensitive or Restricted Information
  Principal Investigators are responsible for knowing whether their project
  uses or generates sensitive or restricted information. Department of Energy
  systems contain data only related to scientific research.

  Sensitive Information: This includes, but is not limited to, personally-identifiable
  information (PII). PII is information that can be used to distinguish or trace an
  individual's identity, either alone or when combined with other information
  that is linked or linkable to a specific individual.",4.434293424763668
"What is the definition of sensitive or restricted information?
","biological, or chemical weapons or any weapons of mass destruction. Authors/generators/owners of information are responsible for its correct categorization as sensitive or non-sensitive. Owners of sensitive information are responsible for its secure handling, transmission, processing, storage, and disposal on OLCF systems. Principal investigators, users, or project delegates that use OLCF resources, or are responsible for overseeing projects that use OLCF resources, are strictly responsible for knowing whether their project generates any of these prohibited data types or information that",4.257094804358694
"What is the definition of sensitive or restricted information?
","biological, or chemical weapons or any weapons of mass destruction. Authors/generators/owners of information are responsible for its correct categorization as sensitive or non-sensitive. Owners of sensitive information are responsible for its secure handling, transmission, processing, storage, and disposal on OLCF systems. Principal investigators, users, or project delegates that use OLCF resources, or are responsible for overseeing projects that use OLCF resources, are strictly responsible for knowing whether their project generates any of these prohibited data types or information that",4.257094804358694
"What should I refrain from doing on the login nodes?
","Login nodes should be used for basic tasks such as file editing, code compilation, data backup, and job submission. Login nodes should not be used for memory- or compute-intensive tasks. Users should also limit the number of simultaneous tasks performed on the login resources. For example, a user should not run (10) simultaneous tar processes on a login node.

Compute-intensive, memory-intensive, or otherwise disruptive processes running on login nodes may be killed without warning.",4.471804074485668
"What should I refrain from doing on the login nodes?
","When you log into an OLCF cluster, you are placed on a login node.  Login node resources are shared by all users of the system. Because of this, users should be mindful when performing tasks on a login node.

Login nodes should be used for basic tasks such as file editing, code compilation, data backup, and job submission. Login nodes should not be used for memory- or compute-intensive tasks. Users should also limit the number of simultaneous tasks performed on the login resources. For example, a user should not run (10) simultaneous tar processes on a login node.",4.369005653365814
"What should I refrain from doing on the login nodes?
","Recall from the System Overview that Frontier contains two node types: Login and Compute. When you connect to the system, you are placed on a login node. Login nodes are used for tasks such as code editing, compiling, etc. They are shared among all users of the system, so it is not appropriate to run tasks that are long/computationally intensive on login nodes. Users should also limit the number of simultaneous tasks on login nodes (e.g. concurrent tar commands, parallel make",4.261745622838017
"How can I disable profiling for Score-P?
","SCOREP_WRAPPER=off disables the instrumentation only in the environment of the configure or cmake command. Subsequent calls to make are not affected and will instrument the application as expected.

<string>:16: (INFO/1) Duplicate implicit target name: ""score-p"".

For more detailed information on using Score-P with CMake or Autotools visit Score-P

To see all available options for instrumentation:  $ scorep --help",4.232368820059157
"How can I disable profiling for Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations. It works in combination with Periscope, Scalasca, Vampir, and Tau.

Steps in a typical Score-P workflow to run on https://docs.olcf.ornl.gov/systems/Scorep.html#summit-user-guide:",4.230647883771997
"How can I disable profiling for Score-P?
","The Score-P measurement infrastructure is a highly scalable and easy-to-use tool suite for profiling, event tracing, and online analysis of HPC applications. Score-P supports analyzing C, C++ and Fortran applications that make use of multi-processing (MPI, SHMEM), thread parallelism (OpenMP, PThreads) and accelerators (CUDA, OpenCL, OpenACC) and combinations.

For detailed information about using Score-P on Summit and the builds available, please see the Score-P Software Page.",4.225710276809161
"What types of data can I process on the SPI?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads.  Because of this, many processes used within the SPI are very similar to those used for standard non-SPI.  This page lists the differences you may see when using OLCF resources to execute SPI workflows.  The page will also point to sections within the site where more information on standard non-SPI use can be found.",4.270353848124205
"What types of data can I process on the SPI?
","The SPI utilizes a mixture of existing resources combined with specialized resources targeted at SPI workloads. Because of this, many processes used within the SPI are very similar to those used for standard non-SPI. This page lists the differences you may see when using OLCF resources to execute SPI resources.",4.260569967826935
"What types of data can I process on the SPI?
","The OLCF's Scalable Protected Infrastructure (SPI) provides resources and protocols that enable researchers to process protected data at scale.  The SPI is built around a framework of security protocols that allows researchers to process large datasets containing private information.  Using this framework researchers can use the center's large HPC resources to compute data containing protected health information (PHI), personally identifiable information (PII), data protected under International Traffic in Arms Regulations, and other types of data that require privacy.",4.213060898702604
"What is an EmptyDir volume on Slate?
",If the application needs access to a temporary space for doing something like generating a configuration file on launch you can mount a EmptyDir volume in the PodSpec which will ensure that whatever user the container is running as will have access to write to that directory.,4.273238385418531
"What is an EmptyDir volume on Slate?
","We hope this provides a starting point for more robust implementations of MinIO, if your workload/project benefits from that. In addition, it gives insight into some of the core building blocks for establishing your own, different, applications on Slate.

This example deployment of MinIO enables two possible configurations (which we configure in the following sections):

MinIO running on a NCCS filesystem (GPFS or NFS - exposing filesystem project space through the MinIO GUI).",4.058975368125767
"What is an EmptyDir volume on Slate?
","MinIO is a high-performance software-defined object storage suite that enables the ability to easily deploy cloud-native data infrastructure for various data workloads. In this example we are deploying a simple, standalone implementation of MinIO on our cloud-native platform, Slate (https://docs.olcf.ornl.gov/systems/minio.html#slate_overview).

This service will only be accessible from inside of ORNL's network.

If your project requires an externally facing service available to the Internet, please contact User Assistance by submitting a help ticket. There is a process to get such approval.",4.014752417968221
"How can I ensure that the GitLab runner is running with the correct token?
","Prior to installation of the GitLab Runner, a registration token for the runner is needed from the GitLab server. This token will allow a GitLab runner to register to the server in the needed location for running CI/CD jobs. Runners themselves may be registered to either a group as a shared runner or a project as a repository specific runner.",4.443246410864057
"How can I ensure that the GitLab runner is running with the correct token?
","If the runner is to be a group shared runner, navigate to the group in GitLab and then go to Settings -> CI/CD. Expand the Runners section of the CI/CD Settings panel. Ensure that the ""Enable shared runners for this group"" toggle is enabled. The registration token should also be available for retrieval from ""Group Runners"" area.",4.388867494750989
"How can I ensure that the GitLab runner is running with the correct token?
","of the CI/CD settings. In the ""Specific Runners"" area, the registration token should be available for retrieval.",4.276163598270671
"What is the name of the Deployment created by the command?
","As with other objects in OpenShift, you can create this Deployment with oc create -f {FILE_NAME}.

Once created, you can check the status of the ongoing deployment process with the command

oc rollout status deploy/{NAME}

To get basic information about the available revisions, if you had done any updates to the deployment since, run

oc rollout history deploy/{NAME}

You can view a specific revision with

oc rollout history deploy/{NAME} --revision=1

For more detailed information about a Deployment, use

oc describe deploy {NAME}

To roll back a deployment, run",4.1841754927133525
"What is the name of the Deployment created by the command?
","In the Deployment make sure to change the YOUR_NAMESPACE string.

Create the Deployment object:

oc create -f deployment.yaml

View the deployment:

oc get deployment nginx-hello-world
NAME                DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-hello-world   3         3         3            3           9s

You should see Desired: 3 and Current: 3

After the deployment has been created it will spin up a pod running NGINX but we need to get traffic from outside the cluster to the pod so that we can display the hello world.",4.123592413027292
"What is the name of the Deployment created by the command?
","From the ArgoCD Applications screen, click the Create Application button. In the General application settings, give the application deployment a name for ArgoCD to refer to in the display. For Project usually the ArgoCD default project created during the ArgoCD instance installation is sufficient. However, your workload may benefit from a different logical grouping by using multiple ArgoCD projects. If it is desired to have ArgoCD automatically deploy resources to an OpenShift namespace, change the Sync Policy to Automatic. Check the Prune Resources to automatically remove objects when they",4.099315792862514
"Can I use MinIO with a different storage system than NCCS or NetApp?
","We hope this provides a starting point for more robust implementations of MinIO, if your workload/project benefits from that. In addition, it gives insight into some of the core building blocks for establishing your own, different, applications on Slate.

This example deployment of MinIO enables two possible configurations (which we configure in the following sections):

MinIO running on a NCCS filesystem (GPFS or NFS - exposing filesystem project space through the MinIO GUI).",4.332651249249352
"Can I use MinIO with a different storage system than NCCS or NetApp?
","MinIO is a high-performance software-defined object storage suite that enables the ability to easily deploy cloud-native data infrastructure for various data workloads. In this example we are deploying a simple, standalone implementation of MinIO on our cloud-native platform, Slate (https://docs.olcf.ornl.gov/systems/minio.html#slate_overview).

This service will only be accessible from inside of ORNL's network.

If your project requires an externally facing service available to the Internet, please contact User Assistance by submitting a help ticket. There is a process to get such approval.",4.293581067329946
"Can I use MinIO with a different storage system than NCCS or NetApp?
","Depending on you how configured your deployment, this could be your NFS or GPFS project space or an isolated volume dedicated/isolated to this MinIO server.

Within the GUI you can create buckets and upload/download data. If you are running this on NFS or GPFS the bucket will map to a directory.",4.211642710178559
"Can I report non-ORNL related bugs and issues in VisIt?
","Past VisIt Tutorials are available on the Visit User's Wiki along with a set of Updated Tutorials available in the VisIt User Manual.

Sample data not pre-packaged with VisIt can be found in the VisIt Data Archive.

Older VisIt Versions with their release notes can be found on the old VisIt website, and Newer Versions can be found on the new VisIt website with release notes found on the VisIt Blog or VisIt Github Releases page.

Non-ORNL related bugs and issues in VisIt can be found and reported on Github.",4.199926610932433
"Can I report non-ORNL related bugs and issues in VisIt?
","Please do not respond to previous tickets with new, unrelated issues. This can slow down response time and make finding relevant information harder thereby slowing down time to resolution.

Let us know if you've solved the issue yourself (and let us know what worked!)

MOST IMPORTANT: do not hesitate to contact us (help@olcf.ornl.gov); we will work through the details with you.",4.098782992847659
"Can I report non-ORNL related bugs and issues in VisIt?
","If you have problems or need helping running on Crusher, please submit a ticket by emailing help@olcf.ornl.gov.



JIRA_CONTENT_HERE",4.082055997743116
"What type of quantum gates does the Quantinuum System use?
","Laser based quantum gates

Linear trap Quantum Charge-Coupled Device (QCCD) architecture with three or more parallel gate zones

Mid-circuit measurement conditioned circuit branching

Qubit reuse after mid-circuit measurement

Native gate set: single-qubit rotations, two-qubit ZZ-gates



Users can access information about Quantinuum's systems, view submitted jobs, look up machine availability, and update job notification preferences on the cloud dashboard on the Quantinuum User Portal.",4.410492625345993
"What type of quantum gates does the Quantinuum System use?
","IBM Quantum Services provides access to more than 20 currently available quantum systems (known as backends).  IBM's quantum processors are made up of superconducting transmon qubits, and users can utilize these systems via the universal, gate-based, circuit model of quantum computation.  Additionally, users have access to 5 different types of simulators, simulating from 32 up to 5000 qubits to represent different aspects of the quantum backends.",4.288828491789096
"What type of quantum gates does the Quantinuum System use?
","New allocation policy in effect Oct. 1st 2022, see https://docs.olcf.ornl.gov/systems/quantinuum.html#quantinuum-alloc section.

Quantinuum offers access to trapped ion quantum computers and emulators, accessible via their API and User Portal. For the complete set of currently available devices, qubit numbers, etc. see the Quantinuum Systems User Guide under the Examples tab on the Quantinuum User Portal.

This guide describes how to use the system once you have access. For instructions on how to gain access, see our :doc:`Quantum Access </quantum/quantum_access>` page instead.",4.23596218383756
"How can I get involved in the development of vendor-neutral, principle-led GitOps practices?
","With GitOps potentially meaning different ideas to different groups, the mission of the Cloud Native Computing Foundation (CNCF) GitOps Working Group (WG) is to define vendor neutral, principle-led meaning for GitOps practices. With the KubeCon NA conference in October, 2021, the GitOps WG released a set of four core GitOps Principles where the desired state of a GitOps managed system must be:

Declarative: A system managed by GitOps must have its desired state expressed declaratively.",4.324486103988993
"How can I get involved in the development of vendor-neutral, principle-led GitOps practices?
","To foster collaboration, discussion, and knowledge sharing, the CNCF GitOps Working Group held GitOpsCon North America 2021 with sessions concerning GitOps in general practice as well as specific tools. Additionally, there are two awesome lists where one may find more information concerning GitOps and tools:

Awesome Argo

Awesome GitOps

The former is curated by one of the committers to the ArgoCD project while the latter is curated by Weaveworks. The remainder of this document is focused solely on the use of the Red Hat OpenShift GitOps operator.",4.209235521770637
"How can I get involved in the development of vendor-neutral, principle-led GitOps practices?
","From the release notes:

Red Hat OpenShift GitOps is a declarative way to implement continuous deployment for cloud native applications. Red Hat OpenShift GitOps ensures consistency in applications when you deploy them to different clusters in different environments, such as: development, staging, and production.",4.165349224810681
"How do I execute a workflow using EnTK 1.13.0 on Summit?
","You will need to know the connection information for both MongoDB and RabbitMQ so that EnTK can be configured to connect to the services.

Then, to use EnTK on Summit, load the module as shown below:

$ module load workflows
$ module load entk/1.13.0

Run the following command to verify that EnTK is available:

$ radical-utils-version
1.13.0

To run EnTK on Summit, you will create two files and then execute two commands from a Summit login node. Currently, EnTK must be run from a Summit login node, rather than within a batch job.",4.43326701029802
"How do I execute a workflow using EnTK 1.13.0 on Summit?
","Finally, run the demo program by executing the following commands from a Summit login node:

$ source setup.bash
$ python3 demo.py3

Congratulations! You should now see interactive output from EnTK while it launches and monitors your job on Summit.",4.348194587277913
"How do I execute a workflow using EnTK 1.13.0 on Summit?
","The Ensemble Toolkit (EnTK) is a Python library developed by the RADICAL Research Group at Rutgers University for developing and executing large-scale ensemble-based workflows. This tutorial shows how to get up and running with EnTK 1.13.0 on Summit specifically. For in-depth information about EnTK itself, please refer to its documentation.

Before using EnTK itself, you will need MongoDB and RabbitMQ services running on https://docs.olcf.ornl.gov/systems/entk.html#Slate<slate>. There are tutorials for MongoDB in this documentation, but the tutorial for RabbitMQ is forthcoming.",4.292143559052149
"Is there a way to acknowledge the use of OLCF resources in my research publications?
","Publications using resources provided by the OLCF are requested to include the following acknowledgment statement: “This research used resources of the Oak Ridge Leadership Computing Facility, which is a DOE Office of Science User Facility supported under Contract DE-AC05-00OR22725.”",4.501323952789704
"Is there a way to acknowledge the use of OLCF resources in my research publications?
","Users should acknowledge the OLCF in all publications and presentations that speak to work performed on OLCF resources:

This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725.



To request software installation, please contact help@olcf.ornl.gov with a description of the software, including the following details:

Software name.

Description of the software and its purpose. Is it export controlled?",4.460866690421308
"Is there a way to acknowledge the use of OLCF resources in my research publications?
","Users should acknowledge the OLCF in all publications and presentations that speak to work performed on OLCF resources:

This research used resources of the Oak Ridge Leadership Computing Facility at the Oak Ridge National Laboratory, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC05-00OR22725.

Many OLCF projects make use of optional subprojects. Subprojects provide a useful means for

dividing allocations among different applications, groups, or individuals

controlling priority

monitoring progress",4.459465904650265
"Where can I find more information and resources on using CuPy?
","You have now discovered what CuPy can provide! Now you can try speeding up your own codes by swapping CuPy and NumPy where you can.

CuPy User Guide

CuPy Website

CuPy API Reference

CuPy Release Notes",4.337729468000626
"Where can I find more information and resources on using CuPy?
","CuPy is a library that implements NumPy arrays on NVIDIA GPUs by utilizing CUDA Toolkit libraries like cuBLAS, cuRAND, cuSOLVER, cuSPARSE, cuFFT, cuDNN and NCCL. Although optimized NumPy is a significant step up from Python in terms of speed, performance is still limited by the CPU (especially at larger data sizes) -- this is where CuPy comes in. Because CuPy's interface is nearly a mirror of NumPy, it acts as a replacement to run existing NumPy/SciPy code on NVIDIA CUDA platforms, which helps speed up calculations further. CuPy supports most of the array operations that NumPy provides,",4.280743211393666
"Where can I find more information and resources on using CuPy?
","As noted in AMD+CuPy limitations, data sizes explored here hang. So, this section currently does not apply to Frontier.

Now that you know how to use CuPy, time to see the actual benefits that CuPy provides for large datasets. More specifically, let's see how much faster CuPy can be than NumPy on Summit. You won't need to fix any errors; this is mainly a demonstration on what CuPy is capable of.

There are a few things to consider when running on GPUs, which also apply to using CuPy:

Higher precision means higher cost (time and space)

The structuring of your data is important",4.272718327967603
"What is the dynamic range of half precision floating point representation?
","The V100 Tensor Cores perform a warp-synchronous multiply and accumulate of 16-bit matrices in the form of D = A * B + C. The operands of this matrix multiplication are 16-bit A and B matrices, while the C and D accumulation matrices may be 16 or 32-bit matrices with comparable performance for either precision.



Half precision floating point representation has a dramatically lower range of numbers than Double or Single precision. Half precision representation consists of 1 sign bit, a 5-bit exponent, and a 10-bit mantissa. This results in a dynamic range of 5.96e-8 to 65,504",4.362990311533811
"What is the dynamic range of half precision floating point representation?
","If you encounter significant differences when running using reduced precision, explore replacing non-converging models in FP16 with BF16, because of the greater dynamic range in BF16. We recommend using BF16 for ML models in general. If you have further questions or encounter issues, contact help@olcf.ornl.gov.

Additional information on MI250X reduced precision can be found at:

The MI250X ISA specification details the flush to zero denorm behavior at: https://developer.amd.com/wp-content/resources/CDNA2_Shader_ISA_18November2021.pdf (See page 41 and 46)",4.1887105739723225
"What is the dynamic range of half precision floating point representation?
","If you encounter significant differences when running using reduced precision, explore replacing non-converging models in FP16 with BF16, because of the greater dynamic range in BF16. We recommend using BF16 for ML models in general. If you have further questions or encounter issues, contact help@olcf.ornl.gov.

Additional information on MI250X reduced precision can be found at:

The MI250X ISA specification details the flush to zero denorm behavior at: https://developer.amd.com/wp-content/resources/CDNA2_Shader_ISA_18November2021.pdf (See page 41 and 46)",4.1887105739723225
"What is the default behavior of MPI rank assignment on Frontier?
","There are many different ways users might choose to perform these mappings, so users are encouraged to clone the hello_mpi_omp program and test whether or not processes and threads are running where intended.

Assigning MPI ranks in a ""round-robin"" (cyclic) manner across L3 cache regions (sockets) is the default behavior on Frontier. This mode will assign consecutive MPI tasks to different sockets before it tries to ""fill up"" a socket.",4.42543716392898
"What is the default behavior of MPI rank assignment on Frontier?
","As has been pointed out previously in the Frontier documentation, notice that GPUs are NOT mapped to MPI ranks in sequential order (e.g., MPI rank 0 is mapped to physical CPU cores 1-7 and GPU 4, MPI rank 1 is mapped to physical CPU cores 9-15 and GPU 5), but this IS expected behavior. It is simply a consequence of the Frontier node architectures as shown in the Frontier Node Diagram and subsequent https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Note on NUMA domains <numa-note>.

Example 2: 1 MPI rank with 7 CPU cores and 1 GPU (single-node)",4.406182347555971
"What is the default behavior of MPI rank assignment on Frontier?
","The most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:",4.359319805544212
"How can I ensure that a Persistent Volume Claim (PVC) is always available for a pod?
","Once the Persistent Volume is created its allocated memory is available to be claimed by a project. This is done through a Persistent Volume Claim. PVC's are created using a YAML file in the same manner that PV's are created. An example of a PVC that claims the PV created in the previous section follows:

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: storage-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

The claim is then placed using:

oc create -f storage-1.yaml",4.440172747321725
"How can I ensure that a Persistent Volume Claim (PVC) is always available for a pod?
","PV Settings

This will redirect you to the status page of your new PVC. You should see Bound in the Status field.

PV Status

Once the claim has been granted to the project a pod within that project can access that storage. To do this, edit the YAML of the pod and under the volume portion of the file add a name and the name of the claim. It will look similar to the following.

volumes:
  name: pvol
  persistentVolumeClaim:
    claimName: storage-1

Then all that is left to do is mount the storage into a container:

volumeMounts:
  - mountPath: /tmp/
    name: pvol",4.379675618611804
"How can I ensure that a Persistent Volume Claim (PVC) is always available for a pod?
","From the main screen of a project go to the Storage option in the hamburger menu on the left hand side of the screen, then click Persistent Volume Claims

Persistent Volume Claim Menu

In the upper right click the Create Persistent Volume Claim button.

Create Storage

This will take you to a screen that allows you to select what you need for your PVC. Give your claim a name and request an amount of storage and click Create. You can also click Edit YAML to edit the PVC object directly.

PV Settings",4.352163789634637
"What is the latest version of VisIt available on Andes?
","Using VisIt on Summit is also an option, as the scalable rendering problem is currently not an issue on Summit (as of Sept. 2021).

As of February 2022, this issue on Andes has been fixed (must use VisIt 3.2.2 or higher).",4.451541957426823
"What is the latest version of VisIt available on Andes?
","VisIt is now available on Frontier through the UMS022 module.

VisIt 3.3.3 is now available on Andes.

VisIt is an interactive, parallel analysis and visualization tool for scientific data. VisIt contains a rich set of visualization features so you can view your data in a variety of ways. It can be used to visualize scalar and vector fields defined on two- and three-dimensional (2D and 3D) structured and unstructured meshes. Further information regarding VisIt can be found at the links provided in the https://docs.olcf.ornl.gov/systems/visit.html#visit-resources section.",4.4054830798420666
"What is the latest version of VisIt available on Andes?
","VisIt uses a client-server architecture. You will obtain the best performance by running the VisIt client on your local computer and running the server on OLCF resources. VisIt for your local computer can be obtained here: VisIt Installation.

Recommended VisIt versions on our systems:

Summit: VisIt 3.1.4

Andes: VisIt 3.3.3

Frontier: VisIt 3.3.3

Using a different version than what is listed above is not guaranteed to work properly.",4.296626344265725
"What is the purpose of the comm.Barrier() function call in the h5py script?
","with h5py.File('output.h5', 'w', driver='mpio', comm=MPI.COMM_WORLD) as f:
    dset = f.create_dataset('test', (42,), dtype='i')
    dset[mpi_rank] = mpi_rank

comm.Barrier()

if (mpi_rank == 0):
    print('42 MPI ranks have finished writing!')

The MPI tasks are going to write to a file named ""output.h5"", which contains a dataset called ""test"" that is of size 42 (assigned to the ""dset"" variable in Python). Each MPI task is going to assign their rank value to the ""dset"" array in Python, so you should end up with a dataset that contains 0-41 in ascending order.",3.991411983117296
"What is the purpose of the comm.Barrier() function call in the h5py script?
","There are various tools that allow users to interact with HDF5 data, but we will be focusing on h5py -- a Python interface to the HDF5 library. h5py provides a simple interface to exploring and manipulating HDF5 data as if they were Python dictionaries or NumPy arrays. For example, you can extract specific variables through slicing, manipulate the shapes of datasets, and even write completely new datasets from external NumPy arrays.",3.896625971466748
"What is the purpose of the comm.Barrier() function call in the h5py script?
","Both HDF5 and h5py can be compiled with MPI support, which allows you to optimize your HDF5 I/O in parallel. MPI support in Python is accomplished through the mpi4py package, which provides complete Python bindings for MPI. Building h5py against mpi4py allows you to write to an HDF5 file using multiple parallel processes, which can be helpful for users handling large datasets in Python. h5Py is available after loading the default Python module on either Summit or Andes, but it has not been built with parallel support.",3.876320871853349
"What is the name of the library that provides the DNS resolution functionality?
","Some libraries still resolved to paths outside of /mnt/bb, and the reason for that is that the executable may have several paths in RPATH.



Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-user-guide.",3.867059215913896
"What is the name of the library that provides the DNS resolution functionality?
","In general, using FQDNs to access a service is more convenient. If your service communicates over HTTP or HTTPS, you can set up a https://docs.olcf.ornl.gov/systems/services.html#slate_routes to achieve this. If your service uses another protocol, you can use https://docs.olcf.ornl.gov/systems/services.html#slate_nodeports.

NodePort is a type of Service that reserves a port across the cluster that can route traffic to your pods. This is more flexible than a Route and can handle any TCP or UDP traffic.",3.843258290086038
"What is the name of the library that provides the DNS resolution functionality?
","OLCF systems provide many software packages and scientific libraries pre-installed at the system-level for users to take advantage of. In order to link these libraries into an application, users must direct the compiler to their location. The module show command can be used to determine the location of a particular library. For example",3.81628732888009
"Can I use `oc` instead of `kubectl` when running `helm status`?
","You can run helm status <release_name> --namespace <project_name> in order to get information about the deployment. In this example, our release name is mysql. Running this command for our mysql installation will give us information on how to connect and authenticate to our newly created database.

The output of helm status will differ from chart to chart, as this output is customizable by the chart itself. If the output has kubectl commands to run, kubectl can be replaced with oc.",4.292721462099149
"Can I use `oc` instead of `kubectl` when running `helm status`?
","Helm is the OLCF preferred package manager for Kubernetes. There are a variety of upstream applications that can be installed with helm, such as databases, web frontends, and monitoring tools. Helm has ""packages"" called ""charts"", which can essentially be thought of as kubernetes object templates. You can pass in values which helm uses to fill in these templates, and create the objects (pods, deployments, services, etc)

Follow https://docs.olcf.ornl.gov/systems/helm_example.html#helm_prerequisite for installing Helm.",4.16705555364111
"Can I use `oc` instead of `kubectl` when running `helm status`?
","$ helm ls
NAME                 NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION
gitlab-runner        myproject       1               2021-09-15 14:10:15.12187869 +0000 UTC  deployed        gitlab-runner-1.0.0     14.2.0
$ oc get pods
NAME                                          READY   STATUS    RESTARTS   AGE
gitlab-runner-gitlab-runner-687486d94-lpmhs   1/1     Running   0          107s",4.118003720183573
"Can I use Python for visualization on Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.38798448398482
"Can I use Python for visualization on Andes?
","Visualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-viz-tools section of the https://docs.olcf.ornl.gov/systems/summit_user_guide.html#andes-user-guide.

For a full list of software available at the OLCF, please see the Software section (coming soon).",4.278141605230982
"Can I use Python for visualization on Andes?
","ParaView was developed to analyze extremely large datasets using distributed memory computing resources. The OLCF provides ParaView server installs on Andes and Summit to facilitate large scale distributed visualizations. The ParaView server running on Andes and Summit may be used in a headless batch processing mode or be used to drive a ParaView GUI client running on your local machine.

For a tutorial of how to get started with ParaView on Andes, see our ParaView at OLCF Tutorial.",4.2355756632706
"How can I see the environment changes made by a module in Andes?
","Changing compilers

If a different compiler is required, it is important to use the correct environment for each compiler. To aid users in pairing the correct compiler and environment, the module system on andes automatically pulls in libraries compiled with a given compiler when changing compilers. The compiler modules will load the correct pairing of compiler version, message passing libraries, and other items required to build and run code. To change the default loaded intel environment to the gcc environment for example, use:

$ module load gcc",4.179579318544894
"How can I see the environment changes made by a module in Andes?
","If a different compiler is required, it is important to use the correct environment for each compiler. To aid users in pairing the correct compiler and environment, the module system on andes automatically pulls in libraries compiled with a given compiler when changing compilers. The compiler modules will load the correct pairing of compiler version, message passing libraries, and other items required to build and run code. To change the default loaded intel environment to the gcc environment for example, use:

$ module load gcc",4.173126233169343
"How can I see the environment changes made by a module in Andes?
",of how to run a Python script using PvBatch on Andes and Summit.,4.130355058885022
"How can I specify the port name for the MongoDB Service in Slate?
","cluster is the Slate cluster (marble, onyx)

The port number should be the one listed from the service command listed above. It may differ from the example, so be sure to update accordingly.

Change the password to the randomly generated one you created during set up.

Once we have finished, we should remove the resources we created.

We have to remove the PVC that was created by the StatefulSet

oc delete service mongo
oc delete statefulset mongo
oc delete persistentvolumeclaim mongo-store-mongo-0
oc delete deployment mongoku",4.249224991571857
"How can I specify the port name for the MongoDB Service in Slate?
","In general, using FQDNs to access a service is more convenient. If your service communicates over HTTP or HTTPS, you can set up a https://docs.olcf.ornl.gov/systems/services.html#slate_routes to achieve this. If your service uses another protocol, you can use https://docs.olcf.ornl.gov/systems/services.html#slate_nodeports.

NodePort is a type of Service that reserves a port across the cluster that can route traffic to your pods. This is more flexible than a Route and can handle any TCP or UDP traffic.",4.2138184457501495
"How can I specify the port name for the MongoDB Service in Slate?
","Steps to configure mongoku

Navigate to http://localhost:3100

Add Server -> ""admin:password@mongo:27017""

Click ""mongo""

We could use the port forwarding technique but that uses a connection that goes through the API server for the cluster which is not very performant. We will change the Service/mongo object so that it creates a NodePort that we can access from outside of the cluster.",4.173245321536287
"How can I use nvprof to profile my GPU application on Summit?
","Prior to Nsight Systems and Nsight Compute, the NVIDIA command line profiling tool was nvprof, which provides both tracing and kernel profiling capabilities. Like with Nsight Systems and Nsight Compute, the profiler data output can be saved and imported into the NVIDIA Visual Profiler for additional graphical analysis. nvprof is in maintenance mode now: it still works on Summit and significant bugs will be fixed, but no new feature development is occurring on this tool.

To use nvprof, the cuda module must be loaded.

summit> module load cuda",4.415531259282808
"How can I use nvprof to profile my GPU application on Summit?
","While using nvprof on the command-line is a quick way to gain insight into your CUDA application, a full visual profile is often even more useful. For information on how to view the output of nvprof in the NVIDIA Visual Profiler, see the NVIDIA Documentation.",4.399797855714427
"How can I use nvprof to profile my GPU application on Summit?
","The first step to GPU profiling is collecting a timeline of your application. (This operation is also sometimes called ""tracing,"" that is, finding the start and stop timestamps of all activities that occurred on the GPU or involved the GPU, such as copying data back and forth.) To do this, we can collect a timeline using the command-line interface, nsys. To use this tool, load the nsight-systems module.

summit> module load nsight-systems

For example, we can profile the vectorAdd CUDA sample (the CUDA samples can be found in $OLCF_CUDA_ROOT/samples if the cuda module is loaded.)",4.369401900690427
