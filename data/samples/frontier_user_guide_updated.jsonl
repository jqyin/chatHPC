{
    "doc": "frontier_user_guide",
    "text": "Frontier User Guide\n\n\n\n\n\nNotable differences between Summit and Frontier:\n\nOrion scratch filesystem\n\nFrontier mounts Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace. Frontier will not mount Alpine and Summit will not mount Orion. Data will not be automatically transferred from Alpine to Orion, so we recommend that users move only needed data between the file systems with Globus.\n\nSee the frontier-data-storage <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-data-storage> section or this recording for more information.\n\nCray Programming Environment\n\nFrontier utilizes the Cray Programming Environment.  Many aspects including LMOD are similar to Summit's environment.  But Cray's compiler wrappers and the Cray and AMD compilers are worth noting.\n\nSee the frontier-compilers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers> section for more information.\n\nAMD GPUs\n\nEach frontier node has 4 AMD MI250X accelerators with two Graphic Compute Dies (GCDs) in each accelerator. The system identifies each GCD as an independent device (so for simplicity we use the term GPU when we talk about a GCD) for a total of 8 GPUs per node (compared to Summit's 6 Nvidia V100 GPUs per node). Each pair of GPUs is associated with a particular NUMA domain (see node diagram in frontier-nodes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes> section) which might affect how your application should lay out data and computation.\n\nSee the amd-gpus <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#amd-gpus> section for more information.\n\nProgramming Models\n\nSince Frontier uses AMD GPUs, code written in Nvidia's CUDA language will not work as is. They need to be converted to use HIP, which is AMD's GPU programming framework, or should be converted to some other GPU framework that supports AMD GPUs as a backend e.g. OpenMP Offload, Kokkos, RAJA, OCCA, SYCL/DPC++ etc .\n\nSee the amd-hip <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#amd-hip> section for more information and links about HIP.\n\nSlurm batch scheduler\n\nFrontier uses SchedMD's Slurm Workload Manager for job scheduling instead of IBM's LSF. Slurm provides similar functionality to LSF, albeit with different commands.  Notable are the separation in batch script submission (sbatch) and interactive batch submission (salloc).\n\nSee the frontier-slurm <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-slurm> section for more infomation including a LSF to Slurm command comparison.\n\nSrun job launcher\n\nFrontier uses Slurm's job launcher, srun, instead of Summit's jsrun to launch parallel jobs within a batch script.  Overall functionality is similar, but commands are notably different. Frontier's compute node layout <frontier-simple> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#compute node layout <frontier-simple>> should also be considered when selecting job layout.\n\nSee the frontier-srun <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-srun> section for more srun information, and see frontier-mapping <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-mapping> for srun examples on Frontier.\n\nOLCF Support\n\nIf you encounter any issues or have questions, please contact the OLCF via the following:\n\nEmail us at help@olcf.ornl.gov\n\nContact your OLCF liaison\n\nSign-up to attend OLCF Office Hours\n\nSystem Overview\n\nFrontier is a HPE Cray EX supercomputer located at the Oak Ridge Leadership Computing Facility. With a theoretical peak double-precision performance of approximately 2 exaflops (2 quintillion calculations per second), it is the fastest system in the world for a wide range of traditional computational science applications. The system has 74 Olympus rack HPE cabinets, each with 128 AMD compute nodes, and a total of 9,408 AMD compute nodes.\n\n\n\nFrontier Compute Nodes\n\nEach Frontier compute node consists of [1x] 64-core AMD \"Optimized 3rd Gen EPYC\" CPU (with 2 hardware threads per physical core) with access to 512 GB of DDR4 memory. Each node also contains [4x] AMD MI250X, each with 2 Graphics Compute Dies (GCDs) for a total of 8 GCDs per node. The programmer can think of the 8 GCDs as 8 separate GPUs, each having 64 GB of high-bandwidth memory (HBM2E). The CPU is connected to each GCD via Infinity Fabric CPU-GPU, allowing a peak host-to-device (H2D) and device-to-host (D2H) bandwidth of 36+36 GB/s. The 2 GCDs on the same MI250X are connected with Infinity Fabric GPU-GPU with a peak bandwidth of 200 GB/s. The GCDs on different MI250X are connected with Infinity Fabric GPU-GPU in the arrangement shown in the Frontier Node Diagram below, where the peak bandwidth ranges from 50-100 GB/s based on the number of Infinity Fabric connections between individual GCDs.\n\nTERMINOLOGY:\n\nThe 8 GCDs contained in the 4 MI250X will show as 8 separate GPUs according to Slurm, ROCR_VISIBLE_DEVICES, and the ROCr runtime, so from this point forward in the quick-start guide, we will simply refer to the GCDs as GPUs.\n\nFrontier node architecture diagram\n\n\n\nThere are [4x] NUMA domains per node and [2x] L3 cache regions per NUMA for a total of [8x] L3 cache regions. The 8 GPUs are each associated with one of the L3 regions as follows:\n\nNUMA 0:\n\nhardware threads 000-007, 064-071 | GPU 4\n\nhardware threads 008-015, 072-079 | GPU 5\n\nNUMA 1:\n\nhardware threads 016-023, 080-087 | GPU 2\n\nhardware threads 024-031, 088-095 | GPU 3\n\nNUMA 2:\n\nhardware threads 032-039, 096-103 | GPU 6\n\nhardware threads 040-047, 104-111 | GPU 7\n\nNUMA 3:\n\nhardware threads 048-055, 112-119 | GPU 0\n\nhardware threads 056-063, 120-127 | GPU 1\n\nBy default, Frontier reserves the first core in each L3 cache region. Frontier uses low-noise mode,\nwhich constrains all system processes to core 0. Low-noise mode cannot be disabled by users.\nIn addition, Frontier uses SLURM core specialization (-S 8 flag at job allocation time, e.g., sbatch)\nto reserve one core from each L3 cache region, leaving 56 allocatable cores. Set -S 0 at job allocation to override this setting.\n\nNode Types\n\nOn Frontier, there are two major types of nodes you will encounter: Login and Compute. While these are\nsimilar in terms of hardware (see: frontier-nodes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes>), they differ considerably in their intended\nuse.\n\n\nThe table provides a breakdown of the different types of nodes found on the Frontier supercomputer. The first row serves as the header and does not require description. The first column, \"Node Type\", lists the types of nodes found on the supercomputer, while the second column, \"Description\", gives a detailed overview of each type. The first type mentioned is the login node, which is where users are placed when they connect to Frontier. This node allows users to write, edit, and compile code, manage data, and submit jobs. However, it is not meant for launching parallel or threaded jobs as it is a shared resource used by multiple users at the same time. The second type of node is the compute node, which makes up the majority of nodes on Frontier. These nodes are where parallel jobs are executed and can be accessed using the srun command. \n\n|Node Type|Description|\n|-|-|\n|Login|When you connect to Frontier, you're placed on a login node. This is the place to write/edit/compile your code, manage data, submit jobs, etc. You should never launch parallel jobs from a login node nor should you run threaded jobs on a login node. Login nodes are shared resources that are in use by many users simultaneously.|\n|Compute|Most of the nodes on Frontier are compute nodes. These are where your parallel job executes. They're accessed via the srun command.|\n\n\n\nSystem Interconnect\n\nThe Frontier nodes are connected with [4x] HPE Slingshot 200 Gbps (25 GB/s) NICs providing a node-injection bandwidth of 800 Gbps (100 GB/s).\n\nFile Systems\n\nFrontier is connected to Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Data will not be automatically transferred from Alpine to Orion. Frontier also has access to the center-wide NFS-based filesystem (which provides user and project home areas). Each compute node has two 1.92TB Non-Volatile Memory storage devices. See frontier-data-storage <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-data-storage> for more information.\n\nFrontier connects to the center’s High Performance Storage System (HPSS) - for user and project archival storage - users can log in to the dtn-user-guide <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#dtn-user-guide> to move data to/from HPSS.\n\nOperating System\n\nFrontier is running Cray OS 2.4 based on SUSE Linux Enterprise Server (SLES) version 15.4.\n\nGPUs\n\nEach Frontier compute node contains 4 AMD MI250X. The AMD MI250X has a peak performance of 53 TFLOPS in double-precision for modeling and simulation. Each MI250X contains 2 GPUs, where each GPU has a peak performance of 26.5 TFLOPS (double-precision), 110 compute units, and 64 GB of high-bandwidth memory (HBM2) which can be accessed at a peak of 1.6 TB/s. The 2 GPUs on an MI250X are connected with Infinity Fabric with a bandwidth of 200 GB/s (in each direction simultaneously).\n\nConnecting\n\nTo connect to Frontier, ssh to frontier.olcf.ornl.gov. For example:\n\n$ ssh <username>@frontier.olcf.ornl.gov\n\nFor more information on connecting to OLCF resources, see connecting-to-olcf <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#connecting-to-olcf>.\n\nBy default, connecting to Frontier will automatically place the user on a random login node. If you need to access a specific login node, you will ssh to that node after your intial connection to Frontier.\n\n[<username>@login12.frontier ~]$ ssh <username>@login01.frontier.olcf.ornl.gov\n\nUsers can connect to any of the 17 Frontier login nodes by replacing login01 with their login node of choice.\n\n\n\n\n\nData and Storage\n\nTransition from Alpine to Orion\n\nFrontier mounts Orion, a parallel filesystem based on Lustre and HPE ClusterStor, with a 679 PB usable namespace (/lustre/orion/). In addition to Frontier, Orion is available on the OLCF's data transfer nodes. It is not available from Summit. Frontier will not mount Alpine when Orion is in production.\n\nData will not be automatically transferred from Alpine to Orion. Users should consider the data needed from Alpine and transfer it. Globus is the preferred method and there is access to both Orion and Alpine through the Globus OLCF DTN endpoint. See data-transferring-data-globus <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-transferring-data-globus>. Use of HPSS to specifically stage data for the Alpine to Orion transfer is discouraged.\n\nOn Alpine, there was no user-exposed concept of file striping, the process of dividing a file between the storage elements of the filesystem. Orion uses a feature called Progressive File Layout (PFL) that changes the striping of files as they grow. Because of this, we ask users not to manually adjust the file striping. If you feel the default striping behavior of Orion is not meeting your needs, please contact help@olcf.ornl.gov.\n\nAs with Alpine, files older than 90 days are purged from Orion.  Please plan your data management and lifecycle at OLCF before generating the data.\n\nFor more detailed information about center-wide file systems and data archiving available on Frontier, please refer to the pages on data-storage-and-transfers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-storage-and-transfers>. The subsections below give a quick overview of NFS, Lustre,and HPSS storage spaces as well as the on node NVMe \"Burst Buffers\" (SSDs).\n\nNFS Filesystem\n\n\nThe following table provides a detailed overview of the settings for the Frontier supercomputer. The table is divided into two rows with the first row serving as the header. The table contains information about the path, type, permissions, quota, backups, purged, retention, and whether it is on the compute nodes for both the User Home and Project Home areas. The User Home area, which is located at /ccs/home/[userid], is an NFS type with user-set permissions and a quota of 50 GB. Backups are enabled and it is not purged. The retention period for this area is 90 days and it is located on the compute nodes. The Project Home area, which is located at /ccs/proj/[projid], is also an NFS type but with a default of 770 permissions and a quota of 50 GB. Backups are enabled and it is not purged. The retention period for this area is also 90 days and it is located on the compute nodes. Overall, this table provides important details about the file storage settings for the Frontier supercomputer, allowing users to easily understand and manage their storage options.\n\n| Area        | Path                 | Type | Permissions | Quota | Backups | Purged | Retention | On Compute Nodes |\n|-------------|----------------------|------|-------------|-------|---------|--------|-----------|------------------|\n| User Home   | /ccs/home/[userid]   | NFS  | User set    | 50 GB | Yes     | No     | 90 days   | Yes              |\n| Project Home| /ccs/proj/[projid]   | NFS  | 770         | 50 GB | Yes     | No     | 90 days   | Yes              |\n\n\n\nThough the NFS filesystem's User Home and Project Home areas are read/write from Frontier's compute nodes,\nwe strongly recommend that users launch and run jobs from the Lustre Orion parallel filesystem\ninstead due to its larger storage capacity and superior performance. Please see below for Lustre\nOrion filesystem storage areas and paths.\n\nLustre Filesystem\n\n\nThe Frontier supercomputer table provides an extensive overview of various work areas and their corresponding paths, types, permissions, quotas, backup options, purged duration, retention settings, and accessibility on compute nodes. These work areas include Member Work, Project Work, and World Work. The paths for these areas include subdirectories such as user ID, project ID, and a designated shared folder. The type of storage used for these work areas is Lustre HPE ClusterStor, which has a capacity of 50 TB per quota. Backup options are not available, and the purged duration is set to 90 days for all work areas. The retention setting is not applicable (N/A) for all work areas. These work areas are accessible on the compute nodes of the Frontier supercomputer. \n\n| Area         | Path                                                   | Type              | Permissions | Quota | Backups | Purged   | Retention | On Compute Nodes |\n|--------------|--------------------------------------------------------|-------------------|-------------|-------|---------|----------|-----------|------------------|\n| Member Work  | /lustre/orion/[projid]/scratch/[userid]               | Lustre HPE ClusterStor | 700       | 50 TB | No      | 90 days  | N/A       | Yes              |\n| Project Work | /lustre/orion/[[projid]/proj-shared                   | Lustre HPE ClusterStor | 770       | 50 TB | No      | 90 days  | N/A       | Yes              |\n| World Work   | /lustre/orion/[[projid]/world-shared                  | Lustre HPE ClusterStor | 775       | 50 TB | No      | 90 days  | N/A       | Yes              |\n\n\n\nHPSS Archival Storage\n\nPlease note that the HPSS is not mounted directly onto Frontier nodes. There are two main methods for accessing and moving data to/from the HPSS. The first is to use the command line utilities hsi and htar. The second is to use the Globus data transfer service. See data-hpss <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#data-hpss> for more information on both of these methods.\n\n\nThe table presents detailed information about the Frontier supercomputer's archives. It includes the areas for Member, Project, and World archives, as well as the respective paths, types, permissions, quotas, backups, purged status, retention period, and whether they are available on compute nodes. The Member Archive has a path of \"/hpss/prod/[projid]/users/$USER\" and HPSS type with permissions set to 700. Its quota is 100 TB and backups are not enabled. The Purged column shows a \"No\" indicating no automatic purging takes place. The retention period for this archive is 90 days and it is not available on compute nodes. The Project Archive has a path of \"/hpss/prod/[projid]/proj-shared\" and HPSS type with permissions set to 770. Its quota, backups, purge status, retention period, and availability on compute nodes follow the same pattern as the Member Archive. The World Archive has a path of \"/hpss/prod/[projid]/world-shared\" and HPSS type with permissions set to 775. Its quota, backups, purge status, retention period, and availability on compute nodes also follow the same pattern as the other archives. Overall, the table provides a comprehensive overview of the Frontier supercomputer's archive organization and management. \n \n\n\nThe following table provides a detailed breakdown of the archives on the Frontier supercomputer. It outlines the Member, Project, and World archives, along with information on their respective paths, types, permissions, quotas, backups, purged status, retention period, and availability on compute nodes. The Member Archive has a path of \"/hpss/prod/[projid]/users/$USER\" and is of HPSS type with permissions set to 700. Its quota is 100 TB and backups are not enabled. The Purged column shows a \"No\" indicating no automatic purging takes place. The retention period for this archive is 90 days and it is not available on compute nodes. The Project Archive has a path of \"/hpss/prod/[projid]/proj-shared\" and is also of HPSS type with permissions set to 770. Its quota, backups, purge status, retention period, and availability on compute nodes follow the same pattern as the Member Archive. The World Archive has a path of \"/hpss/prod/[projid]/world-shared\" and is of HPSS type with permissions set to 775. Its quota, backups, purge status, retention period, and availability on compute nodes also follow the same pattern as the other archives. This table provides a comprehensive overview of the Frontier supercomputer's archive organization and management, presenting all relevant information in an organized manner for accurate understanding and utilization of the archives. \n\n| Area           | Path                             | Type  | Permissions | Quota   | Backups | Purged | Retention | On Compute Nodes |\n|----------------|----------------------------------|-------|-------------|---------|---------|--------|-----------|------------------|\n| Member Archive | /hpss/prod/[projid]/users/$USER | HPSS  | 700         | 100 TB  | No      | No     | 90 days   | No               |\n| Project Archive| /hpss/prod/[projid]/proj-shared | HPSS  | 770         | 100 TB  | No      | No     | 90 days   | No               |\n| World Archive  | /hpss/prod/[projid]/world-shared | HPSS  | 775         | 100 TB  | No      | No     | 90 days   | No               |\n\n\n\nNVMe\n\nEach compute node on Frontier has [2x] 1.92TB Non-Volatile Memory (NVMe) storage devices (SSDs), colloquially known as a \"Burst Buffer\" with a peak sequential performance of 5500 MB/s (read) and 2000 MB/s (write). The purpose of the Burst Buffer system is to bring improved I/O performance to appropriate workloads. Users are not required to use the NVMes. Data can also be written directly to the parallel filesystem.\n\n\n\nThe NVMes on Frontier are local to each node.\n\nNVMe Usage\n\nTo use the NVMe, users must request access during job allocation using the -C nvme option to sbatch, salloc, or srun. Once the devices have been granted to a job, users can access them at /mnt/bb/<userid>. Users are responsible for moving data to/from the NVMe before/after their jobs. Here is a simple example script:\n\n#!/bin/bash\n#SBATCH -A <projid>\n#SBATCH -J nvme_test\n#SBATCH -o %x-%j.out\n#SBATCH -t 00:05:00\n#SBATCH -p batch\n#SBATCH -N 1\n#SBATCH -C nvme\n\ndate\n\n# Change directory to user scratch space (GPFS)\ncd /gpfs/alpine/<projid>/scratch/<userid>\n\necho \" \"\necho \"*****ORIGINAL FILE*****\"\ncat test.txt\necho \"***********************\"\n\n# Move file from GPFS to SSD\nmv test.txt /mnt/bb/<userid>\n\n# Edit file from compute node\nsrun -n1 hostname >> /mnt/bb/<userid>/test.txt\n\n# Move file from SSD back to GPFS\nmv /mnt/bb/<userid>/test.txt .\n\necho \" \"\necho \"*****UPDATED FILE******\"\ncat test.txt\necho \"***********************\"\n\nAnd here is the output from the script:\n\n$ cat nvme_test-<jobid>.out\n\n*****ORIGINAL FILE*****\nThis is my file. There are many like it but this one is mine.\n***********************\n\n*****UPDATED FILE******\nThis is my file. There are many like it but this one is mine.\nfrontier0123\n***********************\n\nUsing Globus to Move Data to Orion\n\nThe following example is intended to help users who are making the transition from Summit to Frontier to move their data between Alpine GPFS and Orion Lustre. We strongly recommend using Globus for this transfer as it is the method that is most efficient for users and that causes the least contention on filesystems and data transfer nodes.\n\nGlobus Warnings:\n\nGlobus transfers do not preserve file permissions. Arriving files will have (rw-r--r--) permissions, meaning arriving file will have user read and write permissions and group and world read permissions. Note that the arriving files will not have any execute permissions, so you will need to use chmod to reset execute permissions before running a Globus-transferred executable.\n\nGlobus will overwrite files at the destination with identically named source files. This is done without warning.\n\nGlobus has restriction of 8 active transfers across all the users. Each user has a limit of 3 active transfers, so it is required to transfer a lot of data on each transfer than less data across many transfers.\n\nIf a folder is constituted with mixed files including thousands of small files (less than 1MB each one), it would be better to tar the small files.  Otherwise, if the files are larger, Globus will handle them.\n\n<string>:5: (INFO/1) Duplicate implicit target name: \"using globus to move data to orion\".\n\nHere is a recording of an example transfer from Alpine to Orion using Globus and the OLCF DTN: Using Globus to Move Data to Orion.\n\nBelow is a summary of the steps for data transfer given in the recording:\n\n1.      Login to globus.org using your globus ID and password. If you do not have a globusID, set one up here:\nGenerate a globusID.\n\nOnce you are logged in, Globus will open the “File Manager” page. Click the left side “Collection” text field in the File Manager and type “OLCF DTN”.\n\nWhen prompted, authenticate into the OLCF DTN endpoint using your OLCF username and PIN followed by your RSA passcode.\n\nClick in the left side “Path” box in the File Manager and enter the path to your data on Alpine. For example, /gpfs/alpine/stf007/proj-shared/my_alpine_data. You should see a list of your files and folders under the left “Path” Box.\n\nClick on all files or folders that you want to transfer in the list. This will highlight them.\n\nClick on the right side “Collection” box in the File Manager and type “OLCF DTN”\n\nClick in the right side “Path” box and enter the path where you want to put your data on Orion, for example, /lustre/orion/stf007/proj-shared/my_orion_data\n\nClick the left \"Start\" button.\n\nClick on “Activity“ in the left blue menu bar to monitor your transfer. Globus will send you an email when the transfer is complete.\n\n<string>:5: (INFO/1) Enumerated list start value not ordinal-1: \"2\" (ordinal 2)\n\n\n\nAMD GPUs\n\nThe AMD Instinct MI200 is built on advanced packaging technologies\nenabling two Graphic Compute Dies (GCDs) to be integrated\ninto a single package in the Open Compute Project (OCP) Accelerator Module (OAM)\nin the MI250 and MI250X products.\nEach GCD is build on the AMD CDNA 2 architecture.\nA single Frontier node contains 4 MI250X OAMs for the total of 8 GCDs.\n\nThe Slurm workload manager and the ROCr runtime treat each GCD as a separate GPU\nand visibility can be controlled using the ROCR_VISIBLE_DEVICES environment variable.\nTherefore, from this point on, the Frontier guide simply refers to a GCD as a GPU.\n\nEach GPU contains 110 Compute Units (CUs) grouped in 4 Compute Engines (CEs).\nPhysically, each GPU contains 112 CUs, but two are disabled.\nA command processor in each GPU receives API commands and transforms them into compute tasks.\nCompute tasks are managed by the 4 compute engines, which dispatch wavefronts to compute units.\nAll wavefronts from a single workgroup are assigned to the same CU.\nIn CUDA terminology, workgroups are \"blocks\", wavefronts are \"warps\", and work-items are \"threads\".\nThe terms are often used interchangeably.\n\nBlock diagram of the AMD Instinct MI200 multi-chip module\n\nThe 110 CUs in each GPU deliver peak performance of 26.5 TFLOPS in double precision.\nAlso, each GPU contains 64 GB of high-bandwidth memory (HBM2) accessible at a peak\nbandwidth of 1.6 TB/s.\nThe 2 GPUs in an MI250X are connected with [4x] GPU-to-GPU Infinity Fabric links\nproviding 200+200 GB/s of bandwidth.\n(Consult the diagram in the frontier-nodes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-nodes> section for information\non how the accelerators are connected to each other, to the CPU, and to the network.\n\nThe X+X GB/s notation describes bidirectional bandwidth, meaning X GB/s in each direction.\n\nTODO: unified memory? If mi250x has it, what is it and how does it work\nTODO: link to HIP from scratch tutorial\nTODO: here are some references https://www.amd.com/system/files/documents/amd-cdna2-white-paper.pdf and https://www.amd.com/system/files/documents/amd-instinct-mi200-datasheet.pdf\n\n\n\nAMD vs NVIDIA Terminology\n\n\nThe table displays a comparison between two leading manufacturers in the computing industry, AMD and NVIDIA. The first column represents the terminology used by AMD for their supercomputers, while the second column represents the terminology used by NVIDIA. The first row serves as a header, specifying the type of terminology being discussed in each column. In the first row, \"Work-items or Threads\" refers to the unit of execution used by AMD, which is equivalent to \"Threads\" in NVIDIA. The second row, \"Workgroup\" for AMD and \"Block\" for NVIDIA, refers to a group of work-items or threads that are executed together. These terms are interchangeable between the two manufacturers. The third row, \"Wavefront\" for AMD and \"Warp\" for NVIDIA, represents a group of threads that operate in a synchronous manner. Finally, the fourth row, \"Grid\" for AMD and \"Grid\" for NVIDIA, refers to the organization of work-items or threads into a two or three-dimensional structure. Overall, this table provides a comprehensive description of key elements used in supercomputers and highlights the similarities and differences between the terminology used by AMD and NVIDIA.\n\n| AMD | NVIDIA |\n|-----|---------|\n| Work-items or Threads | Threads |\n| Workgroup | Block |\n| Wavefront | Warp |\n| Grid | Grid |\n\n\n\nWe will be using these terms interchangeably as they refer to the same concepts in GPU\nprogramming, with the exception that we will only be using \"wavefront\" (which refers to a\nunit of 64 threads) instead of \"warp\" (which refers to a unit of 32 threads) as they mean\ndifferent things.\n\nBlocks (workgroups), Threads (work items), Grids, Wavefronts\n\nWhen kernels are launched on a GPU, a \"grid\" of thread blocks are created, where the\nnumber of thread blocks in the grid and the number of threads within each block are\ndefined by the programmer. The number of blocks in the grid (grid size) and the number of\nthreads within each block (block size) can be specified in one, two, or three dimensions\nduring the kernel launch. Each thread can be identified with a unique id within the\nkernel, indexed along the X, Y, and Z dimensions.\n\nNumber of blocks that can be specified along each dimension in a grid: (2147483647, 2147483647, 2147483647)\n\nMax number of threads that can be specified along each dimension in a block: (1024, 1024, 1024)\n\nHowever, the total of number of threads in a block has an upper limit of 1024\n[i.e. (size of x dimension * size of y dimension * size of z dimension) cannot exceed\n1024].\n\nEach block (or workgroup) of threads is assigned to a single Compute Unit i.e. a single\nblock won’t be split across multiple CUs. The threads in a block are scheduled in units of\n64 threads called wavefronts (similar to warps in CUDA, but warps only have 32 threads\ninstead of 64). When launching a kernel, up to 64KB of block level shared memory called\nthe Local Data Store (LDS) can be statically or dynamically allocated. This shared memory\nbetween the threads in a block allows the threads to access block local data with much\nlower latency compared to using the HBM since the data is in the compute unit itself.\n\nThe Compute Unit\n\nBlock diagram of the AMD Instinct CDNA2 Compute Unit\n\nEach CU has 4 Matrix Core Units (the equivalent of NVIDIA's Tensor core units) and 4\n16-wide SIMD units. For a vector instruction that uses the SIMD units, each wavefront\n(which has 64 threads) is assigned to a single 16-wide SIMD unit such that the wavefront\nas a whole executes the instruction over 4 cycles, 16 threads per cycle. Since other\nwavefronts occupy the other three SIMD units at the same time, the total throughput still\nremains 1 instruction per cycle. Each CU maintains an instructions buffer for 10\nwavefronts and also maintains 256 registers where each register is 64 4-byte wide\nentries.\n\n\n\nHIP\n\nThe Heterogeneous Interface for Portability (HIP) is AMD’s dedicated GPU programming\nenvironment for designing high performance kernels on GPU hardware. HIP is a C++ runtime\nAPI and programming language that allows developers to create portable applications on\ndifferent platforms, including the AMD MI250X. This means that developers can write their GPU applications and with\nvery minimal changes be able to run their code in any environment.  The API is very\nsimilar to CUDA, so if you're already familiar with CUDA there is almost no additional\nwork to learn HIP. See here for a series\nof tutorials on programming with HIP and also converting existing CUDA code to HIP with the hipify tools .\n\nThings To Remember When Programming for AMD GPUs\n\nThe MI250X has different denormal handling for FP16 and BF16 datatypes, which is relevant for ML training. Prefer using the BF16 over the FP16 datatype for ML models as you are more likely to encounter denormal values with FP16 (which get flushed to zero, causing failure in convergence for some ML models). See more in using-reduced-precision <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#using-reduced-precision>.\n\nMemory can be automatically migrated to GPU from CPU on a page fault if XNACK operating mode is set.  No need to explicitly migrate data or provide managed memory. This is useful if you're migrating code from a programming model that relied on 'unified' or 'managed' memory. See more in enabling-gpu-page-migration <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#enabling-gpu-page-migration>. Information about how memory is accessed based on the allocator used and the XNACK mode can be found in migration-of-memory-allocator-xnack <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#migration-of-memory-allocator-xnack>.\n\nHIP has two kinds of memory allocations, coarse grained and fine grained, with tradeoffs between performance and coherence. Particularly relevant if you want to ues the hardware FP atomic instructions. See more in fp-atomic-ops-coarse-fine-allocations <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#fp-atomic-ops-coarse-fine-allocations>.\n\nFP32 atomicAdd operations on Local Data Store (i.e. block shared memory) can be slower than the equivalent FP64 operations. See more in performance-lds-atomicadd <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#performance-lds-atomicadd>.\n\nSee the frontier-compilers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers> section for information on compiling for AMD GPUs, and\nsee the tips-and-tricks <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#tips-and-tricks> section for some detailed information to keep in mind\nto run more efficiently on AMD GPUs.\n\nProgramming Environment\n\nFrontier users are provided with many pre-installed software packages and scientific libraries. To facilitate this, environment management tools are used to handle necessary changes to the shell.\n\nEnvironment Modules (Lmod)\n\nEnvironment modules are provided through Lmod, a Lua-based module system for dynamically altering shell environments. By managing changes to the shell’s environment variables (such as PATH, LD_LIBRARY_PATH, and PKG_CONFIG_PATH), Lmod allows you to alter the software available in your shell environment without the risk of creating package and version combinations that cannot coexist in a single environment.\n\nGeneral Usage\n\nThe interface to Lmod is provided by the module command:\n\n\nThe following table provides a comprehensive list of commands for using the Frontier supercomputer. The first row serves as the header, with the column on the left displaying various commands and the column on the right providing a brief description of each command. These include commands such as \"module -t list\" which shows a list of currently loaded modules, \"module avail\" which displays a table of available modules, \"module help <modulename>\" which provides help information for a specific module, and \"module spider <string>\" which can be used to search for all possible modules based on a given string. Other commands, like \"module load\" and \"module use\", are used to load and add modules to the current environment, while \"module unuse\" and \"module reset\" can remove and reset loaded modules. Lastly, there are also commands like \"module purge\" which unloads all modules and \"module update\" which reloads all currently loaded modules. Overall, this table serves as a useful reference for navigating the various features and capabilities of the Frontier supercomputer.\n\n| Command                | Description                                                               |\n|------------------------|---------------------------------------------------------------------------|\n| module -t list         | Shows a terse list of the currently loaded modules                         |\n| module avail           | Shows a table of the currently available modules                           |\n| module help <modulename>| Shows help information about <modulename>                                  |\n| module show <modulename>| Shows the environment changes made by the <modulename> modulefile         |\n| module spider <string> | Searches all possible modules according to <string>                       |\n| module load <modulename> [...] | Loads the given <modulename>(s) into the current environment      |\n| module use <path>      | Adds <path> to the modulefile search cache and MODULESPATH                |\n| module unuse <path>    | Removes <path> from the modulefile search cache and MODULESPATH           |\n| module purge           | Unloads all modules                                                       |\n| module reset           | Resets loaded modules to system defaults                                  |\n| module update          | Reloads all currently loaded modules                                      |\n\n\n\nSearching for Modules\n\nModules with dependencies are only available when the underlying dependencies, such as compiler families, are loaded. Thus, module avail will only display modules that are compatible with the current state of the environment. To search the entire hierarchy across all possible dependencies, the spider sub-command can be used as summarized in the following table.\n\n\nThe table presents various commands for using the Frontier supercomputer's module system. The first command, \"module spider,\" allows users to view the entire possible graph of available modules on the system. The second command, \"module spider <modulename>,\" enables users to search for specific modules by name within the graph. The third command, \"module spider <modulename>/<version>,\" allows users to search for a specific version of a module. Finally, the fourth command, \"module spider <string>,\" enables users to search for modulefiles containing a specific string. These commands provide users with a comprehensive search functionality for finding and utilizing the various modules available on the Frontier supercomputer.\n\n| Command | Description |\n|---------|-------------|\n| module spider | Shows the entire possible graph of modules |\n| module spider <modulename> | Searches for modules named <modulename> in the graph of possible modules |\n| module spider <modulename>/<version> | Searches for a specific version of <modulename> in the graph of possible modules |\n| module spider <string> | Searches for modulefiles containing <string> |\n\n\n\nCompilers\n\nCray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers. Please see the following frontier-compilers <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compilers> section for more detailed inforation on how to compile using these modules.\n\nCray Programming Environment and Compiler Wrappers\n\nCray provides PrgEnv-<compiler> modules (e.g., PrgEnv-cray) that load compatible components of a specific compiler toolchain. The components include the specified compiler as well as MPI, LibSci, and other libraries. Loading the PrgEnv-<compiler> modules also defines a set of compiler wrappers for that compiler toolchain that automatically add include paths and link in libraries for Cray software. Compiler wrappers are provided for C (cc), C++ (CC), and Fortran (ftn).\n\nUse the -craype-verbose flag to display the full include and link information used by the Cray compiler wrappers. This must be called on a file to see the full output (e.g., CC -craype-verbose test.cpp).\n\nMPI\n\nThe MPI implementation available on Frontier is Cray's MPICH, which is \"GPU-aware\" so GPU buffers can be passed directly to MPI calls.\n\n\n\n\n\nCompiling\n\nCompilers\n\n<string>:581: (INFO/1) Duplicate implicit target name: \"compilers\".\n\nCray, AMD, and GCC compilers are provided through modules on Frontier. The Cray and AMD compilers are both based on LLVM/Clang. There is also a system/OS versions of GCC available in /usr/bin. The table below lists details about each of the module-provided compilers.\n\nIt is highly recommended to use the Cray compiler wrappers (cc, CC, and ftn) whenever possible. See the next section for more details.\n\n\nThe following table outlines the various compilers and programming environments available for use on the Frontier supercomputer. The first column lists the vendors, including Cray, AMD, and GCC. The second column specifies the programming environment, such as prgEnv-cray, prgEnv-amd, and prgEnv-gnu. Next, the compiler module is listed, including cce, amd, and gcc. The fourth column specifies the language, with options for C, C++, and Fortran. The fifth column lists the compiler wrapper, such as cc, craycc, and amdclang. Finally, the last column provides the actual compiler, including craycc or amdclang++, which are used for C and C++ languages, respectively, and crayftn or amdflang, which are used for Fortran. This table is a helpful reference for users of the Frontier supercomputer as it provides a comprehensive list of available compilers and their corresponding environments and modules. \n\n|Vendor|Programming Environment|Compiler Module|Language|Compiler Wrapper|Compiler|\n|---|---|---|---|---|---|\n|Cray|PrgEnv-cray|cce|C|cc|craycc|\n|C++|CC|craycxx or crayCC||||\n|Fortran|ftn|crayftn||||\n|AMD|PrgEnv-amd|amd|C|cc|amdclang|\n|C++|CC|amdclang++||||\n|Fortran|ftn|amdflang||||\n|GCC|PrgEnv-gnu|gcc|C|cc|${GCC_PATH}/bin/gcc|\n|C++|CC|${GCC_PATH}/bin/g++||||\n|Fortran|ftn|${GCC_PATH}/bin/gfortran||||\n\n\n\nCray Programming Environment and Compiler Wrappers\n\n<string>:614: (INFO/1) Duplicate implicit target name: \"cray programming environment and compiler wrappers\".\n\nCray provides PrgEnv-<compiler> modules (e.g., PrgEnv-cray) that load compatible components of a specific compiler toolchain. The components include the specified compiler as well as MPI, LibSci, and other libraries. Loading the PrgEnv-<compiler> modules also defines a set of compiler wrappers for that compiler toolchain that automatically add include paths and link in libraries for Cray software. Compiler wrappers are provided for C (cc), C++ (CC), and Fortran (ftn).\n\nFor example, to load the AMD programming environment, do:\n\nmodule load PrgEnv-amd\n\nThis module will setup your programming environment with paths to software and libraries that are compatible with AMD compilers.\n\nUse the -craype-verbose flag to display the full include and link information used by the Cray compiler wrappers. This must be called on a file to see the full output (e.g., CC -craype-verbose test.cpp).\n\n\n\nExposing The ROCm Toolchain to your Programming Environment\n\nIf you need to add the tools and libraries related to ROCm, the framework for targeting AMD GPUs, to your path, you will need to use a version of ROCm that is compatible with your programming environment.\n\nThe following modules help you expose the ROCm Toolchain to your programming Environment:\n\n\nThe Frontier supercomputer is equipped with a variety of programming environments to cater to different needs. Its programming environment modules include PrgEnv-amd, PrgEnv-cray, and PrgEnv-gnu. Each of these modules provides access to the ROCm Toolchain, a programming toolchain specifically designed for AMD GPUs. To use the PrgEnv-amd module, simply load it using the command \"module load PrgEnv-amd\", and the ROCm Toolchain will automatically be loaded as well. If you prefer to use either PrgEnv-cray or PrgEnv-gnu, you can access the ROCm Toolchain by loading the amd-mixed module using the command \"module load amd-mixed\". With these programming environments and toolchains, users of the Frontier supercomputer have a wide range of options when it comes to developing and running their programs on the powerful AMD GPUs. \n\n| Programming Environment Module | Module that gets you ROCm Toolchain | How you load it: |\n|---------------------------------|-------------------------------------|------------------|\n| PrgEnv-amd                      | amd                                 | module load PrgEnv-amd |\n| PrgEnv-cray or PrgEnv-gnu       | amd-mixed                           | module load amd-mixed |\n\n\n\nBoth the CCE and ROCm compilers are Clang-based, so please be sure to use consistent (major) Clang versions when using them together. You can check which version of Clang is being used with CCE and ROCm by giving the --version flag to CC and hipcc, respectively.\n\nMPI\n\n<string>:653: (INFO/1) Duplicate implicit target name: \"mpi\".\n\nThe MPI implementation available on Frontier is Cray's MPICH, which is \"GPU-aware\" so GPU buffers can be passed directly to MPI calls.\n\n\nThe Frontier supercomputer table lists the various implementations, modules, compilers, and header files and linking used in the creation of the computer. The first row serves as a header and does not require a description. The second row includes the Cray MPICH implementation, its corresponding module - cray-mpich, and the compiler wrappers cc, CC, and ftn used with the Cray compiler. The third row describes the built-in MPI header files and linking found within the Cray compiler wrappers. The fourth row lists the hipcc implementation and includes additional linking options such as -L${MPICH_DIR}/lib and -lmpi. The fifth, sixth, and seventh rows also specify additional linking options and include variables such as ${CRAY_XPMEM_POST_LINK_OPTS}, ${PE_MPICH_GTL_DIR_amd_gfx90a}, and ${PE_MPICH_GTL_LIBS_amd_gfx90a}. Lastly, the eighth row includes an include option for the MPICH directory. \n\n| Implementation | Module      | Compiler                                  | Header Files & Linking                                                                  |\n|----------------|-------------|-------------------------------------------|-----------------------------------------------------------------------------------------|\n| Cray MPICH     | cray-mpich  | cc, CC, ftn (Cray compiler wrappers)      | MPI header files and linking is built into the Cray compiler wrappers                   |\n| hipcc          |             | -L${MPICH_DIR}/lib -lmpi ${CRAY_XPMEM_PO  |                                                                                            |\n|                |             | ST_LINK_OPTS} -lxpmem ${PE_MPICH_GTL_DIR_  |                                                                                            |\n|                |             | amd_gfx90a} ${PE_MPICH_GTL_LIBS_amd_gfx90 |                                                                                            |\n|                |             | a} -I${MPICH_DIR}/include                 |\n\n\n\nhipcc requires the ROCm Toolclain, See exposing-the-rocm-toolchain-to-your-programming-environment <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#exposing-the-rocm-toolchain-to-your-programming-environment>\n\nGPU-Aware MPI\n\nTo use GPU-aware Cray MPICH, with Frontier's PrgEnv modules, users must set the following modules and environment variables:\n\nIf using PrgEnv-amd:\n\nmodule load craype-accel-amd-gfx90a\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nIf using PrgEnv-cray:\n\nmodule load craype-accel-amd-gfx90a\nmodule load amd-mixed\n\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nThere are extra steps needed to enable GPU-aware MPI on Frontier, which depend on the compiler that is used (see 1. and 2. below).\n\n1. Compiling with the Cray compiler wrappers, cc or CC\n\nWhen using GPU-aware Cray MPICH with the Cray compiler wrappers, most of the needed libraries are automatically linked through the environment variables.\n\nThough, the following header files and libraries must be included explicitly:\n\n-I${ROCM_PATH}/include\n-L${ROCM_PATH}/lib -lamdhip64\n\nwhere the include path implies that #include <hip/hip_runtime.h> is included in the source file.\n\n2. Compiling with hipcc\n\nTo use hipcc with GPU-aware Cray MPICH, use the following environment variables to setup the needed header files and libraries.\n\n-I${MPICH_DIR}/include\n-L${MPICH_DIR}/lib -lmpi \\\n  ${CRAY_XPMEM_POST_LINK_OPTS} -lxpmem \\\n  ${PE_MPICH_GTL_DIR_amd_gfx90a} ${PE_MPICH_GTL_LIBS_amd_gfx90a}\n\nHIPFLAGS = --amdgpu-target=gfx90a\n\nDetermining the Compatibility of Cray MPICH and ROCm\n\nReleases of cray-mpich are each built with a specific version of ROCm, and compatibility across multiple versions is not guaranteed. OLCF will maintain compatible default modules when possible. If using non-default modules, you can determine compatibility by reviewing the Product and OS Dependencies section in the cray-mpich release notes. This can be displayed by running module show cray-mpich/<version>. If the notes indicate compatibility with AMD ROCM X.Y or later, only use rocm/X.Y.Z modules. If using a non-default version of cray-mpich, you must add ${CRAY_MPICH_ROOTDIR}/gtl/lib to either your LD_LIBRARY_PATH at run time or your executable's rpath at build time.\n\nThe compatibility table below was determined by linker testing with all current combinations of cray-mpich and ROCm-related modules on Frontier.\n\n\nThe table displays the compatibility of the Frontier supercomputer with the Cray-MPICH and ROCm software. The first row serves as a header, listing the two software versions. The subsequent rows show the compatibility of each version of the Frontier supercomputer with different versions of Cray-MPICH and ROCm. For Cray-MPICH, both versions 8.1.17 and 8.1.23 are compatible with different versions of ROCm, including 5.4.0, 5.3.0, 5.2.0, and 5.1.0. This provides users with different options for utilizing the supercomputer depending on their preferred software versions. The table showcases the flexibility and versatility of Frontier, making it a top choice for high-performance computing needs.\n\nMarkdown Table:\n| cray-mpich | ROCm          |\n|------------|---------------|\n| 8.1.17     | 5.4.0, 5.3.0, 5.2.0, 5.1.0 |\n| 8.1.23     | 5.4.0, 5.3.0, 5.2.0, 5.1.0 |\n\n\n\nOpenMP\n\nThis section shows how to compile with OpenMP using the different compilers covered above.\n\n\nThe table displays the different vendors, modules, languages, and compilers used in the Frontier supercomputer. The first row serves as the header, outlining the different categories. The first column lists the vendors, including Cray, AMD, and GCC. The second column displays the specific module used by each vendor, such as cce for Cray and amd for AMD. The third column indicates the languages available for each vendor, including C, C++, and Fortran. The fourth column lists the compiler used for each language, such as cc for C and ftn for Fortran. The final column shows the OpenMP flag used for each vendor, with some aliases indicated. The Frontier supercomputer offers a variety of options for users to choose from for their programming needs, with each vendor providing a unique set of languages and compilers. \\\n\n| Vendor | Module | Language | Compiler | OpenMP flag (CPU thread) |\n|--------|--------|----------|----------|--------------------------|\n| Cray   | cce    | C, C++   | cc       | -fopenmp                 |\n|        |        |          | cc       | (wraps crayCC)           |\n| Fortran| ftn    | -homp    | -fopenmp | (alias)                  |\n| AMD    | amd    | C, C++   | cc       | -fopenmp                 |\n|        |        | Fortran  | ftn      | (alias)                  |\n| GCC    | gcc    | C, C++   | cc       | -fopenmp                 |\n|        |        | Fortran  | ftn      |                         |\n\n\n\nOpenMP GPU Offload\n\nThis section shows how to compile with OpenMP Offload using the different compilers covered above.\n\nMake sure the craype-accel-amd-gfx90a module is loaded when using OpenMP offload.\n\n\nThe table provides information about the modules, languages, compilers, and OpenMP flags associated with the Frontier supercomputer. The modules listed include \"cce\" and \"amd\", which refer to programming environments offered by the vendors Cray and AMD, respectively. These modules support languages such as C, C++, and Fortran, as shown in the table. Compilers such as \"cc\" and \"ftn\" are used to compile code written in these languages. Additionally, the table provides information on the specific OpenMP flags for each module, with Cray's module using \"-fopenmp\" and AMD's module using \"-fopenmp\" and \"-homp\". The table also mentions additional compilers such as \"cc (wraps crayCC)\" and \"hipcc (requires flags below)\", which are used for specific purposes with certain languages. \n\n| Vendor | Module | Language | Compiler | OpenMP flag (GPU) |\n| --- | --- | --- | --- | --- |\n| Cray | cce | C | cc (wraps craycc) | -fopenmp |\n| | | C++ | CC (wraps crayCC) | -fopenmp |\n| | | Fortran | ftn (wraps crayftn) | -homp |\n| AMD | amd | C | cc (wraps amdclang) | -fopenmp |\n| | | C++ | CC (wraps amdclang++) | -fopenmp |\n| | | Fortran | ftn (wraps amdflang) | -fopenmp (alias) |\n| | | C++ | hipcc (requires flags below) | -fopenmp |\n\n\n\nIf invoking amdclang, amdclang++, or amdflang directly for openmp offload, or using hipcc you will need to add:\n\n-fopenmp -target x86_64-pc-linux-gnu -fopenmp-targets=amdgcn-amd-amdhsa -Xopenmp-target=amdgcn-amd-amdhsa -march=gfx90a.\n\nOpenACC\n\nThis section shows how to compile code with OpenACC. Currently only the Cray compiler supports OpenACC for Fortran. The AMD and\nGNU programming environments do not support OpenACC at all.\nC and C++ support for OpenACC is provided by clacc which maintains a fork of the LLVM\ncompiler with added support for OpenACC. It can be obtained by loading the UMS modules\nums, ums025, and clacc.\n\n\nThe table provides a detailed overview of the specs and support for the Frontier supercomputer. The table presents information regarding the vendor, module, supported languages, compiler, flags, and level of support. The vendor for the supercomputer is Cray and it supports C and C++ languages using the cce compiler. The compiler also supports OpenACC 2.0 with full support for Fortran using ftn. However, partial support is provided for OpenACC 2.x/3.x and UMS. The module for the supercomputer is PrgEnv-cray with UMS version um025 and the compiler is clang. For C and C++ languages, the flags -h acc and -fopenacc are used respectively. The OpenACC support is currently experimental and for more information, one can contact Joel Denny at dennyje@ornl.gov. The last row in the table shows that there is no support for Fortran and it is indicated by an empty cell in the columns for module, language, and compiler.\n\n| Vendor | Module | Language | Compiler | Flags | Support |\n|:-------|:-------|:---------|:---------|:------|:--------|\n| Cray | cce | C, C++ | No support | | |\n| Fortran | ftn (wraps crayftn) | -h acc | Full support for OpenACC 2.0 |\nPartial support for OpenACC 2.x/3.x\nUMS\nmodule | PrgEnv-cray\nums\num025\nclacc | C, C++ | clang | -fopenacc | Experimental. Contact Joel Denny at dennyje@ornl.gov |\nFortran | No support | | |\n\n\n\nHIP\n\n<string>:831: (INFO/1) Duplicate implicit target name: \"hip\".\n\nThis section shows how to compile HIP codes using the Cray compiler wrappers and hipcc compiler driver.\n\nMake sure the craype-accel-amd-gfx90a module is loaded when compiling HIP with the Cray compiler wrappers.\n\n\nThis table provides information about the compilers used for the Frontier supercomputer. The first column lists the different compilers available, including CC, Only with PrgEnv-cray, and PrgEnv-amd. The second column describes the compile and link flags, header files, and libraries for each compiler. For CC, the flags include standard C++11, with options for HIP and ROCclr and specifications for architecture and path. The LFLAGS also specify the ROCm path and use the AMD hip64 library. The third column lists the compiler \"hipcc,\" which can directly compile HIP source files. The last row provides additional information on how to use the compiler, such as the command to see what is being invoked and how to explicitly target a specific architecture. \n\n| Compiler | Compile/Link Flags, Header Files, and Libraries  |\n| -------- | ------------------------------------------------- |\n| CC | CFLAGS = -std=c++11 -D__HIP_ROCclr__ -D__HIP_ARCH_GFX90A__=1 --rocm-path=${ROCM_PATH} --offload-arch=gfx90a -x hip LFLAGS = --rocm-path=${ROCM_PATH} -L${ROCM_PATH}/lib -lamdhip64 |\n| Only with PrgEnv-cray | CFLAGS = -std=c++11 -D__HIP_ROCclr__ -D__HIP_ARCH_GFX90A__=1 --rocm-path=${ROCM_PATH} --offload-arch=gfx90a -x hip LFLAGS = --rocm-path=${ROCM_PATH} -L${ROCM_PATH}/lib -lamdhip64 |\n| PrgEnv-amd | CFLAGS = -std=c++11 -D__HIP_ROCclr__ -D__HIP_ARCH_GFX90A__=1 --rocm-path=${ROCM_PATH} --offload-arch=gfx90a -x hip LFLAGS = --rocm-path=${ROCM_PATH} -L${ROCM_PATH}/lib -lamdhip64 |\n| hipcc | Can be used directly to compile HIP source files. To see what is being invoked within this compiler driver, issue the command, hipcc --verbose. To explicitly target AMD MI250X, use --amdgpu-target=gfx90a |\n\n\n\nhipcc requires the ROCm Toolclain, See exposing-the-rocm-toolchain-to-your-programming-environment <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#exposing-the-rocm-toolchain-to-your-programming-environment>\n\nInformation about compiling code for different XNACK modes (which control page migration between GPU and CPU memory) can be found in the compiling-hip-kernels-for-xnack-modes <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#compiling-hip-kernels-for-xnack-modes> section.\n\nHIP + OpenMP CPU Threading\n\nThis section shows how to compile HIP + OpenMP CPU threading hybrid codes.\n\nMake sure the craype-accel-amd-gfx90a module is loaded when compiling HIP with the Cray compiler wrappers.\n\n\nThe Frontier supercomputer is a high-performance computing system that is designed and built by AMD and Cray. It features a custom compiler, known as CC, which is used to compile and link code targeting the system's specific architecture. The compile and link flags include standard options for C++11, as well as specific options for the HIP programming language and the gfx90a architecture. The compiler also utilizes the Hipcc tool, which can directly compile HIP source files and includes the option to enable OpenMP threading. Additionally, the GNU compiler (CC) can also be used, but it must separate HIP kernels from CPU code and use the CC wrapper during linking. It is important to note that when using cmake, code must be compiled with amdclang++ instead of hipcc in order to properly target the Frontier supercomputer.\n\n| Vendor  | Compiler | Compile/Link Flags,  Header Files, and Libraries |\n| ------- | -------- | ----------------------------------------- |\n| AMD/Cray | CC       | CFLAGS = -std=c++11 -D__HIP_ROCclr__ -D__HIP_ARCH_GFX90A__=1 --rocm-path=${ROCM_PATH} --offload-arch=gfx90a -x hip -fopenmp<br>LFLAGS = --rocm-path=${ROCM_PATH} -fopenmp<br>-L${ROCM_PATH}/lib -lamdhip64 |\n| hipcc   |          | Can be used to directly compile HIP source files, add -fopenmp flag to enable OpenMP threading<br>To explicitly target AMD MI250X, use --amdgpu-target=gfx90a |\n| GNU     | CC       | The GNU compilers cannot be used to compile HIP code, so all HIP kernels must be separated from CPU code.<br>During compilation, all non-HIP files must be compiled with CC while HIP kernels must be compiled with hipcc.<br>Then linking must be performed with the CC wrapper.<br>NOTE: When using cmake, HIP code must currently be compiled using amdclang++ instead of hipcc. |\n\n\n\nhipcc requires the ROCm Toolclain, See exposing-the-rocm-toolchain-to-your-programming-environment <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#exposing-the-rocm-toolchain-to-your-programming-environment>\n\n\n\n\n\nRunning Jobs\n\nComputational work on Frontier is performed by jobs. Jobs typically consist of several componenets:\n\nA batch submission script\n\nA binary executable\n\nA set of input files for the executable\n\nA set of output files created by the executable\n\nIn general, the process for running a job is to:\n\nPrepare executables and input files.\n\nWrite a batch script.\n\nSubmit the batch script to the batch scheduler.\n\nOptionally monitor the job before and during execution.\n\nThe following sections describe in detail how to create, submit, and manage jobs for execution on Frontier. Frontier uses SchedMD's Slurm Workload Manager as the batch scheduling system.\n\nLogin vs Compute Nodes\n\nRecall from the System Overview that Frontier contains two node types: Login and Compute. When you connect to the system, you are placed on a login node. Login nodes are used for tasks such as code editing, compiling, etc. They are shared among all users of the system, so it is not appropriate to run tasks that are long/computationally intensive on login nodes. Users should also limit the number of simultaneous tasks on login nodes (e.g. concurrent tar commands, parallel make\n\nCompute nodes are the appropriate place for long-running, computationally-intensive tasks. When you start a batch job, your batch script (or interactive shell for batch-interactive jobs) runs on one of your allocated compute nodes.\n\nCompute-intensive, memory-intensive, or other disruptive processes running on login nodes may be killed without warning.\n\nUnlike Summit and Titan, there are no launch/batch nodes on Frontier. This means your batch script runs on a node allocated to you rather than a shared node. You still must use the job launcher (srun) to run parallel jobs across all of your nodes, but serial tasks need not be launched with srun.\n\n\n\nSimplified Node Layout\n\nTo easily visualize job examples (see frontier-mapping <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-mapping> further below), the\ncompute node diagram has been simplified to the picture shown below.\n\nSimplified Frontier node architecture diagram\n\nIn the diagram, each physical core on a Frontier compute node is composed\nof two logical cores that are represented by a pair of blue and grey boxes.\nFor a given physical core, the blue box represents the logical core of the\nfirst hardware thread, where the grey box represents the logical core of the\nsecond hardware thread.\n\n\n\nLow-noise Mode Layout\n\nFrontier uses low-noise mode and core specialization (-S flag at job\nallocation, e.g., sbatch).  Low-noise mode constrains all system processes\nto core 0.  Core specialization (by default, -S 8) reserves the first core\nin each L3 region.  This prevents the user running on the core that system\nprocesses are constrained to.  This also means that there are only 56\nallocatable cores by default instead of 64. Therefore, this modifies the\nsimplified node layout to:\n\nSimplified Frontier node architecture diagram (low-noise mode)\n\nTo override this default layout (not recommended), set -S 0 at job allocation.\n\n\n\nSlurm\n\nFrontier uses SchedMD's Slurm Workload Manager for scheduling and managing jobs. Slurm maintains similar functionality to other schedulers such as IBM's LSF, but provides unique control of Frontier's resources through custom commands and options specific to Slurm. A few important commands can be found in the conversion table below, but please visit SchedMD's Rosetta Stone of Workload Managers for a more complete conversion reference.\n\nSlurm documentation for each command is available via the man utility, and on the web at https://slurm.schedmd.com/man_index.html. Additional documentation is available at https://slurm.schedmd.com/documentation.html.\n\nSome common Slurm commands are summarized in the table below. More complete examples are given in the Monitoring and Modifying Batch Jobs section of this guide.\n\n\nThe Frontier supercomputer offers several commands for efficient job management. The first command, `squeue`, can be used to view the current queue and its associated jobs, similar to `bjobs` in the LSF system. `sbatch` allows users to submit batch scripts for execution, which is equivalent to `bsub` in LSF. `salloc` is used for interactive job submission, with a command line argument of `bsub -Is $SHELL` in LSF. `srun` is used to launch parallel jobs, which corresponds to `jsrun` in LSF. `sinfo` can be used to view information about nodes and partitions, which is similar to `bqueues` or `bhosts` in LSF. `sacct` allows users to view accounting information for jobs and job steps, similar to `bacct` in LSF. The `scancel` command can be used to cancel a job or job step, corresponding to `bkill` in LSF. Lastly, `scontrol` can be used to view or modify job configurations, which has similarities to `bstop`, `bresume`, and `bmod` in LSF.\n\n| Command  | Action/Task | LSF Equivalent |\n|----------|-------------|----------------|\n| squeue   | Show the current queue | bjobs |\n| sbatch   | Submit a batch script | bsub |\n| salloc   | Submit an interactive job | bsub -Is $SHELL |\n| srun     | Launch a parallel job | jsrun |\n| sinfo    | Show node/partition info | bqueues or bhosts |\n| sacct    | View accounting information for jobs/job steps | bacct |\n| scancel  | Cancel a job or job step | bkill |\n| scontrol | View or modify job configuration | bstop, bresume, bmod |\n\n\n\nBatch Scripts\n\nThe most common way to interact with the batch system is via batch scripts. A batch script is simply a shell script with added directives to request various resoruces from or provide certain information to the scheduling system.  Aside from these directives, the batch script is simply the series of commands needed to set up and run your job.\n\nTo submit a batch script, use the command sbatch myjob.sl\n\nConsider the following batch script:\n\n#!/bin/bash\n#SBATCH -A ABC123\n#SBATCH -J RunSim123\n#SBATCH -o %x-%j.out\n#SBATCH -t 1:00:00\n#SBATCH -p batch\n#SBATCH -N 1024\n\ncd $MEMBERWORK/abc123/Run.456\ncp $PROJWORK/abc123/RunData/Input.456 ./Input.456\nsrun ...\ncp my_output_file $PROJWORK/abc123/RunData/Output.456\n\nIn the script, Slurm directives are preceded by #SBATCH, making them appear as comments to the shell. Slurm looks for these directives through the first non-comment, non-whitespace line. Options after that will be ignored by Slurm (and the shell).\n\n\nThe table provides a comprehensive overview of the commands and steps involved in running a job on the Frontier supercomputer. It starts with the shell interpreter line, which specifies the type of shell being used. The next row mentions the OLCF (Oak Ridge Leadership Computing Facility) project under which the user will be charged for the job. This is followed by the job name and the location where the standard output file will be saved. The format for the standard output file includes placeholders for the job name and ID. The walltime requested for the job is also specified in HH:MM:SS format. The next row mentions the partition or queue where the job will be executed. The number of compute nodes required for the job is also mentioned. This is followed by a blank line in the table. The next few rows outline the steps involved in running the job, starting with changing into the run directory and copying the input file into its place. The job is then executed, with the layout details mentioned. Finally, the output file is copied to a designated location. This table serves as a handy guide for users to understand the necessary steps and commands involved in submitting jobs to the Frontier supercomputer.\n\n| Line | Description |\n|------|-------------|\n| 1 | Shell interpreter line |\n| 2 | OLCF project to charge |\n| 3 | Job name |\n| 4 | Job standard output file (%x will be replaced with the job name and %j with the Job ID) |\n| 5 | Walltime requested (in HH:MM:SS format). See the table below for other formats. |\n| 6 | Partition (queue) to use |\n| 7 | Number of compute nodes requested |\n| 8 | Blank line |\n| 9 | Change into the run directory |\n| 10 | Copy the input file into place |\n| 11 | Run the job (add layout details) |\n| 12 | Copy the output file to an appropriate location.|\n\n\n\n\n\nInteractive Jobs\n\nMost users will find batch jobs an easy way to use the system, as they allow you to \"hand off\" a job to the scheduler, allowing them to focus on other tasks while their job waits in the queue and eventually runs. Occasionally, it is necessary to run interactively, especially when developing, testing, modifying or debugging a code.\n\nSince all compute resources are managed and scheduled by Slurm, it is not possible to simply log into the system and immediately begin running parallel codes interactively. Rather, you must request the appropriate resources from Slurm and, if necessary, wait for them to become available. This is done through an \"interactive batch\" job. Interactive batch jobs are submitted with the salloc command. Resources are requested via the same options that are passed via #SBATCH in a regular batch script (but without the #SBATCH prefix). For example, to request an interactive batch job with the same resources that the batch script above requests, you would use salloc -A ABC123 -J RunSim123 -t 1:00:00 -p batch -N 1024. Note there is no option for an output file...you are running interactively, so standard output and standard error will be displayed to the terminal.\n\n\n\nCommon Slurm Options\n\nThe table below summarizes options for submitted jobs. Unless otherwise noted, they can be used for either batch scripts or interactive batch jobs. For scripts, they can be added on the sbatch command line or as a #BSUB directive in the batch script. (If they're specified in both places, the command line takes precedence.) This is only a subset of all available options. Check the Slurm Man Pages for a more complete list.\n\n\nDescriptive paragraph: The table provides a comprehensive list of options that can be used when submitting a job to the Frontier supercomputer. These options include specifying the project to which the job should be charged, requesting a certain number of nodes and walltime for the job, specifying the number of active hardware threads per core, and setting job dependencies. There are also options for requesting burst buffer/NVMe on each node, specifying the job name, and directing the standard output and error to specific files. Users can also choose to receive email notifications for certain job actions and can reserve specific nodes for their job. Another option is to reserve a specific number of cores per node, which cannot be used by the application. The table also provides information on how to signal a job and its applications. Users can signal a job to write a checkpoint just before it is killed by the system. Overall, these options allow for more control and customization when utilizing the Frontier supercomputer for various computing needs.\n\n| Option                 | Example Usage                  | Description                                                                                                                                        |\n|------------------------|--------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|\n| -A                     | #SBATCH -A ABC123              | Specifies the project to which the job should be charged                                                                                           |\n| -N                     | #SBATCH -N 1024                | Request 1024 nodes for the job                                                                                                                     |\n| -t                     | #SBATCH -t 4:00:00             | Request a walltime of 4 hours. Walltime requests can be specified as minutes, hours:minutes, hours:minuts:seconds, days-hours, days-hours:minutes, or days-hours:minutes:seconds                                    |\n| --threads-per-core     | #SBATCH --threads-per-core=2   | Number of active hardware threads per core. Can be 1 or 2 (1 is default). Must be used if using --threads-per-core=2 in your srun command                |\n| -d                     | #SBATCH -d afterok:12345       | Specify job dependency (in this example, this job cannot start until job 12345 exits with an exit code of 0. See the Job Dependency section for more information)                                                  |\n| -C                     | #SBATCH -C nvme                | Request the burst buffer/NVMe on each node be made available for your job. See the Burst Buffers section for more information on using them.       |\n| -J                     | #SBATCH -J MyJob123            | Specify the job name (this will show up in queue listings)                                                                                        |\n| -o                     | #SBATCH -o jobout.%j           | File where job STDOUT will be directed (%j will be replaced with the job ID). If no -e option is specified, job STDERR will be placed in this file, too. |\n| -e                     | #SBATCH -e joberr.%j           | File where job STDERR will be directed (%j will be replaced with the job ID). If no -o option is specified, job STDOUT will be placed in this file, too. |\n| --mail-type            | #SBATCH --mail-type=END        | Send email for certain job actions. Can be a comma-separated list. Actions include BEGIN, END, FAIL, REQUEUE, INVALID_DEPEND, STAGE_OUT, ALL, and more. |\n| --mail-user            | #SBATCH --mail-user=user@somewhere.com                                                                             | Email address to be used for notifications. |\n| --reservation          | #SBATCH --reservation=MyReservation.1                                                                             | Instructs Slurm to run a job on nodes that are part of the specified reservation. |\n| -S                     | #SBATCH -S 8                                                                                                         | Instructs Slurm to reserve a specific number of cores per node (default is 8). Reserved cores cannot be used by the application. |\n| --signal               | #SBATCH --signal=USR1@300                                                   | Send the given signal to a job the specified time (in seconds) seconds before the job reaches its walltime. The signal can be by name or by number (i.e. both 10 and USR1 would send SIGUSR1). |\n\n\n\nSlurm Environment Variables\n\nSlurm reads a number of environment variables, many of which can provide the same information as the job options noted above. We recommend using the job options rather than environment variables to specify job options, as it allows you to have everything self-contained within the job submission script (rather than having to remember what options you set for a given job).\n\nSlurm also provides a number of environment variables within your running job. The following table summarizes those that may be particularly useful within your job (e.g. for naming output log files):\n\n\nThe table provides information on the Frontier supercomputer, a high performance computing system that is used for complex and large-scale scientific calculations. It includes five variables and their descriptions related to job submission and management on the supercomputer. The first variable, $SLURM_SUBMIT_DIR, indicates the directory from which the batch job was submitted and how to return to it. The second variable, $SLURM_JOBID, is a unique identifier for each job and is often used for labeling output and error files. The third variable, $SLURM_JOB_NUM_NODES, specifies the number of nodes requested for the job. The fourth variable, $SLURM_JOB_NAME, is the name of the job chosen by the user. The final variable, $SLURM_NODELIST, provides a list of all the nodes assigned to the job. These variables are essential for understanding and managing jobs on the Frontier supercomputer and ensuring efficient and accurate processing of scientific data.\n\n| Variable | Description   | \n|-----------|--------------|\n| $SLURM_SUBMIT_DIR | The directory from which the batch job was submitted. By default, a new job starts in your home directory. You can get back to the directory of job submission with cd $SLURM_SUBMIT_DIR. Note that this is not necessarily the same directory in which the batch script resides. | \n| $SLURM_JOBID | The job’s full identifier. A common use for $SLURM_JOBID is to append the job’s ID to the standard output and error files. | \n| $SLURM_JOB_NUM_NODES | The number of nodes requested. | \n| $SLURM_JOB_NAME | The job name supplied by the user. | \n| $SLURM_NODELIST | The list of nodes assigned to the job. |\n\n\n\nJob States\n\nA job will transition through several states during its lifetime. Common ones include:\n\n\nThe table provides information on the various states and corresponding codes for the Frontier supercomputer. Each state is listed along with its code, along with a brief description of what each state means. The first row of the table is the header with the titles of \"Code\", \"State\", and \"Description\". The first state listed is \"Canceled\" with the code \"CA\", which indicates that the job has been canceled, potentially by either the user or an administrator. The next state is \"Completed\" with the code \"CD\", which signifies that the job has successfully completed with an exit code of 0. \"Completing\" is listed with the code \"CG\", indicating that the job is currently in the process of completing, with some processes still running. The code \"PD\" is associated with the state \"Pending\" which means the job is waiting for resources to be allocated. Lastly, the state of \"Running\" is denoted by the code \"R\", indicating that the job is currently in progress. Overall, this table provides a comprehensive overview of the different states that a job can be in on the Frontier supercomputer.\n\n|Code|State|Description|\n|---|---|---|\n|CA|Canceled|The job was canceled (could've been by the user or an administrator)|\n|CD|Completed|The job completed successfully (exit code 0)|\n|CG|Completing|The job is in the process of completing (some processes may still be running)|\n|PD|Pending|The job is waiting for resources to be allocated|\n|R|Running|The job is currently running|\n\n\n\nJob Reason Codes\n\nIn addition to state codes, jobs that are pending will have a \"reason code\" to explain why the job is pending. Completed jobs will have a reason describing how the job ended. Some codes you might see include:\n\n\nThe following table provides a comprehensive overview of potential reasons and their corresponding meanings for various scenarios that may occur on the Frontier supercomputer. The table's header is not included in the description, as it is self-explanatory. The first column, \"Reason,\" lists all possible reasons for a job being held or experiencing issues, while the second column, \"Meaning,\" provides a detailed explanation for each reason. This table serves as a useful resource for understanding and troubleshooting any problems that may arise when using the Frontier supercomputer. Please see the table below for reference.\n\n| Reason              | Meaning                                                                                                                                                    |\n|---------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Dependency          | The job has dependencies that have not been met, preventing it from running.                                                                                |\n| JobHeldUser         | The job is being held at the request of the user, possibly due to a specific condition or preference.                                                      |\n| JobHeldAdmin        | The job is being held at the request of the system administrator, possibly due to a system-wide issue or maintenance.                                       |\n| Priority            | Other jobs with a higher priority are currently running, causing this job to be held until they complete.                                                   |\n| Reservation         | The job is waiting for its reserved resources to become available before it can run.                                                                       |\n| AssocMaxJobsLimit   | The job is being held because the user or project has reached the limit for running jobs. This limit can be set to prevent resource overloading.          |\n| ReqNodeNotAvail     | The job requested a specific node, but it is currently unavailable due to various reasons such as being in use, reserved, down, or draining.                |\n| JobLaunchFailure    | The job failed to launch, potentially due to system issues or an invalid program name.                                                                     |\n| NonZeroExitCode     | The job exited with an error code other than 0, indicating an issue or failure during execution. This could be due to program errors or incompatible inputs. |\n\n\n\nMany other states and job reason codes exist. For a more complete description, see the squeue man page (either on the system or online).\n\nScheduling Policy\n\nIn a simple batch queue system, jobs run in a first-in, first-out (FIFO) order. This can lead to inefficient use of the system. If a large job is the next to run, a strict FIFO queue can cause nodes to sit idle while waiting for the large job to start. Backfilling would allow smaller, shorter jobs to use those resources that would otherwise remain idle until the large job starts. With the proper algorithm, they would do so without impacting the start time of the large job. While this does make more efficient use of the system, it encourages the submission of smaller jobs.\n\nThe DOE Leadership-Class Job Mandate\n\nAs a DOE Leadership Computing Facility, OLCF has a mandate that a large portion of Frontier's usage come from large, leadership-class (a.k.a. capability) jobs. To ensure that OLCF complies with this directive, we strongly encourage users to run jobs on Frontier that are as large as their code will allow. To that end, OLCF implements queue policies that enable large jobs to run in a timely fashion.\n\nThe OLCF implements queue policies that encourage the submission and timely execution of large, leadership-class jobs on Frontier.\n\nThe basic priority mechanism for jobs waiting in the queue is the time the job has been waiting in the queue. If your jobs require resources outside these policies such as higher priority or longer walltimes, please contact help@olcf.ornl.gov\n\nJob Priority by Node Count\n\nJobs are aged according to the job's requested node count (older\nage equals higher queue priority). Each job's requested node count\nplaces it into a specific bin. Each bin has a different aging\nparameter, which all jobs in the bin receive.\n\n\nThe table presents data on the Frontier supercomputer, specifically regarding the minimum and maximum number of nodes, the maximum walltime in hours, and the aging boost in days. The first row serves as the header for the table. The data is divided into five bins, with each bin containing information on the respective categories. The first bin shows that the minimum number of nodes for the Frontier supercomputer is 5,645 and the maximum number is 9,408, with a maximum walltime of 12 hours and an aging boost period of 8 days. The second bin has a lower minimum number of nodes at 1,882 and a smaller maximum at 5,644, with the same maximum walltime and a shorter aging boost period of 4 days. The third bin has the lowest number of nodes, ranging from 184 to 1,881, with the same maximum walltime and no aging boost. The fourth bin has even fewer nodes, with a range of 92 to 183, and a shorter maximum walltime of 6 hours and no aging boost. Finally, the fifth and last bin only has a single node with a maximum walltime of 2 hours and no aging boost. This table provides a comprehensive overview of the capabilities and configurations of the Frontier supercomputer. \n\n| Bin | Min Nodes | Max Nodes | Max Walltime (Hours) | Aging Boost (Days) |\n| --- | --- | --- | --- | --- |\n| 1 | 5,645 | 9,408 | 12.0 | 8 |\n| 2 | 1,882 | 5,644 | 12.0 | 4 |\n| 3 | 184 | 1,881 | 12.0 | 0 |\n| 4 | 92 | 183 | 6.0 | 0 |\n| 5 | 1 | 91 | 2.0 | 0 |\n\n\n\nbatch Queue Policy\n\nThe batch queue is the default queue for production work on Frontier. Most work on Frontier is handled through this queue. The following policies are enforced for the batch queue:\n\nLimit of four eligible-to-run jobs per user. (Jobs in excess of this number will be held, but will move to the eligible-to-run state at the appropriate time.)\n\nUsers may have only 100 jobs queued in the batch queue at any time (this includes jobs in all states). Additional jobs will be rejected at submit time.\n\ndebug Quality of Service Class\n\nThe debug quality of service (QOS) class can be used to access Frontier's compute resources for short non-production debug tasks. The QOS provides a higher priority compare to jobs of the same job size bin in production queues. Production work and job chaining using the debug QOS is prohibited. Each user is limited to one job in any state at any one point. Attempts to submit multiple jobs to this QOS will be rejected upon job submission.\n\nTo submit a job to the debug QOS, add the -q debug option to your sbatch or salloc command or #SBATCH -q debug to your job script.\n\nAllocation Overuse Policy\n\nProjects that overrun their allocation are still allowed to run on OLCF systems, although at a reduced priority. Like the adjustment for the number of processors requested above, this is an adjustment to the apparent submit time of the job. However, this adjustment has the effect of making jobs appear much younger than jobs submitted under projects that have not exceeded their allocation. In addition to the priority change, these jobs are also limited in the amount of wall time that can be used. For example, consider that job1 is submitted at the same time as job2. The project associated with job1 is over its allocation, while the project for job2 is not. The batch system will consider job2 to have been waiting for a longer time than job1. Additionally, projects that are at 125% of their allocated time will be limited to only 3 running jobs at a time. The adjustment to the apparent submit time depends upon the percentage that the project is over its allocation, as shown in the table below:\n\n\nThe following table outlines the allocation usage and priority levels for the Frontier supercomputer. The first row displays the headers for the columns, which are \"% of Allocation Used\" and \"Priority Reduction.\" The first column lists different ranges for the percentage of allocation used, while the second column specifies the corresponding priority reduction in days. If the allocation usage is less than 100%, there will be no priority reduction. For allocation usage between 100% and 125%, there will be a reduction of 30 days in priority. If the allocation usage exceeds 125%, there will be a reduction of 365 days in priority. This table serves as a guide for understanding the priority levels for allocation usage on the Frontier supercomputer.\n\n| % of Allocation Used | Priority Reduction |\n| --------------------- | ------------------ |\n| < 100%                | none               |\n| >=100% but <=125%     | 30 days            |\n| > 125%                | 365 days           |\n\n\n\nSystem Reservation Policy\n\nProjects may request to reserve a set of nodes for a period of time by contacting help@olcf.ornl.gov. If the reservation is granted, the reserved nodes will be blocked from general use for a given period of time. Only users that have been authorized to use the reservation can utilize those resources. Since no other users can access the reserved resources, it is crucial that groups given reservations take care to ensure the utilization on those resources remains high. To prevent reserved resources from remaining idle for an extended period of time, reservations are monitored for inactivity. If activity falls below 50% of the reserved resources for more than (30) minutes, the reservation will be canceled and the system will be returned to normal scheduling. A new reservation must be requested if this occurs.\n\nThe requesting project’s allocation is charged according to the time window granted, regardless of actual utilization. For example, an 8-hour, 2,000 node reservation on Frontier would be equivalent to using 16,000 Frontier node-hours of a project’s allocation.\n\nReservations should not be confused with priority requests. If quick turnaround is needed for a few jobs or for a period of time, a priority boost should be requested. A reservation should only be requested if users need to guarantee availability of a set of nodes at a given time, such as for a live demonstration at a conference.\n\nJob Dependencies\n\nOftentimes, a job will need data from some other job in the queue, but it's nonetheless convenient to submit the second job before the first finishes. Slurm allows you to submit a job with constraints that will keep it from running until these dependencies are met. These are specified with the -d option to Slurm. Common dependency flags are summarized below. In each of these examples, only a single jobid is shown but you can specify multiple job IDs as a colon-delimited list (i.e. #SBATCH -d afterok:12345:12346:12346). For the after dependency, you can optionally specify a +time value for each jobid.\n\n\nThe Frontier supercomputer is equipped with various flag options to control and schedule jobs. These flags are used to indicate when a job can start based on the status of other jobs. The first flag, #SBATCH -d after:jobid[+time], allows the job to start after a specified job has started or is canceled. The optional argument of +time indicates the minimum number of minutes that must pass before the job can start. The next flag, #SBATCH -d afterany:jobid, allows the job to start after a specific job has ended, regardless of its exit state. Similarly, the #SBATCH -d afternotok:jobid flag enables job start after a specified job has terminated in a failed state, while the #SBATCH -d afterok:jobid flag only allows job start after a successful completion of the specified job. The final flag, #SBATCH -d singleton, enables job start after any previously-launched job with the same name and from the same user has completed, ensuring that jobs are serialized based on username+jobname pairs. Overall, these flags provide flexibility and control over job scheduling on the Frontier supercomputer.\n\n| Flag                         | Meaning (for the dependent job)                                                                                                                                                                                                                                                                                   |\n|------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| #SBATCH -d after:jobid[+time]| The job can start after the specified jobs start or are canceled. The optional +time argument is a number of minutes. If specified, the job cannot start until that many minutes have passed since the listed jobs start/are canceled. If not specified, there is no delay.                                                                  |\n| #SBATCH -d afterany:jobid    | The job can start after the specified jobs have ended (regardless of exit state).                                                                                                                                                                                                                                |\n| #SBATCH -d afternotok:jobid  | The job can start after the specified jobs terminate in a failed (non-zero) state.                                                                                                                                                                                                                              |\n| #SBATCH -d afterok:jobid     | The job can start after the specified jobs complete successfully (i.e. zero exit code).                                                                                                                                                                                                                         |\n| #SBATCH -d singleton         | Job can begin after any previously-launched job with the same name and from the same user have completed. In other words, serialize the running jobs based on username+jobname pairs.                                                                                                                                                                               |\n\n\n\nMonitoring and Modifying Batch Jobs\n\nscontrol hold and scontrol release: Holding and Releasing Jobs\n\nSometimes you may need to place a hold on a job to keep it from starting. For example, you may have submitted it assuming some needed data was in place but later realized that data is not yet available. This can be done with the scontrol hold command. Later, when the data is ready, you can release the job (i.e. tell the system that it's now OK to run the job) with the scontrol release command. For example:\n\n\nThe table provides a detailed overview of the commands scontrol hold and scontrol release for the Frontier supercomputer. The first column lists the commands and the second column provides a brief description of their function. The scontrol hold command is used to place a specific job, designated by the job number 12345, on hold. This means that the job will not be executed until the hold is released. On the other hand, the scontrol release command is used to release a specific job, also designated by the job number 12345, and allows it to be executed by the system. This table serves as a helpful reference for users of the Frontier supercomputer, allowing them to easily manipulate their job status through these commands.\n\n| Command          | Description                                                                                        |\n| -----------------|----------------------------------------------------------------------------------------------------|\n| scontrol hold 12345 | Place job 12345 on hold                                                                             |\n| scontrol release 12345 | Release job 12345 (i.e. tell the system it's OK to run it)                                       |\n\n\n\nscontrol update: Changing Job Parameters\n\nThere may also be occasions where you want to modify a job that's waiting in the queue. For example, perhaps you requested 2,000 nodes but later realized this is a different data set and only needs 1,000 nodes. You can use the scontrol update command for this. For example:\n\n\nThe following table showcases the capabilities of the Frontier supercomputer. In the first row, we see the header \"scontrol update\" which indicates the use of the scontrol command to modify job settings. The first row also shows the command to change the number of nodes requested for job 12345 to 1000. This means that this job will utilize 1000 nodes on the Frontier supercomputer. In the second row, we see the command to change the maximum walltime for job 12345 to 4 hours. This indicates that this job has a time limit of 4 hours for execution on the Frontier supercomputer. Overall, the table highlights some of the customizable settings that can be applied to jobs on the Frontier supercomputer through the use of the scontrol command.\n\n| scontrol update NumNodes=1000 JobID=12345 | Change job 12345's node request to 1000 nodes |\n| scontrol update TimeLimit=4:00:00 JobID=12345 | Change job 12345's max walltime to 4 hours |\n\n\n\nscancel: Cancel or Signal a Job\n\nIn addition to the --signal option for the sbatch/salloc commands described above <common-slurm-options> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#above <common-slurm-options>>, the scancel command can be used to manually signal a job. Typically, this is used to remove a job from the queue. In this use case, you do not need to specify a signal and can simply provide the jobid (i.e. scancel 12345). If you want to send some other signal to the job, use scancel the with the -s option. The -s option allows signals to be specified either by number or by name. Thus, if you want to send SIGUSR1 to a job, you would use scancel -s 10 12345 or scancel -s USR1 12345.\n\nsqueue: View the Queue\n\nThe squeue command is used to show the batch queue. You can filter the level of detail through several command-line options. For example:\n\n\nThe following table presents information related to the Frontier supercomputer's job queue. The first row serves as the header and lists the different commands (squeue -l and squeue -l -u $USER) that can be used to view the job queue. The second row provides a description for each command, explaining that the first one shows all jobs in the queue while the second one specifically shows only the jobs for the current user (denoted by the $USER variable). The rest of the table contains the various parameters and options that can be used with each command to further customize the output. This includes options to view specific job states, partitions, and job details. The information presented in this table is useful for users of the Frontier supercomputer who need to view and manage their pending jobs. \n\n| Command              | Description                                                                         |\n|----------------------|-------------------------------------------------------------------------------------|\n| squeue -l            | Show all jobs currently in the queue.                                                |\n| squeue -l -u $USER   | Show all of your jobs currently in the queue.                                        |\n| -t, --states=[STATE] | Show jobs with a particular state.                                                  |\n| -p, --partition      | Show jobs for specific partition.                                                   |\n| -d, --details        | Show detailed output including job and partition details.                           |\n\nTable in markdown format:\n\n| Command              | Description                                                                         |\n|----------------------|-------------------------------------------------------------------------------------|\n| squeue -l            | Show all jobs currently in the queue.                                                |\n| squeue -l -u $USER   | Show all of your jobs currently in the queue.                                        |\n| -t, --states=[STATE] | Show jobs with a particular state.                                                  |\n| -p, --partition      | Show jobs for specific partition.                                                   |\n| -d, --details        | Show detailed output including job and partition details.                           |\n\n\n\nsacct: Get Job Accounting Information\n\nThe sacct command gives detailed information about jobs currently in the queue and recently-completed jobs. You can also use it to see the various steps within a batch jobs.\n\n\nThe table showcases different variations of the sacct command that can be used to view job allocations and information on the Frontier supercomputer. The first row provides an explanation of the various parameters used in the command. The second row shows the command to display all jobs in the queue while summarizing the entire allocation instead of showing individual steps. The third row displays the command to view all jobs for a specific user, while also showing the individual steps. The fourth row demonstrates how to view all steps for a specific job using its ID number. Lastly, the fifth row showcases a command to view all jobs for a particular user since a specific date and time, with a customized output format displaying the job ID, job name, and node list. This table serves as a helpful guide for users to efficiently manage their jobs on the Frontier supercomputer.\n\n| sacct -a -X | \n| sacct -u $USER | \n| sacct -j 12345 | \n| sacct -u $USER -S 2022-07-01T13:00:00 -o \"jobid%5,jobname%25,nodelist%20\" -X |\n\n\n\nscontrol show job: Get Detailed Job Information\n\nIn addition to holding, releasing, and updating the job, the scontrol command can show detailed job information via the show job subcommand. For example, scontrol show job 12345.\n\n\n\nSrun\n\nThe default job launcher for Frontier is srun . The srun command is used to execute an MPI binary on one or more compute nodes in parallel.\n\nSrun Format\n\nsrun  [OPTIONS... [executable [args...]]]\n\nSingle Command (non-interactive)\n\n$ srun -A <project_id> -t 00:05:00 -p <partition> -N 2 -n 4 --ntasks-per-node=2 ./a.out\n<output printed to terminal>\n\nThe job name and output options have been removed since stdout/stderr are typically desired in the terminal window in this usage mode.\n\nsrun accepts the following common options:\n\n\nThe table provides important information about the Frontier supercomputer. The first row lists the different parameters and their options. The 'Number of nodes' column indicates the total number of nodes available on the supercomputer. The 'Total number of MPI tasks' column specifies the default number of tasks, which is 1. The 'Logical cores per MPI task' column indicates the number of cores allocated for each MPI task. When using '--threads-per-core=1', the '-c' option is equivalent to the physical cores per task. By default, additional cores per task are distributed within one L3 region before moving on to a different L3 region. The '--cpu-bind' option allows for tasks to be bound to specific CPUs, with the recommended 'threads' option automatically generating masks for task-thread binding. The '--threads-per-core' option specifies the maximum number of hardware threads per core, which is 1 by default but can be changed to 2. The '-m' option specifies the distribution of MPI ranks across compute nodes, sockets, and cores. The default distribution is 'block:cyclic:cyclic'. The table also mentions the 'ntasks-per-node' and 'ntasks-per-gpu' options for specifying the number of tasks per node or per GPU. The 'gpus' and 'gpus-per-node' options allow for specifying the total number of GPUs required for the job and the number of GPUs per node, respectively. The table also explains the different options available for binding tasks to GPUs. \n```\n| N | Number of nodes |\n| n | Total number of MPI tasks (default is 1) |\n| c, --cpus-per-task=<ncpus> | Logical cores per MPI task (default is 1) |\n| --cpu-bind=threads | Bind tasks to CPUs. |\n| --threads-per-core=<threads> | In task layout, use the specified maximum number of hardware threads per core (default is 1; there are 2 hardware threads per physical CPU core). Must also be set in salloc or sbatch if using --threads-per-core=2 in your srun command.|\n| -m, --distribution=<value>:<value>:<value> | Specifies the distribution of MPI ranks across compute nodes, sockets, and cores, respectively. The default values are block:cyclic:cyclic, see man srun for more information. Currently, the distribution setting for cores (the third \"<value>\" entry) has no effect on Frontier. |\n| --ntasks-per-node=<ntasks> | If used without -n: requests that a specific number of tasks be invoked on each node. If used with -n: treated as a maximum count of tasks per node. |\n| --gpus | Specify the number of GPUs required for the job (total GPUs across all nodes). |\n| --gpus-per-node | Specify the number of GPUs per node required for the job. |\n| --gpu-bind=closest | Binds each task to the GPU which is on the same NUMA domain as the CPU core the MPI rank is running on. |\n| --gpu-bind=map_gpu:<list> | Bind tasks to specific GPUs by setting GPU masks on tasks (or ranks) as specified where <list> is <gpu_id_for_task_0>,<gpu_id_for_task_1>,.... If the number of tasks (or ranks) exceeds the number of elements in this list, elements in the list will be reused as needed starting from the beginning of the list. To simplify support for large task counts, the lists may follow a map with an asterisk and repetition count. (For example map_gpu:0*4,1*4) |\n| --ntasks-per-gpu=<ntasks> | Request that there are ntasks tasks invoked for every GPU.|\n\n\n\nBelow is a comparison table between srun and jsrun.\n\n\nThe presented table provides a comparison of the different options for job submission on the Frontier supercomputer. The two options, jsrun and srun, are compared in terms of the number of nodes and tasks that can be defined, as well as the number of CPUs, resource sets, and GPUs per task and resource set. Both options also allow for binding tasks to allocated CPUs, but srun also includes a performance binding preference. Additionally, the table highlights the ability to specify a task to resource mapping pattern with the option of launch distribution. This table serves as a helpful guide for users to determine which option best suits their job submission needs on the Frontier supercomputer.\n\n| Option                             | jsrun (Summit) | srun (Frontier) |\n|------------------------------------ |--------------- |---------------- |\n| Number of nodes                     | -nnodes        | -N, --nnodes    |\n| Number of tasks                     | defined with resource set | -n, --ntasks    |\n| Number of tasks per node            | defined with resource set | --ntasks-per-node |\n| Number of CPUs per task             | defined with resource set | -c, --cpus-per-task |\n| Number of resource sets             | -n, --nrs      | N/A             |\n| Number of resource sets per host    | -r, --rs_per_host | N/A           |\n| Number of tasks per resource set    | -a, --tasks_per_rs | N/A           |\n| Number of CPUs per resource set     | -c, --cpus_per_rs | N/A           |\n| Number of GPUs per resource set     | -g, --gpus_per_rs | N/A           |\n| Bind tasks to allocated CPUs        | -b, --bind     | --cpu-bind      |\n| Performance binding preference      | -l,--latency_priority | --hint     |\n| Specify task to resource mapping pattern | --launch_distribution | -m, --distribution |\n\n\n\n\n\nProcess and Thread Mapping Examples\n\nThis section describes how to map processes (e.g., MPI ranks) and process\nthreads (e.g., OpenMP threads) to the CPUs, GPUs, and NICs on Frontier.\n\nUsers are highly encouraged to use the CPU- and GPU-mapping programs used in\nthe following sections to check their understanding of the job steps (i.e.,\nsrun commands) they intend to use in their actual jobs.\n\nFor the frontier-cpu-map <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-cpu-map> and frontier-multi-map <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-multi-map> sections:\n\nA simple MPI+OpenMP \"Hello, World\" program (hello_mpi_omp) will be used to clarify the\nmappings.\n\nFor the frontier-gpu-map <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-gpu-map> section:\n\nAn MPI+OpenMP+HIP \"Hello, World\" program (hello_jobstep) will be used to clarify the GPU\nmappings.\n\nAdditionally, it may be helpful to cross reference the\nsimplified Frontier node diagram <frontier-simple> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#simplified Frontier node diagram <frontier-simple>> -- specifically the\nlow-noise mode diagram <frontier-lownoise> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>>.\n\nUnless specified otherwise, the examples below assume the default low-noise\ncore specialization setting (-S 8).  This means that there are only 56\nallocatable cores by default instead of 64.  See the frontier-lownoise <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-lownoise>\nsection for more details.  Set -S 0 at job allocation to override this setting.\n\n\n\nCPU Mapping\n\nThis subsection covers how to map tasks to the CPU without the presence of\nadditional threads (i.e., solely MPI tasks -- no additional OpenMP threads).\n\nThe intent with both of the following examples is to launch 8 MPI ranks across\nthe node where each rank is assigned its own logical (and, in this case,\nphysical) core.  Using the -m distribution flag, we will cover two common\napproaches to assign the MPI ranks -- in a \"round-robin\" (cyclic)\nconfiguration and in a \"packed\" (block) configuration. Slurm's\nfrontier-interactive <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive> method was used to request an allocation of 1\ncompute node for these examples: salloc -A <project_id> -t 30 -p <parition>\n-N 1\n\nThere are many different ways users might choose to perform these mappings,\nso users are encouraged to clone the hello_mpi_omp program and test whether\nor not processes and threads are running where intended.\n\n8 MPI Ranks (round-robin)\n\nAssigning MPI ranks in a \"round-robin\" (cyclic) manner across L3 cache\nregions (sockets) is the default behavior on Frontier. This mode will assign\nconsecutive MPI tasks to different sockets before it tries to \"fill up\" a\nsocket.\n\nRecall that the -m flag behaves like: -m <node distribution>:<socket\ndistribution>.  Hence, the key setting to achieving the round-robin nature is\nthe -m block:cyclic flag, specifically the cyclic setting provided for\nthe \"socket distribution\". This ensures that the MPI tasks will be distributed\nacross sockets in a cyclic (round-robin) manner.\n\nThe below srun command will achieve the intended 8 MPI \"round-robin\" layout:\n\n$ export OMP_NUM_THREADS=1\n$ srun -N1 -n8 -c1 --cpu-bind=threads --threads-per-core=1 -m block:cyclic ./hello_mpi_omp | sort\n\nMPI 000 - OMP 000 - HWT 001 - Node frontier00144\nMPI 001 - OMP 000 - HWT 009 - Node frontier00144\nMPI 002 - OMP 000 - HWT 017 - Node frontier00144\nMPI 003 - OMP 000 - HWT 025 - Node frontier00144\nMPI 004 - OMP 000 - HWT 033 - Node frontier00144\nMPI 005 - OMP 000 - HWT 041 - Node frontier00144\nMPI 006 - OMP 000 - HWT 049 - Node frontier00144\nMPI 007 - OMP 000 - HWT 057 - Node frontier00144\n\n\n\nBreaking down the srun command, we have:\n\n-N1: indicates we are using 1 node\n\n-n8: indicates we are launching 8 MPI tasks\n\n-c1: indicates we are assigning 1 logical core per MPI task.\nIn this case, because of --threads-per-core=1, this also means 1 physical core per MPI task.\n\n--cpu-bind=threads: binds tasks to threads\n\n--threads-per-core=1: use a maximum of 1 hardware thread per physical core (i.e., only use 1 logical core per physical core)\n\n-m block:cyclic: distribute the tasks in a block layout across nodes (default), and in a cyclic (round-robin) layout across L3 sockets\n\n./hello_mpi_omp: launches the \"hello_mpi_omp\" executable\n\n| sort: sorts the output\n\nAlthough the above command used the default settings -c1,\n--cpu-bind=threads, --threads-per-core=1 and -m block:cyclic, it is\nalways better to be explicit with your srun command to have more control\nover your node layout. The above command is equivalent to srun -N1 -n8.\n\nAs you can see in the node diagram above, this results in the 8 MPI tasks\n(outlined in different colors) being distributed \"vertically\" across L3\nsockets.\n\n7 MPI Ranks (packed)\n\nInstead, you can assign MPI ranks so that the L3 regions are filled in a\n\"packed\" (block) manner.  This mode will assign consecutive MPI tasks to\nthe same L3 region (socket) until it is \"filled up\" or \"packed\" before\nassigning a task to a different socket.\n\nRecall that the -m flag behaves like: -m <node distribution>:<socket\ndistribution>.  Hence, the key setting to achieving the round-robin nature is\nthe -m block:block flag, specifically the block setting provided for\nthe \"socket distribution\". This ensures that the MPI tasks will be distributed\nin a packed manner.\n\nThe below srun command will achieve the intended 7 MPI \"packed\" layout:\n\n$ export OMP_NUM_THREADS=1\n$ srun -N1 -n7 -c1 --cpu-bind=threads --threads-per-core=1 -m block:block ./hello_mpi_omp | sort\n\nMPI 000 - OMP 000 - HWT 001 - Node frontier00144\nMPI 001 - OMP 000 - HWT 002 - Node frontier00144\nMPI 002 - OMP 000 - HWT 003 - Node frontier00144\nMPI 003 - OMP 000 - HWT 004 - Node frontier00144\nMPI 004 - OMP 000 - HWT 005 - Node frontier00144\nMPI 005 - OMP 000 - HWT 006 - Node frontier00144\nMPI 006 - OMP 000 - HWT 007 - Node frontier00144\n\n\n\nBreaking down the srun command, the only difference than the previous example is:\n\n-m block:block: distribute the tasks in a block layout across nodes (default), and in a block (packed) socket layout\n\nAs you can see in the node diagram above, this results in the 7 MPI tasks\n(outlined in different colors) being distributed \"horizontally\" within a\nsocket, rather than being spread across different L3 sockets like with the\nprevious example. However, if an 8th task was requested it would be assigned\nto the next L3 region on core 009.\n\n\n\nMultithreading\n\nBecause a Frontier compute node has two hardware threads available (2 logical\ncores per physical core), this enables the possibility of multithreading your\napplication (e.g., with OpenMP threads). Although the additional hardware\nthreads can be assigned to additional MPI tasks, this is not recommended. It is\nhighly recommended to only use 1 MPI task per physical core and to use OpenMP\nthreads instead on any additional logical cores gained when using both hardware\nthreads.\n\nThe following examples cover multithreading with hybrid MPI+OpenMP\napplications.  In these examples, Slurm's frontier-interactive <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive> method\nwas used to request an allocation of 1 compute node:\nsalloc -A <project_id> -t 30 -p <parition> -N 1\n\nThere are many different ways users might choose to perform these mappings,\nso users are encouraged to clone the hello_mpi_omp program and test whether\nor not processes and threads are running where intended.\n\n2 MPI ranks - each with 2 OpenMP threads\n\nIn this example, the intent is to launch 2 MPI ranks, each of which spawn 2\nOpenMP threads, and have all of the 4 OpenMP threads run on different physical\nCPU cores.\n\nFirst (INCORRECT) attempt\n\nTo set the number of OpenMP threads spawned per MPI rank, the\nOMP_NUM_THREADS environment variable can be used. To set the number of MPI\nranks launched, the srun flag -n can be used.\n\n$ export OMP_NUM_THREADS=2\n$ srun -N1 -n2 ./hello_mpi_omp | sort\n\nWARNING: Requested total thread count and/or thread affinity may result in\noversubscription of available CPU resources!  Performance may be degraded.\nExplicitly set OMP_WAIT_POLICY=PASSIVE or ACTIVE to suppress this message.\nSet CRAY_OMP_CHECK_AFFINITY=TRUE to print detailed thread-affinity messages.\nWARNING: Requested total thread count and/or thread affinity may result in\noversubscription of available CPU resources!  Performance may be degraded.\nExplicitly set OMP_WAIT_POLICY=PASSIVE or ACTIVE to suppress this message.\nSet CRAY_OMP_CHECK_AFFINITY=TRUE to print detailed thread-affinity messages.\n\nMPI 000 - OMP 000 - HWT 001 - Node frontier001\nMPI 000 - OMP 001 - HWT 001 - Node frontier001\nMPI 001 - OMP 000 - HWT 009 - Node frontier001\nMPI 001 - OMP 001 - HWT 009 - Node frontier001\n\nThe first thing to notice here is the WARNING about oversubscribing the\navailable CPU cores. Also, the output shows each MPI rank did spawn 2 OpenMP\nthreads, but both OpenMP threads ran on the same logical core (for a given\nMPI rank). This was not the intended behavior; each OpenMP thread was meant to\nrun on its own physical CPU core.\n\nThe problem here arises from two default settings; 1) each MPI rank is only\nallocated 1 logical core (-c 1) and, 2) only 1 hardware thread per physical\nCPU core is enabled (--threads-per-core=1).  When using\n--threads-per-core=1 and --cpu-bind=threads (the default setting), 1\nlogical core in -c is equivalent to 1 physical core.  So in this case, each\nMPI rank only has 1 physical core (with 1 hardware thread) to run on -\nincluding any threads the process spawns - hence the WARNING and undesired\nbehavior.\n\nSecond (CORRECT) attempt\n\nRecall that in this scenario, because of the --threads-per-core=1 setting,\n1 logical core is equivalent to 1 physical core when using -c.  Therefore,\nin order for each OpenMP thread to run on its own physical CPU core, each MPI\nrank should be given 2 physical CPU cores (-c 2).  Now the OpenMP threads\nwill be mapped to unique hardware threads on separate physical CPU cores.\n\n$ export OMP_NUM_THREADS=2\n$ srun -N1 -n2 -c2 ./hello_mpi_omp | sort\n\nMPI 000 - OMP 000 - HWT 001 - Node frontier001\nMPI 000 - OMP 001 - HWT 002 - Node frontier001\nMPI 001 - OMP 000 - HWT 009 - Node frontier001\nMPI 001 - OMP 001 - HWT 010 - Node frontier001\n\nNow the output shows that each OpenMP thread ran on its own physical CPU core.\nMore specifically (see the Frontier Compute Node diagram), OpenMP thread 000 of\nMPI rank 000 ran on logical core 001 (i.e., physical CPU core 01), OpenMP\nthread 001 of MPI rank 000 ran on logical core 002 (i.e., physical CPU core\n02), OpenMP thread 000 of MPI rank 001 ran on logical core 009 (i.e., physical\nCPU core 09), and OpenMP thread 001 of MPI rank 001 ran on logical core 010\n(i.e., physical CPU core 10) - as intended.\n\nThird attempt - Using multiple threads per core\n\nTo use both available hardware threads per core, the job must be allocated\nwith --threads-per-core=2 (as opposed to only the job step - i.e., srun\ncommand). That value will then be inherited by srun unless explcitly\noverridden with --threads-per-core=1. Because we are using\n--threads-per-core=2, the usage of -c goes back to purely meaning the\namount of logical cores (i.e., it is no longer equivalent to 1 physical core).\n\n$ salloc -N1 -A <project_id> -t <time> -p <partition> --threads-per-core=2\n\n$ export OMP_NUM_THREADS=2\n$ srun -N1 -n2 -c2 ./hello_mpi_omp | sort\n\nMPI 000 - OMP 000 - HWT 001 - Node frontier001\nMPI 000 - OMP 001 - HWT 065 - Node frontier001\nMPI 001 - OMP 000 - HWT 009 - Node frontier001\nMPI 001 - OMP 001 - HWT 073 - Node frontier001\n\nComparing this output to the Frontier Compute Node diagram, we see that each\npair of OpenMP threads is contained within a single physical core. MPI rank 000\nran on logical cores 001 and 065 (i.e. physical CPU core 01) and MPI rank\n001 ran on logical cores 009 and 073 (i.e. physical CPU core 09).\n\n\n\nGPU Mapping\n\nIn this sub-section, an MPI+OpenMP+HIP \"Hello, World\" program (hello_jobstep) will be used to show how to make\nonly specific GPUs available to processes - which we will refer to as \"GPU\nmapping\". Again, Slurm's frontier-interactive <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-interactive> method was used to request\nan allocation of 2 compute nodes for these examples: salloc -A <project_id>\n-t 30 -p <parition> -N 2. The CPU mapping part of this example is very\nsimilar to the example used above in the Multithreading sub-section, so the\nfocus here will be on the GPU mapping part.\n\nIn general, GPU mapping can be accomplished in different ways. For example, an\napplication might map GPUs to MPI ranks programmatically within the code using,\nsay, hipSetDevice. In this case, there might not be a need to map GPUs using\nSlurm (since it can be done in the code itself). However, many applications\nexpect only 1 GPU to be available to each rank. It is this latter case that the\nfollowing examples refer to.\n\nAlso, recall that the CPU cores in a given L3 cache region are connected to a\nspecific GPU (see the Frontier Node Diagram and subsequent\nNote on NUMA domains <numa-note> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Note on NUMA domains <numa-note>> for more information). In the examples\nbelow, knowledge of these details will be assumed.\n\nThere are many different ways users might choose to perform these mappings,\nso users are encouraged to clone the hello_jobstep program and test whether\nprocesses and threads are mapped to the CPU cores and GPUs as intended..\n\nDue to the unique architecture of Frontier compute nodes and the way that\nSlurm currently allocates GPUs and CPU cores to job steps, it is suggested that\nall 8 GPUs on a node are allocated to the job step to ensure that optimal\nbindings are possible.\n\nhello_jobstep output\n\nBefore jumping into the examples, it is helpful to understand the output from the hello_jobstep program:\n\n\nThe table presents information on the Frontier supercomputer, specifically related to MPI ranks and OpenMP threads running on different compute nodes and GPUs. The first row shows the headers for the various data points, including MPI rank ID, OpenMP thread ID, CPU hardware thread, compute node, GPU ID, runtime GPU ID, and physical Bus ID. The information in the table is useful for understanding the distribution of work on the Frontier supercomputer, as well as the performance of specific nodes and GPUs. It can also help in troubleshooting any performance or access issues related to GPUs. The table provides a comprehensive view of the utilization of MPI ranks and OpenMP threads, with corresponding data for the hardware and runtime IDs for GPUs. This information can be used by researchers and engineers for optimizing their codes and ensuring efficient use of the Frontier supercomputer's resources. \n\nID|Description\n---|---\nMPI|MPI rank ID\nOMP|OpenMP thread ID\nHWT|CPU hardware thread the MPI rank or OpenMP thread ran on\nNode|Compute node the MPI rank or OpenMP thread ran on\nGPU_ID|GPU ID the MPI rank or OpenMP thread had access to\nRT_GPU_ID|The runtime GPU ID\nBus_ID|The physical Bus ID associated with a GPU\n\n\n\nMapping 1 GPU per task\n\nIn the following examples, 1 GPU will be mapped to each MPI rank (and any OpenMP threads it might spawn). The relevant srun options for GPU mapping used in these examples are:\n\n\nThe table provides a detailed overview of the Slurm options available for the Frontier supercomputer. The first row serves as a header for the two columns, \"Slurm Option\" and \"Description\". The first option listed under \"Slurm Option\" is \"--gpus-per-task\" which allows the user to specify the number of GPUs needed for a particular job on each task. This option requires the user to explicitly state the task count using the \"-n\" flag. The second option listed is \"--gpu-bind=closest\" which binds each task to the GPU(s) that are closest to it. In this case, \"closest\" refers to the GPU connected to the L3 where the MPI rank (message passing interface rank) is mapped to. This feature allows for optimized use of resources and potential performance improvement in parallel computing tasks. \n\n|Slurm Option   |Description   |\n|---            |---           |\n|--gpus-per-task|Specify the number of GPUs required for the job on each task. This option requires an explicit task count, e.g. -n |\n|--gpu-bind=closest|Bind each task to the GPU(s) which are closest. Here, closest refers to the GPU connected to the L3 where the MPI rank is mapped to.|\n\n\n\nExample 1: 8 MPI ranks - each with 7 CPU cores and 1 GPU (single-node)\n\nThe most common use case for running on Frontier is to run with 8 MPI ranks per node, where each rank has access to 7 physical CPU cores and 1 GPU (recall it is 7 CPU cores here instead of 8 due to core specialization: see low-noise mode diagram <frontier-lownoise> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#low-noise mode diagram <frontier-lownoise>>). The MPI rank can use the 7 CPU cores to e.g., spawn OpenMP threads on (if OpenMP CPU threading is available in the application). Here is an example of such a job step on a single node:\n\n$ OMP_NUM_THREADS=7 srun -N1 -n8 -c7 --gpus-per-task=1 --gpu-bind=closest ./hello_jobstep | sort\nMPI 000 - OMP 000 - HWT 001 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 001 - HWT 002 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 002 - HWT 003 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 003 - HWT 004 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 004 - HWT 005 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 005 - HWT 006 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 006 - HWT 007 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 009 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 001 - HWT 010 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 002 - HWT 011 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 003 - HWT 012 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 004 - HWT 013 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 005 - HWT 014 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 006 - HWT 015 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 002 - OMP 000 - HWT 017 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 001 - HWT 018 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 002 - HWT 019 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 003 - HWT 020 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 004 - HWT 021 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 005 - HWT 022 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 006 - HWT 023 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 003 - OMP 000 - HWT 025 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 001 - HWT 026 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 002 - HWT 027 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 003 - HWT 028 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 004 - HWT 029 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 005 - HWT 030 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 006 - HWT 031 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 004 - OMP 000 - HWT 033 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 001 - HWT 034 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 002 - HWT 035 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 003 - HWT 036 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 004 - HWT 037 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 005 - HWT 038 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 006 - HWT 039 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 005 - OMP 000 - HWT 041 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 001 - HWT 042 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 002 - HWT 043 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 003 - HWT 044 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 004 - HWT 045 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 005 - HWT 046 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 006 - HWT 047 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 006 - OMP 000 - HWT 049 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 001 - HWT 050 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 002 - HWT 051 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 003 - HWT 052 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 004 - HWT 053 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 005 - HWT 054 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 006 - HWT 055 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 007 - OMP 000 - HWT 057 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 001 - HWT 058 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 002 - HWT 059 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 003 - HWT 060 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 004 - HWT 061 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 005 - HWT 062 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 006 - HWT 063 - Node frontier00256 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\n<string>:5: (INFO/1) Duplicate explicit target name: \"frontier node diagram\".\n\nAs has been pointed out previously in the Frontier documentation, notice that\nGPUs are NOT mapped to MPI ranks in sequential order (e.g., MPI rank 0 is\nmapped to physical CPU cores 1-7 and GPU 4, MPI rank 1 is mapped to physical\nCPU cores 9-15 and GPU 5), but this IS expected behavior. It is simply a\nconsequence of the Frontier node architectures as shown in the Frontier Node\nDiagram and\nsubsequent Note on NUMA domains <numa-note> <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#Note on NUMA domains <numa-note>>.\n\nExample 2: 1 MPI rank with 7 CPU cores and 1 GPU (single-node)\n\nWhen new users first attempt to run their application on Frontier, they often\nwant to test with 1 MPI rank that has access to 7 CPU cores and 1 GPU. Although\nthe job step used here is very similar to Example 1, the behavior is different:\n\n$ OMP_NUM_THREADS=7 srun -N1 -n1 -c7 --gpus-per-task=1 --gpu-bind=closest ./hello_jobstep | sort\nMPI 000 - OMP 000 - HWT 049 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 000 - OMP 001 - HWT 050 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 000 - OMP 002 - HWT 051 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 000 - OMP 003 - HWT 052 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 000 - OMP 004 - HWT 053 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 000 - OMP 005 - HWT 054 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 000 - OMP 006 - HWT 055 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\n\nNotice that our MPI rank did not get mapped to CPU cores 1-7 and GPU 4, but\ninstead to GPU 0 and CPU cores 49-55. The apparent reason for this can be found\nin the --gpu-bind section in the srun man page: GPU binding is\nignored if there is only one task.. Here, Slurm appears to give the first GPU\nit sees and maps it to the CPU cores that are closest. So although the mapping\ndoesn't occur as expected, the rank is still mapped to the correct GPU given\nthe CPU cores it ran on.\n\nExample 3: 16 MPI ranks - each with 7 CPU cores and 1 GPU (multi-node)\n\nThis example simply extends Example 1 to run on 2 nodes, which simply requires\nchanging the number of nodes to 2 (-N2) and the number of MPI ranks to 16\n(-n16).\n\n$ OMP_NUM_THREADS=7 srun -N2 -n16 -c7 --gpus-per-task=1 --gpu-bind=closest ./hello_jobstep | sort\nMPI 000 - OMP 000 - HWT 001 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 001 - HWT 002 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 002 - HWT 003 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 003 - HWT 004 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 004 - HWT 005 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 005 - HWT 006 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 000 - OMP 006 - HWT 007 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 009 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 001 - HWT 010 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 002 - HWT 011 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 003 - HWT 012 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 004 - HWT 013 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 005 - HWT 014 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 001 - OMP 006 - HWT 015 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 002 - OMP 000 - HWT 017 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 001 - HWT 018 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 002 - HWT 019 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 003 - HWT 020 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 004 - HWT 021 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 005 - HWT 022 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 002 - OMP 006 - HWT 023 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 003 - OMP 000 - HWT 025 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 001 - HWT 026 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 002 - HWT 027 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 003 - HWT 028 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 004 - HWT 029 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 005 - HWT 030 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 003 - OMP 006 - HWT 031 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 004 - OMP 000 - HWT 033 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 001 - HWT 034 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 002 - HWT 035 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 003 - HWT 036 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 004 - HWT 037 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 005 - HWT 038 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 004 - OMP 006 - HWT 039 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 005 - OMP 000 - HWT 041 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 001 - HWT 042 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 002 - HWT 043 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 003 - HWT 044 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 004 - HWT 045 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 005 - HWT 046 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 005 - OMP 006 - HWT 047 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 006 - OMP 000 - HWT 049 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 001 - HWT 050 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 002 - HWT 051 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 003 - HWT 052 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 004 - HWT 053 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 005 - HWT 054 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 006 - OMP 006 - HWT 055 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 007 - OMP 000 - HWT 057 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 001 - HWT 058 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 002 - HWT 059 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 003 - HWT 060 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 004 - HWT 061 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 005 - HWT 062 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 007 - OMP 006 - HWT 063 - Node frontier04086 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 008 - OMP 000 - HWT 001 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 008 - OMP 001 - HWT 002 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 008 - OMP 002 - HWT 003 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 008 - OMP 003 - HWT 004 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 008 - OMP 004 - HWT 005 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 008 - OMP 005 - HWT 006 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 008 - OMP 006 - HWT 007 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 009 - OMP 000 - HWT 009 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 009 - OMP 001 - HWT 010 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 009 - OMP 002 - HWT 011 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 009 - OMP 003 - HWT 012 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 009 - OMP 004 - HWT 013 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 009 - OMP 005 - HWT 014 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 009 - OMP 006 - HWT 015 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 010 - OMP 000 - HWT 017 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 010 - OMP 001 - HWT 018 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 010 - OMP 002 - HWT 019 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 010 - OMP 003 - HWT 020 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 010 - OMP 004 - HWT 021 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 010 - OMP 005 - HWT 022 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 010 - OMP 006 - HWT 023 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 011 - OMP 000 - HWT 025 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 011 - OMP 001 - HWT 026 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 011 - OMP 002 - HWT 027 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 011 - OMP 003 - HWT 028 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 011 - OMP 004 - HWT 029 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 011 - OMP 005 - HWT 030 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 011 - OMP 006 - HWT 031 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 012 - OMP 000 - HWT 033 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 012 - OMP 001 - HWT 034 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 012 - OMP 002 - HWT 035 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 012 - OMP 003 - HWT 036 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 012 - OMP 004 - HWT 037 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 012 - OMP 005 - HWT 038 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 012 - OMP 006 - HWT 039 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 013 - OMP 000 - HWT 041 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 013 - OMP 001 - HWT 042 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 013 - OMP 002 - HWT 043 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 013 - OMP 003 - HWT 044 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 013 - OMP 004 - HWT 045 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 013 - OMP 005 - HWT 046 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 013 - OMP 006 - HWT 047 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 014 - OMP 000 - HWT 049 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 001 - HWT 050 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 002 - HWT 051 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 003 - HWT 052 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 004 - HWT 053 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 005 - HWT 054 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 006 - HWT 055 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 015 - OMP 000 - HWT 057 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 001 - HWT 058 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 002 - HWT 059 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 003 - HWT 060 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 004 - HWT 061 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 005 - HWT 062 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 006 - HWT 063 - Node frontier04087 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nMapping multiple MPI ranks to a single GPU\n\nIn the following examples, 2 MPI ranks will be mapped to 1 GPU. For brevity,\nOMP_NUM_THREADS will be set to 1, so -c1 will be used unless\notherwise specified. A new srun option will also be introduced to\naccomplish the new mapping:\n\n\nThe table provides a comprehensive overview of the Slurm options specifically for the Frontier supercomputer. The first row serves as a header, indicating that the table pertains to Slurm options and their respective descriptions. The second row presents the \"ntasks-per-gpu\" option, which specifies the number of MPI ranks that can concurrently access a single GPU. In other words, this option allows for efficient utilization of the GPUs on Frontier by dividing them among multiple MPI ranks. This table, in a concise manner, highlights an important feature of Frontier that enables optimum performance for users in scientific and research fields. \n\n| Slurm Option  | Description                                                                 |\n|---------------|-----------------------------------------------------------------------------|\n| ntasks-per-gpu | Specifies the number of MPI ranks that will share access to a GPU on Frontier|\n\n\n\nOn AMD's MI250X, multi-process service (MPS) is not needed since multiple\nMPI ranks per GPU is supported natively.\n\nExample 4: 16 MPI ranks - where 2 ranks share a GPU (round-robin, single-node)\n\nThis example launches 16 MPI ranks (-n16), each with 1 physical CPU core\n(-c1) to launch 1 OpenMP thread (OMP_NUM_THREADS=1) on. The MPI ranks\nwill be assigned to GPUs in a round-robin fashion so that each of the 8 GPUs on\nthe node are shared by 2 MPI ranks.\n\n$ OMP_NUM_THREADS=1 srun -N1 -n16 -c1 --ntasks-per-gpu=2 --gpu-bind=closest ./hello_jobstep | sort\nMPI 000 - OMP 000 - HWT 001 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 009 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 002 - OMP 000 - HWT 017 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 003 - OMP 000 - HWT 025 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 004 - OMP 000 - HWT 033 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 005 - OMP 000 - HWT 041 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 006 - OMP 000 - HWT 049 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 007 - OMP 000 - HWT 057 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 008 - OMP 000 - HWT 002 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 009 - OMP 000 - HWT 010 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 010 - OMP 000 - HWT 018 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 011 - OMP 000 - HWT 026 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 012 - OMP 000 - HWT 034 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 013 - OMP 000 - HWT 042 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 014 - OMP 000 - HWT 050 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 015 - OMP 000 - HWT 058 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nThe output shows the round-robin (cyclic) distribution of MPI ranks to\nGPUs. In fact, it is a round-robin distribution of MPI ranks to L3 cache\nregions (the default distribution). The GPU mapping is a consequence of where\nthe MPI ranks are distributed; --gpu-bind=closest simply maps the GPU in an\nL3 cache region to the MPI ranks in the same L3 region.\n\nExample 5: 32 MPI ranks - where 2 ranks share a GPU (round-robin, multi-node)\n\nThis example is an extension of Example 4 to run on 2 nodes.\n\n$ OMP_NUM_THREADS=1 srun -N2 -n32 -c1 --ntasks-per-gpu=2 --gpu-bind=closest ./hello_jobstep | sort\nMPI 000 - OMP 000 - HWT 001 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 009 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 002 - OMP 000 - HWT 017 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 003 - OMP 000 - HWT 025 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 004 - OMP 000 - HWT 033 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 005 - OMP 000 - HWT 041 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 006 - OMP 000 - HWT 049 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 007 - OMP 000 - HWT 057 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 008 - OMP 000 - HWT 002 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 009 - OMP 000 - HWT 010 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 010 - OMP 000 - HWT 018 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 011 - OMP 000 - HWT 026 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 012 - OMP 000 - HWT 034 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 013 - OMP 000 - HWT 042 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 014 - OMP 000 - HWT 050 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 015 - OMP 000 - HWT 058 - Node frontier04975 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 016 - OMP 000 - HWT 001 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 017 - OMP 000 - HWT 009 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 018 - OMP 000 - HWT 017 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 019 - OMP 000 - HWT 025 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 020 - OMP 000 - HWT 033 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 021 - OMP 000 - HWT 041 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 022 - OMP 000 - HWT 049 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 023 - OMP 000 - HWT 057 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 024 - OMP 000 - HWT 002 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 025 - OMP 000 - HWT 010 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 026 - OMP 000 - HWT 018 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 027 - OMP 000 - HWT 026 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 028 - OMP 000 - HWT 034 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 029 - OMP 000 - HWT 042 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 030 - OMP 000 - HWT 050 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 031 - OMP 000 - HWT 058 - Node frontier04976 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nExample 6: 16 MPI ranks - where 2 ranks share a GPU (packed, single-node)\n\nThis example assumes the use of a core specialization of -S 0.  Because\nFrontier's default core specialization (-S 8) reserves the first core in\neach L3 region, the \"packed\" mode can be problematic because the 7 cores\navailable in each L3 region won't necessarily divide evenly. This can lead to\ntasks potentially spanning multiple L3 regions with its assigned cores, which\ncreates problems when Slurm tries to assign GPUs to a given task.\n\nThis example launches 16 MPI ranks (-n16), each with 4 physical CPU cores\n(-c4) to launch 1 OpenMP thread (OMP_NUM_THREADS=1) on. The MPI ranks\nwill be assigned to GPUs in a packed fashion so that each of the 8 GPUs on the\nnode are shared by 2 MPI ranks. Similar to Example 4, -ntasks-per-gpu=2\nwill be used, but a new srun flag will be used to change the default\nround-robin (cyclic) distribution of MPI ranks across NUMA domains:\n\n\nThe table provides a detailed breakdown of the Slurm Option \"-distribution\". It specifies the distribution of MPI ranks across the compute nodes, sockets, and cores of the Frontier supercomputer. The default values for this option are block, cyclic, and cyclic which correspond to the distribution of MPI ranks across blocks, sockets, and cores respectively. This information gives users a better understanding of how their tasks are being distributed and how to optimize their usage of the Frontier supercomputer. \n\n\n| Slurm Option | Description |\n| --- | --- |\n| --distribution=<value>[:<value>][:<value>]|Specifies the distribution of MPI ranks across compute nodes, sockets\n(L3 cache regions on Frontier), and cores, respectively. The default values are\nblock:cyclic:cyclic, which is where the cyclic assignment comes from in the previous\nexamples. |\n\n\n\nIn the job step for this example, --distribution=*:block is used, where\n* represents the default value of block for the distribution of MPI\nranks across compute nodes and the distribution of MPI ranks across L3 cache\nregions has been changed to block from its default value of cyclic.\n\nBecause the distribution across L3 cache regions has been changed to a\n\"packed\" (block) configuration, caution must be taken to ensure MPI ranks\nend up in the L3 cache regions where the GPUs they intend to be mapped to are\nlocated. To accomplish this, the number of physical CPU cores assigned to an\nMPI rank was increased - in this case to 4. Doing so ensures that only 2 MPI\nranks can fit into a single L3 cache region. If the value of -c was left at\n1, all 8 MPI ranks would be \"packed\" into the first L3 region, where the\n\"closest\" GPU would be GPU 4 - the only GPU in that L3 region.\n\n$ OMP_NUM_THREADS=1 srun -N1 -n16 -c4 --ntasks-per-gpu=2 --gpu-bind=closest --distribution=*:block ./hello_jobstep | sort\nMPI 000 - OMP 000 - HWT 000 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 004 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 002 - OMP 000 - HWT 008 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 003 - OMP 000 - HWT 012 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 004 - OMP 000 - HWT 016 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 005 - OMP 000 - HWT 020 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 006 - OMP 000 - HWT 024 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 007 - OMP 000 - HWT 028 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 008 - OMP 000 - HWT 032 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 009 - OMP 000 - HWT 036 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 010 - OMP 000 - HWT 040 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 011 - OMP 000 - HWT 044 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 012 - OMP 000 - HWT 048 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 013 - OMP 000 - HWT 052 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 000 - HWT 056 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 000 - HWT 060 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nThe overall effect of using --distribution=*:block and increasing the\nnumber of physical CPU cores available to each MPI rank is to place the first\ntwo MPI ranks in the first L3 cache region with GPU 4, the next two MPI ranks\nin the second L3 cache region with GPU 5, and so on.\n\nExample 7: 32 MPI ranks - where 2 ranks share a GPU (packed, multi-node)\n\nThis example assumes the use of a core specialization of -S 0.  Because\nFrontier's default core specialization (-S 8) reserves the first core in\neach L3 region, the \"packed\" mode can be problematic because the 7 cores\navailable in each L3 region won't necessarily divide evenly. This can lead to\ntasks potentially spanning multiple L3 regions with its assigned cores, which\ncreates problems when Slurm tries to assign GPUs to a given task.\n\nThis example is an extension of Example 6 to use 2 compute nodes. With the\nappropriate changes put in place in Example 6, it is a straightforward exercise\nto change to using 2 nodes (-N2) and 32 MPI ranks (-n32).\n\n$ OMP_NUM_THREADS=1 srun -N2 -n32 -c4 --ntasks-per-gpu=2 --gpu-bind=closest --distribution=*:block ./hello_jobstep | sort\nMPI 000 - OMP 000 - HWT 000 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 004 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 002 - OMP 000 - HWT 010 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 003 - OMP 000 - HWT 012 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 004 - OMP 000 - HWT 016 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 005 - OMP 000 - HWT 021 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 006 - OMP 000 - HWT 024 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 007 - OMP 000 - HWT 028 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 008 - OMP 000 - HWT 032 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 009 - OMP 000 - HWT 037 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 010 - OMP 000 - HWT 041 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 011 - OMP 000 - HWT 044 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 012 - OMP 000 - HWT 049 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 013 - OMP 000 - HWT 052 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 014 - OMP 000 - HWT 056 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 015 - OMP 000 - HWT 060 - Node frontier002 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 016 - OMP 000 - HWT 000 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 017 - OMP 000 - HWT 004 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 018 - OMP 000 - HWT 008 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 019 - OMP 000 - HWT 013 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 020 - OMP 000 - HWT 016 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 021 - OMP 000 - HWT 020 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 022 - OMP 000 - HWT 024 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 023 - OMP 000 - HWT 028 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 024 - OMP 000 - HWT 034 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 025 - OMP 000 - HWT 036 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 026 - OMP 000 - HWT 040 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 027 - OMP 000 - HWT 044 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 028 - OMP 000 - HWT 048 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 029 - OMP 000 - HWT 052 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 030 - OMP 000 - HWT 056 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 031 - OMP 000 - HWT 060 - Node frontier004 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nExample 8: 56 MPI ranks - where 7 ranks share a GPU (packed, single-node)\n\nAn alternative solution to Example 6 and 7's -S 8 issue is to use -c 1\ninstead.  There is no problem when running with 1 core per MPI rank (i.e., 7\nranks per GPU) because the task can’t span multiple L3s.\n\n$ OMP_NUM_THREADS=1 srun -N1 -n56 -c1 --ntasks-per-gpu=7 --gpu-bind=closest --distribution=*:block ./hello_jobstep | sort\nMPI 000 - OMP 000 - HWT 001 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 001 - OMP 000 - HWT 002 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 002 - OMP 000 - HWT 003 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 003 - OMP 000 - HWT 004 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 004 - OMP 000 - HWT 005 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 005 - OMP 000 - HWT 006 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 006 - OMP 000 - HWT 007 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nMPI 007 - OMP 000 - HWT 009 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 008 - OMP 000 - HWT 010 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 009 - OMP 000 - HWT 011 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 010 - OMP 000 - HWT 012 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 011 - OMP 000 - HWT 013 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 012 - OMP 000 - HWT 014 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 013 - OMP 000 - HWT 015 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nMPI 014 - OMP 000 - HWT 017 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 015 - OMP 000 - HWT 018 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 016 - OMP 000 - HWT 019 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 017 - OMP 000 - HWT 020 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 018 - OMP 000 - HWT 021 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 019 - OMP 000 - HWT 022 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 020 - OMP 000 - HWT 023 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nMPI 021 - OMP 000 - HWT 025 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 022 - OMP 000 - HWT 026 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 023 - OMP 000 - HWT 027 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 024 - OMP 000 - HWT 028 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 025 - OMP 000 - HWT 029 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 026 - OMP 000 - HWT 030 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 027 - OMP 000 - HWT 031 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nMPI 028 - OMP 000 - HWT 033 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 029 - OMP 000 - HWT 034 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 030 - OMP 000 - HWT 035 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 031 - OMP 000 - HWT 036 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 032 - OMP 000 - HWT 037 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 033 - OMP 000 - HWT 038 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 034 - OMP 000 - HWT 039 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nMPI 035 - OMP 000 - HWT 041 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 036 - OMP 000 - HWT 042 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 037 - OMP 000 - HWT 043 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 038 - OMP 000 - HWT 044 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 039 - OMP 000 - HWT 045 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 040 - OMP 000 - HWT 046 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 041 - OMP 000 - HWT 047 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\nMPI 042 - OMP 000 - HWT 049 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 043 - OMP 000 - HWT 050 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 044 - OMP 000 - HWT 051 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 045 - OMP 000 - HWT 052 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 046 - OMP 000 - HWT 053 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 047 - OMP 000 - HWT 054 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 048 - OMP 000 - HWT 055 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nMPI 049 - OMP 000 - HWT 057 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 050 - OMP 000 - HWT 058 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 051 - OMP 000 - HWT 059 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 052 - OMP 000 - HWT 060 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 053 - OMP 000 - HWT 061 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 054 - OMP 000 - HWT 062 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nMPI 055 - OMP 000 - HWT 063 - Node frontier08413 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\n\nMultiple Independent Job Steps\n\nExample 9: 8 independent and simultaneous job steps running on a single node\n\nThis example shows how to run multiple independent, simultaneous job steps on a single compute node. Specifically, it shows how to run 8 independent hello_jobstep programs running on their own CPU core and GPU.\n\nSubmission script:\n\n#!/bin/bash\n\n#SBATCH -A stf016_frontier\n#SBATCH -N 1\n#SBATCH -t 5\n\nfor idx in {1..8};\n\n    do\n        date\n\n        OMP_NUM_THREADS=1 srun -u --gpus-per-task=1 --gpu-bind=closest -N1 -n1 -c1 ./hello_jobstep &\n\n        sleep 1\n    done\n\nwait\n\nOutput:\n\nFri 02 Jun 2023 03:33:45 PM EDT\nMPI 000 - OMP 000 - HWT 049 - Node frontier04724 - RT_GPU_ID 0 - GPU_ID 0 - Bus_ID c1\nFri 02 Jun 2023 03:33:46 PM EDT\nMPI 000 - OMP 000 - HWT 057 - Node frontier04724 - RT_GPU_ID 0 - GPU_ID 1 - Bus_ID c6\nFri 02 Jun 2023 03:33:47 PM EDT\nMPI 000 - OMP 000 - HWT 017 - Node frontier04724 - RT_GPU_ID 0 - GPU_ID 2 - Bus_ID c9\nFri 02 Jun 2023 03:33:48 PM EDT\nMPI 000 - OMP 000 - HWT 025 - Node frontier04724 - RT_GPU_ID 0 - GPU_ID 3 - Bus_ID ce\nFri 02 Jun 2023 03:33:49 PM EDT\nMPI 000 - OMP 000 - HWT 001 - Node frontier04724 - RT_GPU_ID 0 - GPU_ID 4 - Bus_ID d1\nFri 02 Jun 2023 03:33:50 PM EDT\nMPI 000 - OMP 000 - HWT 009 - Node frontier04724 - RT_GPU_ID 0 - GPU_ID 5 - Bus_ID d6\nFri 02 Jun 2023 03:33:51 PM EDT\nMPI 000 - OMP 000 - HWT 033 - Node frontier04724 - RT_GPU_ID 0 - GPU_ID 6 - Bus_ID d9\nFri 02 Jun 2023 03:33:52 PM EDT\nMPI 000 - OMP 000 - HWT 041 - Node frontier04724 - RT_GPU_ID 0 - GPU_ID 7 - Bus_ID de\n\nThe output shows that each independent process ran on its own CPU core and GPU\non the same single node. To show that the ranks ran simultaneously, date\nwas called before each job step and a 20 second sleep was added to the end of\nthe hello_jobstep program. So the output also shows that the first job step\nwas submitted at :45 and the subsequent job steps were all submitted\nbetween :46 and :52. But because each hello_jobstep sleeps for 20\nseconds, the subsequent job steps must have all been running while the first\njob step was still sleeping (and holding up its resources). And the same\nargument can be made for the other job steps.\n\nThe wait command is needed so the job script (and allocation) do not immediately end after launching the job steps in the background.\n\nThe sleep 1 is needed to give Slurm sufficient time to launch each job step.\n\nMultiple GPUs per MPI rank\n\nAs mentioned previously, all GPUs are accessible by all MPI ranks by default,\nso it is possible to programatically map any combination of GPUs to MPI\nranks. It should be noted however that Cray MPICH does not support GPU-aware\nMPI for multiple GPUs per rank, so this binding is not suggested.\n\nNIC Mapping\n\n<string>:5: (INFO/1) Duplicate explicit target name: \"frontier node diagram\".\n\nAs shown in the Frontier Node Diagram, each of the 4\nNICs on a compute node is connected to a specific MI250X, and each MI250X is\n(in turn) connected to a specific NUMA domain - so each NUMA domain is\ncorrelated to a specific NIC. By default, processes (e.g., MPI ranks) that are\nmapped to CPU cores in a specific NUMA domain are mapped (by CrayMPICH) to the\nNIC that is correlated to that NUMA domain.\n\nIf a user attempts to map a process to a set of cores that span more than 1\nNUMA domain using the default NIC mapping, they will see an error such as\nMPICH ERROR: Unable to use a NIC_POLICY of 'NUMA'. Rank 0 is not confined\nto a single NUMA node.. This is expected behavior for the default NIC\npolicy.\n\nThe default behavior can be changed by using the MPICH_OFI_NIC_POLICY\nenvironment variable (see man mpi for available options).\n\nTips for Launching at Scale\n\nSBCAST your executable and libraries\n\nSlurm contains a utility called sbcast. This program takes a file and broadcasts it to each node's node-local storage (ie, /tmp, NVMe).\nThis is useful for sharing large input files, binaries and shared libraries, while reducing the overhead on shared file systems and overhead at startup.\nThis is highly recommended at scale if you have multiple shared libraries on Lustre/NFS file systems.\n\nSBCASTing a single file\n\nHere is a simple example of a file sbcast from a user's scratch space on Lustre to each node's NVMe drive:\n\n#!/bin/bash\n#SBATCH -A <projid>\n#SBATCH -J sbcast_to_nvme\n#SBATCH -o %x-%j.out\n#SBATCH -t 00:05:00\n#SBATCH -p batch\n#SBATCH -N 2\n#SBATCH -C nvme\n\ndate\n\n# Change directory to user scratch space (Orion)\ncd /lustre/orion/<projid>/scratch/<userid>\n\necho \"This is an example file\" > test.txt\necho\necho \"*****ORIGINAL FILE*****\"\ncat test.txt\necho \"***********************\"\n\n# SBCAST file from Orion to NVMe -- NOTE: ``-C nvme`` is required to use the NVMe drive\nsbcast -pf test.txt /mnt/bb/$USER/test.txt\nif [ ! \"$?\" == \"0\" ]; then\n    # CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,\n    # your application may pick up partially complete shared library files, which would give you confusing errors.\n    echo \"SBCAST failed!\"\n    exit 1\nfi\n\necho\necho \"*****DISPLAYING FILES ON EACH NODE IN THE ALLOCATION*****\"\n# Check to see if file exists\nsrun -N ${SLURM_NNODES} -n ${SLURM_NNODES} --ntasks-per-node=1 bash -c \"echo \\\"\\$(hostname): \\$(ls -lh /mnt/bb/$USER/test.txt)\\\"\"\necho \"*********************************************************\"\n\necho\n# Showing the file on the current node -- this will be the same on all other nodes in the allocation\necho \"*****SBCAST FILE ON CURRENT NODE******\"\ncat /mnt/bb/$USER/test.txt\necho \"**************************************\"\n\nand here is the output from that script:\n\nFri 03 Mar 2023 03:43:30 PM EST\n\n*****ORIGINAL FILE*****\nThis is an example file\n***********************\n\n*****DISPLAYING FILES ON EACH NODE IN THE ALLOCATION*****\nfrontier00001: -rw-r--r-- 1 hagertnl hagertnl 24 Mar  3 15:43 /mnt/bb/hagertnl/test.txt\nfrontier00002: -rw-r--r-- 1 hagertnl hagertnl 24 Mar  3 15:43 /mnt/bb/hagertnl/test.txt\n*********************************************************\n\n*****SBCAST FILE ON CURRENT NODE******\nThis is an example file\n**************************************\n\nSBCASTing a binary with libraries stored on shared file systems\n\nsbcast also handles binaries and their libraries:\n\n#!/bin/bash\n#SBATCH -A <projid>\n#SBATCH -J sbcast_binary_to_nvme\n#SBATCH -o %x-%j.out\n#SBATCH -t 00:05:00\n#SBATCH -p batch\n#SBATCH -N 2\n#SBATCH -C nvme\n\ndate\n\n# Change directory to user scratch space (Orion)\ncd /lustre/orion/<projid>/scratch/<userid>\n\n# For this example, I use a HIP-enabled LAMMPS binary, with dependencies to MPI, HIP, and HWLOC\nexe=\"lmp\"\n\necho \"*****ldd ./${exe}*****\"\nldd ./${exe}\necho \"*************************\"\n\n# SBCAST executable from Orion to NVMe -- NOTE: ``-C nvme`` is needed in SBATCH headers to use the NVMe drive\n# NOTE: dlopen'd files will NOT be picked up by sbcast\n# SBCAST automatically excludes several directories: /lib,/usr/lib,/lib64,/usr/lib64,/opt\n#   - These directories are node-local and are very fast to read from, so SBCASTing them isn't critical\n#   - see ``$ scontrol show config | grep BcastExclude`` for current list\n#   - OLCF-provided libraries in ``/sw`` are not on the exclusion list. ``/sw`` is an NFS shared file system\n#   - To override, add ``--exclude=NONE`` to arguments\nsbcast --send-libs -pf ${exe} /mnt/bb/$USER/${exe}\nif [ ! \"$?\" == \"0\" ]; then\n    # CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,\n    # your application may pick up partially complete shared library files, which would give you confusing errors.\n    echo \"SBCAST failed!\"\n    exit 1\nfi\n\n# Check to see if file exists\necho \"*****ls -lh /mnt/bb/$USER*****\"\nls -lh /mnt/bb/$USER/\necho \"*****ls -lh /mnt/bb/$USER/${exe}_libs*****\"\nls -lh /mnt/bb/$USER/${exe}_libs\n\n# SBCAST sends all libraries detected by `ld` (minus any excluded), and stores them in the same directory in each node's node-local storage\n# Any libraries opened by `dlopen` are NOT sent, since they are not known by the linker at run-time.\n\n# At minimum: prepend the node-local path to LD_LIBRARY_PATH to pick up the SBCAST libraries\n# It is also recommended that you **remove** any paths that you don't need, like those that contain the libraries that you just SBCAST'd\n# Failure to remove may result in unnecessary calls to stat shared file systems\nexport LD_LIBRARY_PATH=\"/mnt/bb/$USER/${exe}_libs:${LD_LIBRARY_PATH}\"\n\n# If you SBCAST **all** your libraries (ie, `--exclude=NONE`), you may use the following line:\n#export LD_LIBRARY_PATH=\"/mnt/bb/$USER/${exe}_libs:$(pkg-config --variable=libdir libfabric)\"\n# Use with caution -- certain libraries may use ``dlopen`` at runtime, and that is NOT covered by sbcast\n# If you use this option, we recommend you contact OLCF Help Desk for the latest list of additional steps required\n\n# You may notice that some libraries are still linked from /sw/frontier, even after SBCASTing.\n# This is because the Spack-build modules use RPATH to find their dependencies.\necho \"*****ldd /mnt/bb/$USER/${exe}*****\"\nldd /mnt/bb/$USER/${exe}\necho \"*************************************\"\n\nand here is the output from that script:\n\nTue 28 Mar 2023 05:01:41 PM EDT\n*****ldd ./lmp*****\n    linux-vdso.so.1 (0x00007fffeda02000)\n    libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007fffed5bb000)\n    libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fffed398000)\n    libm.so.6 => /lib64/libm.so.6 (0x00007fffed04d000)\n    librt.so.1 => /lib64/librt.so.1 (0x00007fffece44000)\n    libamdhip64.so.4 => /opt/rocm-4.5.2/lib/libamdhip64.so.4 (0x00007fffec052000)\n    libmpi_cray.so.12 => /opt/cray/pe/lib64/libmpi_cray.so.12 (0x00007fffe96cc000)\n    libmpi_gtl_hsa.so.0 => /opt/cray/pe/lib64/libmpi_gtl_hsa.so.0 (0x00007fffe9469000)\n    libhsa-runtime64.so.1 => /opt/rocm-5.3.0/lib/libhsa-runtime64.so.1 (0x00007fffe8fdc000)\n    libhwloc.so.15 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/hwloc-2.5.0-4p6jkgf5ez6wr27pytkzyptppzpugu3e/lib/libhwloc.so.15 (0x00007fffe8d82000)\n    libdl.so.2 => /lib64/libdl.so.2 (0x00007fffe8b7e000)\n    libhipfft.so => /opt/rocm-5.3.0/lib/libhipfft.so (0x00007fffed9d2000)\n    libstdc++.so.6 => /usr/lib64/libstdc++.so.6 (0x00007fffe875b000)\n    libc.so.6 => /lib64/libc.so.6 (0x00007fffe8366000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007fffed7da000)\n    libamd_comgr.so.2 => /opt/rocm-5.3.0/lib/libamd_comgr.so.2 (0x00007fffe06e0000)\n    libnuma.so.1 => /usr/lib64/libnuma.so.1 (0x00007fffe04d4000)\n    libfabric.so.1 => /opt/cray/libfabric/1.15.2.0/lib64/libfabric.so.1 (0x00007fffe01e2000)\n    libatomic.so.1 => /usr/lib64/libatomic.so.1 (0x00007fffdffd9000)\n    libpmi.so.0 => /opt/cray/pe/lib64/libpmi.so.0 (0x00007fffdfdd7000)\n    libpmi2.so.0 => /opt/cray/pe/lib64/libpmi2.so.0 (0x00007fffdfb9e000)\n    libquadmath.so.0 => /usr/lib64/libquadmath.so.0 (0x00007fffdf959000)\n    libmodules.so.1 => /opt/cray/pe/lib64/cce/libmodules.so.1 (0x00007fffed9b5000)\n    libfi.so.1 => /opt/cray/pe/lib64/cce/libfi.so.1 (0x00007fffdf3b4000)\n    libcraymath.so.1 => /opt/cray/pe/lib64/cce/libcraymath.so.1 (0x00007fffed8ce000)\n    libf.so.1 => /opt/cray/pe/lib64/cce/libf.so.1 (0x00007fffed83b000)\n    libu.so.1 => /opt/cray/pe/lib64/cce/libu.so.1 (0x00007fffdf2ab000)\n    libcsup.so.1 => /opt/cray/pe/lib64/cce/libcsup.so.1 (0x00007fffed832000)\n    libamdhip64.so.5 => /opt/rocm-5.3.0/lib/libamdhip64.so.5 (0x00007fffdda8a000)\n    libelf.so.1 => /usr/lib64/libelf.so.1 (0x00007fffdd871000)\n    libdrm.so.2 => /usr/lib64/libdrm.so.2 (0x00007fffdd65d000)\n    libdrm_amdgpu.so.1 => /usr/lib64/libdrm_amdgpu.so.1 (0x00007fffdd453000)\n    libpciaccess.so.0 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libpciaccess-0.16-6evcvl74myxfxdrddmnsvbc7sgfx6s5j/lib/libpciaccess.so.0 (0x00007fffdd24a000)\n    libxml2.so.2 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libxml2-2.9.12-h7fe2yauotu3xit7rsaixaowd3noknur/lib/libxml2.so.2 (0x00007fffdcee6000)\n    librocfft.so.0 => /opt/rocm-5.3.0/lib/librocfft.so.0 (0x00007fffdca1a000)\n    libz.so.1 => /lib64/libz.so.1 (0x00007fffdc803000)\n    libtinfo.so.6 => /lib64/libtinfo.so.6 (0x00007fffdc5d5000)\n    libcxi.so.1 => /usr/lib64/libcxi.so.1 (0x00007fffdc3b0000)\n    libcurl.so.4 => /usr/lib64/libcurl.so.4 (0x00007fffdc311000)\n    libjson-c.so.3 => /usr/lib64/libjson-c.so.3 (0x00007fffdc101000)\n    libpals.so.0 => /opt/cray/pe/lib64/libpals.so.0 (0x00007fffdbefc000)\n    libgfortran.so.5 => /opt/cray/pe/gcc-libs/libgfortran.so.5 (0x00007fffdba50000)\n    liblzma.so.5 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/xz-5.2.5-zwra4gpckelt5umczowf3jtmeiz3yd7u/lib/liblzma.so.5 (0x00007fffdb82a000)\n    libiconv.so.2 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libiconv-1.16-coz5dzhtoeq5unhjirayfn2xftnxk43l/lib/libiconv.so.2 (0x00007fffdb52e000)\n    librocfft-device-0.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-0.so.0 (0x00007fffa11a0000)\n    librocfft-device-1.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-1.so.0 (0x00007fff6491b000)\n    librocfft-device-2.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-2.so.0 (0x00007fff2a828000)\n    librocfft-device-3.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-3.so.0 (0x00007ffef7eee000)\n    libnghttp2.so.14 => /usr/lib64/libnghttp2.so.14 (0x00007ffef7cc6000)\n    libidn2.so.0 => /usr/lib64/libidn2.so.0 (0x00007ffef7aa9000)\n    libssh.so.4 => /usr/lib64/libssh.so.4 (0x00007ffef783b000)\n    libpsl.so.5 => /usr/lib64/libpsl.so.5 (0x00007ffef7629000)\n    libssl.so.1.1 => /usr/lib64/libssl.so.1.1 (0x00007ffef758a000)\n    libcrypto.so.1.1 => /usr/lib64/libcrypto.so.1.1 (0x00007ffef724a000)\n    libgssapi_krb5.so.2 => /usr/lib64/libgssapi_krb5.so.2 (0x00007ffef6ff8000)\n    libldap_r-2.4.so.2 => /usr/lib64/libldap_r-2.4.so.2 (0x00007ffef6da4000)\n    liblber-2.4.so.2 => /usr/lib64/liblber-2.4.so.2 (0x00007ffef6b95000)\n    libzstd.so.1 => /usr/lib64/libzstd.so.1 (0x00007ffef6865000)\n    libbrotlidec.so.1 => /usr/lib64/libbrotlidec.so.1 (0x00007ffef6659000)\n    libunistring.so.2 => /usr/lib64/libunistring.so.2 (0x00007ffef62d6000)\n    libjitterentropy.so.3 => /usr/lib64/libjitterentropy.so.3 (0x00007ffef60cf000)\n    libkrb5.so.3 => /usr/lib64/libkrb5.so.3 (0x00007ffef5df6000)\n    libk5crypto.so.3 => /usr/lib64/libk5crypto.so.3 (0x00007ffef5bde000)\n    libcom_err.so.2 => /lib64/libcom_err.so.2 (0x00007ffef59da000)\n    libkrb5support.so.0 => /usr/lib64/libkrb5support.so.0 (0x00007ffef57cb000)\n    libresolv.so.2 => /lib64/libresolv.so.2 (0x00007ffef55b3000)\n    libsasl2.so.3 => /usr/lib64/libsasl2.so.3 (0x00007ffef5396000)\n    libbrotlicommon.so.1 => /usr/lib64/libbrotlicommon.so.1 (0x00007ffef5175000)\n    libkeyutils.so.1 => /usr/lib64/libkeyutils.so.1 (0x00007ffef4f70000)\n    libselinux.so.1 => /lib64/libselinux.so.1 (0x00007ffef4d47000)\n    libpcre.so.1 => /usr/lib64/libpcre.so.1 (0x00007ffef4abe000)\n*************************\n*****ls -lh /mnt/bb/hagertnl*****\ntotal 236M\n-rwxr-xr-x 1 hagertnl hagertnl 236M Mar 28 17:01 lmp\ndrwx------ 2 hagertnl hagertnl  114 Mar 28 17:01 lmp_libs\n*****ls -lh /mnt/bb/hagertnl/lmp_libs*****\ntotal 9.2M\n-rwxr-xr-x 1 hagertnl hagertnl 1.6M Oct  6  2021 libhwloc.so.15\n-rwxr-xr-x 1 hagertnl hagertnl 1.6M Oct  6  2021 libiconv.so.2\n-rwxr-xr-x 1 hagertnl hagertnl 783K Oct  6  2021 liblzma.so.5\n-rwxr-xr-x 1 hagertnl hagertnl 149K Oct  6  2021 libpciaccess.so.0\n-rwxr-xr-x 1 hagertnl hagertnl 5.2M Oct  6  2021 libxml2.so.2\n*****ldd /mnt/bb/hagertnl/lmp*****\n    linux-vdso.so.1 (0x00007fffeda02000)\n    libgcc_s.so.1 => /lib64/libgcc_s.so.1 (0x00007fffed5bb000)\n    libpthread.so.0 => /lib64/libpthread.so.0 (0x00007fffed398000)\n    libm.so.6 => /lib64/libm.so.6 (0x00007fffed04d000)\n    librt.so.1 => /lib64/librt.so.1 (0x00007fffece44000)\n    libamdhip64.so.4 => /opt/rocm-4.5.2/lib/libamdhip64.so.4 (0x00007fffec052000)\n    libmpi_cray.so.12 => /opt/cray/pe/lib64/libmpi_cray.so.12 (0x00007fffe96cc000)\n    libmpi_gtl_hsa.so.0 => /opt/cray/pe/lib64/libmpi_gtl_hsa.so.0 (0x00007fffe9469000)\n    libhsa-runtime64.so.1 => /opt/rocm-5.3.0/lib/libhsa-runtime64.so.1 (0x00007fffe8fdc000)\n    libhwloc.so.15 => /mnt/bb/hagertnl/lmp_libs/libhwloc.so.15 (0x00007fffe8d82000)\n    libdl.so.2 => /lib64/libdl.so.2 (0x00007fffe8b7e000)\n    libhipfft.so => /opt/rocm-5.3.0/lib/libhipfft.so (0x00007fffed9d2000)\n    libstdc++.so.6 => /usr/lib64/libstdc++.so.6 (0x00007fffe875b000)\n    libc.so.6 => /lib64/libc.so.6 (0x00007fffe8366000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007fffed7da000)\n    libamd_comgr.so.2 => /opt/rocm-5.3.0/lib/libamd_comgr.so.2 (0x00007fffe06e0000)\n    libnuma.so.1 => /usr/lib64/libnuma.so.1 (0x00007fffe04d4000)\n    libfabric.so.1 => /opt/cray/libfabric/1.15.2.0/lib64/libfabric.so.1 (0x00007fffe01e2000)\n    libatomic.so.1 => /usr/lib64/libatomic.so.1 (0x00007fffdffd9000)\n    libpmi.so.0 => /opt/cray/pe/lib64/libpmi.so.0 (0x00007fffdfdd7000)\n    libpmi2.so.0 => /opt/cray/pe/lib64/libpmi2.so.0 (0x00007fffdfb9e000)\n    libquadmath.so.0 => /usr/lib64/libquadmath.so.0 (0x00007fffdf959000)\n    libmodules.so.1 => /opt/cray/pe/lib64/cce/libmodules.so.1 (0x00007fffed9b5000)\n    libfi.so.1 => /opt/cray/pe/lib64/cce/libfi.so.1 (0x00007fffdf3b4000)\n    libcraymath.so.1 => /opt/cray/pe/lib64/cce/libcraymath.so.1 (0x00007fffed8ce000)\n    libf.so.1 => /opt/cray/pe/lib64/cce/libf.so.1 (0x00007fffed83b000)\n    libu.so.1 => /opt/cray/pe/lib64/cce/libu.so.1 (0x00007fffdf2ab000)\n    libcsup.so.1 => /opt/cray/pe/lib64/cce/libcsup.so.1 (0x00007fffed832000)\n    libamdhip64.so.5 => /opt/rocm-5.3.0/lib/libamdhip64.so.5 (0x00007fffdda8a000)\n    libelf.so.1 => /usr/lib64/libelf.so.1 (0x00007fffdd871000)\n    libdrm.so.2 => /usr/lib64/libdrm.so.2 (0x00007fffdd65d000)\n    libdrm_amdgpu.so.1 => /usr/lib64/libdrm_amdgpu.so.1 (0x00007fffdd453000)\n    libpciaccess.so.0 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libpciaccess-0.16-6evcvl74myxfxdrddmnsvbc7sgfx6s5j/lib/libpciaccess.so.0 (0x00007fffdd24a000)\n    libxml2.so.2 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libxml2-2.9.12-h7fe2yauotu3xit7rsaixaowd3noknur/lib/libxml2.so.2 (0x00007fffdcee6000)\n    librocfft.so.0 => /opt/rocm-5.3.0/lib/librocfft.so.0 (0x00007fffdca1a000)\n    libz.so.1 => /lib64/libz.so.1 (0x00007fffdc803000)\n    libtinfo.so.6 => /lib64/libtinfo.so.6 (0x00007fffdc5d5000)\n    libcxi.so.1 => /usr/lib64/libcxi.so.1 (0x00007fffdc3b0000)\n    libcurl.so.4 => /usr/lib64/libcurl.so.4 (0x00007fffdc311000)\n    libjson-c.so.3 => /usr/lib64/libjson-c.so.3 (0x00007fffdc101000)\n    libpals.so.0 => /opt/cray/pe/lib64/libpals.so.0 (0x00007fffdbefc000)\n    libgfortran.so.5 => /opt/cray/pe/gcc-libs/libgfortran.so.5 (0x00007fffdba50000)\n    liblzma.so.5 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/xz-5.2.5-zwra4gpckelt5umczowf3jtmeiz3yd7u/lib/liblzma.so.5 (0x00007fffdb82a000)\n    libiconv.so.2 => /sw/frontier/spack-envs/base/opt/linux-sles15-x86_64/gcc-7.5.0/libiconv-1.16-coz5dzhtoeq5unhjirayfn2xftnxk43l/lib/libiconv.so.2 (0x00007fffdb52e000)\n    librocfft-device-0.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-0.so.0 (0x00007fffa11a0000)\n    librocfft-device-1.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-1.so.0 (0x00007fff6491b000)\n    librocfft-device-2.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-2.so.0 (0x00007fff2a828000)\n    librocfft-device-3.so.0 => /opt/rocm-5.3.0/lib/librocfft-device-3.so.0 (0x00007ffef7eee000)\n    libnghttp2.so.14 => /usr/lib64/libnghttp2.so.14 (0x00007ffef7cc6000)\n    libidn2.so.0 => /usr/lib64/libidn2.so.0 (0x00007ffef7aa9000)\n    libssh.so.4 => /usr/lib64/libssh.so.4 (0x00007ffef783b000)\n    libpsl.so.5 => /usr/lib64/libpsl.so.5 (0x00007ffef7629000)\n    libssl.so.1.1 => /usr/lib64/libssl.so.1.1 (0x00007ffef758a000)\n    libcrypto.so.1.1 => /usr/lib64/libcrypto.so.1.1 (0x00007ffef724a000)\n    libgssapi_krb5.so.2 => /usr/lib64/libgssapi_krb5.so.2 (0x00007ffef6ff8000)\n    libldap_r-2.4.so.2 => /usr/lib64/libldap_r-2.4.so.2 (0x00007ffef6da4000)\n    liblber-2.4.so.2 => /usr/lib64/liblber-2.4.so.2 (0x00007ffef6b95000)\n    libzstd.so.1 => /usr/lib64/libzstd.so.1 (0x00007ffef6865000)\n    libbrotlidec.so.1 => /usr/lib64/libbrotlidec.so.1 (0x00007ffef6659000)\n    libunistring.so.2 => /usr/lib64/libunistring.so.2 (0x00007ffef62d6000)\n    libjitterentropy.so.3 => /usr/lib64/libjitterentropy.so.3 (0x00007ffef60cf000)\n    libkrb5.so.3 => /usr/lib64/libkrb5.so.3 (0x00007ffef5df6000)\n    libk5crypto.so.3 => /usr/lib64/libk5crypto.so.3 (0x00007ffef5bde000)\n    libcom_err.so.2 => /lib64/libcom_err.so.2 (0x00007ffef59da000)\n    libkrb5support.so.0 => /usr/lib64/libkrb5support.so.0 (0x00007ffef57cb000)\n    libresolv.so.2 => /lib64/libresolv.so.2 (0x00007ffef55b3000)\n    libsasl2.so.3 => /usr/lib64/libsasl2.so.3 (0x00007ffef5396000)\n    libbrotlicommon.so.1 => /usr/lib64/libbrotlicommon.so.1 (0x00007ffef5175000)\n    libkeyutils.so.1 => /usr/lib64/libkeyutils.so.1 (0x00007ffef4f70000)\n    libselinux.so.1 => /lib64/libselinux.so.1 (0x00007ffef4d47000)\n    libpcre.so.1 => /usr/lib64/libpcre.so.1 (0x00007ffef4abe000)\n*************************************\n\nNotice that the libraries are sent to the ${exe}_libs directory in the same prefix as the executable.\nOnce libraries are here, you cannot tell where they came from, so consider doing an ldd of your executable prior to sbcast.\n\nAlternative: SBCASTing a binary with all libraries\n\nAs mentioned above, you can alternatively use --exclude=NONE on sbcast to send all libraries along with the binary.\nUsing --exclude=NONE requires more effort but substantially simplifies the linker configuration at run-time.\nA job script for the previous example, modified for sending all libraries is shown below.\n\n#!/bin/bash\n#SBATCH -A <projid>\n#SBATCH -J sbcast_binary_to_nvme\n#SBATCH -o %x-%j.out\n#SBATCH -t 00:05:00\n#SBATCH -p batch\n#SBATCH -N 2\n#SBATCH -C nvme\n\ndate\n\n# Change directory to user scratch space (Orion)\ncd /lustre/orion/<projid>/scratch/<userid>\n\n# For this example, I use a HIP-enabled LAMMPS binary, with dependencies to MPI, HIP, and HWLOC\nexe=\"lmp\"\n\necho \"*****ldd ./${exe}*****\"\nldd ./${exe}\necho \"*************************\"\n\n# SBCAST executable from Orion to NVMe -- NOTE: ``-C nvme`` is needed in SBATCH headers to use the NVMe drive\n# NOTE: dlopen'd files will NOT be picked up by sbcast\nsbcast --send-libs --exclude=NONE -pf ${exe} /mnt/bb/$USER/${exe}\nif [ ! \"$?\" == \"0\" ]; then\n    # CHECK EXIT CODE. When SBCAST fails, it may leave partial files on the compute nodes, and if you continue to launch srun,\n    # your application may pick up partially complete shared library files, which would give you confusing errors.\n    echo \"SBCAST failed!\"\n    exit 1\nfi\n\n# Check to see if file exists\necho \"*****ls -lh /mnt/bb/$USER*****\"\nls -lh /mnt/bb/$USER/\necho \"*****ls -lh /mnt/bb/$USER/${exe}_libs*****\"\nls -lh /mnt/bb/$USER/${exe}_libs\n\n# SBCAST sends all libraries detected by `ld` (minus any excluded), and stores them in the same directory in each node's node-local storage\n# Any libraries opened by `dlopen` are NOT sent, since they are not known by the linker at run-time.\n\n# All required libraries now reside in /mnt/bb/$USER/${exe}_libs\nexport LD_LIBRARY_PATH=\"/mnt/bb/$USER/${exe}_libs\"\n\n# libfabric dlopen's several libraries:\nexport LD_LIBRARY_PATH=\"${LD_LIBRARY_PATH}:$(pkg-config --variable=libdir libfabric)\"\n\n# cray-mpich dlopen's libhsa-runtime64.so and libamdhip64.so (non-versioned), so symlink on each node:\nsrun -N ${SLURM_NNODES} -n ${SLURM_NNODES} --ntasks-per-node=1 --label -D /mnt/bb/$USER/${exe}_libs \\\n    bash -c \"if [ -f libhsa-runtime64.so.1 ]; then ln -s libhsa-runtime64.so.1 libhsa-runtime64.so; fi;\n    if [ -f libamdhip64.so.5 ]; then ln -s libamdhip64.so.5 libamdhip64.so; fi\"\n\n# RocBLAS has over 1,000 device libraries that may be `dlopen`'d by RocBLAS during a run.\n# It's impractical to SBCAST all of these, so you can set this path instead, if you use RocBLAS:\n#export ROCBLAS_TENSILE_LIBPATH=${ROCM_PATH}/lib/rocblas/library\n\n# You may notice that some libraries are still linked from /sw/crusher, even after SBCASTing.\n# This is because the Spack-build modules use RPATH to find their dependencies. This behavior cannot be changed.\necho \"*****ldd /mnt/bb/$USER/${exe}*****\"\nldd /mnt/bb/$USER/${exe}\necho \"*************************************\"\n\nSome libraries still resolved to paths outside of /mnt/bb, and the reason for that is that the executable may have several paths in RPATH.\n\n\n\nSoftware\n\nVisualization and analysis tasks should be done on the Andes cluster. There are a few tools provided for various visualization tasks, as described in the andes-viz-tools <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-viz-tools> section of the andes-user-guide <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#andes-user-guide>.\n\nFor a full list of software availability and latest news at the OLCF, please reference the Software <https://docs.olcf.ornl.gov/software/software-news.html> section in OLCF's User Documentation.\n\nDebugging\n\nLinaro DDT\n\nLinaro DDT is an advanced debugging tool used for scalar, multi-threaded,\nand large-scale parallel applications. In addition to traditional\ndebugging features (setting breakpoints, stepping through code,\nexamining variables), DDT also supports attaching to already-running\nprocesses and memory debugging. In-depth details of DDT can be found in\nthe Official DDT User Guide, and\ninstructions for how to use it on OLCF systems can be found on the Debugging Software <https://docs.olcf.ornl.gov/software/debugging/index.html> page. DDT is the\nOLCF's recommended debugging software for large parallel applications.\n\nOne of the most useful features of DDT is its remote debugging feature. This allows you to connect to a debugging session on Frontier from a client running on your workstation. The local client provides much faster interaction than you would have if using the graphical client on Frontier. For guidance in setting up the remote client see the Debugging Software <https://docs.olcf.ornl.gov/software/debugging/index.html> page.\n\nGDB\n\nGDB, the GNU Project Debugger,\nis a command-line debugger useful for traditional debugging and\ninvestigating code crashes. GDB lets you debug programs written in Ada,\nC, C++, Objective-C, Pascal (and many other languages).\n\nGDB is availableon Summit under all compiler families:\n\nmodule load gdb\n\nTo use GDB to debug your application run:\n\ngdb ./path_to_executable\n\nAdditional information about GDB usage can befound on the GDB Documentation Page.\n\nValgrind4hpc\n\nValgrind4hpc is a Valgrind-based debugging tool to aid in the detection of memory leaks\nand errors in parallel applications. Valgrind4hpc aggregates any duplicate\nmessages across ranks to help provide an understandable picture of\nprogram behavior. Valgrind4hpc manages starting and redirecting output from many\ncopies of Valgrind, as well as deduplicating and filtering Valgrind messages.\nIf your program can be debugged with Valgrind, it can be debugged with\nValgrind4hpc.\n\nValgrind4hpc is available on Frontier under all compiler families:\n\nmodule load valgrind4hpc\n\nAdditional information about Valgrind4hpc usage can be found on the HPE Cray Programming Environment User Guide Page.\n\nProfiling Applications\n\nGetting Started with the HPE Performance Analysis Tools (PAT)\n\nThe Performance Analysis Tools (PAT), formerly CrayPAT, are a suite of utilities that enable users to capture and analyze performance data generated during program execution. These tools provide an integrated infrastructure for measurement, analysis, and visualization of computation, communication, I/O, and memory utilization to help users optimize programs for faster execution and more efficient computing resource usage.\n\nThere are three programming interfaces available: (1) Perftools-lite, (2) Perftools, and (3) Perftools-preload.\n\nBelow are two examples that generate an instrumented executable using Perftools, which is an advanced interface that provides full-featured data collection and analysis capability, including full traces with timeline displays.\n\nThe first example generates an instrumented executable using a PrgEnv-amd build:\n\nmodule load PrgEnv-amd\nmodule load craype-accel-amd-gfx90a\nmodule load rocm\nmodule load perftools\n\nexport PATH=\"${PATH}:${ROCM_PATH}/llvm/bin\"\nexport CXX='CC -x hip'\nexport CXXFLAGS='-ggdb -O3 -std=c++17 –Wall'\nexport LD='CC'\nexport LDFLAGS=\"${CXXFLAGS} -L${ROCM_PATH}/lib\"\nexport LIBS='-lamdhip64'\n\nmake clean\nmake\n\npat_build -g hip,io,mpi -w -f <executable>\n\nThe second example generates an instrumened executable using a hipcc build:\n\nmodule load perftools\nmodule load craype-accel-amd-gfx90a\nmodule load rocm\n\nexport CXX='hipcc'\nexport CXXFLAGS=\"$(pat_opts include hipcc) \\\n  $(pat_opts pre_compile hipcc) -g -O3 -std=c++17 -Wall \\\n  --offload-arch=gfx90a -I${CRAY_MPICH_DIR}/include \\\n  $(pat_opts post_compile hipcc)\"\nexport LD='hipcc'\nexport LDFLAGS=\"$(pat_opts pre_link hipcc) ${CXXFLAGS} \\\n  -L${CRAY_MPICH_DIR}/lib ${PE_MPICH_GTL_DIR_amd_gfx908}\"\nexport LIBS=\"-lmpi ${PE_MPICH_GTL_LIBS_amd_gfx908} \\\n  $(pat_opts post_link hipcc)\"\n\nmake clean\nmake\n\npat_build -g hip,io,mpi -w -f <executable>\n\nThe pat_build command in the above examples generates an instrumented executable with +pat appended to the executable name (e.g., hello_jobstep+pat).\n\nWhen run, the instrumented executable will trace HIP, I/O, MPI, and all user functions and generate a folder of results (e.g., hello_jobstep+pat+39545-2t).\n\nTo analyze these results, use the pat_report command, e.g.:\n\npat_report hello_jobstep+pat+39545-2t\n\nThe resulting report includes profiles of functions, profiles of maximum function times, details on load imbalance, details on program energy and power usages, details on memory high water mark, and more.\n\nMore detailed information on the HPE Performance Analysis Tools can be found in the HPE Performance Analysis Tools User Guide.\n\nWhen using perftools-lite-gpu, there is a known issue causing ld.lld not to be found. A workaround this issue can be found here.\n\nGetting Started with HPCToolkit\n\nHPCToolkit is an integrated suite of tools for measurement and analysis of program performance on computers ranging from multicore desktop systems to the nation's largest supercomputers. HPCToolkit provides accurate measurements of a program's work, resource consumption, and inefficiency, correlates these metrics with the program's source code, works with multilingual, fully optimized binaries, has very low measurement overhead, and scales to large parallel systems. HPCToolkit's measurements provide support for analyzing a program execution cost, inefficiency, and scaling characteristics both within and across nodes of a parallel system.\n\nProgramming models supported by HPCToolkit include MPI, OpenMP, OpenACC, CUDA, OpenCL, DPC++, HIP, RAJA, Kokkos, and others.\n\nBelow is an example that generates a profile and loads the results in their GUI-based viewer.\n\nA full list of available HPCToolkit versions can be seen with the module spider hpctoolkit command.\n\nmodule load hpctoolkit/2022.05.15-rocm\n\n# 1. Profile and trace an application using CPU time and GPU performance counters\nsrun <srun_options> hpcrun -o <measurement_dir> -t -e CPUTIME -e gpu=amd <application>\n\n# 2. Analyze the binary of executables and its dependent libraries\nhpcstruct <measurement_dir>\n\n# 3. Combine measurements with program structure information and generate a database\nhpcprof -o <database_dir> <measurement_dir>\n\n# 4. Understand performance issues by analyzing profiles and traces with the GUI\nhpcviewer <database_dir>\n\nMore detailed information on HPCToolkit can be found in the HPCToolkit User's Manual.\n\nHPCToolkit does not require a recompile to profile the code. It is recommended to use the -g optimization flag for attribution to source lines.\n\nGetting Started with the ROCm Profiler\n\nrocprof gathers metrics on kernels run on AMD GPU architectures. The profiler works for HIP kernels, as well as offloaded kernels from OpenMP target offloading, OpenCL, and abstraction layers such as Kokkos.\nFor a simple view of kernels being run, rocprof --stats --timestamp on is a great place to start.\nWith the --stats option enabled, rocprof will generate a file that is named results.stats.csv by default, but named <output>.stats.csv if the -o flag is supplied.\nThis file will list all kernels being run, the number of times they are run, the total duration and the average duration (in nanoseconds) of the kernel, and the GPU usage percentage.\nMore detailed infromation on rocprof profiling modes can be found at ROCm Profiler documentation.\n\nIf you are using sbcast, you need to explicitly sbcast the AQL profiling library found in ${ROCM_PATH}/hsa-amd-aqlprofile/lib/libhsa-amd-aqlprofile64.so.\nA symbolic link to this library can also be found in ${ROCM_PATH}/lib.\nAlternatively, you may leave ${ROCM_PATH}/lib in your LD_LIBRARY_PATH.\n\nRoofline Profiling with the ROCm Profiler\n\nThe Roofline performance model is an increasingly popular way to demonstrate and understand application performance.\nThis section documents how to construct a simple roofline model for a single kernel using rocprof.\nThis roofline model is designed to be comparable to rooflines constructed by NVIDIA's NSight Compute.\nA roofline model plots the achieved performance (in floating-point operations per second, FLOPS/s) as a function of arithmetic (or operational) intensity (in FLOPS per Byte).\nThe model detailed here calculates the bytes moved as they move to and from the GPU's HBM.\n\nInteger instructions and cache levels are currently not documented here.\n\nTo get started, you will need to make an input file for rocprof, to be passed in through rocprof -i <input_file> --timestamp on -o my_output.csv <my_exe>.\nBelow is an example, and contains the information needed to roofline profile GPU 0, as seen by each rank:\n\npmc : TCC_EA_RDREQ_32B_sum TCC_EA_RDREQ_sum TCC_EA_WRREQ_sum TCC_EA_WRREQ_64B_sum SQ_INSTS_VALU_ADD_F16 SQ_INSTS_VALU_MUL_F16 SQ_INSTS_VALU_FMA_F16 SQ_INSTS_VALU_TRANS_F16 SQ_INSTS_VALU_ADD_F32 SQ_INSTS_VALU_MUL_F32 SQ_INSTS_VALU_FMA_F32 SQ_INSTS_VALU_TRANS_F32\npmc : SQ_INSTS_VALU_ADD_F64 SQ_INSTS_VALU_MUL_F64 SQ_INSTS_VALU_FMA_F64 SQ_INSTS_VALU_TRANS_F64 SQ_INSTS_VALU_MFMA_MOPS_F16 SQ_INSTS_VALU_MFMA_MOPS_BF16 SQ_INSTS_VALU_MFMA_MOPS_F32 SQ_INSTS_VALU_MFMA_MOPS_F64\ngpu: 0\n\nIn an application with more than one kernel, you should strongly consider filtering by kernel name by adding a line like: kernel: <kernel_name> to the rocprof input file.\n\nThis provides the minimum set of metrics used to construct a roofline model, in the minimum number of passes.\nEach line that begins with pmc indicates that the application will be re-run, and the metrics in that line will be collected.\nrocprof can collect up to 8 counters from each block (SQ, TCC) in each application re-run.\nTo gather metrics across multiple MPI ranks, you will need to use a command that redirects the output of rocprof to a unique file for each task.\nFor example:\n\nsrun -N 2 -n 16 --ntasks-per-node=8 --gpus-per-node=8 --gpu-bind=closest bash -c 'rocprof -o ${SLURM_JOBID}_${SLURM_PROCID}.csv -i <input_file> --timestamp on <exe>'\n\nThe gpu: filter in the rocprof input file identifies GPUs by the number the MPI rank would see them as. In the srun example above,\neach MPI rank only has 1 GPU, so each rank sees its GPU as GPU 0.\n\nTheoretical Roofline\n\nThe theoretical (not attainable) peak roofline constructs a theoretical maximum performance for each operational intensity.\n\ntheoretical peak is determined by the hardware specifications and is not attainable in practice. attaiable peak is the performance as measured by\nin-situ microbenchmarks designed to best utilize the hardware. achieved performance is what the profiled application actually achieves.\n\nThe theoretical roofline can be constructed as:\n\nFLOPS_{peak} = minimum(ArithmeticIntensity * BW_{HBM}, TheoreticalFLOPS)\n\nOn Frontier, the memory bandwidth for HBM is 1.6 TB/s, and the theoretical peak floating-point FLOPS/s when using vector registers is calculated by:\n\nTheoreticalFLOPS = 128 FLOP/cycle/CU * 110 CU * 1700000000 cycles/second = 23.9 TFLOP/s\n\nHowever, when using MFMA instructions, the theoretical peak floating-point FLOPS/s is calculated by:\n\nTheoreticalFLOPS = flop\\_per\\_cycle(precision) FLOP/cycle/CU * 110 CU * 1700000000 cycles/second\n\nwhere flop_per_cycle(precision) is the published floating-point operations per clock cycle, per compute unit.\nThose values are:\n\n\nThe following table displays the specifications for the Frontier supercomputer. It is a highly advanced and powerful supercomputer that has the capability to perform a wide range of tasks. The first row of the table serves as the header and provides the categories for the data types being presented. The table shows the number of Flops (floating-point operations) that can be performed per clock cycle per compute unit (CU) for different data types. The lower the data type, the higher the number of Flops, with the highest being 1024 for both FP16 and BF16. This means that the Frontier supercomputer is specifically designed to handle calculations and tasks that require a high amount of precision and accuracy. It has 256 Flops for both FP64 and FP32, making it capable of performing complex calculations in double-precision and single-precision floating-point formats. Additionally, the table shows that the Frontier supercomputer has a significantly high Flops rate for the INT8 data type, at 1024. This indicates its impressive abilities in handling integer-based calculations. Overall, this table highlights the cutting-edge technology and immense computational power of the Frontier supercomputer.\n\n| Data Type | Flops/Clock/CU | \n| --------- | -------------- | \n| FP64      | 256            | \n| FP32      | 256            | \n| FP16      | 1024           | \n| BF16      | 1024           | \n| INT8      | 1024           |\n\n\n\nAttainable peak rooflines are constructed using microbenchmarks, and are not currently discussed here.\nAttainable rooflines consider the limitations of cooling and power consumption and are more representative of what an application can achieve.\n\nAchieved FLOPS/s\n\nWe calculate the achieved performance at the desired level (here, double-precision floating point, FP64), by summing each metric count and weighting the FMA metric by 2, since a fused multiply-add is considered 2 floating point operations.\nAlso note that these SQ_INSTS_VALU_<ADD,MUL,TRANS> metrics are reported as per-simd, so we mutliply by the wavefront size as well.\nThe SQ_INSTS_VALU_MFMA_MOPS_* instructions should be multiplied by the Flops/Cycle/CU value listed above.\nWe use this equation to calculate the number of double-precision FLOPS:\n\nFP64\\_FLOPS =   64  *&(SQ\\_INSTS\\_VALU\\_ADD\\_F64         \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_MUL\\_F64       \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_TRANS\\_F64     \\\\\\\\\n                     &+ 2 * SQ\\_INSTS\\_VALU\\_FMA\\_F64)  \\\\\\\\\n              + 256 *&(SQ\\_INSTS\\_VALU\\_MFMA\\_MOPS\\_F64)\n\nWhen SQ_INSTS_VALU_MFMA_MOPS_*_F64 instructions are used, then 47.8 TF/s is considered the theoretical maximum FLOPS/s.\nIf only SQ_INSTS_VALU_<ADD,MUL,TRANS> are found, then 23.9 TF/s is the theoretical maximum FLOPS/s.\nThen, we divide the number of FLOPS by the elapsed time of the kernel to find FLOPS per second.\nThis is found from subtracting the rocprof metrics EndNs by BeginNs, provided by --timestamp on, then converting from nanoseconds to seconds by dividing by 1,000,000,000 (power(10,9)).\n\nFor ROCm/5.2.0 and earlier, there is a known issue with the timings provided by --timestamp on. See crusher-known-issues <https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#crusher-known-issues>.\n\nCalculating for all precisions\n\nThe above formula can be adapted to compute the total FLOPS across all floating-point precisions (INT excluded).\n\nTOTAL\\_FLOPS =   64  *&(SQ\\_INSTS\\_VALU\\_ADD\\_F16         \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_MUL\\_F16       \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_TRANS\\_F16     \\\\\\\\\n                     &+ 2 * SQ\\_INSTS\\_VALU\\_FMA\\_F16)  \\\\\\\\\n              + 64  *&(SQ\\_INSTS\\_VALU\\_ADD\\_F32         \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_MUL\\_F32       \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_TRANS\\_F32     \\\\\\\\\n                     &+ 2 * SQ\\_INSTS\\_VALU\\_FMA\\_F32)  \\\\\\\\\n              + 64  *&(SQ\\_INSTS\\_VALU\\_ADD\\_F64         \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_MUL\\_F64       \\\\\\\\\n                     &+ SQ\\_INSTS\\_VALU\\_TRANS\\_F64     \\\\\\\\\n                     &+ 2 * SQ\\_INSTS\\_VALU\\_FMA\\_F64)  \\\\\\\\\n              + 1024 &*(SQ\\_INSTS\\_VALU\\_MFMA\\_MOPS\\_F16) \\\\\\\\\n              + 1024 &*(SQ\\_INSTS\\_VALU\\_MFMA\\_MOPS\\_BF16) \\\\\\\\\n              + 256 *&(SQ\\_INSTS\\_VALU\\_MFMA\\_MOPS\\_F32) \\\\\\\\\n              + 256 *&(SQ\\_INSTS\\_VALU\\_MFMA\\_MOPS\\_F64) \\\\\\\\\n\nArithmetic Intensity\n\nArithmetic intensity calculates the ratio of FLOPS to bytes moved between HBM and L2 cache.\nWe calculated FLOPS above (FP64_FLOPS).\nWe can calculate the number of bytes moved using the rocprof metrics TCC_EA_WRREQ_64B, TCC_EA_WRREQ_sum, TCC_EA_RDREQ_32B, and TCC_EA_RDREQ_sum.\nTCC refers to the L2 cache, and EA is the interface between L2 and HBM.\nWRREQ and RDREQ are write-requests and read-requests, respectively.\nEach of these requests is either 32 bytes or 64 bytes.\nSo we calculate the number of bytes traveling over the EA interface as:\n\nBytesMoved = BytesWritten + BytesRead\n\nwhere\n\nBytesWritten = 64 * TCC\\_EA\\_WRREQ\\_64B\\_sum + 32 * (TCC\\_EA\\_WRREQ\\_sum - TCC\\_EA\\_WRREQ\\_64B\\_sum)\n\nBytesRead = 32 * TCC\\_EA\\_RDREQ\\_32B\\_sum + 64 * (TCC\\_EA\\_RDREQ\\_sum - TCC\\_EA\\_RDREQ\\_32B\\_sum)\n\n\n\n\n\nTips and Tricks\n\nThis section details 'tips and tricks' and information of interest to users when porting from Summit to Frontier.\n\n\n\nUsing reduced precision (FP16 and BF16 datatypes)\n\nUsers leveraging BF16 and FP16 datatypes for applications such as ML/AI training and low-precision matrix multiplication should be aware that the AMD MI250X GPU has different denormal handling than the V100 GPUs on Summit. On the MI250X, the V_DOT2 and the matrix instructions for FP16 and BF16 flush input and output denormal values to zero. FP32 and FP64 MFMA instructions do not flush input and output denormal values to zero.\n\nWhen training deep learning models using FP16 precision, some models may fail to converge with FP16 denorms flushed to zero. This occurs in operations encountering denormal values, and so is more likely to occur in FP16 because of a small dynamic range. BF16 numbers have a larger dynamic range than FP16 numbers and are less likely to encounter denormal values.\n\nAMD has provided a solution in ROCm 5.0 which modifies the behavior of Tensorflow, PyTorch, and rocBLAS. This modification starts with FP16 input values, casting the intermediate FP16 values to BF16, and then casting back to FP16 output after the accumulate FP32 operations. In this way, the input and output types are unchanged. The behavior is enabled by default in machine learning frameworks. This behavior requires user action in rocBLAS, via a special enum type. For more information, see the rocBLAS link below.\n\nIf you encounter significant differences when running using reduced precision, explore replacing non-converging models in FP16 with BF16, because of the greater dynamic range in BF16. We recommend using BF16 for ML models in general. If you have further questions or encounter issues, contact help@olcf.ornl.gov.\n\nAdditional information on MI250X reduced precision can be found at:\n\nThe MI250X ISA specification details the flush to zero denorm behavior at: https://developer.amd.com/wp-content/resources/CDNA2_Shader_ISA_18November2021.pdf (See page 41 and 46)\n\nAMD rocBLAS library reference guide details this behavior at: https://rocblas.readthedocs.io/en/master/API_Reference_Guide.html#mi200-gfx90a-considerations\n\n\n\nEnabling GPU Page Migration\n\nThe AMD MI250X and operating system on Frontier supports unified virtual addressing across the entire host and device memory, and automatic page migration between CPU and GPU memory. Migratable, universally addressable memory is sometimes called 'managed' or 'unified' memory, but neither of these terms fully describes how memory may behave on Frontier. In the following section we'll discuss how the heterogenous memory space on a Frontier node is surfaced within your application.\n\nThe accessibility of memory from GPU kernels and whether pages may migrate depends three factors: how the memory was allocated; the XNACK operating mode of the GPU; whether the kernel was compiled to support page migration. The latter two factors are intrinsically linked, as the MI250X GPU operating mode restricts the types of kernels which may run.\n\nXNACK (pronounced X-knack) refers to the AMD GPU's ability to retry memory accesses that fail due to a page fault. The XNACK mode of an MI250X can be changed by setting the environment variable HSA_XNACK before starting a process that uses the GPU. Valid values are 0 (disabled) and 1 (enabled), and all processes connected to a GPU must use the same XNACK setting. The default MI250X on Frontier is HSA_XNACK=0.\n\nIf HSA_XNACK=0, page faults in GPU kernels are not handled and will terminate the kernel. Therefore all memory locations accessed by the GPU must either be resident in the GPU HBM or mapped by the HIP runtime. Memory regions may be migrated between the host DDR4 and GPU HBM using explicit HIP library functions such as hipMemAdvise and hipPrefetchAsync, but memory will not be automatically migrated based on access patterns alone.\n\nIf HSA_XNACK=1, page faults in GPU kernels will trigger a page table lookup. If the memory location can be made accessible to the GPU, either by being migrated to GPU HBM or being mapped for remote access, the appropriate action will occur and the access will be replayed. Page migration  will happen between CPU DDR4 and GPU HBM according to page touch. The exceptions are if the programmer uses a HIP library call such as hipPrefetchAsync to request migration, or if a preferred location is set via hipMemAdvise, or if GPU HBM becomes full and the page must forcibly be evicted back to CPU DDR4 to make room for other data.\n\nIf ``HSA_XNACK=1``, page faults in GPU kernels will trigger a page table lookup. If the memory location can be made accessible to the GPU, either by being migrated to GPU HBM or being mapped for remote access, the appropriate action will occur and the access will be replayed. Once a memory region has been migrated to GPU HBM it typically stays there rather than migrating back to CPU DDR4. The exceptions are if the programmer uses a HIP library call such as ``hipPrefetchAsync`` to request migration, or if GPU HBM becomes full and the page must forcibly be evicted back to CPU DDR4 to make room for other data.\n\n\n\nMigration of Memory by Allocator and XNACK Mode\n\nMost applications that use \"managed\" or \"unified\" memory on other platforms will want to enable XNACK to take advantage of automatic page migration on Frontier. The following table shows how common allocators currently behave with XNACK enabled. The behavior of a specific memory region may vary from the default if the programmer uses certain API calls.\n\nThe page migration behavior summarized by the following tables represents the current, observable behavior. Said behavior will likely change in the near future.\n\nCPU accesses to migratable memory may behave differently than other platforms you're used to. On Frontier, pages will not migrate from GPU HBM to CPU DDR4 based on access patterns alone. Once a page has migrated to GPU HBM it will remain there even if the CPU accesses it, and all accesses which do not resolve in the CPU cache will occur over the Infinity Fabric between the AMD \"Optimized 3rd Gen EPYC\" CPU and AMD MI250X GPU. Pages will only *automatically* migrate back to CPU DDR4 if they are forcibly evicted to free HBM capacity, although programmers may use HIP APIs to manually migrate memory regions.\n\nHSA_XNACK=1 Automatic Page Migration Enabled\n\n\nThe following table provides a comprehensive overview of the Frontier supercomputer and its different memory allocation options. The first row serves as the header for the table and includes the different allocators available. The first column lists the allocators, such as System Allocator (malloc,new,allocate), hipMallocManaged, hipHostMalloc, and hipMalloc. These allocators are then compared based on their initial physical location, which includes CPU DDR4 and GPU HBM. The table also illustrates the CPU access after GPU first touch, which is either a migration to CPU DDR4 or zero copy read/write over Infinity Fabric. Similarly, the default behavior for GPU access is shown for each allocator, which varies from migrating to GPU HBM on touch to zero copy read/write over Infinity Fabric. This table serves as a helpful reference for users to understand the various memory allocation options available on the Frontier supercomputer, allowing them to make an informed decision based on their specific needs.\n\n |Allocator |Initial Physical Location |CPU Access after GPU First Touch |Default Behavior for GPU Access|\n |----------|---------------------------|---------------------------------|--------------------------------|\n |System Allocator (malloc, new, allocate, etc)|CPU DDR4|Migrate to CPU DDR4 on touch|Migrate to GPU HBM on touch|\n |hipMallocManaged|CPU DDR4|Migrate to CPU DDR4 on touch|Migrate to GPU HBM on touch|\n |hipHostMalloc|CPU DDR4|Local read/write|Zero copy read/write over Infinity Fabric|\n |hipMalloc|GPU HBM|Zero copy read/write over Infinity Fabric|Local read/write|\n\n\n\nDisabling XNACK will not necessarily result in an application failure, as most types of memory can still be accessed by the AMD \"Optimized 3rd Gen EPYC\" CPU and AMD MI250X GPU. In most cases, however, the access will occur in a zero-copy fashion over the Infinity Fabric. The exception is memory allocated through standard system allocators such as malloc, which cannot be accessed directly from GPU kernels without previously being registered via a HIP runtime call such as hipHostRegister. Access to malloc'ed and unregistered memory from GPU kernels will result in fatal unhandled page faults. The table below shows how common allocators behave with XNACK disabled.\n\nHSA_XNACK=0 Automatic Page Migration Disabled\n\n\nThe following table provides an overview of the different allocators available for the Frontier supercomputer. The first column lists the name of the allocator, followed by its initial physical location in the system. The third column describes the default behavior for CPU access, which is local read/write. In contrast, the fourth column explains the default behavior for GPU access, which is dependent on the allocator. The System Allocator, which includes functions such as malloc, new, and allocate, has local read/write access for CPU and Fatal Unhandled Page Fault for GPU. hipMallocManaged also has local read/write access for CPU, but enables zero copy read/write over Infinity Fabric for GPU. hipHostMalloc, similar to hipMallocManaged, provides local read/write access for CPU and enables zero copy read/write over Infinity Fabric for GPU. Finally, hipMalloc, specifically for GPU HBM, allows for zero copy read/write over Infinity Fabric for GPU, but has local read/write access for CPU. This table showcases the various options for memory allocation in the Frontier supercomputer, depending on the needs and capabilities of the user. \n\n| Allocator | Initial Physical Location | Default Behavior for CPU Access | Default Behavior for GPU Access | \n| ------ | ------ | ------ | ------ | \n| System Allocator (malloc, new, allocate, etc) | CPU DDR4 | Local read/write | Fatal Unhandled Page Fault | \n| hipMallocManaged | CPU DDR4 | Local read/write | Zero copy read/write over Infinity Fabric | \n| hipHostMalloc | CPU DDR4 | Local read/write | Zero copy read/write over Infinity Fabric | \n| hipMalloc | GPU HBM | Zero copy read/write over Infinity Fabric | Local read/write |\n\n\n\n\n\nCompiling HIP kernels for specific XNACK modes\n\nAlthough XNACK is a capability of the MI250X GPU, it does require that kernels be able to recover from page faults. Both the ROCm and CCE HIP compilers will default to generating code that runs correctly with both XNACK enabled and disabled. Some applications may benefit from using the following compilation options to target specific XNACK modes.\n\nhipcc --amdgpu-target=gfx90a or CC --offload-arch=gfx90a -x hip\n\nKernels are compiled to a single \"xnack any\" binary, which will run correctly with both XNACK enabled and XNACK disabled.\n\nhipcc --amdgpu-target=gfx90a:xnack+ or CC --offload-arch=gfx90a:xnack+ -x hip\n\nKernels are compiled in \"xnack plus\" mode and will only be able to run on GPUs with HSA_XNACK=1 to enable XNACK. Performance may be better than \"xnack any\", but attempts to run with XNACK disabled will fail.\n\nhipcc --amdgpu-target=gfx90a:xnack- or CC --offload-arch=gfx90a:xnack- -x hip\n\nKernels are compiled in \"xnack minus\" mode and will only be able to run on GPUs with HSA_XNACK=0 and XNACK disabled. Performance may be better than \"xnack any\", but attempts to run with XNACK enabled will fail.\n\nhipcc --amdgpu-target=gfx90a:xnack- --amdgpu-target=gfx90a:xnack+ -x hip or CC --offload-arch=gfx90a:xnack- --offload-arch=gfx90a:xnack+ -x hip\n\nTwo versions of each kernel will be generated, one that runs with XNACK disabled and one that runs if XNACK is enabled. This is different from \"xnack any\" in that two versions of each kernel are compiled and HIP picks the appropriate one at runtime, rather than there being a single version compatible with both. A \"fat binary\" compiled in this way will have the same performance of \"xnack+\" with HSA_XNACK=1 and as \"xnack-\" with HSA_XNACK=0, but the final executable will be larger since it contains two copies of every kernel.\n\nIf the HIP runtime cannot find a kernel image that matches the XNACK mode of the device, it will fail with hipErrorNoBinaryForGpu.\n\n$ HSA_XNACK=0 srun -n 1 -N 1 -t 1 ./xnack_plus.exe\n\"hipErrorNoBinaryForGpu: Unable to find code object for all current devices!\"\nsrun: error: frontier002: task 0: Aborted\nsrun: launch/slurm: _step_signal: Terminating StepId=74100.0\n\nNOTE: This works in my shell because I used cpan to install the URI::Encode perl modules.\nThis won't work generically unless those get installed, so commenting out this block now.\n\nThe AMD tool `roc-obj-ls` will let you see what code objects are in a binary.\n\n.. code::\n    $ hipcc --amdgpu-target=gfx90a:xnack+ square.hipref.cpp -o xnack_plus.exe\n    $ roc-obj-ls -v xnack_plus.exe\n    Bundle# Entry ID:                                                              URI:\n    1       host-x86_64-unknown-linux                                           file://xnack_plus.exe#offset=8192&size=0\n    1       hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+                              file://xnack_plus.exe#offset=8192&size=9752\n\nIf no XNACK flag is specificed at compilation the default is \"xnack any\", and objects in `roc-obj-ls` with not have an XNACK mode specified.\n\n.. code::\n    $ hipcc --amdgpu-target=gfx90a square.hipref.cpp -o xnack_any.exe\n    $ roc-obj-ls -v xnack_any.exe\n    Bundle# Entry ID:                                                              URI:\n    1       host-x86_64-unknown-linux                                           file://xnack_any.exe#offset=8192&size=0\n    1       hipv4-amdgcn-amd-amdhsa--gfx90a                                     file://xnack_any.exe#offset=8192&size=9752\n\nOne way to diagnose hipErrorNoBinaryForGpu messages is to set the environment variable AMD_LOG_LEVEL to 1 or greater:\n\n$ AMD_LOG_LEVEL=1 HSA_XNACK=0 srun -n 1 -N 1 -t 1 ./xnack_plus.exe\n:1:rocdevice.cpp            :1573: 43966598070 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966598762 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966599392 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966599970 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966600550 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966601109 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966601673 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:rocdevice.cpp            :1573: 43966602248 us: HSA_AMD_AGENT_INFO_SVM_DIRECT_HOST_ACCESS query failed.\n:1:hip_code_object.cpp      :460 : 43966602806 us: hipErrorNoBinaryForGpu: Unable to find code object for all current devices!\n:1:hip_code_object.cpp      :461 : 43966602810 us:   Devices:\n:1:hip_code_object.cpp      :464 : 43966602811 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602811 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602812 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602813 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602813 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602814 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602814 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :464 : 43966602815 us:     amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack- - [Not Found]\n:1:hip_code_object.cpp      :468 : 43966602816 us:   Bundled Code Objects:\n:1:hip_code_object.cpp      :485 : 43966602817 us:     host-x86_64-unknown-linux - [Unsupported]\n:1:hip_code_object.cpp      :483 : 43966602818 us:     hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+ - [code object v4 is amdgcn-amd-amdhsa--gfx90a:xnack+]\n\"hipErrorNoBinaryForGpu: Unable to find code object for all current devices!\"\nsrun: error: frontier129: task 0: Aborted\nsrun: launch/slurm: _step_signal: Terminating StepId=74102.0\n\nThe above log messages indicate the type of image required by each device, given its current mode (amdgcn-amd-amdhsa--gfx90a:sramecc+:xnack-) and the images found in the binary (hipv4-amdgcn-amd-amdhsa--gfx90a:xnack+).\n\n\n\n\n\nFloating-Point (FP) Atomic Operations and Coarse/Fine Grained Memory Allocations\n\nThe Frontier system, equipped with CDNA2-based architecture MI250X cards, offers a coherent host interface that enables advanced memory and unique cache coherency capabilities.\nThe AMD driver leverages the Heterogeneous Memory Management (HMM) support in the Linux kernel to perform seamless page migrations to/from CPU/GPUs.\nThis new capability comes with a memory model that needs to be understood completely to avoid unexpected behavior in real applications. For more details, please visit the previous section.\n\nAMD GPUs can allocate two different types of memory locations: 1) Coarse grained and 2) Fine grained.\n\nCoarse grained memory is only guaranteed to be coherent outside of GPU kernels that modify it, enabling higher performance memory operations. Changes applied to coarse-grained memory by a GPU kernel are  only visible to the rest of the system (CPU or other GPUs) when the kernel has completed. A GPU kernel is only guaranteed to see changes applied to coarse grained memory by the rest of the system (CPU or other GPUs) if those changes were made before the kernel launched.\n\nFine grained memory allows CPUs and GPUs to synchronize (via atomics) and coherently communicate with each other while the GPU kernel is running, allowing more advanced programming patterns. The additional visibility impacts the performance of fine grained allocated memory.\n\nThe fast hardware-based Floating point (FP) atomic operations available on MI250X are assumed to be working on coarse grained memory regions; when these instructions are applied to a fine-grained memory region, they will silently produce a no-op. To avoid returning incorrect results, the compiler never emits hardware-based FP atomics instructions by default, even when applied to coarse grained memory regions. Currently, users can use the -munsafe-fp-atomics flag to force the compiler to emit hardware-based FP atomics.\nUsing hardware-based FP atomics translates in a substantial performance improvement over the default choice.\n\nUsers applying floating point atomic operations (e.g., atomicAdd) on memory regions allocated via regular hipMalloc() can safely apply the -munsafe-fp-atomics flags to their codes to get the best possible performance and leverage hardware supported floating point atomics.\nAtomic operations supported in hardware on non-FP datatypes  (e.g., INT32) will work correctly regardless of the nature of the memory region used.\n\nIn ROCm-5.1 and earlier versions, the flag -munsafe-fp-atomics is interpreted as a suggestion by the compiler, whereas from ROCm-5.2 the flag will always enforce the use of fast hardware-based FP atomics.\n\nThe following tables summarize the result granularity of various combinations of allocators, flags and arguments.\n\nFor hipHostMalloc(), the following table shows the nature of the memory returned based on the flag passed as argument.\n\n\nThe table provides a comparison of two different flags for the hipHostMalloc() API in the Frontier supercomputer. The first flag, hipHostMallocDefault, results in a fine-grained allocation of memory, while the second flag, hipHostMallocNonCoherent, results in a coarse-grained allocation. This information is important for developers using the Frontier supercomputer, as it allows them to choose the flag that best fits their needs based on the desired level of granularity for memory allocation. This table serves as a useful resource for understanding the capabilities and options available for this specific API on the Frontier supercomputer.\n\n\n\n| API             | Flag                    | Results      |\n| --------------- | ----------------------- | ------------ |\n| hipHostMalloc() | hipHostMallocDefault    | Fine grained |\n| hipHostMalloc() | hipHostMallocNonCoherent| Coarse grained |\n\n\n\nThe following table shows the nature of the memory returned based on the flag passed as argument to hipExtMallocWithFlags().\n\n\nThe table presents the different flags and results for the API function hipExtMallocWithFlags(), specifically for the Frontier supercomputer. The first row serves as a header and provides the names of the columns: API, Flag, and Result. The following rows list the different flags, including hipDeviceMallocDefault and hipDeviceMallocFineGrained, and the corresponding results of using them with the hipExtMallocWithFlags() function. These results are listed as \"Coarse grained\" and \"Fine grained,\" respectively, indicating the level of granularity achieved with memory allocation. This table offers a clear and concise overview of the different options and outcomes available when using the hipExtMallocWithFlags() function on the Frontier supercomputer. In summary, the table portrays the capabilities of this API function and the potential impact of utilizing different flags for memory allocation. \n\n| API                     | Flag                     | Result        |\n| ----------------------- | ------------------------ | ------------- |\n| hipExtMallocWithFlags() | hipDeviceMallocDefault   | Coarse grained|\n| hipExtMallocWithFlags() | hipDeviceMallocFinegrained| Fine grained  |\n\n\n\nFinally, the following table summarizes the nature of the memory returned based on the flag passed as argument to hipMallocManaged() and the use of CPU regular malloc() routine with the possible use of hipMemAdvise().\n\n\nThe table provided outlines the different memory allocation options for the Frontier supercomputer. The first column lists the different APIs, or application programming interfaces, which are used to allocate memory in a computer program. The second column, labeled \"MemAdvice\", provides information on how to use the given API. The third column, titled \"Result\", indicates the level of granularity for the memory allocation. The first row, labeled as the header, does not contain any information as it only serves as a reference for the table. The following rows provide details for the \"hipMallocManaged()\" and \"malloc()\" APIs, with and without the use of the \"hipMemAdviseSetCoarseGrain\" function, resulting in a \"Fine grained\" or \"Coarse grained\" memory allocation, respectively. This table showcases the various options available for memory allocation on the Frontier supercomputer.\n\n| API           | MemAdvice                               | Result        |\n|---------------|-----------------------------------------|---------------|\n| hipMallocManaged() |                                       | Fine grained  |\n| hipMallocManaged() | hipMemAdvise (hipMemAdviseSetCoarseGrain)| Coarse grained |\n| malloc()         |                                       | Fine grained  |\n| malloc()         | hipMemAdvise (hipMemAdviseSetCoarseGrain)| Coarse grained |\n\n\n\nPerformance considerations for LDS FP atomicAdd()\n\nHardware FP atomic operations performed in LDS memory are usually always faster than an equivalent CAS loop, in particular when contention on LDS memory locations is high.\nBecause of a hardware design choice, FP32 LDS atomicAdd() operations can be slower than equivalent FP64 LDS atomicAdd(), in particular when contention on memory locations is low (e.g. random access pattern).\nThe aforementioned behavior is only true for FP atomicAdd() operations. Hardware atomic operations for CAS/Min/Max on FP32 are usually faster than the FP64 counterparts.\nIn cases when contention is very low, a FP32 CAS loop implementing an atomicAdd() operation could be faster than an hardware FP32 LDS atomicAdd().\nApplications using single precision FP atomicAdd() are encouraged to experiment with the use of double precision to evaluate the trade-off between high atomicAdd() performance vs. potential lower occupancy due to higher LDS usage.\n\nLibrary considerations with atomic operations\n\nSome functionality provided by the rocBLAS and hipBLAS libraries use atomic operations to improve performance by default. This can cause results to not be bit-wise reproducible.\nLevel 2 functions that may use atomic operations include: gemv, hemv, and symv, which introduced atomic operations in ROCm 5.5. All of the Level 3 functions, along with Level 2 trsv, may use atomic operations where dependent\non gemm. Atomic operations are used for problem sizes where they are shown to improve performance.\nIf it is necessary to have bit-wise reproducible results from these libraries, it is recommended to turn the atomic operations off by setting the mode via the rocBLAS or hipBLAS handle:\n\n...\nrocblas_create_handle(handle);\nrocblas_set_atomics_mode(handle, rocblas_atomics_not_allowed);\n\nhipblasCreate(&handle);\nhipblasSetAtomicsMode(handle, HIPBLAS_ATOMICS_NOT_ALLOWED);\n\n\n\nSystem Updates\n\n2023-09-19\n\nOn Tuesday, September 19, 2023, Frontier's system software was upgraded. The following changes took place:\n\nThe system was upgraded to Slingshot Host Software 2.1.0.\n\nROCm 5.6.0 and 5.7.0 are now available via the rocm/5.6.0 and rocm/5.7.0 modulefiles, respectively.\n\nHPE/Cray Programming Environments (PE) 23.09 is now available via the cpe/23.09 modulefile.\n\nROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.\n\n2023-07-18\n\nOn Tuesday, July 18, 2023, Frontier was upgraded to a new version of the system software stack. During the upgrade, the following changes took place:\n\nThe system was upgraded to Cray OS 2.5, Slingshot Host Software 2.0.2-112, and the AMD GPU 6.0.5 device driver (ROCm 5.5.1 release).\n\nROCm 5.5.1 is now available via the rocm/5.5.1 modulefile.\n\nHPE/Cray Programming Environments (PE) 23.05 is now available via the cpe/23.05 modulefile.\n\nHPE/Cray PE 23.05 introduces support for ROCm 5.5.1. However, due to issues identified during testing, ROCm 5.3.0 and HPE/Cray PE 22.12 remain as default.\n\n2023-05-09\n\nOn Tuesday, May 9, 2023, the darshan-runtime modulefile was added to DefApps and is now loaded by default on Frontier. This module will allow users to profile the I/O of their applications with minimal impact. The logs are available to users on the Orion file system in /lustre/orion/darshan/<system>/<yyyy>/<mm>/<dd>. Unloading darshan-runtime is recommended for users profiling their applications with other profilers to prevent conflicts.\n\n\n\nKnown Issues\n\nJIRA_CONTENT_HERE"
}